Questions,Answers
"<p>Also how do <code>LEFT JOIN</code>, <code>RIGHT JOIN</code> and <code>FULL JOIN</code> fit in?</p>
","<p><strong>Use prepared statements and parameterized queries.</strong> These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.</p>

<p>You basically have two options to achieve this:</p>

<ol>
<li><p>Using <a href=""http://php.net/manual/en/book.pdo.php"" rel=""noreferrer"">PDO</a> (for any supported database driver):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $pdo-&gt;prepare('SELECT * FROM employees WHERE name = :name');

$stmt-&gt;execute(array('name' =&gt; $name));

foreach ($stmt as $row) {
    // do something with $row
}
</code></pre></li>
<li><p>Using <a href=""http://php.net/manual/en/book.mysqli.php"" rel=""noreferrer"">MySQLi</a> (for MySQL):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $dbConnection-&gt;prepare('SELECT * FROM employees WHERE name = ?');
$stmt-&gt;bind_param('s', $name); // 's' specifies the variable type =&gt; 'string'

$stmt-&gt;execute();

$result = $stmt-&gt;get_result();
while ($row = $result-&gt;fetch_assoc()) {
    // do something with $row
}
</code></pre></li>
</ol>

<p>If you're connecting to a database other than MySQL, there is a driver-specific second option that you can refer to (e.g. <code>pg_prepare()</code> and <code>pg_execute()</code> for PostgreSQL). PDO is the universal option.</p>

<h2>Correctly setting up the connection</h2>

<p>Note that when using <code>PDO</code> to access a MySQL database <em>real</em> prepared statements are <strong>not used by default</strong>. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:</p>

<pre class=""lang-php prettyprint-override""><code>$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8', 'user', 'pass');

$dbConnection-&gt;setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
</code></pre>

<p>In the above example the error mode isn't strictly necessary, <strong>but it is advised to add it</strong>. This way the script will not stop with a <code>Fatal Error</code> when something goes wrong. And it gives the developer the chance to <code>catch</code> any error(s) which are <code>throw</code>n as <code>PDOException</code>s.</p>

<p>What is <strong>mandatory</strong>, however, is the first <code>setAttribute()</code> line, which tells PDO to disable emulated prepared statements and use <em>real</em> prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).</p>

<p>Although you can set the <code>charset</code> in the options of the constructor, it's important to note that 'older' versions of PHP (&lt; 5.3.6) <a href=""http://php.net/manual/en/ref.pdo-mysql.connection.php"" rel=""noreferrer"">silently ignored the charset parameter</a> in the DSN.</p>

<h2>Explanation</h2>

<p>What happens is that the SQL statement you pass to <code>prepare</code> is parsed and compiled by the database server. By specifying parameters (either a <code>?</code> or a named parameter like <code>:name</code> in the example above) you tell the database engine where you want to filter on. Then when you call <code>execute</code>, the prepared statement is combined with the parameter values you specify. </p>

<p>The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend. Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the <code>$name</code> variable contains <code>'Sarah'; DELETE FROM employees</code> the result would simply be a search for the string <code>""'Sarah'; DELETE FROM employees""</code>, and you will not end up with <a href=""http://xkcd.com/327/"" rel=""noreferrer"">an empty table</a>.</p>

<p>Another benefit of using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.</p>

<p>Oh, and since you asked about how to do it for an insert, here's an example (using PDO):</p>

<pre class=""lang-php prettyprint-override""><code>$preparedStatement = $db-&gt;prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement-&gt;execute(array('column' =&gt; $unsafeValue));
</code></pre>

<h2>Can prepared statements be used for dynamic queries?</h2>

<p>While you can still use prepared statements for the query parameters, the structure of the dynamic query itself cannot be parametrized and certain query features cannot be parametrized.</p>

<p>For these specific scenarios, the best thing to do is use a whitelist filter that restricts the possible values.</p>

<pre><code>// Value whitelist
// $dir can only be 'DESC' otherwise it will be 'ASC'
if (empty($dir) || $dir !== 'DESC') {
   $dir = 'ASC';
}
</code></pre>
"
"<p>In <strong>SQL Server</strong>, it's possible to <code>insert</code> into a table using a <code>SELECT</code> statement:</p>

<pre><code>INSERT INTO Table (col1, col2, col3)
SELECT col1, col2, col3 
FROM other_table 
WHERE sql = 'cool'
</code></pre>

<p>Is it also possible to <em>update</em> via a <code>SELECT</code>? I have a temporary table containing the values, and would like to update another table using those values. Perhaps something like this:</p>

<pre><code>UPDATE Table SET col1, col2
SELECT col1, col2 
FROM other_table 
WHERE sql = 'cool'
WHERE Table.id = other_table.id
</code></pre>
","<pre><code>UPDATE
    Table_A
SET
    Table_A.col1 = Table_B.col1,
    Table_A.col2 = Table_B.col2
FROM
    Some_Table AS Table_A
    INNER JOIN Other_Table AS Table_B
        ON Table_A.id = Table_B.id
WHERE
    Table_A.col3 = 'cool'
</code></pre>
"
"<p>In <strong>SQL Server</strong>, it's possible to <code>insert</code> into a table using a <code>SELECT</code> statement:</p>

<pre><code>INSERT INTO Table (col1, col2, col3)
SELECT col1, col2, col3 
FROM other_table 
WHERE sql = 'cool'
</code></pre>

<p>Is it also possible to <em>update</em> via a <code>SELECT</code>? I have a temporary table containing the values, and would like to update another table using those values. Perhaps something like this:</p>

<pre><code>UPDATE Table SET col1, col2
SELECT col1, col2 
FROM other_table 
WHERE sql = 'cool'
WHERE Table.id = other_table.id
</code></pre>
","<pre><code>UPDATE
    Table_A
SET
    Table_A.col1 = Table_B.col1,
    Table_A.col2 = Table_B.col2
FROM
    Some_Table AS Table_A
    INNER JOIN Other_Table AS Table_B
        ON Table_A.id = Table_B.id
WHERE
    Table_A.col3 = 'cool'
</code></pre>
"
"<p>If user input is inserted without modification into an SQL query, then the application becomes vulnerable to <a href=""https://stackoverflow.com/a/332367/"">SQL injection</a>, like in the following example:</p>

<pre class=""lang-php prettyprint-override""><code>$unsafe_variable = $_POST['user_input']; 

mysql_query(""INSERT INTO `table` (`column`) VALUES ('$unsafe_variable')"");
</code></pre>

<p>That's because the user can input something like <code>value'); DROP TABLE table;--</code>, and the query becomes:</p>

<pre><code>INSERT INTO `table` (`column`) VALUES('value'); DROP TABLE table;--')
</code></pre>

<p>What can be done to prevent this from happening?</p>
","<p><strong>Use prepared statements and parameterized queries.</strong> These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.</p>

<p>You basically have two options to achieve this:</p>

<ol>
<li><p>Using <a href=""http://php.net/manual/en/book.pdo.php"" rel=""noreferrer"">PDO</a> (for any supported database driver):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $pdo-&gt;prepare('SELECT * FROM employees WHERE name = :name');

$stmt-&gt;execute(array('name' =&gt; $name));

foreach ($stmt as $row) {
    // do something with $row
}
</code></pre></li>
<li><p>Using <a href=""http://php.net/manual/en/book.mysqli.php"" rel=""noreferrer"">MySQLi</a> (for MySQL):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $dbConnection-&gt;prepare('SELECT * FROM employees WHERE name = ?');
$stmt-&gt;bind_param('s', $name); // 's' specifies the variable type =&gt; 'string'

$stmt-&gt;execute();

$result = $stmt-&gt;get_result();
while ($row = $result-&gt;fetch_assoc()) {
    // do something with $row
}
</code></pre></li>
</ol>

<p>If you're connecting to a database other than MySQL, there is a driver-specific second option that you can refer to (e.g. <code>pg_prepare()</code> and <code>pg_execute()</code> for PostgreSQL). PDO is the universal option.</p>

<h2>Correctly setting up the connection</h2>

<p>Note that when using <code>PDO</code> to access a MySQL database <em>real</em> prepared statements are <strong>not used by default</strong>. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:</p>

<pre class=""lang-php prettyprint-override""><code>$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8', 'user', 'pass');

$dbConnection-&gt;setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
</code></pre>

<p>In the above example the error mode isn't strictly necessary, <strong>but it is advised to add it</strong>. This way the script will not stop with a <code>Fatal Error</code> when something goes wrong. And it gives the developer the chance to <code>catch</code> any error(s) which are <code>throw</code>n as <code>PDOException</code>s.</p>

<p>What is <strong>mandatory</strong>, however, is the first <code>setAttribute()</code> line, which tells PDO to disable emulated prepared statements and use <em>real</em> prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).</p>

<p>Although you can set the <code>charset</code> in the options of the constructor, it's important to note that 'older' versions of PHP (&lt; 5.3.6) <a href=""http://php.net/manual/en/ref.pdo-mysql.connection.php"" rel=""noreferrer"">silently ignored the charset parameter</a> in the DSN.</p>

<h2>Explanation</h2>

<p>What happens is that the SQL statement you pass to <code>prepare</code> is parsed and compiled by the database server. By specifying parameters (either a <code>?</code> or a named parameter like <code>:name</code> in the example above) you tell the database engine where you want to filter on. Then when you call <code>execute</code>, the prepared statement is combined with the parameter values you specify. </p>

<p>The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend. Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the <code>$name</code> variable contains <code>'Sarah'; DELETE FROM employees</code> the result would simply be a search for the string <code>""'Sarah'; DELETE FROM employees""</code>, and you will not end up with <a href=""http://xkcd.com/327/"" rel=""noreferrer"">an empty table</a>.</p>

<p>Another benefit of using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.</p>

<p>Oh, and since you asked about how to do it for an insert, here's an example (using PDO):</p>

<pre class=""lang-php prettyprint-override""><code>$preparedStatement = $db-&gt;prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement-&gt;execute(array('column' =&gt; $unsafeValue));
</code></pre>

<h2>Can prepared statements be used for dynamic queries?</h2>

<p>While you can still use prepared statements for the query parameters, the structure of the dynamic query itself cannot be parametrized and certain query features cannot be parametrized.</p>

<p>For these specific scenarios, the best thing to do is use a whitelist filter that restricts the possible values.</p>

<pre><code>// Value whitelist
// $dir can only be 'DESC' otherwise it will be 'ASC'
if (empty($dir) || $dir !== 'DESC') {
   $dir = 'ASC';
}
</code></pre>
"
"<p>If user input is inserted without modification into an SQL query, then the application becomes vulnerable to <a href=""https://stackoverflow.com/a/332367/"">SQL injection</a>, like in the following example:</p>

<pre class=""lang-php prettyprint-override""><code>$unsafe_variable = $_POST['user_input']; 

mysql_query(""INSERT INTO `table` (`column`) VALUES ('$unsafe_variable')"");
</code></pre>

<p>That's because the user can input something like <code>value'); DROP TABLE table;--</code>, and the query becomes:</p>

<pre><code>INSERT INTO `table` (`column`) VALUES('value'); DROP TABLE table;--')
</code></pre>

<p>What can be done to prevent this from happening?</p>
","<p><strong>Use prepared statements and parameterized queries.</strong> These are SQL statements that are sent to and parsed by the database server separately from any parameters. This way it is impossible for an attacker to inject malicious SQL.</p>

<p>You basically have two options to achieve this:</p>

<ol>
<li><p>Using <a href=""http://php.net/manual/en/book.pdo.php"" rel=""noreferrer"">PDO</a> (for any supported database driver):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $pdo-&gt;prepare('SELECT * FROM employees WHERE name = :name');

$stmt-&gt;execute(array('name' =&gt; $name));

foreach ($stmt as $row) {
    // do something with $row
}
</code></pre></li>
<li><p>Using <a href=""http://php.net/manual/en/book.mysqli.php"" rel=""noreferrer"">MySQLi</a> (for MySQL):</p>

<pre class=""lang-php prettyprint-override""><code>$stmt = $dbConnection-&gt;prepare('SELECT * FROM employees WHERE name = ?');
$stmt-&gt;bind_param('s', $name); // 's' specifies the variable type =&gt; 'string'

$stmt-&gt;execute();

$result = $stmt-&gt;get_result();
while ($row = $result-&gt;fetch_assoc()) {
    // do something with $row
}
</code></pre></li>
</ol>

<p>If you're connecting to a database other than MySQL, there is a driver-specific second option that you can refer to (e.g. <code>pg_prepare()</code> and <code>pg_execute()</code> for PostgreSQL). PDO is the universal option.</p>

<h2>Correctly setting up the connection</h2>

<p>Note that when using <code>PDO</code> to access a MySQL database <em>real</em> prepared statements are <strong>not used by default</strong>. To fix this you have to disable the emulation of prepared statements. An example of creating a connection using PDO is:</p>

<pre class=""lang-php prettyprint-override""><code>$dbConnection = new PDO('mysql:dbname=dbtest;host=127.0.0.1;charset=utf8', 'user', 'pass');

$dbConnection-&gt;setAttribute(PDO::ATTR_EMULATE_PREPARES, false);
$dbConnection-&gt;setAttribute(PDO::ATTR_ERRMODE, PDO::ERRMODE_EXCEPTION);
</code></pre>

<p>In the above example the error mode isn't strictly necessary, <strong>but it is advised to add it</strong>. This way the script will not stop with a <code>Fatal Error</code> when something goes wrong. And it gives the developer the chance to <code>catch</code> any error(s) which are <code>throw</code>n as <code>PDOException</code>s.</p>

<p>What is <strong>mandatory</strong>, however, is the first <code>setAttribute()</code> line, which tells PDO to disable emulated prepared statements and use <em>real</em> prepared statements. This makes sure the statement and the values aren't parsed by PHP before sending it to the MySQL server (giving a possible attacker no chance to inject malicious SQL).</p>

<p>Although you can set the <code>charset</code> in the options of the constructor, it's important to note that 'older' versions of PHP (&lt; 5.3.6) <a href=""http://php.net/manual/en/ref.pdo-mysql.connection.php"" rel=""noreferrer"">silently ignored the charset parameter</a> in the DSN.</p>

<h2>Explanation</h2>

<p>What happens is that the SQL statement you pass to <code>prepare</code> is parsed and compiled by the database server. By specifying parameters (either a <code>?</code> or a named parameter like <code>:name</code> in the example above) you tell the database engine where you want to filter on. Then when you call <code>execute</code>, the prepared statement is combined with the parameter values you specify. </p>

<p>The important thing here is that the parameter values are combined with the compiled statement, not an SQL string. SQL injection works by tricking the script into including malicious strings when it creates SQL to send to the database. So by sending the actual SQL separately from the parameters, you limit the risk of ending up with something you didn't intend. Any parameters you send when using a prepared statement will just be treated as strings (although the database engine may do some optimization so parameters may end up as numbers too, of course). In the example above, if the <code>$name</code> variable contains <code>'Sarah'; DELETE FROM employees</code> the result would simply be a search for the string <code>""'Sarah'; DELETE FROM employees""</code>, and you will not end up with <a href=""http://xkcd.com/327/"" rel=""noreferrer"">an empty table</a>.</p>

<p>Another benefit of using prepared statements is that if you execute the same statement many times in the same session it will only be parsed and compiled once, giving you some speed gains.</p>

<p>Oh, and since you asked about how to do it for an insert, here's an example (using PDO):</p>

<pre class=""lang-php prettyprint-override""><code>$preparedStatement = $db-&gt;prepare('INSERT INTO table (column) VALUES (:column)');

$preparedStatement-&gt;execute(array('column' =&gt; $unsafeValue));
</code></pre>

<h2>Can prepared statements be used for dynamic queries?</h2>

<p>While you can still use prepared statements for the query parameters, the structure of the dynamic query itself cannot be parametrized and certain query features cannot be parametrized.</p>

<p>For these specific scenarios, the best thing to do is use a whitelist filter that restricts the possible values.</p>

<pre><code>// Value whitelist
// $dir can only be 'DESC' otherwise it will be 'ASC'
if (empty($dir) || $dir !== 'DESC') {
   $dir = 'ASC';
}
</code></pre>
"
"<p>Optimizing SQLite is tricky. Bulk-insert performance of a C application can vary from 85 inserts per second to over 96,000 inserts per second!</p>

<p><strong>Background:</strong> We are using SQLite as part of a desktop application. We have large amounts of configuration data stored in XML files that are parsed and loaded into an SQLite database for further processing when the application is initialized. SQLite is ideal for this situation because it's fast, it requires no specialized configuration, and the database is stored on disk as a single file.</p>

<p><strong>Rationale:</strong> <em>Initially I was disappointed with the performance I was seeing.</em> It turns-out that the performance of SQLite can vary significantly (both for bulk-inserts and selects) depending on how the database is configured and how you're using the API. It was not a trivial matter to figure out what all of the options and techniques were, so I thought it prudent to create this community wiki entry to share the results with Stack&nbsp;Overflow readers in order to save others the trouble of the same investigations.</p>

<p><strong>The Experiment:</strong> Rather than simply talking about performance tips in the general sense (i.e. <em>""Use a transaction!""</em>), I thought it best to write some C code and <em>actually measure</em> the impact of various options. We're going to start with some simple data:</p>

<ul>
<li>A 28 MB TAB-delimited text file (approximately 865,000 records) of the <a href=""http://www.toronto.ca/open/datasets/ttc-routes"" rel=""noreferrer"">complete transit schedule for the city of Toronto</a></li>
<li>My test machine is a 3.60 GHz P4 running Windows XP.</li>
<li>The code is compiled with <a href=""http://en.wikipedia.org/wiki/Visual_C%2B%2B#32-bit_versions"" rel=""noreferrer"">Visual C++</a> 2005 as ""Release"" with ""Full Optimization"" (/Ox) and Favor Fast Code (/Ot).</li>
<li>I'm using the SQLite ""Amalgamation"", compiled directly into my test application. The SQLite version I happen to have is a bit older (3.6.7), but I suspect these results will be comparable to the latest release (please leave a comment if you think otherwise).</li>
</ul>

<p><em>Let's write some code!</em></p>

<p><strong>The Code:</strong> A simple C program that reads the text file line-by-line, splits the string into values and then will inserts the data into an SQLite database. In this ""baseline"" version of the code, the database is created, but we won't actually insert data:</p>

<pre><code>/*************************************************************
    Baseline code to experiment with SQLite performance.

    Input data is a 28 MB TAB-delimited text file of the
    complete Toronto Transit System schedule/route info
    from http://www.toronto.ca/open/datasets/ttc-routes/

**************************************************************/
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;time.h&gt;
#include &lt;string.h&gt;
#include ""sqlite3.h""

#define INPUTDATA ""C:\\TTC_schedule_scheduleitem_10-27-2009.txt""
#define DATABASE ""c:\\TTC_schedule_scheduleitem_10-27-2009.sqlite""
#define TABLE ""CREATE TABLE IF NOT EXISTS TTC (id INTEGER PRIMARY KEY, Route_ID TEXT, Branch_Code TEXT, Version INTEGER, Stop INTEGER, Vehicle_Index INTEGER, Day Integer, Time TEXT)""
#define BUFFER_SIZE 256

int main(int argc, char **argv) {

    sqlite3 * db;
    sqlite3_stmt * stmt;
    char * sErrMsg = 0;
    char * tail = 0;
    int nRetCode;
    int n = 0;

    clock_t cStartClock;

    FILE * pFile;
    char sInputBuf [BUFFER_SIZE] = ""\0"";

    char * sRT = 0;  /* Route */
    char * sBR = 0;  /* Branch */
    char * sVR = 0;  /* Version */
    char * sST = 0;  /* Stop Number */
    char * sVI = 0;  /* Vehicle */
    char * sDT = 0;  /* Date */
    char * sTM = 0;  /* Time */

    char sSQL [BUFFER_SIZE] = ""\0"";

    /*********************************************/
    /* Open the Database and create the Schema */
    sqlite3_open(DATABASE, &amp;db);
    sqlite3_exec(db, TABLE, NULL, NULL, &amp;sErrMsg);

    /*********************************************/
    /* Open input file and import into Database*/
    cStartClock = clock();

    pFile = fopen (INPUTDATA,""r"");
    while (!feof(pFile)) {

        fgets (sInputBuf, BUFFER_SIZE, pFile);

        sRT = strtok (sInputBuf, ""\t"");     /* Get Route */
        sBR = strtok (NULL, ""\t"");            /* Get Branch */
        sVR = strtok (NULL, ""\t"");            /* Get Version */
        sST = strtok (NULL, ""\t"");            /* Get Stop Number */
        sVI = strtok (NULL, ""\t"");            /* Get Vehicle */
        sDT = strtok (NULL, ""\t"");            /* Get Date */
        sTM = strtok (NULL, ""\t"");            /* Get Time */

        /* ACTUAL INSERT WILL GO HERE */

        n++;
    }
    fclose (pFile);

    printf(""Imported %d records in %4.2f seconds\n"", n, (clock() - cStartClock) / (double)CLOCKS_PER_SEC);

    sqlite3_close(db);
    return 0;
}
</code></pre>

<hr>

<h2>The ""Control""</h2>

<p>Running the code as-is doesn't actually perform any database operations, but it will give us an idea of how fast the raw C file I/O and string processing operations are.</p>

<blockquote>
  <p>Imported 864913 records in 0.94
  seconds</p>
</blockquote>

<p>Great! We can do 920,000 inserts per second, provided we don't actually do any inserts :-)</p>

<hr>

<h2>The ""Worst-Case-Scenario""</h2>

<p>We're going to generate the SQL string using the values read from the file and invoke that SQL operation using sqlite3_exec:</p>

<pre><code>sprintf(sSQL, ""INSERT INTO TTC VALUES (NULL, '%s', '%s', '%s', '%s', '%s', '%s', '%s')"", sRT, sBR, sVR, sST, sVI, sDT, sTM);
sqlite3_exec(db, sSQL, NULL, NULL, &amp;sErrMsg);
</code></pre>

<p>This is going to be slow because the SQL will be compiled into VDBE code for every insert and every insert will happen in its own transaction. <em>How slow?</em></p>

<blockquote>
  <p>Imported 864913 records in 9933.61
  seconds</p>
</blockquote>

<p>Yikes! 2 hours and 45 minutes! That's only <strong>85 inserts per second.</strong></p>

<h2>Using a Transaction</h2>

<p>By default, SQLite will evaluate every INSERT / UPDATE statement within a unique transaction. If performing a large number of inserts, it's advisable to wrap your operation in a transaction:</p>

<pre><code>sqlite3_exec(db, ""BEGIN TRANSACTION"", NULL, NULL, &amp;sErrMsg);

pFile = fopen (INPUTDATA,""r"");
while (!feof(pFile)) {

    ...

}
fclose (pFile);

sqlite3_exec(db, ""END TRANSACTION"", NULL, NULL, &amp;sErrMsg);
</code></pre>

<blockquote>
  <p>Imported 864913 records in 38.03
  seconds</p>
</blockquote>

<p>That's better. Simply wrapping all of our inserts in a single transaction improved our performance to <strong>23,000 inserts per second.</strong></p>

<h2>Using a Prepared Statement</h2>

<p>Using a transaction was a huge improvement, but recompiling the SQL statement for every insert doesn't make sense if we using the same SQL over-and-over. Let's use <code>sqlite3_prepare_v2</code> to compile our SQL statement once and then bind our parameters to that statement using <code>sqlite3_bind_text</code>:</p>

<pre><code>/* Open input file and import into the database */
cStartClock = clock();

sprintf(sSQL, ""INSERT INTO TTC VALUES (NULL, @RT, @BR, @VR, @ST, @VI, @DT, @TM)"");
sqlite3_prepare_v2(db,  sSQL, BUFFER_SIZE, &amp;stmt, &amp;tail);

sqlite3_exec(db, ""BEGIN TRANSACTION"", NULL, NULL, &amp;sErrMsg);

pFile = fopen (INPUTDATA,""r"");
while (!feof(pFile)) {

    fgets (sInputBuf, BUFFER_SIZE, pFile);

    sRT = strtok (sInputBuf, ""\t"");   /* Get Route */
    sBR = strtok (NULL, ""\t"");        /* Get Branch */
    sVR = strtok (NULL, ""\t"");        /* Get Version */
    sST = strtok (NULL, ""\t"");        /* Get Stop Number */
    sVI = strtok (NULL, ""\t"");        /* Get Vehicle */
    sDT = strtok (NULL, ""\t"");        /* Get Date */
    sTM = strtok (NULL, ""\t"");        /* Get Time */

    sqlite3_bind_text(stmt, 1, sRT, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 2, sBR, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 3, sVR, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 4, sST, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 5, sVI, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 6, sDT, -1, SQLITE_TRANSIENT);
    sqlite3_bind_text(stmt, 7, sTM, -1, SQLITE_TRANSIENT);

    sqlite3_step(stmt);

    sqlite3_clear_bindings(stmt);
    sqlite3_reset(stmt);

    n++;
}
fclose (pFile);

sqlite3_exec(db, ""END TRANSACTION"", NULL, NULL, &amp;sErrMsg);

printf(""Imported %d records in %4.2f seconds\n"", n, (clock() - cStartClock) / (double)CLOCKS_PER_SEC);

sqlite3_finalize(stmt);
sqlite3_close(db);

return 0;
</code></pre>

<blockquote>
  <p>Imported 864913 records in 16.27
  seconds</p>
</blockquote>

<p>Nice! There's a little bit more code (don't forget to call <code>sqlite3_clear_bindings</code> and <code>sqlite3_reset</code>), but we've more than doubled our performance to <strong>53,000 inserts per second.</strong></p>

<h2>PRAGMA synchronous = OFF</h2>

<p>By default, SQLite will pause after issuing a OS-level write command. This guarantees that the data is written to the disk. By setting <code>synchronous = OFF</code>, we are instructing SQLite to simply hand-off the data to the OS for writing and then continue. There's a chance that the database file may become corrupted if the computer suffers a catastrophic crash (or power failure) before the data is written to the platter:</p>

<pre><code>/* Open the database and create the schema */
sqlite3_open(DATABASE, &amp;db);
sqlite3_exec(db, TABLE, NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""PRAGMA synchronous = OFF"", NULL, NULL, &amp;sErrMsg);
</code></pre>

<blockquote>
  <p>Imported 864913 records in 12.41
  seconds</p>
</blockquote>

<p>The improvements are now smaller, but we're up to <strong>69,600 inserts per second.</strong></p>

<h2>PRAGMA journal_mode = MEMORY</h2>

<p>Consider storing the rollback journal in memory by evaluating <code>PRAGMA journal_mode = MEMORY</code>. Your transaction will be faster, but if you lose power or your program crashes during a transaction you database could be left in a corrupt state with a partially-completed transaction:</p>

<pre><code>/* Open the database and create the schema */
sqlite3_open(DATABASE, &amp;db);
sqlite3_exec(db, TABLE, NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""PRAGMA journal_mode = MEMORY"", NULL, NULL, &amp;sErrMsg);
</code></pre>

<blockquote>
  <p>Imported 864913 records in 13.50
  seconds</p>
</blockquote>

<p>A little slower than the previous optimization at <strong>64,000 inserts per second.</strong></p>

<h2>PRAGMA synchronous = OFF <em>and</em> PRAGMA journal_mode = MEMORY</h2>

<p>Let's combine the previous two optimizations. It's a little more risky (in case of a crash), but we're just importing data (not running a bank):</p>

<pre><code>/* Open the database and create the schema */
sqlite3_open(DATABASE, &amp;db);
sqlite3_exec(db, TABLE, NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""PRAGMA synchronous = OFF"", NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""PRAGMA journal_mode = MEMORY"", NULL, NULL, &amp;sErrMsg);
</code></pre>

<blockquote>
  <p>Imported 864913 records in 12.00
  seconds</p>
</blockquote>

<p>Fantastic! We're able to do <strong>72,000 inserts per second.</strong></p>

<h2>Using an In-Memory Database</h2>

<p>Just for kicks, let's build upon all of the previous optimizations and redefine the database filename so we're working entirely in RAM:</p>

<pre><code>#define DATABASE "":memory:""
</code></pre>

<blockquote>
  <p>Imported 864913 records in 10.94
  seconds</p>
</blockquote>

<p>It's not super-practical to store our database in RAM, but it's impressive that we can perform <strong>79,000 inserts per second.</strong></p>

<h2>Refactoring C Code</h2>

<p>Although not specifically an SQLite improvement, I don't like the extra <code>char*</code> assignment operations in the <code>while</code> loop. Let's quickly refactor that code to pass the output of <code>strtok()</code> directly into <code>sqlite3_bind_text()</code>, and let the compiler try to speed things up for us:</p>

<pre><code>pFile = fopen (INPUTDATA,""r"");
while (!feof(pFile)) {

    fgets (sInputBuf, BUFFER_SIZE, pFile);

    sqlite3_bind_text(stmt, 1, strtok (sInputBuf, ""\t""), -1, SQLITE_TRANSIENT); /* Get Route */
    sqlite3_bind_text(stmt, 2, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Branch */
    sqlite3_bind_text(stmt, 3, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Version */
    sqlite3_bind_text(stmt, 4, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Stop Number */
    sqlite3_bind_text(stmt, 5, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Vehicle */
    sqlite3_bind_text(stmt, 6, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Date */
    sqlite3_bind_text(stmt, 7, strtok (NULL, ""\t""), -1, SQLITE_TRANSIENT);    /* Get Time */

    sqlite3_step(stmt);        /* Execute the SQL Statement */
    sqlite3_clear_bindings(stmt);    /* Clear bindings */
    sqlite3_reset(stmt);        /* Reset VDBE */

    n++;
}
fclose (pFile);
</code></pre>

<p><strong>Note: We are back to using a real database file. In-memory databases are fast, but not necessarily practical</strong></p>

<blockquote>
  <p>Imported 864913 records in 8.94
  seconds</p>
</blockquote>

<p>A slight refactoring to the string processing code used in our parameter binding has allowed us to perform <strong>96,700 inserts per second.</strong> I think it's safe to say that this is <em>plenty fast</em>. As we start to tweak other variables (i.e. page size, index creation, etc.) this will be our benchmark.</p>

<hr>

<h2>Summary (so far)</h2>

<p><em>I hope you're still with me!</em> The reason we started down this road is that bulk-insert performance varies so wildly with SQLite, and it's not always obvious what changes need to be made to speed-up our operation. Using the same compiler (and compiler options), the same version of SQLite and the same data we've optimized our code and our usage of SQLite to go <strong>from a worst-case scenario of 85 inserts per second to over 96,000 inserts per second!</strong></p>

<hr>

<h2>CREATE INDEX then INSERT vs. INSERT then CREATE INDEX</h2>

<p>Before we start measuring <code>SELECT</code> performance, we know that we'll be creating indexes. It's been suggested in one of the answers below that when doing bulk inserts, it is faster to create the index after the data has been inserted (as opposed to creating the index first then inserting the data). Let's try:</p>

<p><strong>Create Index then Insert Data</strong></p>

<pre><code>sqlite3_exec(db, ""CREATE  INDEX 'TTC_Stop_Index' ON 'TTC' ('Stop')"", NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""BEGIN TRANSACTION"", NULL, NULL, &amp;sErrMsg);
...
</code></pre>

<blockquote>
  <p>Imported 864913 records in 18.13
  seconds</p>
</blockquote>

<p><strong>Insert Data then Create Index</strong></p>

<pre><code>...
sqlite3_exec(db, ""END TRANSACTION"", NULL, NULL, &amp;sErrMsg);
sqlite3_exec(db, ""CREATE  INDEX 'TTC_Stop_Index' ON 'TTC' ('Stop')"", NULL, NULL, &amp;sErrMsg);
</code></pre>

<blockquote>
  <p>Imported 864913 records in 13.66
  seconds</p>
</blockquote>

<p>As expected, bulk-inserts are slower if one column is indexed, but it does make a difference if the index is created after the data is inserted. Our no-index baseline is 96,000 inserts per second. <strong>Creating the index first then inserting data gives us 47,700 inserts per second, whereas inserting the data first then creating the index gives us 63,300 inserts per second.</strong></p>

<hr>

<p>I'd gladly take suggestions for other scenarios to try... And will be compiling similar data for SELECT queries soon.</p>
","<p>There are a few steps to see the tables in an SQLite database:</p>

<ol>
<li><p>List the tables in your database:</p>

<pre><code>.tables
</code></pre></li>
<li><p>List how the table looks:</p>

<pre><code>.schema tablename
</code></pre></li>
<li><p>Print the entire table:</p>

<pre><code>SELECT * FROM tablename;
</code></pre></li>
<li><p>List all of the available SQLite prompt commands:</p>

<pre><code>.help
</code></pre></li>
</ol>
"
"<p>How can a column with a default value be added to an existing table in <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server#Genesis"" rel=""noreferrer"">SQL Server 2000</a> / <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server#SQL_Server_2005"" rel=""noreferrer"">SQL Server 2005</a>?</p>
","<h2>Syntax:</h2>

<pre><code>ALTER TABLE {TABLENAME} 
ADD {COLUMNNAME} {TYPE} {NULL|NOT NULL} 
CONSTRAINT {CONSTRAINT_NAME} DEFAULT {DEFAULT_VALUE}
WITH VALUES
</code></pre>

<h2>Example:</h2>

<pre><code>ALTER TABLE SomeTable
        ADD SomeCol Bit NULL --Or NOT NULL.
 CONSTRAINT D_SomeTable_SomeCol --When Omitted a Default-Constraint Name is autogenerated.
    DEFAULT (0)--Optional Default-Constraint.
WITH VALUES --Add if Column is Nullable and you want the Default Value for Existing Records.
</code></pre>

<h2>Notes:</h2>

<p><strong>Optional Constraint Name:</strong><br />
If you leave out <code>CONSTRAINT D_SomeTable_SomeCol</code> then SQL Server will autogenerate<br />
&nbsp; &nbsp; a Default-Contraint with a funny Name like: <code>DF__SomeTa__SomeC__4FB7FEF6</code><br /></p>

<p><strong>Optional With-Values Statement:</strong><br />
The <code>WITH VALUES</code> is only needed when your Column is Nullable<br />
&nbsp; &nbsp; and you want the Default Value used for Existing Records.<br />
If your Column is <code>NOT NULL</code>, then it will automatically use the Default Value<br />
&nbsp; &nbsp; for all Existing Records, whether you specify <code>WITH VALUES</code> or not.</p>

<p><strong>How Inserts work with a Default-Constraint:</strong><br />
If you insert a Record into <code>SomeTable</code> and do <strong><em>not</em></strong> Specify <code>SomeCol</code>'s value, then it will Default to <code>0</code>.<br />
If you insert a Record <strong><em>and</em></strong> Specify <code>SomeCol</code>'s value as <code>NULL</code> (and your column allows nulls),<br />
&nbsp; &nbsp; then the Default-Constraint will <strong><em>not</em></strong> be used and <code>NULL</code> will be inserted as the Value.<br /></p>

<p>Notes were based on everyone's great feedback below.<br />
Special Thanks to:<br />
&nbsp; &nbsp; @Yatrix, @WalterStabosz, @YahooSerious, and @StackMan for their Comments.</p>
"
"<p>How can a column with a default value be added to an existing table in <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server#Genesis"" rel=""noreferrer"">SQL Server 2000</a> / <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server#SQL_Server_2005"" rel=""noreferrer"">SQL Server 2005</a>?</p>
","<h2>Syntax:</h2>

<pre><code>ALTER TABLE {TABLENAME} 
ADD {COLUMNNAME} {TYPE} {NULL|NOT NULL} 
CONSTRAINT {CONSTRAINT_NAME} DEFAULT {DEFAULT_VALUE}
WITH VALUES
</code></pre>

<h2>Example:</h2>

<pre><code>ALTER TABLE SomeTable
        ADD SomeCol Bit NULL --Or NOT NULL.
 CONSTRAINT D_SomeTable_SomeCol --When Omitted a Default-Constraint Name is autogenerated.
    DEFAULT (0)--Optional Default-Constraint.
WITH VALUES --Add if Column is Nullable and you want the Default Value for Existing Records.
</code></pre>

<h2>Notes:</h2>

<p><strong>Optional Constraint Name:</strong><br />
If you leave out <code>CONSTRAINT D_SomeTable_SomeCol</code> then SQL Server will autogenerate<br />
&nbsp; &nbsp; a Default-Contraint with a funny Name like: <code>DF__SomeTa__SomeC__4FB7FEF6</code><br /></p>

<p><strong>Optional With-Values Statement:</strong><br />
The <code>WITH VALUES</code> is only needed when your Column is Nullable<br />
&nbsp; &nbsp; and you want the Default Value used for Existing Records.<br />
If your Column is <code>NOT NULL</code>, then it will automatically use the Default Value<br />
&nbsp; &nbsp; for all Existing Records, whether you specify <code>WITH VALUES</code> or not.</p>

<p><strong>How Inserts work with a Default-Constraint:</strong><br />
If you insert a Record into <code>SomeTable</code> and do <strong><em>not</em></strong> Specify <code>SomeCol</code>'s value, then it will Default to <code>0</code>.<br />
If you insert a Record <strong><em>and</em></strong> Specify <code>SomeCol</code>'s value as <code>NULL</code> (and your column allows nulls),<br />
&nbsp; &nbsp; then the Default-Constraint will <strong><em>not</em></strong> be used and <code>NULL</code> will be inserted as the Value.<br /></p>

<p>Notes were based on everyone's great feedback below.<br />
Special Thanks to:<br />
&nbsp; &nbsp; @Yatrix, @WalterStabosz, @YahooSerious, and @StackMan for their Comments.</p>
"
"<p>Would you recommend using a <a href=""https://dev.mysql.com/doc/refman/5.0/en/datetime.html"" rel=""noreferrer"">datetime</a> or a <a href=""https://dev.mysql.com/doc/refman/5.0/en/datetime.html"" rel=""noreferrer"">timestamp</a> field, and why (using MySQL)? </p>

<p>I'm working with PHP on the server side.</p>
","<p>Timestamps in MySQL are generally used to track changes to records, and are often updated every time the record is changed. If you want to store a specific value you should use a datetime field.</p>

<p>If you meant that you want to decide between using a UNIX timestamp or a native MySQL datetime field, go with the native format. You can do calculations within MySQL that way 
<code>(""SELECT DATE_ADD(my_datetime, INTERVAL 1 DAY)"")</code> and it is simple to change the format of the value to a UNIX timestamp <code>(""SELECT UNIX_TIMESTAMP(my_datetime)"")</code> when you query the record if you want to operate on it with PHP.</p>
"
"<p>I need to add a specific column if it does not exist. I have something like the following, but it always returns false:</p>

<pre><code>IF EXISTS(SELECT *
          FROM   INFORMATION_SCHEMA.COLUMNS
          WHERE  TABLE_NAME = 'myTableName'
                 AND COLUMN_NAME = 'myColumnName') 
</code></pre>

<p>How can I check if a column exists in a table of the SQL Server database?</p>
","<p>SQL Server 2005 onwards:</p>

<pre><code>IF EXISTS(SELECT 1 FROM sys.columns 
          WHERE Name = N'columnName'
          AND Object_ID = Object_ID(N'schemaName.tableName'))
BEGIN
    -- Column Exists
END
</code></pre>

<p>Martin Smith's version is shorter:</p>

<pre><code>IF COL_LENGTH('schemaName.tableName', 'columnName') IS NOT NULL
BEGIN
    -- Column Exists
END
</code></pre>
"
"<p>I need to add a specific column if it does not exist. I have something like the following, but it always returns false:</p>

<pre><code>IF EXISTS(SELECT *
          FROM   INFORMATION_SCHEMA.COLUMNS
          WHERE  TABLE_NAME = 'myTableName'
                 AND COLUMN_NAME = 'myColumnName') 
</code></pre>

<p>How can I check if a column exists in a table of the SQL Server database?</p>
","<p>SQL Server 2005 onwards:</p>

<pre><code>IF EXISTS(SELECT 1 FROM sys.columns 
          WHERE Name = N'columnName'
          AND Object_ID = Object_ID(N'schemaName.tableName'))
BEGIN
    -- Column Exists
END
</code></pre>

<p>Martin Smith's version is shorter:</p>

<pre><code>IF COL_LENGTH('schemaName.tableName', 'columnName') IS NOT NULL
BEGIN
    -- Column Exists
END
</code></pre>
"
"<p>I need to add a specific column if it does not exist. I have something like the following, but it always returns false:</p>

<pre><code>IF EXISTS(SELECT *
          FROM   INFORMATION_SCHEMA.COLUMNS
          WHERE  TABLE_NAME = 'myTableName'
                 AND COLUMN_NAME = 'myColumnName') 
</code></pre>

<p>How can I check if a column exists in a table of the SQL Server database?</p>
","<p>SQL Server 2005 onwards:</p>

<pre><code>IF EXISTS(SELECT 1 FROM sys.columns 
          WHERE Name = N'columnName'
          AND Object_ID = Object_ID(N'schemaName.tableName'))
BEGIN
    -- Column Exists
END
</code></pre>

<p>Martin Smith's version is shorter:</p>

<pre><code>IF COL_LENGTH('schemaName.tableName', 'columnName') IS NOT NULL
BEGIN
    -- Column Exists
END
</code></pre>
"
"<p>What command or short key can I use to exit the PostgreSQL command line utility <code>psql</code>?</p>
","<p>Try this (in the <code>psql</code> command-line tool):</p>

<pre><code>\d+ tablename
</code></pre>

<p>See <a href=""http://www.postgresql.org/docs/current/interactive/app-psql.html#APP-PSQL-META-COMMANDS"" rel=""noreferrer"">the manual</a> for more info.</p>
"
"<p>What command or short key can I use to exit the PostgreSQL command line utility <code>psql</code>?</p>
","<p>Type <code>\q</code> and then press <code>ENTER</code> to quit <code>psql</code>.</p>

<p><strong>UPDATE: 19-OCT-2018</strong></p>

<p>As of <em>PostgreSQL 11</em>, the keywords ""<code>quit</code>"" and ""<code>exit</code>"" in the PostgreSQL command-line interface have been included to help make it easier to leave the command-line tool.</p>
"
"<p>What command or short key can I use to exit the PostgreSQL command line utility <code>psql</code>?</p>
","<p>Try this (in the <code>psql</code> command-line tool):</p>

<pre><code>\d+ tablename
</code></pre>

<p>See <a href=""http://www.postgresql.org/docs/current/interactive/app-psql.html#APP-PSQL-META-COMMANDS"" rel=""noreferrer"">the manual</a> for more info.</p>
"
"<p>I have two tables, <code>movies</code> and <code>categories</code>, and I get an ordered list by <strong>categoryID</strong> first and then by <strong>Name</strong>.</p>

<p>The movie table has three columns, <strong>ID, Name, and CategoryID</strong>.
The category table two has columns, <strong>ID, and Name</strong>.</p>

<p>I tried something like the following, but it didn't work.</p>

<pre><code>var movies = _db.Movies.OrderBy( m =&gt; { m.CategoryID, m.Name })
</code></pre>
","<p>This should work for you:</p>

<pre><code>var movies = _db.Movies.OrderBy(c =&gt; c.Category).ThenBy(n =&gt; n.Name)
</code></pre>
"
"<p>I'm using the MySQL command line utility and can navigate through a database. Now I need to see a list of user accounts. How can I do this?</p>

<p>I'm using MySQL version <code>5.4.1</code>.</p>
","<p>Use this query:</p>

<pre><code>SELECT User FROM mysql.user;
</code></pre>

<p>Which will output a table like this:</p>

<pre><code>+-------+
| User  |
+-------+
| root  |
+-------+
| user2 |
+-------+
</code></pre>

<p>As Matthew Scharley <a href=""https://stackoverflow.com/questions/1135245/how-to-get-a-list-of-mysql-user-accounts/1135251#comment953239_1135251"">points out in the comments on this answer</a>, you can group by the <code>User</code> column if you'd only like to see unique usernames.</p>
"
"<p>I am trying to <code>INSERT INTO</code> a table using the input from another table. Although this is entirely feasible for many database engines, I always seem to struggle to remember the correct syntax for the <code>SQL</code> engine of the day (<a href=""http://en.wikipedia.org/wiki/MySQL"" rel=""noreferrer"">MySQL</a>, <a href=""http://en.wikipedia.org/wiki/Oracle_Database"" rel=""noreferrer"">Oracle</a>, <a href=""http://en.wikipedia.org/wiki/Microsoft_SQL_Server"" rel=""noreferrer"">SQL Server</a>, <a href=""http://en.wikipedia.org/wiki/IBM_Informix"" rel=""noreferrer"">Informix</a>, and <a href=""http://en.wikipedia.org/wiki/IBM_DB2"" rel=""noreferrer"">DB2</a>).</p>

<p>Is there a silver-bullet syntax coming from an SQL standard (for example, <a href=""http://en.wikipedia.org/wiki/SQL-92"" rel=""noreferrer"">SQL-92</a>) that would allow me to insert the values without worrying about the underlying database?</p>
","<p>Try:</p>

<pre><code>INSERT INTO table1 ( column1 )
SELECT  col1
FROM    table2  
</code></pre>

<p>This is standard ANSI SQL and should work on any DBMS</p>

<p>It definitely works for:</p>

<ul>
<li>Oracle  </li>
<li>MS SQL Server  </li>
<li>MySQL  </li>
<li>Postgres  </li>
<li>SQLite v3  </li>
<li>Teradata</li>
<li>DB2  </li>
<li>Sybase</li>
<li>Vertica</li>
<li>HSQLDB</li>
<li>H2</li>
<li>AWS RedShift</li>
<li>SAP HANA</li>
</ul>
"
"<p>I want to query something with SQL's <code>like</code> query:</p>

<pre><code>SELECT * FROM users  WHERE name LIKE '%m%'
</code></pre>

<p>How to do I achieve the same in MongoDB?  I can't find an operator for <code>like</code> in the <a href=""http://www.mongodb.org/display/DOCS/Advanced+Queries"" rel=""noreferrer"">documentation</a>.</p>
","<p>That would have to be:</p>

<pre><code>db.users.find({""name"": /.*m.*/})
</code></pre>

<p>or, similar:</p>

<pre><code>db.users.find({""name"": /m/})
</code></pre>

<p>You're looking for something that contains ""m"" somewhere (SQL's '<code>%</code>' operator is equivalent to Regexp's '<code>.*</code>'), not something that has ""m"" anchored to the beginning of the string.</p>
"
"<p>I need to update this table in <strong>SQL Server 2005</strong> with data from its 'parent' table, see below:</p>

<p><strong>sale</strong></p>

<pre><code>id (int)
udid (int)
assid (int)
</code></pre>

<p><strong>ud</strong></p>

<pre><code>id  (int)
assid  (int)
</code></pre>

<p><code>sale.assid</code> contains the correct value to update <code>ud.assid</code>. </p>

<p>What query will do this? I'm thinking a <code>join</code> but I'm not sure if it's possible.</p>
","<p>Syntax strictly depends on which SQL DBMS you're using. Here are some ways to do it in ANSI/ISO (aka should work on any SQL DBMS), MySQL, SQL Server, and Oracle. Be advised that my suggested ANSI/ISO method will typically be much slower than the other two methods, but if you're using a SQL DBMS other than MySQL, SQL Server, or Oracle, then it may be the only way to go (e.g. if your SQL DBMS doesn't support <code>MERGE</code>):</p>

<p>ANSI/ISO:</p>

<pre><code>update ud 
     set assid = (
          select sale.assid 
          from sale 
          where sale.udid = ud.id
     )
 where exists (
      select * 
      from sale 
      where sale.udid = ud.id
 );
</code></pre>

<p>MySQL:</p>

<pre><code>update ud u
inner join sale s on
    u.id = s.udid
set u.assid = s.assid
</code></pre>

<p>SQL Server:</p>

<pre><code>update u
set u.assid = s.assid
from ud u
    inner join sale s on
        u.id = s.udid
</code></pre>

<p>PostgreSQL:</p>

<pre><code>update ud
  set ud.assid = s.assid
from sale s 
where ud.id = s.udid;
</code></pre>

<p>Note that the target table must not be repeated in the <code>FROM</code> clause for Postgres.</p>

<p>Oracle:</p>

<pre><code>update
    (select
        u.assid as new_assid,
        s.assid as old_assid
    from ud u
        inner join sale s on
            u.id = s.udid) up
set up.new_assid = up.old_assid
</code></pre>

<p>SQLite:</p>

<pre><code>update ud 
     set assid = (
          select sale.assid 
          from sale 
          where sale.udid = ud.id
     )
 where RowID in (
      select RowID 
      from ud 
      where sale.udid = ud.id
 );
</code></pre>
"
"<p>What SQL can be used to list the tables, and the rows within those tables in a SQLite database file - once I have attached it with the <code>ATTACH</code> command on the SQLite 3 command line tool?</p>
","<p>There are a few steps to see the tables in an SQLite database:</p>

<ol>
<li><p>List the tables in your database:</p>

<pre><code>.tables
</code></pre></li>
<li><p>List how the table looks:</p>

<pre><code>.schema tablename
</code></pre></li>
<li><p>Print the entire table:</p>

<pre><code>SELECT * FROM tablename;
</code></pre></li>
<li><p>List all of the available SQLite prompt commands:</p>

<pre><code>.help
</code></pre></li>
</ol>
"
"<p>I want to delete using <code>INNER JOIN</code> in <strong>SQL Server 2008</strong>.</p>

<p>But I get this error:</p>

<blockquote>
  <p>Msg 156, Level 15, State 1, Line 15<br>
  Incorrect syntax near the keyword 'INNER'.</p>
</blockquote>

<p>My code:</p>

<pre><code>DELETE FROM WorkRecord2 
INNER JOIN Employee ON EmployeeRun=EmployeeNo
WHERE Company = '1' AND Date = '2013-05-06'
</code></pre>
","<p>You need to specify what table you are deleting from, here is a version with an alias:</p>

<pre><code>DELETE w
FROM WorkRecord2 w
INNER JOIN Employee e
  ON EmployeeRun=EmployeeNo
WHERE Company = '1' AND Date = '2013-05-06'
</code></pre>
"
"<p>I'm trying to install version 1.2.2 of the MySQL_python adaptor, using a fresh virtualenv created with the <code>--no-site-packages</code> option. The current version shown in PyPi is <a href=""http://pypi.python.org/pypi/MySQL-python/1.2.3"" rel=""noreferrer"">1.2.3</a>. Is there a way to install the older version? I found an article stating that this should do it:</p>

<pre><code>pip install MySQL_python==1.2.2
</code></pre>

<p>When installed, however, it still shows <code>MySQL_python-1.2.3-py2.6.egg-info</code> in the site packages. Is this a problem specific to this package, or am I doing something wrong?</p>
","<p>It seems mysql_config is missing on your system or the installer could not find it.
Be sure mysql_config is really installed.</p>

<p>For example on Debian/Ubuntu you must install the package:</p>

<pre><code>sudo apt-get install libmysqlclient-dev
</code></pre>

<p>Maybe the mysql_config is not in your path, it will be the case when you compile by yourself
the mysql suite.</p>

<p><strong>Update:</strong> For recent versions of debian/ubuntu (as of 2018) it is </p>

<pre><code>sudo apt install default-libmysqlclient-dev
</code></pre>
"
"<p>What is the difference between returning <code>IQueryable&lt;T&gt;</code> vs. <code>IEnumerable&lt;T&gt;</code>?</p>

<pre><code>IQueryable&lt;Customer&gt; custs = from c in db.Customers
where c.City == ""&lt;City&gt;""
select c;

IEnumerable&lt;Customer&gt; custs = from c in db.Customers
where c.City == ""&lt;City&gt;""
select c;
</code></pre>

<p>Will both be deferred execution and when should one be preferred over the other?</p>
","<p>Yes, both will give you <a href=""https://msdn.microsoft.com/en-us/library/bb738633(v=vs.110).aspx#Anchor_0"" rel=""noreferrer"">deferred execution</a>.</p>

<p>The difference is that <a href=""https://msdn.microsoft.com/en-us/library/bb351562.aspx"" rel=""noreferrer""><code>IQueryable&lt;T&gt;</code></a> is the interface that allows LINQ-to-SQL (LINQ.-to-anything really) to work. So if you further refine your query on an <a href=""https://msdn.microsoft.com/en-us/library/bb351562.aspx"" rel=""noreferrer""><code>IQueryable&lt;T&gt;</code></a>, that query will be executed in the database, if possible. </p>

<p>For the <a href=""https://msdn.microsoft.com/en-us/library/9eekhta0.aspx"" rel=""noreferrer""><code>IEnumerable&lt;T&gt;</code></a> case, it will be LINQ-to-object, meaning that all objects matching the original query will have to be loaded into memory from the database.</p>

<p>In code:</p>

<pre><code>IQueryable&lt;Customer&gt; custs = ...;
// Later on...
var goldCustomers = custs.Where(c =&gt; c.IsGold);
</code></pre>

<p>That code will execute SQL to only select gold customers. The following code, on the other hand, will execute the original query in the database, then filtering out the non-gold customers in the memory:</p>

<pre><code>IEnumerable&lt;Customer&gt; custs = ...;
// Later on...
var goldCustomers = custs.Where(c =&gt; c.IsGold);
</code></pre>

<p>This is quite an important difference, and working on <a href=""https://msdn.microsoft.com/en-us/library/bb351562.aspx"" rel=""noreferrer""><code>IQueryable&lt;T&gt;</code></a> can in many cases save you from returning too many rows from the database. Another prime example is doing paging: If you use <a href=""https://msdn.microsoft.com/en-us/library/bb300906.aspx"" rel=""noreferrer""><code>Take</code></a> and <a href=""https://msdn.microsoft.com/en-us/library/bb357513.aspx"" rel=""noreferrer""><code>Skip</code></a> on <a href=""https://msdn.microsoft.com/en-us/library/system.linq.iqueryable.aspx"" rel=""noreferrer""><code>IQueryable</code></a>, you will only get the number of rows requested; doing that on an <a href=""https://msdn.microsoft.com/en-us/library/9eekhta0.aspx"" rel=""noreferrer""><code>IEnumerable&lt;T&gt;</code></a> will cause all of your rows to be loaded in memory.</p>
"
"<p>If your application requires a database and it comes with built in data, what is the best way to ship that application? Should I:</p>

<ol>
<li><p>Precreate the SQLite database and include it in the <code>.apk</code>?</p></li>
<li><p>Include the SQL commands with the application and have it create the database and insert the data on first use?</p></li>
</ol>

<p>The drawbacks I see are:</p>

<ol>
<li><p>Possible SQLite version mismatches might cause problems and I currently don't know where the database should go and how to access it.</p></li>
<li><p>It may take a really long time to create and populate the database on the device.</p></li>
</ol>

<p>Any suggestions? Pointers to the documentation regarding any issues would be greatly appreciated.</p>
","<p>I just found a way to do this in <em><code>ReignDesign blog</code></em> in an article titled <a href=""http://www.reigndesign.com/blog/using-your-own-sqlite-database-in-android-applications/"" rel=""noreferrer"">Using your own SQLite database in Android applications</a>. Basically you precreate your database, put it in your assets directory in your apk, and on first use copy to <code>/data/data/YOUR_PACKAGE/databases/</code> directory.</p>
"
"<p>What is the easiest way to save PL/pgSQL output from a PostgreSQL database to a CSV file? </p>

<p>I'm using PostgreSQL 8.4 with pgAdmin III and PSQL plugin where I run queries from.</p>
","<p>Do you want the resulting file on the server, or on the client?</p>

<h1>Server side</h1>

<p>If you want something easy to re-use or automate, you can use Postgresql's built in <a href=""http://www.postgresql.org/docs/current/interactive/sql-copy.html"" rel=""noreferrer"">COPY</a> command. e.g.</p>

<pre><code>Copy (Select * From foo) To '/tmp/test.csv' With CSV DELIMITER ',';
</code></pre>

<p><strong>This approach runs entirely on the remote server</strong> - it can't write to your local PC. It also needs to be run as a Postgres ""superuser"" (normally called ""root"") because Postgres can't stop it doing nasty things with that machine's local filesystem.</p>

<p>That doesn't actually mean you have to be connected as a superuser (automating that would be a security risk of a different kind), because you can use <a href=""http://www.postgresql.org/docs/current/interactive/sql-createfunction.html"" rel=""noreferrer"">the <code>SECURITY DEFINER</code> option to <code>CREATE FUNCTION</code></a> to make a function which <em>runs as though you were a superuser</em>. </p>

<p>The crucial part is that your function is there to perform additional checks, not just by-pass the security - so you could write a function which exports the exact data you need, or you could write something which can accept various options as long as they meet a strict whitelist. You need to check two things:</p>

<ol>
<li>Which <strong>files</strong> should the user be allowed to read/write on disk? This might be a particular directory, for instance, and the filename might have to have a suitable prefix or extension.</li>
<li>Which <strong>tables</strong> should the user be able to read/write in the database? This would normally be defined by <code>GRANT</code>s in the database, but the function is now running as a superuser, so tables which would normally be ""out of bounds"" will be fully accessible. You probably dont want to let someone invoke your function and add rows on the end of your users table</li>
</ol>

<p>I've written <a href=""http://rwec.co.uk/q/pg-copy"" rel=""noreferrer"">a blog post expanding on this approach</a>, including some examples of functions that export (or import) files and tables meeting strict conditions.</p>

<hr>

<h1>Client side</h1>

<p>The other approach is to <strong>do the file handling on the client side</strong>, i.e. in your application or script. The Postgres server doesn't need to know what file you're copying to, it just spits out the data and the client puts it somewhere.</p>

<p>The underlying syntax for this is the <code>COPY TO STDOUT</code> command, and graphical tools like pgAdmin will wrap it for you in a nice dialog.</p>

<p>The <strong><code>psql</code> command-line client</strong> has a special ""meta-command"" called <strong><code>\copy</code></strong>, which takes all the same options as the ""real"" <code>COPY</code>, but is run inside the client:</p>

<pre><code>\copy (Select * From foo) To '/tmp/test.csv' With CSV
</code></pre>

<p>Note that there is no terminating <code>;</code>, because meta-commands are terminated by newline, unlike SQL commands.</p>

<p>From <a href=""http://www.postgresql.org/docs/current/interactive/app-psql.html#APP-PSQL-META-COMMANDS-COPY"" rel=""noreferrer"">the docs</a>:</p>

<blockquote>
  <p>Do not confuse COPY with the psql instruction \copy. \copy invokes COPY FROM STDIN or COPY TO STDOUT, and then fetches/stores the data in a file accessible to the psql client. Thus, file accessibility and access rights depend on the client rather than the server when \copy is used.</p>
</blockquote>

<p>Your application programming language <em>may</em> also have support for pushing or fetching the data, but you cannot generally use <code>COPY FROM STDIN</code>/<code>TO STDOUT</code> within a standard SQL statement, because there is no way of connecting the input/output stream. PHP's PostgreSQL handler (<em>not</em> PDO) includes very basic <a href=""http://www.php.net/manual/en/function.pg-copy-from.php"" rel=""noreferrer""><code>pg_copy_from</code></a> and <a href=""http://www.php.net/manual/en/function.pg-copy-to.php"" rel=""noreferrer""><code>pg_copy_to</code></a> functions which copy to/from a PHP array, which may not be efficient for large data sets.</p>
"
"<p>I've been learning Functions and Stored Procedure for quite a while but I don't know why and when I should use a function or a stored procedure. They look same to me, maybe because I am kinda newbie about that.</p>

<p>Can some one tell me why?</p>
","<p>Functions are computed values and cannot perform permanent environmental changes to SQL Server (i.e. no INSERT or UPDATE statements allowed).</p>

<p>A function can be used inline in SQL statements if it returns a scalar value, or can be joined upon if it returns a result set.</p>

<p><strong>A point worth noting from comments, which summarize the answer. Thanks to @Sean K Anderson:</strong></p>

<blockquote>
  <p>Functions follow the computer-sciency definition in that they MUST return a value and cannot alter the data they receive as parameters
  (the arguments). Functions are not allowed to change anything, must
  have at least one parameter, and they must return a value. Stored
  procs do not have to have a parameter, can change database objects,
  and do not have to return a value.</p>
</blockquote>
"
"<p>In the MongoDB shell, how do I list all collections for the current database that I'm using?</p>
","<p>You can do...</p>

<p>JS (shell):</p>

<pre><code>db.getCollectionNames()
</code></pre>

<p>node.js:</p>

<pre><code>db.listCollections()
</code></pre>

<p>non-JS (shell only):</p>

<pre><code>show collections
</code></pre>

<p>The reason I call that non-JS is because:</p>

<pre><code>$ mongo prodmongo/app --eval ""show collections""
MongoDB shell version: 3.2.10
connecting to: prodmongo/app
2016-10-26T19:34:34.886-0400 E QUERY    [thread1] SyntaxError: missing ; before statement @(shell eval):1:5

$ mongo prodmongo/app --eval ""db.getCollectionNames()""
MongoDB shell version: 3.2.10
connecting to: prodmongo/app
[
    ""Profiles"",
    ""Unit_Info""
]
</code></pre>

<p>If you really want that sweet, sweet <code>show collections</code> output, you can:</p>

<pre><code>$ mongo prodmongo/app --eval ""db.getCollectionNames().join('\n')""
MongoDB shell version: 3.2.10
connecting to: prodmongo/app
Profiles
Unit_Info
</code></pre>
"
"<p>I am using the Ruby on Rails 3.1 pre version. I like to use PostgreSQL, but the problem is installing the <code>pg</code> gem. It gives me the following error:</p>

<pre><code>$ gem install pg
Building native extensions.  This could take a while...
ERROR:  Error installing pg:
    ERROR: Failed to build gem native extension.

        /home/u/.rvm/rubies/ruby-1.9.2-p0/bin/ruby extconf.rb
checking for pg_config... no
No pg_config... trying anyway. If building fails, please try again with
 --with-pg-config=/path/to/pg_config
checking for libpq-fe.h... no
Can't find the 'libpq-fe.h header
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of
necessary libraries and/or headers.  Check the mkmf.log file for more
details.  You may need configuration options.

Provided configuration options:
    --with-opt-dir
    --without-opt-dir
    --with-opt-include
    --without-opt-include=${opt-dir}/include
    --with-opt-lib
    --without-opt-lib=${opt-dir}/lib
    --with-make-prog
    --without-make-prog
    --srcdir=.
    --curdir
    --ruby=/home/u/.rvm/rubies/ruby-1.9.2-p0/bin/ruby
    --with-pg
    --without-pg
    --with-pg-dir
    --without-pg-dir
    --with-pg-include
    --without-pg-include=${pg-dir}/include
    --with-pg-lib
    --without-pg-lib=${pg-dir}/lib
    --with-pg-config
    --without-pg-config
    --with-pg_config
    --without-pg_config


Gem files will remain installed in /home/u/.rvm/gems/ruby-1.9.2-p0/gems/pg-0.11.0 for inspection.
Results logged to /home/u/.rvm/gems/ruby-1.9.2-p0/gems/pg-0.11.0/ext/gem_make.out
</code></pre>

<p>How do I solve this problem?</p>
","<p>It looks like in Ubuntu that header is part of the <a href=""http://packages.ubuntu.com/natty/i386/libpq-dev/filelist"" rel=""noreferrer""><code>libpq-dev</code> package</a> (at least in the following Ubuntu versions: 
<a href=""http://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_11.04_.28Natty_Narwhal.29"" rel=""noreferrer"">11.04</a> (Natty Narwhal), <a href=""https://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_10.04_LTS_.28Lucid_Lynx.29"" rel=""noreferrer"">10.04</a> (Lucid Lynx), <a href=""http://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_11.10_.28Oneiric_Ocelot.29"" rel=""noreferrer"">11.10</a> (Oneiric Ocelot), <a href=""http://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_12.04_LTS_.28Precise_Pangolin.29"" rel=""noreferrer"">12.04</a> (Precise Pangolin), <a href=""https://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_14.04_LTS_.28Trusty_Tahr.29"" rel=""noreferrer"">14.04</a> (Trusty Tahr) and <a href=""https://en.wikipedia.org/wiki/Ubuntu_version_history#Ubuntu_18.04_LTS_(Bionic_Beaver)"" rel=""noreferrer"">18.04</a> (Bionic Beaver)):</p>

<pre><code>...
/usr/include/postgresql/libpq-fe.h
...
</code></pre>

<p>So try installing <code>libpq-dev</code> or its equivalent for your OS:</p>

<ul>
<li>For Ubuntu/Debian systems: <code>sudo apt-get install libpq-dev</code></li>
<li>On <a href=""http://en.wikipedia.org/wiki/Red_Hat_Linux"" rel=""noreferrer"">Red Hat Linux</a> (RHEL) systems: <code>yum install postgresql-devel</code></li>
<li>For Mac <a href=""https://en.wikipedia.org/wiki/Homebrew_%28package_management_software%29"" rel=""noreferrer"">Homebrew</a>: <code>brew install postgresql</code></li>
<li>For Mac <a href=""http://en.wikipedia.org/wiki/MacPorts"" rel=""noreferrer"">MacPorts</a> PostgreSQL: <code>gem install pg -- --with-pg-config=/opt/local/lib/postgresql[version number]/bin/pg_config</code></li>
<li>For <a href=""https://en.wikipedia.org/wiki/OpenSUSE"" rel=""noreferrer"">OpenSuse</a>: <code>zypper in postgresql-devel</code></li>
<li>For <a href=""https://en.wikipedia.org/wiki/Arch_Linux"" rel=""noreferrer"">ArchLinux</a>: <code>pacman -S postgresql-libs</code></li>
</ul>
"
"<p>I'm trying to update the column <code>visited</code> to give it the value 1. I use MySQL workbench, and I'm writing the statement in the SQL editor from inside the workbench. I'm writing the following command:</p>

<pre><code>UPDATE tablename SET columnname=1;
</code></pre>

<p>It gives me the following error:</p>

<blockquote>
  <p>You are using safe update mode and you tried to update a table without
  a WHERE that uses a KEY column To disable safe mode, toggle the option
  ....</p>
</blockquote>

<p>I followed the instructions, and I unchecked the <code>safe update</code> option from the <code>Edit</code> menu then <code>Preferences</code> then <code>SQL Editor</code>. The same error still appear &amp; I'm not able to update this value. Please, tell me what is wrong?</p>
","<p>It looks like your MySql session has the <a href=""http://dev.mysql.com/doc/refman/5.5/en/mysql-command-options.html#option_mysql_safe-updates"" rel=""noreferrer"">safe-updates option</a> set. This means that you can't update or delete records without specifying a key (ex. <code>primary key</code>) in the where clause.</p>

<p>Try:</p>

<pre><code>SET SQL_SAFE_UPDATES = 0;
</code></pre>

<p>Or you can modify your query to follow the rule (use <code>primary key</code> in <code>where clause</code>).</p>
"
"<p>I have inserted records into a SQL Server database table. The table had a primary key defined and the auto increment identity seed is set to Yes. This is done primarily because in SQL Azure, each table has to have a primary key and identity defined. </p>

<p>But since I have to delete some records from the table, the identity seed for those tables will be disturbed and the index column (which is auto-generated with an increment of 1) will get disturbed.</p>

<p><strong>How can I reset the identity column after I deleted the records so that the column has sequence in ascending numerical order?</strong></p>

<p>The identity column is not used as a foreign key anywhere in database.</p>
","<p>The <a href=""http://technet.microsoft.com/en-us/library/ms176057.aspx"" rel=""noreferrer""><code>DBCC CHECKIDENT</code></a> management command is used to reset identity counter. The command syntax is:</p>

<pre><code>DBCC CHECKIDENT (table_name [, { NORESEED | { RESEED [, new_reseed_value ]}}])
[ WITH NO_INFOMSGS ]
</code></pre>

<p>Example: </p>

<pre><code>DBCC CHECKIDENT ('[TestTable]', RESEED, 0);
GO
</code></pre>

<p>It was not supported in a previous versions of Azure SQL Database, but is supported now.</p>

<hr>

<p>Please note that <code>new_reseed_value</code> argument is varied across SQL Server versions <a href=""https://docs.microsoft.com/en-us/sql/t-sql/database-console-commands/dbcc-checkident-transact-sql"" rel=""noreferrer"">according to documentation</a>: </p>

<blockquote>
  <p>If rows are present in the table, the next row is inserted with the <em>new_reseed_value</em> value. In version SQL Server 2008 R2 and earlier, the next row inserted uses <em>new_reseed_value</em> + the current increment value.</p>
</blockquote>

<p>However, <strong>I find this information misleading</strong> (just plain wrong actually) because observed behaviour indicates that at least SQL Server 2012 is still uses <em>new_reseed_value</em> + the current increment value logic. Microsoft even contradicts with its own <code>Example C</code> found on same page:</p>

<blockquote>
  <p>C. Forcing the current identity value to a new value</p>
  
  <p>The following example forces the current identity value in the
  AddressTypeID column in the AddressType table to a value of 10.
  Because the table has existing rows, the next row inserted will use 11
  as the value, that is, the new current increment value defined for the
  column value plus 1.</p>
</blockquote>

<pre><code>USE AdventureWorks2012;  
GO  
DBCC CHECKIDENT ('Person.AddressType', RESEED, 10);  
GO
</code></pre>

<p>Still, this all leaves an option for different behaviour on newer SQL Server versions. I guess the only way to be sure, until Microsoft clear up things in its own documentation, is to do actual tests before usage.</p>
"
"<p>Several months ago I learned from an answer on Stack Overflow how to perform multiple updates at once in MySQL using the following syntax:</p>

<pre><code>INSERT INTO table (id, field, field2) VALUES (1, A, X), (2, B, Y), (3, C, Z)
ON DUPLICATE KEY UPDATE field=VALUES(Col1), field2=VALUES(Col2);
</code></pre>

<p>I've now switched over to PostgreSQL and apparently this is not correct. It's referring to all the correct tables so I assume it's a matter of different keywords being used but I'm not sure where in the PostgreSQL documentation this is covered.</p>

<p>To clarify, I want to insert several things and if they already exist to update them.</p>
","<p><strong>Warning: this is not safe if executed from multiple sessions at the same time</strong> (see caveats below).</p>

<hr>

<p>Another clever way to do an ""UPSERT"" in postgresql is to do two sequential UPDATE/INSERT statements that are each designed to succeed or have no effect.</p>

<pre><code>UPDATE table SET field='C', field2='Z' WHERE id=3;
INSERT INTO table (id, field, field2)
       SELECT 3, 'C', 'Z'
       WHERE NOT EXISTS (SELECT 1 FROM table WHERE id=3);
</code></pre>

<p>The UPDATE will succeed if a row with ""id=3"" already exists, otherwise it has no effect.</p>

<p>The INSERT will succeed only if row with ""id=3"" does not already exist.</p>

<p>You can combine these two into a single string and run them both with a single SQL statement execute from your application.  Running them together in a single transaction is highly recommended.</p>

<p>This works very well when run in isolation or on a locked table, but is subject to race conditions that mean it might still fail with duplicate key error if a row is inserted concurrently, or might terminate with no row inserted when a row is deleted concurrently. A <code>SERIALIZABLE</code> transaction on PostgreSQL 9.1 or higher will handle it reliably at the cost of a very high serialization failure rate, meaning you'll have to retry a lot. See <a href=""http://www.depesz.com/2012/06/10/why-is-upsert-so-complicated/"" rel=""noreferrer"">why is upsert so complicated</a>, which discusses this case in more detail.</p>

<p>This approach is also <a href=""https://dba.stackexchange.com/q/78510/7788"">subject to lost updates in <code>read committed</code> isolation unless the application checks the affected row counts and verifies that either the <code>insert</code> or the <code>update</code> affected a row</a>.</p>
"
"<p>I've created database, for example 'mydb'. </p>

<pre><code>CREATE DATABASE mydb CHARACTER SET utf8 COLLATE utf8_bin;
CREATE USER 'myuser'@'%' IDENTIFIED BY PASSWORD '*HASH';
GRANT ALL ON mydb.* TO 'myuser'@'%';
GRANT ALL ON mydb TO 'myuser'@'%';
GRANT CREATE ON mydb TO 'myuser'@'%';
FLUSH PRIVILEGES;
</code></pre>

<p>Now i can login to database from everywhere, but can't create tables.</p>

<p>How to grant all privileges on that database and (in the future) tables. I can't create tables in 'mydb' database. I always get:</p>

<pre><code>CREATE TABLE t (c CHAR(20) CHARACTER SET utf8 COLLATE utf8_bin);
ERROR 1142 (42000): CREATE command denied to user 'myuser'@'...' for table 't'
</code></pre>
","<pre><code>GRANT ALL PRIVILEGES ON mydb.* TO 'myuser'@'%' WITH GRANT OPTION;
</code></pre>

<p>This is how I create my ""Super User"" privileges (although I would normally specify a host).</p>

<h1>IMPORTANT NOTE</h1>

<p>While this answer can solve the problem of access, <code>WITH GRANT OPTION</code> creates a MySQL user that can <a href=""https://dev.mysql.com/doc/refman/5.6/en/privileges-provided.html#priv_grant-option"" rel=""noreferrer"">edit the permissions of other users</a>. </p>

<blockquote>
  <p>The GRANT OPTION privilege enables you to give to other users or remove from other users those privileges that you yourself possess. </p>
</blockquote>

<p>For security reasons, you should not use this type of user account for any process that the public will have access to (i.e. a website). It is recommended that you <a href=""https://stackoverflow.com/a/15707789/2370483"">create a user with only database privileges</a> for that kind of use.</p>
"
"<p>This should be dead simple, but I <em>cannot</em> get it to work for the life of me.<br>
I'm just trying to connect remotely to my MySQL server.</p>

<p>connecting as</p>

<pre><code>mysql -u root -h localhost -p  
</code></pre>

<p>works fine, but trying  </p>

<pre><code>mysql -u root -h 'any ip address here' -p
</code></pre>

<p>fails with the error</p>

<pre><code>ERROR 1130 (00000): Host ''xxx.xx.xxx.xxx'' is not allowed to connect to this MySQL server
</code></pre>

<p>In the <code>mysql.user</code> table, there is exactly the same entry for user 'root' with host 'localhost' as another with host '%'.</p>

<p>I'm at my wits' end, and have no idea how to proceed.
Any ideas are welcome.</p>
","<p>Possibly a security precaution. You could try adding a new administrator account:</p>

<pre><code>mysql&gt; CREATE USER 'monty'@'localhost' IDENTIFIED BY 'some_pass';
mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'monty'@'localhost'
    -&gt;     WITH GRANT OPTION;
mysql&gt; CREATE USER 'monty'@'%' IDENTIFIED BY 'some_pass';
mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'monty'@'%'
    -&gt;     WITH GRANT OPTION;
</code></pre>

<p>Although as Pascal and others have noted it's not a great idea to have a user with this kind of access open to any IP. If you need an administrative user, use root, and leave it on localhost. For any other action specify exactly the privileges you need and limit the accessibility of the user as Pascal has suggest below.</p>

<p>Edit:</p>

<p>From the MySQL FAQ:</p>

<blockquote>
  <p>If you cannot figure out why you get
  Access denied, remove from the user 
  table all entries that have Host
  values containing wildcards (entries
  that contain '%' or '_' characters). A
  very common error is to insert a new
  entry with Host='%' and
  User='some_user', thinking that this
  allows you to specify localhost to
  connect from the same machine. The
  reason that this does not work is that
  the default privileges include an
  entry with Host='localhost' and
  User=''. Because that entry has a Host
  value 'localhost' that is more
  specific than '%', it is used in
  preference to the new entry when
  connecting from localhost! The correct
  procedure is to insert a second entry
  with Host='localhost' and
  User='some_user', or to delete the
  entry with Host='localhost' and
  User=''. After deleting the entry,
  remember to issue a FLUSH PRIVILEGES
  statement to reload the grant tables.
  See also Section 5.4.4, Access
  Control, Stage 1: Connection
  Verification.</p>
</blockquote>
"
"<p>I have a project in which I'd like to use some of the .NET 4.0 features but a core requirement is that I can use the System.Data.SQLite framework which is compiled against 2.X. I see mention of this being possible such as <a href=""https://stackoverflow.com/questions/1609894/how-to-reference-net-4-0-assembly-within-net-3-5-projects"">the accepted answer here</a> but I don't see how to actually achieve this.</p>

<p>When I just try and run my 4.0 project while referencing the 2.X assembly I get:</p>

<blockquote>
  <p><em>Mixed mode assembly is built against version 'v2.0.50727' of the runtime 
  and cannot be loaded in the 4.0 runtime without additional 
  configuration information.</em></p>
</blockquote>

<p>What ""additional configuration"" is necessary?</p>
","<p>In order to use a CLR 2.0 <em>mixed mode assembly</em>, you need to modify your App.Config file to include:</p>

<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;<br/>&lt;configuration&gt;<br/>  &lt;startup <b>useLegacyV2RuntimeActivationPolicy=&quot;true&quot;</b>&gt;<br/>    &lt;supportedRuntime version=&quot;v4.0&quot; sku=&quot;.NETFramework,Version=v4.0&quot;/&gt;<br/>  &lt;/startup&gt;<br/>&lt;/configuration&gt;</code></pre>

<p>The key is the <code>useLegacyV2RuntimeActivationPolicy</code> flag.  This causes the CLR to use the latest version (4.0) to load your mixed mode assembly.  Without this, it will not work.</p>

<p>Note that this only matters for mixed mode (C++/CLI) assemblies.  You can load all managed CLR 2 assemblies without specifying this in <code>app.config</code>.</p>
"
"<p>I have a table <code>story_category</code> in my database with corrupt entries. The next query returns the corrupt entries:</p>

<pre><code>SELECT * 
FROM  story_category 
WHERE category_id NOT IN (
    SELECT DISTINCT category.id 
    FROM category INNER JOIN 
       story_category ON category_id=category.id);
</code></pre>

<p>I tried to delete them executing:</p>

<pre><code>DELETE FROM story_category 
WHERE category_id NOT IN (
    SELECT DISTINCT category.id 
    FROM category 
      INNER JOIN story_category ON category_id=category.id);
</code></pre>

<p>But I get the next error: </p>

<blockquote>
  <p>#1093 - You can't specify target table 'story_category' for update in FROM clause </p>
</blockquote>

<p>How can I overcome this?</p>
","<p>The problem is that MySQL, for whatever inane reason, doesn't allow you to write queries like this:</p>

<pre><code>UPDATE myTable
SET myTable.A =
(
    SELECT B
    FROM myTable
    INNER JOIN ...
)
</code></pre>

<p>That is, if you're doing an <code>UPDATE</code>/<code>INSERT</code>/<code>DELETE</code> on a table, you can't reference that table in an inner query <em>(you <strong>can</strong> however reference a field from that outer table...)</em></p>

<hr>

<p>The solution is to replace the instance of <code>myTable</code> in the sub-query with <code>(SELECT * FROM myTable)</code>, like this</p>

<pre><code>UPDATE myTable
SET myTable.A =
(
    SELECT B
    FROM (SELECT * FROM myTable) AS something
    INNER JOIN ...
)
</code></pre>

<p>This apparently causes the necessary fields to be implicitly copied into a temporary table, so it's allowed.</p>

<p>I found this solution <a href=""http://www.xaprb.com/blog/2006/06/23/how-to-select-from-an-update-target-in-mysql/"" rel=""noreferrer"">here</a>.  A note from that article:</p>

<blockquote>
  <p>You dont want to just <code>SELECT * FROM table</code> in the subquery in real life; I just wanted to keep the examples simple. In reality, you should only be selecting the columns you need in that innermost query, and adding a good <code>WHERE</code> clause to limit the results, too.</p>
</blockquote>
"
"<p>Is there a way to restrict certain tables from the mysqldump command?</p>

<p>For example, I'd use the following syntax to dump <em>only</em> table1 and table2:</p>

<pre><code>mysqldump -u username -p database table1 table2 &gt; database.sql
</code></pre>

<p>But is there a similar way to dump all the tables <em>except</em> table1 and table2? I haven't found anything in the mysqldump documentation, so is brute-force (specifying all the table names) the only way to go?</p>
","<p>You can use the <a href=""http://dev.mysql.com/doc/refman/5.1/en/mysqldump.html#option_mysqldump_ignore-table"" rel=""noreferrer"">--ignore-table</a> option.  So you could do</p>

<pre><code>mysqldump -u USERNAME -pPASSWORD DATABASE --ignore-table=DATABASE.table1 &gt; database.sql
</code></pre>

<p>There is no whitespace after <code>-p</code> (this is not a typo).</p>

<p>If you want to ignore multiple tables you can use a simple script like this</p>

<pre><code>#!/bin/bash
PASSWORD=XXXXXX
HOST=XXXXXX
USER=XXXXXX
DATABASE=databasename
DB_FILE=dump.sql
EXCLUDED_TABLES=(
table1
table2
table3
table4
tableN   
)

IGNORED_TABLES_STRING=''
for TABLE in ""${EXCLUDED_TABLES[@]}""
do :
   IGNORED_TABLES_STRING+="" --ignore-table=${DATABASE}.${TABLE}""
done

echo ""Dump structure""
mysqldump --host=${HOST} --user=${USER} --password=${PASSWORD} --single-transaction --no-data --routines ${DATABASE} &gt; ${DB_FILE}

echo ""Dump content""
mysqldump --host=${HOST} --user=${USER} --password=${PASSWORD} ${DATABASE} --no-create-info --skip-triggers ${IGNORED_TABLES_STRING} &gt;&gt; ${DB_FILE}
</code></pre>
"
"<p>I have a table with ~500k rows; varchar(255) UTF8 column <code>filename</code> contains a file name;</p>

<p>I'm trying to strip out various strange characters out of the filename - thought I'd use a character class: <code>[^a-zA-Z0-9()_ .\-]</code></p>

<p>Now, <strong>is there a function in MySQL that lets you replace through a regular expression</strong>? I'm looking for a similar functionality to REPLACE() function - simplified example follows:</p>

<pre><code>SELECT REPLACE('stackowerflow', 'ower', 'over');

Output: ""stackoverflow""

/* does something like this exist? */
SELECT X_REG_REPLACE('Stackoverflow','/[A-Zf]/','-'); 

Output: ""-tackover-low""
</code></pre>

<p>I know about <a href=""https://stackoverflow.com/a/6943142/19746"">REGEXP/RLIKE</a>, but those only check <em>if</em> there is a match, not <em>what</em> the match is.</p>

<p>(I <em>could</em> do a ""<code>SELECT pkey_id,filename FROM foo WHERE filename RLIKE '[^a-zA-Z0-9()_ .\-]'</code>"" from a PHP script, do a <code>preg_replace</code> and then ""<code>UPDATE foo ... WHERE pkey_id=...</code>"", but that looks like a last-resort slow &amp; ugly hack)</p>
","<p>No.</p>

<p>But if you have access to your server, you could use a user defined function (UDF)  like <a href=""https://github.com/hholzgra/mysql-udf-regexp"" rel=""noreferrer"">mysql-udf-regexp</a>.</p>

<p><strong>EDIT:</strong> MySQL 8.0+ you could use natively REGEXP_REPLACE. More in answer above</p>
"
"<p>I am having some problems when trying to install <code>mysql2</code> gem for Rails. When I try to install it by running <code>bundle install</code> or <code>gem install mysql2</code> it gives me the following error:</p>

<blockquote>
  <p>Error installing mysql2: ERROR: Failed to build gem native extension.</p>
</blockquote>

<p>How can I fix this and successfully install <code>mysql2</code>?</p>
","<p>On Ubuntu/Debian and other distributions using aptitude:</p>

<pre><code>sudo apt-get install libmysql-ruby libmysqlclient-dev
</code></pre>

<p>Package <code>libmysql-ruby</code> has been phased out and replaced by <code>ruby-mysql</code>. <a href=""https://askubuntu.com/a/641953"">This</a> is where I found the solution.</p>

<p>If the above command doesn't work because <code>libmysql-ruby</code> cannot be found, the following should be sufficient:</p>

<pre><code>sudo apt-get install libmysqlclient-dev
</code></pre>

<p>On Red Hat/CentOS and other distributions using yum:</p>

<pre><code>sudo yum install mysql-devel
</code></pre>

<p>On Mac OS X with <a href=""http://mxcl.github.com/homebrew/"" rel=""noreferrer"">Homebrew</a>:</p>

<pre><code>brew install mysql
</code></pre>
"
"<p>When I executed the following command:</p>

<pre><code>ALTER TABLE `mytable` ADD UNIQUE (
`column1` ,
`column2`
);
</code></pre>

<p>I got this error message:</p>

<pre><code>#1071 - Specified key was too long; max key length is 767 bytes
</code></pre>

<p>Information about column1 and column2:</p>

<pre><code>column1 varchar(20) utf8_general_ci
column2  varchar(500) utf8_general_ci
</code></pre>

<p>I think <code>varchar(20)</code> only requires 21 bytes while <code>varchar(500)</code> only requires 501 bytes. So the total bytes are 522, less than 767. So why did I get the error message?</p>

<pre><code>#1071 - Specified key was too long; max key length is 767 bytes
</code></pre>
","<p>767 bytes is the <a href=""http://dev.mysql.com/doc/refman/5.1/en/create-index.html"" rel=""noreferrer"">stated prefix limitation</a> for InnoDB tables in MySQL version 5.6 (and prior versions). It's 1,000 bytes long for MyISAM tables. In MySQL version 5.7 and upwards this limit has been increased to 3072 bytes.</p>

<p>You also have to be aware that if you set an index on a big char or varchar field which is utf8mb4 encoded, you have to divide the max index prefix length of 767 bytes (or 3072 bytes) by 4 resulting in 191. This is because the maximum length of a utf8mb4 character is four bytes. For a utf8 character it would be three bytes resulting in max index prefix length of 254.</p>

<p>One option you have is to just place lower limit on your VARCHAR fields.</p>

<p>Another option (according to the <a href=""http://bugs.mysql.com/bug.php?id=6604"" rel=""noreferrer"">response to this issue</a>) is to get the subset of the column rather than the entire amount, i.e.:</p>

<pre><code>ALTER TABLE `mytable` ADD UNIQUE ( column1(15), column2(200) );
</code></pre>

<p>Tweak as you need to get the key to apply, but I wonder if it would be worth it to review your data model regarding this entity to see if there's improvements that would allow you to implement the intended business rules without hitting the MySQL limitation.</p>
"
"<p>How do I rename a column in table <code>xyz</code>? The columns are:</p>

<pre><code>Manufacurerid, name, status, AI, PK, int
</code></pre>

<p>I want to rename to <code>manufacturerid</code></p>

<p>I tried using PHPMyAdmin panel, but I get this error:</p>

<pre><code>MySQL said: Documentation
#1025 - Error on rename of '.\shopping\#sql-c98_26' to '.\shopping\tblmanufacturer' (errno: 150)
</code></pre>
","<p>Lone Ranger is very close... in fact, you also need to specify the datatype of the renamed column. For example:</p>

<pre><code>ALTER TABLE `xyz` CHANGE `manufacurerid` `manufacturerid` INT;
</code></pre>

<p>Remember  :</p>

<ul>
<li>Replace INT with whatever your column data type is (REQUIRED)</li>
<li>Tilde/ Backtick (`) is optional</li>
</ul>
"
"<p>I just installed SQL Server Express 2012 on my home server.  I'm trying to connect to it from Visual Studio 2012 from my desktop PC, and repeatedly getting the well-known error:</p>

<blockquote>
  <p>A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: Named Pipes Provider, error: 40 - Could not open a connection to SQL Server)</p>
</blockquote>

<p>What I've done to try to fix this:</p>

<ul>
<li>Run SQL Server Configuration Manager on the server and enable SQL Server Browser</li>
<li>Add a Windows Firewall exception on the server for TCP, ports 1433 and 1434 on the local subnet.</li>
<li>Verify that I have a login on the SQL Server instance for the user I'm logged in as on the desktop.</li>
<li>Verify that I'm using Windows Authentication on the SQL Server instance.</li>
<li>Repeatedly restart SQL Server and the whole dang server.</li>
<li>Pull all my hair out.</li>
</ul>

<p><strong>How can I get SQL Server 2012 Express to allow remote connections!?</strong></p>
","<p>Well, <a href=""https://stackoverflow.blog/2012/05/22/encyclopedia-stack-exchange/"">glad I asked</a>.  The solution I finally discovered was here:</p>

<p><a href=""http://support.webecs.com/KB/a868/how-do-i-configure-sql-server-express-to-allow-remote.aspx"" rel=""noreferrer"">How do I configure SQL Server Express to allow remote tcp/ip connections on port 1433?</a></p>

<ol>
<li>Run SQL Server Configuration Manager.</li>
<li>Go to SQL Server Network Configuration > Protocols for SQLEXPRESS.</li>
<li>Make sure TCP/IP is enabled.</li>
</ol>

<p>So far, so good, and entirely expected.  But then:</p>

<ol>
<li>Right-click on TCP/IP and select <strong>Properties</strong>.</li>
<li>Verify that, under IP2, the IP Address is set to the computer's IP address on the local subnet.</li>
<li>Scroll down to IPAll.</li>
<li>Make sure that <strong>TCP Dynamic Ports</strong> is <strong>blank</strong>.  (Mine was set to some 5-digit port number.)</li>
<li>Make sure that <strong>TCP Port</strong> is set to <strong>1433</strong>.  (Mine was blank.)</li>
</ol>

<p>(Also, if you follow these steps, it's <em>not</em> necessary to enable SQL Server Browser, and you only need to allow port 1433, not 1434.)</p>

<p>These extra five steps are something I can't remember ever having had to do in a previous version of SQL Server, Express or otherwise.  They appear to have been necessary because I'm using a named instance (myservername\SQLEXPRESS) on the server instead of a default instance.  See here:</p>

<p><a href=""https://msdn.microsoft.com/en-us/library/ms177440.aspx"" rel=""noreferrer"">Configure a Server to Listen on a Specific TCP Port (SQL Server Configuration Manager)</a></p>
"
"<p>Is the database query faster if I insert multiple rows at once:</p>

<p>like</p>

<pre><code>INSERT....

UNION

INSERT....

UNION
</code></pre>

<p>(I need to insert like 2-3000 rows)</p>
","<blockquote>
  <p><code>INSERT</code> statements that use <code>VALUES</code> syntax can insert multiple rows. To do this, include multiple lists of column values, each enclosed within parentheses and separated by commas.</p>
</blockquote>

<p><strong>Example:</strong></p>

<pre><code>INSERT INTO tbl_name
    (a,b,c)
VALUES
    (1,2,3),
    (4,5,6),
    (7,8,9);
</code></pre>

<p><a href=""http://dev.mysql.com/doc/refman/5.5/en/insert.html"" rel=""noreferrer"">Source</a></p>
"
"<p>How do I limit a SQL Server Profiler trace to a specific database? I can't see how to filter the trace to not see events for all databases on the instance I connect to.</p>
","<p>Under Trace properties > Events Selection tab > select show all columns. Now under column filters, you should see the database name. Enter the database name for the Like section and you should see traces only for that database.</p>
"
"<p>I installed <a href=""http://en.wikipedia.org/wiki/LAMP_%28software_bundle%29"">LAMP</a> on <a href=""http://en.wikipedia.org/wiki/List_of_Ubuntu_releases#Ubuntu_12.04_LTS_.28Precise_Pangolin.29"">Ubuntu&nbsp;12.04 LTS</a> (Precise Pangolin) and then set root password on <a href=""http://en.wikipedia.org/wiki/PhpMyAdmin"">phpMyAdmin</a>. I forgot the password and now I am unable to login. When I try to change password through terminal I get:</p>

<blockquote>
  <p>ERROR 2002 (HY000): Can't connect to local MySQL server through socket
   '/var/run/mysqld/mysqld.sock' (2)</p>
</blockquote>

<p>How can I fix this? I am unable to open LAMP, uninstall it or reinstall it.</p>
","<p>I once had this problem and solved it by installing <code>mysql-server</code>, so make sure that you have installed the <code>mysql-server</code>, not the <code>mysql-client</code> or something else.</p>

<p>That error means the file <code>/var/run/mysqld/mysqld.sock</code> doesn't exists, if you didn't install <code>mysql-server</code>, then the file would not exist. But if the <code>mysql-server</code> is already installed and is running, then you need to check the config files.</p>

<p>The config files are:</p>

<pre><code>/etc/my.cnf
/etc/mysql/my.cnf
/var/lib/mysql/my.cnf
</code></pre>

<p>In <code>/etc/my.cnf</code>, the socket file config may be <code>/tmp/mysql.sock</code> and in <code>/etc/mysql/my.cnf</code> the socket file config may be <code>/var/run/mysqld/mysqld.sock</code>. So, remove or rename <code>/etc/mysql/my.cnf</code>, let mysql use <code>/etc/my.cnf</code>, then the problem may solved.</p>
"
"<p>In our place we're split between using mysqli and PDO for stuff like prepared statements and transaction support. Some projects use one, some the other. There is little realistic likelihood of us ever moving to another RDBMS.</p>

<p>I prefer PDO for the single reason that it allows named parameters for prepared statements, and as far as I am aware mysqli does not.</p>

<p>Are there any other pros and cons to choosing one over the other as a standard as we consolidate our projects to use just one approach?</p>
","<p>Well, you could argue with the object oriented aspect, the prepared statements, the fact that it becomes a standard, etc. But I know that most of the time, convincing somebody works better with a killer feature. So there it is:</p>

<p>A really nice thing with PDO is you can fetch the data, injecting it automatically in an object. If you don't want to use an <a href=""http://en.wikipedia.org/wiki/Object-relational_mapping"" rel=""noreferrer"">ORM</a> (cause it's a just a quick script) but you do like object mapping, it's REALLY cool :</p>

<pre><code>class Student {

    public $id;
    public $first_name;
    public $last_name

    public function getFullName() {
        return $this-&gt;first_name.' '.$this-&gt;last_name
    }
}

try 
{
    $dbh = new PDO(""mysql:host=$hostname;dbname=school"", $username, $password)

    $stmt = $dbh-&gt;query(""SELECT * FROM students"");

    /* MAGIC HAPPENS HERE */

    $stmt-&gt;setFetchMode(PDO::FETCH_INTO, new Student);


    foreach($stmt as $student)
    {
        echo $student-&gt;getFullName().'&lt;br /&gt;';
    } 

    $dbh = null;
}
catch(PDOException $e)
{
    echo $e-&gt;getMessage();
}
</code></pre>
"
"<p>What the difference is between <code>flush()</code> and <code>commit()</code> in SQLAlchemy?</p>

<p>I've read the docs, but am none the wiser - they seem to assume a pre-understanding that I don't have.</p>

<p>I'm particularly interested in their impact on memory usage. I'm loading some data into a database from a series of files (around 5 million rows in total) and my session is occasionally falling over - it's a large database and a machine with not much memory. </p>

<p>I'm wondering if I'm using too many <code>commit()</code> and not enough <code>flush()</code> calls - but without really understanding what the difference is, it's hard to tell!</p>
","<p>Just as an FYI, you can also specify those things as column attributes. For instance, I might have done:</p>

<pre><code>.order_by(model.Entry.amount.desc())
</code></pre>

<p>This is handy since you can use it on other places such as in a relation definition, etc.</p>

<p>For more information, you can <a href=""http://docs.sqlalchemy.org/en/latest/orm/tutorial.html"" rel=""noreferrer"">refer this</a> </p>
"
"<p>I want to use Oracle SQL Developer to generate an ER diagram for my DB tables but I am new to Oracle and this tool.</p>

<p>What is the process for creating an ER diagram in SQL Developer?</p>
","<p>Create a diagram for existing database schema or its subset as follows:</p>

<ol>
<li>Click <strong>File  Data Modeler  Import  Data Dictionary</strong>.</li>
<li>Select a DB connection (add one if none).</li>
<li>Click <strong>Next</strong>.</li>
<li>Check one or more schema names.</li>
<li>Click <strong>Next</strong>.</li>
<li>Check one or more objects to import.</li>
<li>Click <strong>Next</strong>.</li>
<li>Click <strong>Finish</strong>.</li>
</ol>

<p>The ERD is displayed.</p>

<p>Export the diagram as follows:</p>

<ol>
<li>Click <strong>File  Data Modeler  Print Diagram  To Image File</strong>.</li>
<li>Browse to and select the export file location.</li>
<li>Click <strong>Save</strong>.</li>
</ol>

<p>The diagram is exported. To export in a vector format, use <strong>To PDF File</strong>, instead. This allows for simplified editing using <a href=""https://inkscape.org/"" rel=""noreferrer"">Inkscape</a> (or other vector image editor).</p>

<p>These instructions may work for SQL Developer 3.2.09.23 to 4.1.3.20.</p>
"
"<p>Is there a nice easy way to drop all tables from a MySQL database, ignoring any foreign key constraints that may be in there?</p>
","<p>The best and most efficient way is to catch the ""table not found"" exception: this avoids the overhead of checking if the table exists twice; and doesn't suffer from the problem that if the DROP fails for some other reason (that might be important) the exception is still raised to the caller:</p>

<pre><code>BEGIN
   EXECUTE IMMEDIATE 'DROP TABLE ' || table_name;
EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
</code></pre>

<p><strong>ADDENDUM</strong>
For reference, here are the equivalent blocks for other object types:</p>

<p>Sequence</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP SEQUENCE ' || sequence_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -2289 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>View</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP VIEW ' || view_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -942 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Trigger</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP TRIGGER ' || trigger_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -4080 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Index</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP INDEX ' || index_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -1418 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Column</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'ALTER TABLE ' || table_name
                || ' DROP COLUMN ' || column_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -904 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Database Link</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP DATABASE LINK ' || dblink_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -2024 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Materialized View</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP MATERIALIZED VIEW ' || mview_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -12003 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Type</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP TYPE ' || type_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -4043 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Constraint</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'ALTER TABLE ' || table_name
            || ' DROP CONSTRAINT ' || constraint_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -2443 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Scheduler Job</p>

<pre><code>BEGIN
  DBMS_SCHEDULER.drop_job(job_name);
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -27475 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>User / Schema</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP USER ' || user_name;
  /* you may or may not want to add CASCADE */
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -1918 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Package</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP PACKAGE ' || package_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -4043 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Procedure</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP PROCEDURE ' || procedure_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -4043 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Function</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP FUNCTION ' || function_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -4043 THEN
      RAISE;
    END IF;
END;
</code></pre>

<p>Tablespace</p>

<pre><code>BEGIN
  EXECUTE IMMEDIATE 'DROP TABLESPACE' || tablespace_name;
EXCEPTION
  WHEN OTHERS THEN
    IF SQLCODE != -959 THEN
      RAISE;
    END IF;
END;
</code></pre>
"
"<p>I have upgraded my system and have installed MySql 5.7.9 with php for a web application I am working on. I have a query that is dynamically created, and when run in older versions of MySql it works fine.  Since upgrading to 5.7 I get this error:</p>

<blockquote>
  <p>Expression #1 of SELECT list is not in GROUP BY clause and contains
  nonaggregated column 'support_desk.mod_users_groups.group_id' which is
  not functionally dependent on columns in GROUP BY clause; this is
  incompatible with sql_mode=only_full_group_by</p>
</blockquote>

<p>Note the Manual page for Mysql 5.7 on the topic of <a href=""http://dev.mysql.com/doc/refman/5.7/en/sql-mode.html"">Server SQL Modes</a>.</p>

<p>This is the query that is giving me trouble:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT mod_users_groups.group_id AS 'value', 
       group_name AS 'text' 
FROM mod_users_groups
LEFT JOIN mod_users_data ON mod_users_groups.group_id = mod_users_data.group_id 
WHERE  mod_users_groups.active = 1 
  AND mod_users_groups.department_id = 1 
  AND mod_users_groups.manage_work_orders = 1 
  AND group_name != 'root' 
  AND group_name != 'superuser' 
GROUP BY group_name 
HAVING COUNT(`user_id`) &gt; 0 
ORDER BY group_name
</code></pre>

<p>I did some googling on the issue, but I don't understand <code>only_full_group_by</code> enough to figure out what I need to do to fix the query. Can I just turn off the <code>only_full_group_by</code> option, or is there something else I need to do?</p>

<p>Let me know if you need more information.</p>
","<p>You can try to disable the <code>only_full_group_by</code> setting by executing the following:</p>

<pre><code>mysql&gt; set global sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';
mysql&gt; set session sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION';
</code></pre>

<p>MySQL 8 does not accept <code>NO_AUTO_CREATE_USER</code> so that needs to be removed.</p>
"
"<p>How do I get:</p>

<pre><code>id       Name       Value
1          A          4
1          B          8
2          C          9
</code></pre>

<p>to</p>

<pre><code>id          Column
1          A:4, B:8
2          C:9
</code></pre>
","<p><strong>No CURSOR, WHILE loop, or User-Defined Function needed</strong>.  </p>

<p>Just need to be creative with FOR XML and PATH.</p>

<p>[Note: This solution only works on SQL 2005 and later. Original question didn't specify the version in use.]</p>

<pre><code>CREATE TABLE #YourTable ([ID] INT, [Name] CHAR(1), [Value] INT)

INSERT INTO #YourTable ([ID],[Name],[Value]) VALUES (1,'A',4)
INSERT INTO #YourTable ([ID],[Name],[Value]) VALUES (1,'B',8)
INSERT INTO #YourTable ([ID],[Name],[Value]) VALUES (2,'C',9)

SELECT 
  [ID],
  STUFF((
    SELECT ', ' + [Name] + ':' + CAST([Value] AS VARCHAR(MAX)) 
    FROM #YourTable 
    WHERE (ID = Results.ID) 
    FOR XML PATH(''),TYPE).value('(./text())[1]','VARCHAR(MAX)')
  ,1,2,'') AS NameValues
FROM #YourTable Results
GROUP BY ID

DROP TABLE #YourTable
</code></pre>
"
"<p>I am running this query on MySQL</p>

<pre><code>SELECT ID FROM (
    SELECT ID, msisdn
    FROM (
        SELECT * FROM TT2
    )
);
</code></pre>

<p>and it is giving this error:</p>

<blockquote>
  <p>Every derived table must have its own alias. </p>
</blockquote>

<p>What's causing this error?</p>
","<p>Every derived table (AKA sub-query) must indeed have an alias. I.e. each query in brackets must be given an alias (<code>AS whatever</code>), which can the be used to refer to it in the rest of the outer query.</p>

<pre><code>SELECT ID FROM (
    SELECT ID, msisdn FROM (
        SELECT * FROM TT2
    ) AS T
) AS T
</code></pre>

<p>In your case, of course, the entire query could be replaced with:</p>

<pre><code>SELECT ID FROM TT2
</code></pre>
"
"<p>What is the difference between SQL, PL-SQL and T-SQL?</p>

<p>Can anyone explain what the differences between these three are, and provide scenarios where each would be relevantly used?</p>
","<ul>
<li><p><code>SQL</code> is a query language to operate on sets.</p>

<p>It is more or less standardized, and used by almost all relational database management systems: SQL Server, Oracle, MySQL, PostgreSQL, DB2, Informix, etc.</p></li>
<li><p><code>PL/SQL</code> is a proprietary procedural language used by Oracle</p></li>
<li><p><code>PL/pgSQL</code> is a procedural language used by PostgreSQL</p></li>
<li><p><code>TSQL</code> is a proprietary procedural language used by Microsoft in SQL Server.</p></li>
</ul>

<p>Procedural languages are designed to extend SQL's abilities while being able to integrate well with SQL. Several features such as local variables and string/data processing are added. These features make the language Turing-complete.</p>

<p>They are also used to write stored procedures: pieces of code residing on the server to manage complex business rules that are hard or impossible to manage with pure set-based operations.</p>
"
"<p>In Microsoft SQL Server how can I get a query execution plan for a query / stored procedure?</p>
","<p>There are a number of methods of obtaining an execution plan, which one to use will depend on your circumstances.  Usually you can use SQL Server Management Studio to get a plan, however if for some reason you can't run your query in SQL Server Management Studio then you might find it helpful to be able to obtain a plan via SQL Server Profiler or by inspecting the plan cache.</p>

<h2>Method 1 - Using SQL Server Management Studio</h2>

<p>SQL Server comes with a couple of neat features that make it very easy to capture an execution plan, simply make sure that the ""Include Actual Execution Plan"" menu item (found under the ""Query"" menu) is ticked and run your query as normal.</p>

<p><img src=""https://i.stack.imgur.com/0iCgU.png"" alt=""Include Action Execution Plan menu item""></p>

<p>If you are trying to obtain the execution plan for statements in a stored procedure then you should execute the stored procedure, like so:</p>

<pre><code>exec p_Example 42
</code></pre>

<p>When your query completes you should see an extra tab entitled ""Execution plan"" appear in the results pane.  If you ran many statements then you may see many plans displayed in this tab.</p>

<p><img src=""https://i.stack.imgur.com/iHKOE.png"" alt=""Screenshot of an Execution Plan""></p>

<p>From here you can inspect the execution plan in SQL Server Management Studio, or right click on the plan and select ""Save Execution Plan As ..."" to save the plan to a file in XML format.</p>

<h2>Method 2 - Using SHOWPLAN options</h2>

<p>This method is very similar to method 1 (in fact this is what SQL Server Management Studio does internally), however I have included it for completeness or if you don't have SQL Server Management Studio available.</p>

<p>Before you run your query, run <strong>one</strong> of the following statements.  The statement must be the only statement in the batch, i.e. you cannot execute another statement at the same time:</p>

<pre><code>SET SHOWPLAN_TEXT ON
SET SHOWPLAN_ALL ON
SET SHOWPLAN_XML ON
SET STATISTICS PROFILE ON
SET STATISTICS XML ON -- The is the recommended option to use
</code></pre>

<p>These are connection options and so you only need to run this once per connection. From this point on all statements run will be acompanied by an <strong>additional resultset</strong> containing your execution plan in the desired format - simply run your query as you normally would to see the plan.</p>

<p>Once you are done you can turn this option off with the following statement:</p>

<pre><code>SET &lt;&lt;option&gt;&gt; OFF
</code></pre>

<h3>Comparison of execution plan formats</h3>

<p>Unless you have a strong preference my recommendation is to use the <code>STATISTICS XML</code> option. This option is equivalent to the ""Include Actual Execution Plan"" option in SQL Server Management Studio and supplies the most information in the most convenient format.</p>

<ul>
<li><code>SHOWPLAN_TEXT</code> - Displays a basic text based estimated execution plan, without executing the query</li>
<li><code>SHOWPLAN_ALL</code> - Displays a text based estimated execution plan with cost estimations, without executing the query</li>
<li><code>SHOWPLAN_XML</code> - Displays an XML based estimated execution plan with cost estimations, without executing the query. This is equivalent to the ""Display Estimated Execution Plan..."" option in SQL Server Management Studio.</li>
<li><code>STATISTICS PROFILE</code> - Executes the query and displays a text based actual execution plan.</li>
<li><code>STATISTICS XML</code> - Executes the query and displays an XML based actual execution plan. This is equivalent to the ""Include Actual Execution Plan"" option in SQL Server Management Studio.</li>
</ul>

<h2>Method 3 - Using SQL Server Profiler</h2>

<p>If you can't run your query directly (or your query doesn't run slowly when you execute it directly - remember we want a plan of the query performing badly), then you can capture a plan using a SQL Server Profiler trace.  The idea is to run your query while a trace that is capturing one of the ""Showplan"" events is running.</p>

<p>Note that depending on load you <strong>can</strong> use this method on a production environment, however you should obviously use caution.  The SQL Server profiling mechanisms are designed to minimize impact on the database but this doesn't mean that there won't be <em>any</em> performance impact.  You may also have problems filtering and identifying the correct plan in your trace if your database is under heavy use.  You should obviously check with your DBA to see if they are happy with you doing this on their precious database!</p>

<ol>
<li>Open SQL Server Profiler and create a new trace connecting to the desired database against which you wish to record the trace.</li>
<li>Under the ""Events Selection"" tab check ""Show all events"", check the ""Performance"" -> ""Showplan XML"" row and run the trace.</li>
<li>While the trace is running, do whatever it is you need to do to get the slow running query to run.</li>
<li>Wait for the query to complete and stop the trace.</li>
<li>To save the trace right click on the plan xml in SQL Server Profiler and select ""Extract event data..."" to save the plan to file in XML format.</li>
</ol>

<p>The plan you get is equivalent to the ""Include Actual Execution Plan"" option in SQL Server Management Studio.</p>

<h2>Method 4 - Inspecting the query cache</h2>

<p>If you can't run your query directly and you also can't capture a profiler trace then you can still obtain an estimated plan by inspecting the SQL query plan cache.</p>

<p>We inspect the plan cache by querying SQL Server <a href=""http://msdn.microsoft.com/en-us/library/ms188754.aspx"">DMVs</a>.  The following is a basic query which will list all cached query plans (as xml) along with their SQL text.  On most database you will also need to add additional filtering clauses to filter the results down to just the plans you are interested in.</p>

<pre><code>SELECT UseCounts, Cacheobjtype, Objtype, TEXT, query_plan
FROM sys.dm_exec_cached_plans 
CROSS APPLY sys.dm_exec_sql_text(plan_handle)
CROSS APPLY sys.dm_exec_query_plan(plan_handle)
</code></pre>

<p>Execute this query and click on the  plan XML to open up the plan in a new window - right click and select ""Save execution plan as..."" to save the plan to file in XML format.</p>

<h1>Notes:</h1>

<p>Because there are so many factors involved (ranging from the table and index schema down to the data stored and the table statistics) you should <strong>always</strong> try to obtain an execution plan from the database you are interested in (normally the one that is experiencing a performance problem).</p>

<p>You can't capture an execution plan for encrypted stored procedures.</p>

<h2>""actual"" vs ""estimated"" execution plans</h2>

<p>An <em>actual</em> execution plan is one where SQL Server actually runs the query, whereas an <em>estimated</em> execution plan SQL Server works out what it <em>would</em> do without executing the query.  Although logically equivalent, an actual execution plan is much more useful as it contains additional details and statistics about what actually happened when executing the query.  This is essential when diagnosing problems where SQL Servers estimations are off (such as when statistics are out of date).</p>

<ul>
<li><a href=""http://sqlserverpedia.com/blog/sql-server-bloggers/estimated-and-actual-execution-plan-revisited/"">Estimated and Actual execution plan revisited</a></li>
</ul>

<h2>How do I interpret a query execution plan?</h2>

<p>This is a topic worthy enough for a (free) <a href=""https://www.simple-talk.com/books/sql-books/sql-server-execution-plans-second-edition-by-grant-fritchey/"">book</a> in its own right.</p>

<h2>See also:</h2>

<ul>
<li><a href=""http://www.simple-talk.com/sql/performance/execution-plan-basics/"">Execution Plan Basics</a></li>
<li><a href=""http://msdn.microsoft.com/en-us/library/ms178086.aspx"">SHOWPLAN Permission and Transact-SQL Batches</a></li>
<li><a href=""http://www.dbsnaps.com/sql-server/sql-server-query-hashes/"">SQL Server 2008  Using Query Hashes and Query Plan Hashes</a></li>
<li><a href=""http://www.mssqltips.com/sqlservertip/1661/analyzing-the-sql-server-plan-cache/"">Analyzing the SQL Server Plan Cache</a></li>
</ul>
"
"<p>I want to keep a backup of all my MySQL databases. I have more than 100 MySQL databases. I want to export all of them at the same time and again import all of them into my MySQL server at one time. How can I do that?</p>
","<h2>Export:</h2>

<pre class=""lang-none prettyprint-override""><code>mysqldump -u root -p --all-databases &gt; alldb.sql
</code></pre>

<p>Look up the <a href=""http://dev.mysql.com/doc/refman/5.5/en/mysqldump.html"" rel=""noreferrer"">documentation for mysqldump</a>. You may want to use some of the options mentioned in comments:</p>

<pre class=""lang-none prettyprint-override""><code>mysqldump -u root -p --opt --all-databases &gt; alldb.sql
mysqldump -u root -p --all-databases --skip-lock-tables &gt; alldb.sql
</code></pre>

<h2>Import:</h2>

<pre class=""lang-none prettyprint-override""><code>mysql -u root -p &lt; alldb.sql
</code></pre>
"
"<p>I'm trying to setup up MySQL on mac os 10.6 using Homebrew by <code>brew install mysql 5.1.52</code>.</p>

<p>Everything goes well and I am also successful with the <code>mysql_install_db</code>.<br>
However when I try to connect to the server using:</p>

<pre><code>/usr/local/Cellar/mysql/5.1.52/bin/mysqladmin -u root password 'mypass'
</code></pre>

<p>I get: </p>

<pre>
/usr/local/Cellar/mysql/5.1.52/bin/mysqladmin: connect to server at 'localhost' 
failed error: 'Access denied for user 'root'@'localhost' (using password: NO)'</pre>

<p>I've tried to access <code>mysqladmin or mysql using -u root -proot</code> as well,<br>
but it doesn't work with or without password. </p>

<p>This is a brand new installation on a brand new machine and as far as I know the new installation must be accessible without a root password. I also tried:</p>

<pre><code>/usr/local/Cellar/mysql/5.1.52/bin/mysql_secure_installation
</code></pre>

<p>but I also get </p>

<pre>
ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO)</pre>
","<p>I think one can end up in this position with older versions of mysql already installed. I had the same problem and none of the above solutions worked for me. I fixed it thus:</p>

<p>Used brew's <code>remove</code> &amp; <code>cleanup</code> commands, unloaded the <code>launchctl</code> script, then deleted the mysql directory in <code>/usr/local/var</code>, deleted my existing <code>/etc/my.cnf</code> (leave that one up to you, should it apply) and launchctl plist</p>

<p>Updated the string for the plist. Note also your alternate security script directory will be based on which version of MySQL you are installing.</p>

<p>Step-by-step:</p>

<pre><code>brew remove mysql

brew cleanup

launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist

rm ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist

sudo rm -rf /usr/local/var/mysql
</code></pre>

<p>I then started from scratch:</p>

<ol>
<li>installed mysql with <code>brew install mysql</code></li>
<li><p>ran the commands brew suggested: (see note: below)</p>

<pre><code>unset TMPDIR

mysql_install_db --verbose --user=`whoami` --basedir=""$(brew --prefix mysql)"" --datadir=/usr/local/var/mysql --tmpdir=/tmp
</code></pre></li>
<li><p>Start mysql with <code>mysql.server start</code> command, to be able to log on it</p></li>
<li><p>Used the alternate security script:</p>

<pre><code>/usr/local/Cellar/mysql/5.5.10/bin/mysql_secure_installation
</code></pre></li>
<li><p>Followed the <code>launchctl</code> section from the brew package script output such as,</p>

<pre><code>#start
launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist

#stop
launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist
</code></pre></li>
<li><p>Boom.</p></li>
</ol>

<p>Hope that helps someone!</p>

<p><strong>Note:</strong> the <code>--force</code> bit on <code>brew cleanup</code> will also cleanup outdated kegs, think it's a new-ish homebrew feature.</p>

<p><strong>Note the second:</strong> a commenter says step 2 is not required. I don't want to test it, so YMMV! </p>
"
"<p>I want to create a varchar column in SQL that should contain <code>N'guid'</code> while <code>guid</code> is a generated GUID by .NET (<a href=""https://docs.microsoft.com/en-us/dotnet/api/system.guid.newguid"" rel=""nofollow noreferrer"">Guid.NewGuid</a>) - class System.Guid.</p>

<p>What is the length of the <code>varchar</code> I should expect from a GUID?
Is it a static length?</p>

<p>Should I use <code>nvarchar</code> (will GUID ever use Unicode characters)?</p>

<pre><code>varchar(Guid.Length)
</code></pre>

<p>PS. I don't want to use a SQL row guid data-type. I am just asking what is <code>Guid.MaxLength</code>.</p>
","<p>It depends on how you format the Guid:</p>

<ul>
<li><p><code>Guid.NewGuid().ToString()</code> => <strong>36</strong> characters  (Hyphenated)<br>
outputs: <code>12345678-1234-1234-1234-123456789abc</code></p></li>
<li><p><code>Guid.NewGuid().ToString(""D"")</code> => <strong>36</strong> characters (Hyphenated, same as <code>ToString()</code>)<br>
outputs: <code>12345678-1234-1234-1234-123456789abc</code></p></li>
<li><p><code>Guid.NewGuid().ToString(""N"")</code> => <strong>32</strong> characters (Digits only)<br>
outputs: <code>12345678123412341234123456789abc</code></p></li>
<li><p><code>Guid.NewGuid().ToString(""B"")</code> => <strong>38</strong> characters (Braces)<br>
outputs: <code>{12345678-1234-1234-1234-123456789abc}</code></p></li>
<li><p><code>Guid.NewGuid().ToString(""P"")</code> => <strong>38</strong> characters (Parentheses)<br>
outputs: <code>(12345678-1234-1234-1234-123456789abc)</code></p></li>
<li><p><code>Guid.NewGuid().ToString(""X"")</code> => <strong>68</strong> characters (Hexadecimal)<br>
outputs: <code>{0x12345678,0x1234,0x1234,{0x12,0x34,0x12,0x34,0x56,0x78,0x9a,0xbc}}</code></p></li>
</ul>
"
"<p>I have a table with a primary key that is a varchar(255). Some cases have arisen where 255 characters isn't enough. I tried changing the field to a text, but I get the following error:</p>

<pre><code>BLOB/TEXT column 'message_id' used in key specification without a key length
</code></pre>

<p>how can I fix this?</p>

<p>edit: I should also point out this table has a composite primary key with multiple columns.</p>
","<p>The error happens because MySQL can index only the first N chars of a BLOB or <code>TEXT</code> column. So The error mainly happens when there is a field/column type of <code>TEXT</code> or BLOB or those belong to <code>TEXT</code> or <code>BLOB</code> types such as <code>TINYBLOB</code>, <code>MEDIUMBLOB</code>, <code>LONGBLOB</code>, <code>TINYTEXT</code>, <code>MEDIUMTEXT</code>, and <code>LONGTEXT</code> that you try to make a primary key or index. With full <code>BLOB</code> or <code>TEXT</code> without the length value, MySQL is unable to guarantee the uniqueness of the column as its of variable and dynamic size. So, when using <code>BLOB</code> or <code>TEXT</code> types as an index, the value of N must be supplied so that MySQL can determine the key length. However, MySQL doesnt support a key length limit on <code>TEXT</code> or <code>BLOB</code>. <code>TEXT(88)</code> simply wont work.</p>

<p>The error will also pop up when you try to convert a table column from <code>non-TEXT</code> and <code>non-BLOB</code> type such as <code>VARCHAR</code> and <code>ENUM</code> into <code>TEXT</code> or <code>BLOB</code> type, with the column already been defined as unique constraints or index. The Alter Table SQL command will fail.</p>

<p>The solution to the problem is to remove the <code>TEXT</code> or <code>BLOB</code> column from the index or unique constraint or set another field as primary key. If you can't do that, and wanting to place a limit on the <code>TEXT</code> or <code>BLOB</code> column, try to use <code>VARCHAR</code> type and place a limit of length on it. By default, <code>VARCHAR</code> is limited to a maximum of 255 characters and its limit must be specified implicitly within a bracket right after its declaration, i.e <code>VARCHAR(200)</code> will limit it to 200 characters long only.</p>

<p>Sometimes, even though you dont use <code>TEXT</code> or <code>BLOB</code> related type in your table, the Error 1170 may also appear. It happens in a situation such as when you specify <code>VARCHAR</code> column as primary key, but wrongly set its length or characters size. <code>VARCHAR</code> can only accepts up to 256 characters, so anything such as <code>VARCHAR(512)</code> will force MySQL to auto-convert the <code>VARCHAR(512)</code> to a <code>SMALLTEXT</code> datatype, which subsequently fails with error 1170 on key length if the column is used as primary key or unique or non-unique index. To solve this problem, specify a figure less than 256 as the size for <code>VARCHAR</code> field.</p>

<p>Reference: <a href=""http://www.mydigitallife.info/2007/07/09/mysql-error-1170-42000-blobtext-column-used-in-key-specification-without-a-key-length/"" rel=""noreferrer"">MySQL Error 1170 (42000): BLOB/TEXT Column Used in Key Specification Without a Key Length</a></p>
"
"<p>I want to export and import a .sql file to and from a MySQL database from command line.</p>

<p>Is there any command to export .sql file in MySQL? Then how do I import it?</p>

<p>When doing the import, there may be constraints like enable/disable <strong>foreign key check</strong> or <strong>export only table structure</strong>.</p>

<p>Can we set those <strong>options</strong> with <code>mysqldump</code>?</p>
","<p>Looks like you have a query that is taking longer than it should.
From your stack trace and your code you should be able to determine exactly what query that is.</p>

<p>This type of timeout can have three causes; </p>

<ol>
<li>There's a deadlock somewhere</li>
<li>The database's statistics and/or query plan cache are incorrect</li>
<li>The query is too complex and needs to be tuned</li>
</ol>

<p>A deadlock can be difficult to fix, but it's easy to determine whether that is the case. Connect to your database with Sql Server Management Studio. In the left pane right-click on the server node and select <em>Activity Monitor</em>. Take a look at the running processes.
Normally most will be idle or running. When the problem occurs you can identify any blocked process by the process state. If you right-click on the process and select <em>details</em> it'll show you the last query executed by the process.</p>

<p>The second issue will cause the database to use a sub-optimal query plan. It can be resolved by clearing the statistics:</p>

<pre><code>exec sp_updatestats
</code></pre>

<p>If that doesn't work you could also try</p>

<pre><code>dbcc freeproccache
</code></pre>

<p>You should not do this when your server is under heavy load because it will temporarily incur a big performace hit as all stored procs and queries are recompiled when first executed. 
However, since you state the issue occurs <em>sometimes</em>, and the stack trace indicates your application is starting up, I think you're running a query that is only run on occasionally. You may be better off by forcing SQL Server not to reuse a previous query plan. See <a href=""https://stackoverflow.com/a/8590152/64096"">this answer</a> for details on how to do that.</p>

<p>I've already touched on the third issue, but you can easily determine whether the query needs tuning by executing the query manually, for example using Sql Server Management Studio. If the query takes too long to complete, even after resetting the statistics you'll probably need to tune it. For help with that, you should post the exact query in a new question.</p>
"
"<p>I am working on a query in Sql Server 2005 where I need to convert a value in <code>DateTime</code> variable into a <code>varchar</code> variable in <code>yyyy-mm-dd</code> format (without time part). How do I do that?</p>
","<p>Here's some test sql for all the styles.</p>

<pre><code>DECLARE @now datetime
SET @now = GETDATE()
select convert(nvarchar(MAX), @now, 0) as output, 0 as style 
union select convert(nvarchar(MAX), @now, 1), 1
union select convert(nvarchar(MAX), @now, 2), 2
union select convert(nvarchar(MAX), @now, 3), 3
union select convert(nvarchar(MAX), @now, 4), 4
union select convert(nvarchar(MAX), @now, 5), 5
union select convert(nvarchar(MAX), @now, 6), 6
union select convert(nvarchar(MAX), @now, 7), 7
union select convert(nvarchar(MAX), @now, 8), 8
union select convert(nvarchar(MAX), @now, 9), 9
union select convert(nvarchar(MAX), @now, 10), 10
union select convert(nvarchar(MAX), @now, 11), 11
union select convert(nvarchar(MAX), @now, 12), 12
union select convert(nvarchar(MAX), @now, 13), 13
union select convert(nvarchar(MAX), @now, 14), 14
--15 to 19 not valid
union select convert(nvarchar(MAX), @now, 20), 20
union select convert(nvarchar(MAX), @now, 21), 21
union select convert(nvarchar(MAX), @now, 22), 22
union select convert(nvarchar(MAX), @now, 23), 23
union select convert(nvarchar(MAX), @now, 24), 24
union select convert(nvarchar(MAX), @now, 25), 25
--26 to 99 not valid
union select convert(nvarchar(MAX), @now, 100), 100
union select convert(nvarchar(MAX), @now, 101), 101
union select convert(nvarchar(MAX), @now, 102), 102
union select convert(nvarchar(MAX), @now, 103), 103
union select convert(nvarchar(MAX), @now, 104), 104
union select convert(nvarchar(MAX), @now, 105), 105
union select convert(nvarchar(MAX), @now, 106), 106
union select convert(nvarchar(MAX), @now, 107), 107
union select convert(nvarchar(MAX), @now, 108), 108
union select convert(nvarchar(MAX), @now, 109), 109
union select convert(nvarchar(MAX), @now, 110), 110
union select convert(nvarchar(MAX), @now, 111), 111
union select convert(nvarchar(MAX), @now, 112), 112
union select convert(nvarchar(MAX), @now, 113), 113
union select convert(nvarchar(MAX), @now, 114), 114
union select convert(nvarchar(MAX), @now, 120), 120
union select convert(nvarchar(MAX), @now, 121), 121
--122 to 125 not valid
union select convert(nvarchar(MAX), @now, 126), 126
union select convert(nvarchar(MAX), @now, 127), 127
--128, 129 not valid
union select convert(nvarchar(MAX), @now, 130), 130
union select convert(nvarchar(MAX), @now, 131), 131
--132 not valid
order BY style
</code></pre>

<p>Here's the result</p>

<pre><code>output                   style
Apr 28 2014  9:31AM          0
04/28/14                     1
14.04.28                     2
28/04/14                     3
28.04.14                     4
28-04-14                     5
28 Apr 14                    6
Apr 28, 14                   7
09:31:28                     8
Apr 28 2014  9:31:28:580AM   9
04-28-14                     10
14/04/28                     11
140428                       12
28 Apr 2014 09:31:28:580     13
09:31:28:580                 14
2014-04-28 09:31:28          20
2014-04-28 09:31:28.580      21
04/28/14  9:31:28 AM         22
2014-04-28                   23
09:31:28                     24
2014-04-28 09:31:28.580      25
Apr 28 2014  9:31AM          100
04/28/2014                   101
2014.04.28                   102
28/04/2014                   103
28.04.2014                   104
28-04-2014                   105
28 Apr 2014                  106
Apr 28, 2014                 107
09:31:28                     108
Apr 28 2014  9:31:28:580AM   109
04-28-2014                   110
2014/04/28                   111
20140428                     112
28 Apr 2014 09:31:28:580     113
09:31:28:580                 114
2014-04-28 09:31:28          120
2014-04-28 09:31:28.580      121
2014-04-28T09:31:28.580      126
2014-04-28T09:31:28.580      127
28   1435  9:31:28:580AM    130
28/06/1435  9:31:28:580AM    131
</code></pre>

<p>Make <code>nvarchar(max)</code> shorter to trim the time. For example:</p>

<pre><code>select convert(nvarchar(11), GETDATE(), 0)
union select convert(nvarchar(max), GETDATE(), 0)
</code></pre>

<p>outputs:</p>

<pre><code>May 18 2018
May 18 2018  9:57AM
</code></pre>
"
"<p>With the two classes below, I've tried connect to a MySQL database. However, I always get this error:</p>

<blockquote>
Wed Dec 09 22:46:52 CET 2015 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
</blockquote>

<p>This is the test class with the <code>main</code> method:</p>

<pre><code>public class TestDatabase {

    public static void main(String[] args) {
        Database db = new Database();
        try {
            db.connect();
        } catch (Exception e) {
            e.printStackTrace();
        }
        db.close();
    }
}
</code></pre>

<p>This is the <code>Database</code> class:</p>

<pre><code>import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;

public class Database {

    private Connection con;

    public void connect() throws Exception{

        if(con != null) return;

        try {
            Class.forName(""com.mysql.jdbc.Driver"");
        } catch (ClassNotFoundException e) {
            throw new Exception(""No database"");
        }

        String connectionURL = ""jdbc:mysql://localhost:3306/Peoples"";

        con = DriverManager.getConnection(connectionURL, ""root"", ""milos23"");        
    }

    public void close(){
        if(con != null){
            try {
                con.close();
            } catch (SQLException e) {
                e.printStackTrace();
            }
        }
    }
}
</code></pre>
","<p>Your connection URL should look like the below,</p>

<pre><code>jdbc:mysql://localhost:3306/Peoples?autoReconnect=true&amp;useSSL=false
</code></pre>

<p>This will disable SSL and also suppress the SSL errors.</p>
"
"<p>I have created my tables in my <code>SQLiteOpenHelper</code> <code>onCreate()</code> but receive</p>

<pre><code>SQLiteException: no such table
</code></pre>

<p>or</p>

<pre><code>SQLiteException: no such column
</code></pre>

<p>errors. Why?</p>

<blockquote>
  <p><strong>NOTE:</strong></p>
  
  <p>(This is the amalgamated summary of tens of similar questions every week. Attempting to provide a ""canonical"" community wiki question/answer here so that all those questions can be directed to a good reference.)</p>
</blockquote>
","<p><a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html""><code>SQLiteOpenHelper</code></a> <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#onCreate%28android.database.sqlite.SQLiteDatabase%29""><code>onCreate()</code></a> and <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#onUpgrade%28android.database.sqlite.SQLiteDatabase,%20int,%20int%29""><code>onUpgrade()</code></a> callbacks are invoked when the database is actually opened, for example by a call to <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#getWritableDatabase%28%29""><code>getWritableDatabase()</code></a>. The database is not opened when the database helper object itself is created.</p>

<p><code>SQLiteOpenHelper</code> versions the database files. The version number is the <code>int</code> argument passed to the <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#SQLiteOpenHelper%28android.content.Context,%20java.lang.String,%20android.database.sqlite.SQLiteDatabase.CursorFactory,%20int%29"">constructor</a>. In the database file, the version number is stored in <a href=""http://www.sqlite.org/pragma.html#pragma_schema_version""><code>PRAGMA user_version</code></a>.</p>

<p><code>onCreate()</code> is only run when the database file did not exist and was just created. If <code>onCreate()</code> returns successfully (doesn't throw an exception), the database is assumed to be created with the requested version number. As an implication, you should not catch <code>SQLException</code>s in <code>onCreate()</code> yourself.</p>

<p><code>onUpgrade()</code> is only called when the database file exists but the stored version number is lower than requested in constructor. The <code>onUpgrade()</code> should update the table schema to the requested version.</p>

<p>When changing the table schema in code (<code>onCreate()</code>), you should make sure the database is updated. Two main approaches:</p>

<ol>
<li><p>Delete the old database file so that <code>onCreate()</code> is run again. This is often preferred at development time where you have control over the installed versions and data loss is not an issue. Some ways to to delete the database file:</p>

<ul>
<li><p>Uninstall the application. Use the application manager or <code>adb uninstall your.package.name</code> from shell.</p></li>
<li><p>Clear application data. Use the application manager.</p></li>
</ul></li>
<li><p>Increment the database version so that <code>onUpgrade()</code> is invoked. This is slightly more complicated as more code is needed.</p>

<ul>
<li><p>For development time schema upgrades where data loss is not an issue, you can just use <code>execSQL(""DROP TABLE IF EXISTS &lt;tablename&gt;"")</code> in to remove your existing tables and call <code>onCreate()</code> to recreate the database.</p></li>
<li><p>For released versions, you should implement data migration in <code>onUpgrade()</code> so your users don't lose their data.</p></li>
</ul></li>
</ol>
"
"<p>I have created my tables in my <code>SQLiteOpenHelper</code> <code>onCreate()</code> but receive</p>

<pre><code>SQLiteException: no such table
</code></pre>

<p>or</p>

<pre><code>SQLiteException: no such column
</code></pre>

<p>errors. Why?</p>

<blockquote>
  <p><strong>NOTE:</strong></p>
  
  <p>(This is the amalgamated summary of tens of similar questions every week. Attempting to provide a ""canonical"" community wiki question/answer here so that all those questions can be directed to a good reference.)</p>
</blockquote>
","<p><a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html""><code>SQLiteOpenHelper</code></a> <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#onCreate%28android.database.sqlite.SQLiteDatabase%29""><code>onCreate()</code></a> and <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#onUpgrade%28android.database.sqlite.SQLiteDatabase,%20int,%20int%29""><code>onUpgrade()</code></a> callbacks are invoked when the database is actually opened, for example by a call to <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#getWritableDatabase%28%29""><code>getWritableDatabase()</code></a>. The database is not opened when the database helper object itself is created.</p>

<p><code>SQLiteOpenHelper</code> versions the database files. The version number is the <code>int</code> argument passed to the <a href=""http://developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html#SQLiteOpenHelper%28android.content.Context,%20java.lang.String,%20android.database.sqlite.SQLiteDatabase.CursorFactory,%20int%29"">constructor</a>. In the database file, the version number is stored in <a href=""http://www.sqlite.org/pragma.html#pragma_schema_version""><code>PRAGMA user_version</code></a>.</p>

<p><code>onCreate()</code> is only run when the database file did not exist and was just created. If <code>onCreate()</code> returns successfully (doesn't throw an exception), the database is assumed to be created with the requested version number. As an implication, you should not catch <code>SQLException</code>s in <code>onCreate()</code> yourself.</p>

<p><code>onUpgrade()</code> is only called when the database file exists but the stored version number is lower than requested in constructor. The <code>onUpgrade()</code> should update the table schema to the requested version.</p>

<p>When changing the table schema in code (<code>onCreate()</code>), you should make sure the database is updated. Two main approaches:</p>

<ol>
<li><p>Delete the old database file so that <code>onCreate()</code> is run again. This is often preferred at development time where you have control over the installed versions and data loss is not an issue. Some ways to to delete the database file:</p>

<ul>
<li><p>Uninstall the application. Use the application manager or <code>adb uninstall your.package.name</code> from shell.</p></li>
<li><p>Clear application data. Use the application manager.</p></li>
</ul></li>
<li><p>Increment the database version so that <code>onUpgrade()</code> is invoked. This is slightly more complicated as more code is needed.</p>

<ul>
<li><p>For development time schema upgrades where data loss is not an issue, you can just use <code>execSQL(""DROP TABLE IF EXISTS &lt;tablename&gt;"")</code> in to remove your existing tables and call <code>onCreate()</code> to recreate the database.</p></li>
<li><p>For released versions, you should implement data migration in <code>onUpgrade()</code> so your users don't lose their data.</p></li>
</ul></li>
</ol>
"
"<p>PostgreSQL just introduced <a href=""http://www.depesz.com/2014/03/25/waiting-for-9-4-introduce-jsonb-a-structured-format-for-storing-json/"">JSONB</a> and it's already trending on <a href=""https://news.ycombinator.com/item?id=7457197"">hacker news</a>.  It would be great if someone could explain how it's different from Hstore and JSON previously present in PostgreSQL. What are it's advantages and limitations and when should someone consider using it?</p>
","<p>First, <a href=""http://www.postgresql.org/docs/9.3/static/hstore.html"" rel=""noreferrer""><code>hstore</code></a> is a contrib module, which only allows you to store key => value pairs, where keys and values can only be <code>text</code>s (however values can be sql <code>NULL</code>s too).</p>

<p>Both <code>json</code> &amp; <code>jsonb</code> allows you to store a valid JSON <em>value</em> (defined in its <a href=""http://json.org"" rel=""noreferrer"">spec</a>).</p>

<p>F.ex. these are valid JSON representations: <code>null</code>, <code>true</code>, <code>[1,false,""string"",{""foo"":""bar""}]</code>, <code>{""foo"":""bar"",""baz"":[null]}</code> - <code>hstore</code> is just a little subset compared to what JSON is capable (but if you only need this subset, it's fine).</p>

<p>The only difference between <code>json</code> &amp; <code>jsonb</code> is their storage:</p>

<ul>
<li><code>json</code> is stored in its plain text format, while</li>
<li><code>jsonb</code> is stored in some binary representation</li>
</ul>

<p>There are 3 major consequences of this:</p>

<ul>
<li><code>jsonb</code> usually takes more disk space to store than <code>json</code> (sometimes not)</li>
<li><code>jsonb</code> takes more time to build from its input representation than <code>json</code></li>
<li><code>json</code> operations take <em>significantly</em> more time than <code>jsonb</code> (&amp; parsing also needs to be done each time you do some operation at a <code>json</code> typed value)</li>
</ul>

<p>When <code>jsonb</code> will be available with a stable release, there will be two major use cases, when you can easily select between them:</p>

<ol>
<li>If you only work with the JSON representation in your application, PostgreSQL is only used to store &amp; retrieve this representation, you should use <code>json</code>.</li>
<li>If you do a lot of operations on the JSON value in PostgreSQL, or use indexing on some JSON field, you should use <code>jsonb</code>.</li>
</ol>
"
"<p>I am connecting MySQL - 8.0 with MySQL Workbench and getting the below error:</p>

<blockquote>
  <p>Authentication plugin 'caching_sha2_password' cannot be loaded:
  dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image
  not found</p>
</blockquote>

<p>I have tried with other client tool as well.</p>

<p>Any solution for this?</p>
","<p>Note: For MAC OS</p>

<ol>
<li>Open MySQL from System Preferences > Initialize Database ></li>
<li>Type your new password.</li>
<li>Choose 'Use legacy password'</li>
<li>Start the Server again.</li>
<li>Now connect the MySQL Workbench</li>
</ol>

<p><img src=""https://i.stack.imgur.com/qGdmc.jpg"" alt=""Image description""></p>
"
"<p>Is there any command to find all the databases size in Postgres? </p>

<p>I am able to find the size of a specific database by using following command:</p>

<pre><code>select pg_database_size('databaseName');
</code></pre>
","<p>GRANT on the database is not what you need.  Grant on the tables directly.</p>

<p>Granting privileges on the database mostly is used to grant or revoke connect privileges.  This allows you to specify who may do stuff in the database if they have sufficient other permissions.</p>

<p>You want instead:</p>

<pre><code> GRANT ALL PRIVILEGES ON TABLE side_adzone TO jerry;
</code></pre>

<p>This will take care of this issue.</p>
"
"<p>My question is rather simple. I'm aware of the concept of a UUID and I want to generate one to refer to each 'item' from a 'store' in my DB with. Seems reasonable right?</p>

<p>The problem is the following line returns an error:</p>

<pre><code>honeydb=# insert into items values(
uuid_generate_v4(), 54.321, 31, 'desc 1', 31.94);
ERROR:  function uuid_generate_v4() does not exist
LINE 2: uuid_generate_v4(), 54.321, 31, 'desc 1', 31.94);
        ^
HINT:  No function matches the given name and argument types. You might need to add explicit type casts.
</code></pre>

<p>I've read the page at: <a href=""http://www.postgresql.org/docs/current/static/uuid-ossp.html"" rel=""noreferrer"">http://www.postgresql.org/docs/current/static/uuid-ossp.html</a></p>

<p><img src=""https://i.stack.imgur.com/BDeZ3.png"" alt=""enter image description here""></p>

<p>I'm running Postgres 8.4 on Ubuntu 10.04 x64.</p>
","<p><code>uuid-ossp</code> is a contrib module, so it isn't loaded into the server by default. You must load it into your database to use it.</p>

<p>For modern PostgreSQL versions (9.1  and newer) that's easy:</p>

<pre><code>CREATE EXTENSION IF NOT EXISTS ""uuid-ossp"";
</code></pre>

<p>but for 9.0 and below you must instead run the SQL script to load the extension. See <a href=""http://www.postgresql.org/docs/8.4/static/contrib.html"" rel=""noreferrer"">the documentation for contrib modules in 8.4</a>.</p>

<p>For Pg 9.1 and newer instead read <a href=""http://www.postgresql.org/docs/current/static/contrib.html"" rel=""noreferrer"">the current contrib docs</a> and <a href=""http://www.postgresql.org/docs/current/static/sql-createextension.html"" rel=""noreferrer""><code>CREATE EXTENSION</code></a>. These features do not exist in 9.0 or older versions, like your 8.4.</p>

<p>If you're using a packaged version of PostgreSQL you might need to install a separate package containing the contrib modules and extensions. Search your package manager database for 'postgres' and 'contrib'.</p>
"
"<p>I have many users on my web site (20000-60000 per day), which is a download site for mobile files.  I have remote access to my server (windows server 2008-R2). <br/>  I've received <em>""Server is unavailable""</em> errors before, but am now seeing a connection timeout error.
<br/>  I'm not familiar with this - why does it occur and how can I fix it? </p>

<p>The full error is below:</p>

<blockquote>
  <p>Server Error in '/' Application. Timeout expired.  The timeout period
  elapsed prior to completion of the operation or the server is not
  responding. The statement has been terminated. Description: An
  unhandled exception occurred during the execution of the current web
  request. Please review the stack trace for more information about the
  error and where it originated in the code.</p>
  
  <p>Exception Details: System.Data.SqlClient.SqlException: Timeout
  expired.  The timeout period elapsed prior to completion of the
  operation or the server is not responding. The statement has been
  terminated.</p>
  
  <p>Source Error:</p>
  
  <p>An unhandled exception was generated during the execution of the
  current web request. Information regarding the origin and location of
  the exception can be identified using the exception stack trace below.</p>
  
  <p>Stack Trace:</p>
  
  <p>[SqlException (0x80131904): Timeout expired.  The timeout period
  elapsed prior to completion of the operation or the server is not
  responding. The statement has been terminated.]<br>
  System.Data.SqlClient.SqlConnection.OnError(SqlException exception,
  Boolean breakConnection) +404<br>
  System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning() +412<br>
  System.Data.SqlClient.TdsParser.Run(RunBehavior runBehavior,
  SqlCommand cmdHandler, SqlDataReader dataStream,
  BulkCopySimpleResultSet bulkCopyHandler, TdsParserStateObject
  stateObj) +1363<br>
  System.Data.SqlClient.SqlCommand.FinishExecuteReader(SqlDataReader ds,
  RunBehavior runBehavior, String resetOptionsString) +6387741<br>
  System.Data.SqlClient.SqlCommand.RunExecuteReaderTds(CommandBehavior
  cmdBehavior, RunBehavior runBehavior, Boolean returnStream, Boolean
  async) +6389442<br>
  System.Data.SqlClient.SqlCommand.RunExecuteReader(CommandBehavior
  cmdBehavior, RunBehavior runBehavior, Boolean returnStream, String
  method, DbAsyncResult result) +538<br>
  System.Data.SqlClient.SqlCommand.InternalExecuteNonQuery(DbAsyncResult
  result, String methodName, Boolean sendToPipe) +689<br>
  System.Data.SqlClient.SqlCommand.ExecuteNonQuery() +327<br>
  NovinMedia.Data.DbObject.RunProcedure(String storedProcName,
  IDataParameter[] parameters, Int32&amp; rowsAffected) +209<br>
  DataLayer.OnlineUsers.Update_SessionEnd_And_Online(Object Session_End,
  Boolean Online) +440<br>
  NiceFileExplorer.Global.Application_Start(Object sender, EventArgs e)
  +163</p>
  
  <p>[HttpException (0x80004005): Timeout expired.  The timeout period
  elapsed prior to completion of the operation or the server is not
  responding. The statement has been terminated.]<br>
  System.Web.HttpApplicationFactory.EnsureAppStartCalledForIntegratedMode(HttpContext
  context, HttpApplication app) +4052053<br>
  System.Web.HttpApplication.RegisterEventSubscriptionsWithIIS(IntPtr
  appContext, HttpContext context, MethodInfo[] handlers) +191<br>
  System.Web.HttpApplication.InitSpecial(HttpApplicationState state,
  MethodInfo[] handlers, IntPtr appContext, HttpContext context) +352<br>
  System.Web.HttpApplicationFactory.GetSpecialApplicationInstance(IntPtr
  appContext, HttpContext context) +407<br>
  System.Web.Hosting.PipelineRuntime.InitializeApplication(IntPtr
  appContext) +375</p>
  
  <p>[HttpException (0x80004005): Timeout expired.  The timeout period
  elapsed prior to completion of the operation or the server is not
  responding. The statement has been terminated.]<br>
  System.Web.HttpRuntime.FirstRequestInit(HttpContext context) +11686928
  System.Web.HttpRuntime.EnsureFirstRequestInit(HttpContext context)
  +141    System.Web.HttpRuntime.ProcessRequestNotificationPrivate(IIS7WorkerRequest
  wr, HttpContext context) +4863749</p>
</blockquote>

<hr>

<p><strong>EDIT AFTER ANSWERS:</strong><br>
my <code>Application_Start</code> in <code>Global.asax</code> is like below:  </p>

<pre><code>protected void Application_Start(object sender, EventArgs e)
{
    Application[""OnlineUsers""] = 0;

    OnlineUsers.Update_SessionEnd_And_Online(
        DateTime.Now,
        false);

    AddTask(""DoStuff"", 10);
}
</code></pre>

<p>The stored procedure being called is:</p>

<pre><code>ALTER Procedure [dbo].[sp_OnlineUsers_Update_SessionEnd_And_Online]
    @Session_End datetime,
    @Online bit
As
Begin
    Update OnlineUsers
    SET
        [Session_End] = @Session_End,
        [Online] = @Online

End
</code></pre>

<p>I have two methods for getting online users:</p>

<ol>
<li>using <code>Application[""OnlineUsers""] = 0;</code>  </li>
<li>the other one using database</li>
</ol>

<p>So, for method #2 I reset all OnlineUsers at <code>Application_Start</code>.   There are over 482,751 records in that table.  </p>
","<p><strong>Updated</strong></p>

<p>For MS SQL Server 2012 and above</p>

<pre><code>USE [master];

DECLARE @kill varchar(8000) = '';  
SELECT @kill = @kill + 'kill ' + CONVERT(varchar(5), session_id) + ';'  
FROM sys.dm_exec_sessions
WHERE database_id  = db_id('MyDB')

EXEC(@kill);
</code></pre>

<p>For MS SQL Server 2000, 2005, 2008</p>

<pre><code>USE master;

DECLARE @kill varchar(8000); SET @kill = '';  
SELECT @kill = @kill + 'kill ' + CONVERT(varchar(5), spid) + ';'  
FROM master..sysprocesses  
WHERE dbid = db_id('MyDB')

EXEC(@kill); 
</code></pre>
"
"<p>I'm using a SQLdatareader to build POCOs from a database. The code works except when it encounters a null value in the database. For example, if the FirstName column in the database contains a null value, an exception is thrown. </p>

<pre><code>employee.FirstName = sqlreader.GetString(indexFirstName);
</code></pre>

<p>What is the best way to handle null values in this situation?</p>
","<p>You need to check for <code>IsDBNull</code>:</p>

<pre><code>if(!SqlReader.IsDBNull(indexFirstName))
{
  employee.FirstName = sqlreader.GetString(indexFirstName);
}
</code></pre>

<p>That's your only reliable way to detect and handle this situation.</p>

<p>I wrapped those things into extension methods and tend to return a default value if the column is indeed <code>null</code>:</p>

<pre><code>public static string SafeGetString(this SqlDataReader reader, int colIndex)
{
   if(!reader.IsDBNull(colIndex))
       return reader.GetString(colIndex);
   return string.Empty;
}
</code></pre>

<p>Now you can call it like this:</p>

<pre><code>employee.FirstName = SqlReader.SafeGetString(indexFirstName);
</code></pre>

<p>and you'll never have to worry about an exception or a <code>null</code> value again.</p>
"
"<p>I have the following error during sqlite3-ruby install:</p>

<pre>
Building native extensions.  This could take a while...
ERROR:  Error installing sqlite3-ruby:
    ERROR: Failed to build gem native extension.

/usr/bin/ruby1.8 extconf.rb
checking for sqlite3.h... no
sqlite3.h is missing. Try 'port install sqlite3 +universal' or 'yum install sqlite3-devel'
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of
necessary libraries and/or headers.  Check the mkmf.log file for more
details.  You may need configuration options.

Provided configuration options:
    --with-opt-dir
    --without-opt-dir
    --with-opt-include
    --without-opt-include=${opt-dir}/include
    --with-opt-lib
    --without-opt-lib=${opt-dir}/lib
    --with-make-prog
    --without-make-prog
    --srcdir=.
    --curdir
    --ruby=/usr/bin/ruby1.8
    --with-sqlite3-dir
    --without-sqlite3-dir
    --with-sqlite3-include
    --without-sqlite3-include=${sqlite3-dir}/include
    --with-sqlite3-lib
    --without-sqlite3-lib=${sqlite3-dir}/lib


Gem files will remain installed in /usr/lib/ruby/gems/1.8/gems/sqlite3-ruby-1.3.1 for inspection.
Results logged to /usr/lib/ruby/gems/1.8/gems/sqlite3-ruby-1.3.1/ext/sqlite3/gem_make.out
</pre>

<p>sqlite3.h is located in /usr/include/</p>

<pre>
sudo gem install sqlite3-ruby --without-sqlite3-include=/usr/include
</pre>

<p>doesn't work</p>

<pre>

ERROR:  While executing gem ... (OptionParser::InvalidOption)
    invalid option: --without-sqlite3-include=/usr/include
</pre>

<p>Ubuntu 10.04</p>
","<p>You need the SQLite3 development headers for the gems native extension to compile against. You can install them by running (possibly with <code>sudo</code>):</p>

<pre><code>apt-get install libsqlite3-dev
</code></pre>
"
"<p>I seem to be unable to re-create a simple user I've deleted, even as root in MySQL.</p>

<p>My case: user 'jack' existed before, but I deleted it from mysql.user in order to recreate it. I see no vestiges of this in that table. If I execute this command for some other, random username, say 'jimmy', it works fine (just as it originally did for 'jack').</p>

<p>What have I done to corrupt user 'jack' and how can I undo that corruption in order to re-create 'jack' as a valid user for this installation of MySQL?</p>

<p>See example below. (Of course, originally, there was much time between the creation of 'jack' and his removal.)</p>

<pre><code>mysql&gt; CREATE USER 'jack'@'localhost' IDENTIFIED BY 'test123';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; select user,host from user;
+------------------+-----------------+
| user             | host            |
+------------------+-----------------+
| root             | 127.0.0.1       |
| debian-sys-maint | localhost       |
| jack             | localhost       |
| root             | localhost       |
| root             | russ-elite-book |
+------------------+-----------------+
5 rows in set (0.00 sec)

mysql&gt; delete from user where user = 'jack';
Query OK, 1 row affected (0.00 sec)

mysql&gt; select user,host from user;
+------------------+-----------------+
| user             | host            |
+------------------+-----------------+
| root             | 127.0.0.1       |
| debian-sys-maint | localhost       |
| root             | localhost       |
| root             | russ-elite-book |
+------------------+-----------------+
4 rows in set (0.00 sec)

mysql&gt; CREATE USER 'jack'@'localhost' IDENTIFIED BY 'test123';
ERROR 1396 (HY000): Operation CREATE USER failed for 'jack'@'localhost'
mysql&gt; CREATE USER 'jimmy'@'localhost' IDENTIFIED BY 'test123';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; select user,host from user;
+------------------+-----------------+
| user             | host            |
+------------------+-----------------+
| root             | 127.0.0.1       |
| debian-sys-maint | localhost       |
| jimmy            | localhost       |
| root             | localhost       |
| root             | russ-elite-book |
+------------------+-----------------+
5 rows in set (0.00 sec)
</code></pre>
","<p>yes this bug is there. However, I found a small workaround.</p>

<ul>
<li>Assume the user is there, so drop the user</li>
<li>After deleting the user, there is need to flush the mysql privileges</li>
<li>Now create the user.</li>
</ul>

<p>That should solve it. Assuming we want to create the user admin @ localhost, these would be the commands:</p>

<pre>
drop user admin@localhost;
flush privileges;
create user admin@localhost identified by '<i>admins_password</i>'
</pre>

<p>Cheers</p>
"
"<p>Error
SQL query: </p>

<pre><code>--
-- Database: `work`
--
-- --------------------------------------------------------
--
-- Table structure for table `administrators`
--
CREATE TABLE IF NOT EXISTS `administrators` (

`user_id` varchar( 30 ) NOT NULL ,
`password` varchar( 30 ) NOT NULL ) ENGINE = InnoDB DEFAULT CHARSET = latin1;
</code></pre>

<p>MySQL said:  </p>

<pre><code>#1046 - No database selected
</code></pre>

<p>need some help here.</p>
","<p>You need to tell MySQL which database to use:</p>

<pre><code>USE database_name;
</code></pre>

<p>before you create a table.</p>

<p>In case the database does not exist, you need to create it as:</p>

<pre><code>CREATE DATABASE database_name;
</code></pre>

<p>followed by: </p>

<pre><code>USE database_name;
</code></pre>
"
"<p>How can I call <strong>psql</strong> so that it <strong>doesn't prompt for a password</strong>?</p>

<p>This is what I have:</p>

<pre><code>psql -Umyuser &lt; myscript.sql
</code></pre>

<p>However, I couldn't find the argument that passes the password, and so psql always prompts for it.</p>
","<p>There are several ways to authenticate to PostgreSQL. You may wish to investigate alternatives to password authentication at <a href=""https://www.postgresql.org/docs/current/static/client-authentication.html"" rel=""noreferrer"">https://www.postgresql.org/docs/current/static/client-authentication.html</a>.</p>

<p>To answer your question, there are a few ways provide a password for password-based authentication.  The obvious way is via the password prompt.  Instead of that, you can provide the password in a pgpass file or through the <code>PGPASSWORD</code> environment variable.  See these:</p>

<ul>
<li><a href=""https://www.postgresql.org/docs/9.0/static/libpq-pgpass.html"" rel=""noreferrer"">https://www.postgresql.org/docs/9.0/static/libpq-pgpass.html</a></li>
<li><a href=""https://www.postgresql.org/docs/9.0/interactive/libpq-envars.html"" rel=""noreferrer"">https://www.postgresql.org/docs/9.0/interactive/libpq-envars.html</a></li>
</ul>

<p>There is no option to provide the password as a command line argument because that information is often available to all users, and therefore insecure.  However, in Linux/Unix environments you can provide an environment variable for a single command like this:</p>

<pre><code>PGPASSWORD=yourpass psql ...
</code></pre>
"
"<p>I have an MS SQL Server 2008 Express system which contains a database that I would like to 'copy and rename' (for testing purposes) but I am unaware of a simple way to achieve this.</p>

<p>I notice that in the R2 version of SQL Server there is a copy database wizard, but sadly I can't upgrade.</p>

<p>The database in question is around a gig.
I attempted to restore a backup of the database I want to copy into a new database, but with no luck.</p>
","<ol>
<li><p>Install Microsoft SQL Management Studio, which you can download for free from Microsoft's website:</p>

<p><strong>Version 2008</strong>       </p>

<p>Microsoft SQL Management Studio 2008 is part of <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=1842"" rel=""nofollow noreferrer"">SQL Server 2008 Express with Advanced Services</a></p>

<p><strong>Version 2012</strong></p>

<p>Click <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=29062"" rel=""nofollow noreferrer"">download button</a> and check <code>ENU\x64\SQLManagementStudio_x64_ENU.exe</code></p>

<p><strong>Version 2014</strong></p>

<p>Click <a href=""http://www.microsoft.com/en-us/download/details.aspx?id=42299"" rel=""nofollow noreferrer"">download button</a> and check MgmtStudio <code>64BIT\SQLManagementStudio_x64_ENU.exe</code></p></li>
<li><p>Open <strong>Microsoft SQL Management Studio</strong>. </p></li>
<li>Backup original database to .BAK file (db -> Task -> Backup).</li>
<li>Create empty database with new name (clone). Note comments below as this is optional. </li>
<li>Click to clone database and open restore dialog (see image)
<img src=""https://i.stack.imgur.com/4reCD.png"" alt=""restore dialog""></li>
<li>Select Device and add the backup file from step 3.
<img src=""https://i.stack.imgur.com/szMBC.png"" alt=""add backup file""></li>
<li>Change destination to test database
<img src=""https://i.stack.imgur.com/qxFWk.png"" alt=""change destination""></li>
<li>Change location of database files, it must be different from the original. You can type directly into text box, just add postfix. (NOTE: Order is important. Select checkbox, then change the filenames.)
<img src=""https://i.stack.imgur.com/pBzT4.png"" alt=""change location""></li>
<li>Check WITH REPLACE and WITH KEEP_REPLICATION
<img src=""https://i.stack.imgur.com/QvGoB.png"" alt=""with replace""></li>
</ol>
"
"<p>I'm having a bit of a strange problem. I'm trying to add a foreign key to one table that references another, but it is failing for some reason. With my limited knowledge of MySQL, the only thing that could possibly be suspect is that there is a foreign key on a different table referencing the one I am trying to reference.</p>

<p>Here is a picture of my table relationships, generated via phpMyAdmin:
<a href=""http://img14.imageshack.us/img14/5415/phpmyadminrelation.png"" rel=""noreferrer"">Relationships</a></p>

<p>I've done a <code>SHOW CREATE TABLE</code> query on both tables, <code>sourcecodes_tags</code> is the table with the foreign key, <code>sourcecodes</code> is the referenced table.</p>

<pre><code>CREATE TABLE `sourcecodes` (
 `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
 `user_id` int(11) unsigned NOT NULL,
 `language_id` int(11) unsigned NOT NULL,
 `category_id` int(11) unsigned NOT NULL,
 `title` varchar(40) CHARACTER SET utf8 NOT NULL,
 `description` text CHARACTER SET utf8 NOT NULL,
 `views` int(11) unsigned NOT NULL,
 `downloads` int(11) unsigned NOT NULL,
 `time_posted` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
 PRIMARY KEY (`id`),
 KEY `user_id` (`user_id`),
 KEY `language_id` (`language_id`),
 KEY `category_id` (`category_id`),
 CONSTRAINT `sourcecodes_ibfk_3` FOREIGN KEY (`language_id`) REFERENCES `languages` (`id`) ON DELETE CASCADE ON UPDATE CASCADE,
 CONSTRAINT `sourcecodes_ibfk_1` FOREIGN KEY (`user_id`) REFERENCES `users` (`id`) ON DELETE CASCADE ON UPDATE CASCADE,
 CONSTRAINT `sourcecodes_ibfk_2` FOREIGN KEY (`category_id`) REFERENCES `categories` (`id`) ON DELETE CASCADE ON UPDATE CASCADE
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=latin1

CREATE TABLE `sourcecodes_tags` (
 `sourcecode_id` int(11) unsigned NOT NULL,
 `tag_id` int(11) unsigned NOT NULL,
 KEY `sourcecode_id` (`sourcecode_id`),
 KEY `tag_id` (`tag_id`),
 CONSTRAINT `sourcecodes_tags_ibfk_1` FOREIGN KEY (`tag_id`) REFERENCES `tags` (`id`) ON DELETE CASCADE ON UPDATE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=latin1
</code></pre>

<p>It would be great if anyone could tell me what is going on here, I've had no formal training or anything with MySQL :)</p>

<p>Thanks.</p>

<p><strong>Edit:</strong> This is the code that generates the error:</p>

<pre><code>ALTER TABLE sourcecodes_tags ADD FOREIGN KEY (sourcecode_id) REFERENCES sourcecodes (id) ON DELETE CASCADE ON UPDATE CASCADE
</code></pre>
","<p>Quite likely your <code>sourcecodes_tags</code> table contains <code>sourcecode_id</code> values that no longer exists in your <code>sourcecodes</code> table. You have to get rid of those first.</p>

<p>Here's a query that can find those IDs:</p>

<pre><code>SELECT DISTINCT sourcecode_id FROM 
   sourcecodes_tags tags LEFT JOIN sourcecodes sc ON tags.sourcecode_id=sc.id 
WHERE sc.id IS NULL;
</code></pre>
"
"<p>Why would someone use <code>WHERE 1=1 AND &lt;conditions&gt;</code> in a SQL clause (Either SQL obtained through concatenated strings, either view definition)</p>

<p>I've seen somewhere that this would be used to protect against SQL Injection, but it seems very weird.</p>

<p>If there is injection <code>WHERE 1 = 1 AND injected OR 1=1</code> would have the same result as <code>injected OR 1=1</code>.</p>

<p>Later edit: What about the usage in a view definition?</p>

<hr>

<p>Thank you for your answers.</p>

<p>Still,
I don't understand why would someone use this construction for defining a view, or use it inside a stored procedure.</p>

<p>Take this for example:</p>

<pre><code>CREATE VIEW vTest AS
SELECT FROM Table WHERE 1=1 AND table.Field=Value
</code></pre>
","<p>If the list of conditions is not known at compile time and is instead built at run time, you don't have to worry about whether you have one or more than one condition. You can generate them all like:</p>

<pre><code>and &lt;condition&gt;
</code></pre>

<p>and concatenate them all together. With the <code>1=1</code> at the start, the initial <code>and</code> has something to associate with.</p>

<p>I've never seen this used for any kind of injection protection, as you say it doesn't seem like it would help much. I <em>have</em> seen it used as an implementation convenience. The SQL query engine will end up ignoring the <code>1=1</code> so it should have no performance impact.</p>
"
"<p>When should I use <code>KEY</code>, <code>PRIMARY KEY</code>, <code>UNIQUE KEY</code> and <code>INDEX</code>?</p>
","<p><code>KEY</code> and <code>INDEX</code> are synonyms in MySQL. They mean the same thing. In databases you would use <a href=""http://en.wikipedia.org/wiki/Index_(database)"" rel=""noreferrer"">indexes</a> to improve the speed of data retrieval. An index is typically created on columns used in <code>JOIN</code>, <code>WHERE</code>, and <code>ORDER BY</code> clauses. </p>

<p>Imagine you have a table called <code>users</code> and you want to search for all the users which have the last name 'Smith'. Without an index, the database would have to go through all the records of the table: this is slow, because the more records you have in your database, the more work it has to do to find the result. On the other hand, an index will help the database skip quickly to the relevant pages where the 'Smith' records are held. This is very similar to how we, humans, go through a phone book directory to find someone by the last name: We don't start searching through the directory from cover to cover, as long we inserted the information in some order that we can use to skip quickly to the 'S' pages.</p>

<p>Primary keys and unique keys are similar. A primary key is a column, or a combination of columns, that can uniquely identify a row. It is a special case of <a href=""http://en.wikipedia.org/wiki/Primary_key"" rel=""noreferrer"">unique key</a>. A table can have at most one primary key, but more than one unique key. When you specify a unique key on a column, no two distinct rows in a table can have the same value.</p>

<p>Also note that columns defined as primary keys or unique keys are automatically indexed in MySQL.</p>
"
"<p>Assuming we have a database project called <code>MyDatabase</code> then a file called <code>MyDatabase.jfm</code> appears in the root of the project directory.</p>

<ul>
<li>It is exclusively locked while the project is open in Visual Studio</li>
<li>It is a binary file</li>
<li>It has only started appearing recently (past couple of days)</li>
</ul>

<p>I have done a Google search, which has not offered any insight. There are some references to some old software, but most of the results are spam/trojanware. I have also looked in SO, but that has not produced any results either.</p>

<p><strong>Does anyone know what it is and why it's there?</strong></p>

<p>The plan is to add it to the <a href=""https://github.com/github/gitignore"">gitignore file for Visual Studio</a>, but I need to know what it is first to submit the PR...</p>

<p><strong>UPDATE</strong></p>

<p>This is now breaking the Team Explorer Changes view. There are no changes being shown with the following from the Output window:</p>

<p><a href=""https://i.stack.imgur.com/wUWHC.png""><img src=""https://i.stack.imgur.com/wUWHC.png"" alt=""Screenshot of the error from the output tab""></a></p>

<p>As such I will be adding the file to the .gitignore file and <a href=""https://github.com/github/gitignore/pull/2028"">submitting the PR</a>. It would be good to eventually learn what this file is and where it came from...</p>
","<p>This is an issue caused by the <a href=""https://en.wikipedia.org/wiki/Extensible_Storage_Engine"" rel=""noreferrer"">ESENT engine</a> relied on by SQL Projects adding in a new file. This is a new feature in Windows 10 Anniversary Edition to avoid data loss, but the fact SQL Projects store the .dbmdl cache file under the project root means this locked file is added to the Git source control.</p>

<p>Notes:</p>

<ul>
<li>A <a href=""https://github.com/github/gitignore/pull/2104"" rel=""noreferrer"">pull request</a> to fix this in GitIgnore has been accepted and the Visual Studio team is working to include this in future updates. This will ensure that for new projects the .gitignore file includes .jfm and the problem will not occur</li>
<li>The core SSDT team is working to provide a solution to this in a future update, but for now the best solution is to manually add as discussed in the comments. </li>
</ul>

<p><strong>Disclosure</strong>: I work on the SSDT team for Microsoft.</p>
"
"<p>I can't believe I couldn't find a working solution to this after an hour of searching. I'm following <a href=""http://www.dotnetcurry.com/showarticle.aspx?ID=941"">this article</a> on Entity Framework 6.0 which gives a simple walk-through on Code First. I created the project and installed the latest <a href=""https://www.nuget.org/packages/EntityFramework/6.0.2"">EF Nuget package</a> for the project to compile. I also verified that I have Microsoft SQL Server 2012 Express LocalDB installed which came with Visual Studio 2013. I don't have any other instances of SQL installed on my local computer. The program runs and entries are added to the database and outputted in the console. But when the article says ""check your localdb"" it doesn't say how! I don't see any '.mdf' or '.ldf' files created under the project folder. I tried every way to connect Visual Studio's Server Explorer to LocalDB. The wizard cannot locate <code>(localdb)</code> or cannot find any provider in Server Explorer to accept connection string like <code>(localdb)\v11.0;Integrated Security=true;</code> I've seen this asked several places in StackOverflow but no answer works or marked as answer. Please help, this doesn't have to be this frustrating!</p>

<p>What are the steps to connect Visual Studio Server Explorer to LocalDB?</p>
","<p>In <strong>Visual Studio 2012</strong> all I had to do was enter:</p>

<pre><code>(localdb)\v11.0
</code></pre>

<p><strong>Visual Studio 2015</strong> and <strong>Visual Studio 2017</strong> changed to:</p>

<pre><code>(localdb)\MSSQLLocalDB
</code></pre>

<p>as the server name when adding a <code>Microsoft SQL Server Data</code> source in:</p>

<pre><code>View/Server Explorer/(Right click) Data Connections/Add Connection
</code></pre>

<p>and then the database names were populated. I didn't need to do all the other steps in the accepted answer, although it would be nice if the server name was available automatically in the server name combo box.</p>

<p>You can also browse the LocalDB database names available on your machine using:</p>

<pre><code>View/SQL Server Object Explorer.
</code></pre>
"
"<p>I'm trying to determine what instances of sql server/sql express I have installed (either manually or programmatically) but all of the examples are telling me to run a SQL query to determine this which assumes I'm already connected to a particular instance.</p>
","<p>Maybe you have to execute the following in the <strong><em>Visual Studio Tools</em></strong> command prompt:</p>

<pre><code>aspnet_regiis -i
</code></pre>

<p>You can read more about the <strong>ASP.NET IIS Registration Tool (Aspnet_regiis.exe)</strong> <a href=""http://msdn.microsoft.com/en-us/library/k6h9cz8h%28v=vs.100%29.aspx"">here</a>.</p>
"
"<p>After installing SQL Server 2008, I cannot find the <code>SQL Server Configuration Manager</code> in <code>Start / SQL Server 2008 / Configuration Tools</code> menu. </p>

<p>What should I do to install this tool?</p>
","<p>If you happen to be using Windows 8 and up, here's how to get to it:</p>

<ul>
<li>The newer Microsoft SQL Server Configuration Manager is a snap-in for
the Microsoft Management Console program.</li>
<li>It  is not a stand-alone
program as used in the previous versions of Microsoft Windows
operating systems,</li>
<li>SQL Server Configuration Manager doesnt appear as
an application when running Windows 8.</li>
<li><p>To open SQL Server
Configuration Manager, in the Search charm, under Apps, type:</p>

<p><code>SQLServerManager14.msc</code> for [SQL Server 2017] or</p>

<p><code>SQLServerManager13.msc</code> for [SQL Server 2016] or</p>

<p><code>SQLServerManager12.msc</code> for [SQL Server 2014] or</p>

<p><code>SQLServerManager11.msc</code> for [SQL Server 2012] or</p>

<p><code>SQLServerManager10.msc</code> for [SQL Server 2008], and then press <kbd>Enter</kbd>.</p></li>
</ul>

<p>Text kindly reproduced from <a href=""http://babyraj.com/sql-server-configuration-manager-changes-windows-8/"" rel=""noreferrer"">SQL Server Configuration Manager changes in Windows 8</a></p>

<hr>

<p>Detailed info from MSDN: <a href=""http://technet.microsoft.com/en-us/library/ms174212.aspx"" rel=""noreferrer"">SQL Server Configuration Manager</a></p>
"
"<p>I am looking for some docs and/or examples for the new JSON functions in PostgreSQL 9.2. </p>

<p>Specifically, given a series of JSON records:</p>

<pre><code>[
  {name: ""Toby"", occupation: ""Software Engineer""},
  {name: ""Zaphod"", occupation: ""Galactic President""}
]
</code></pre>

<p>How would I write the SQL to find a record by name?</p>

<p>In vanilla SQL:</p>

<pre><code>SELECT * from json_data WHERE ""name"" = ""Toby""
</code></pre>

<p>The official dev manual is quite sparse:</p>

<ul>
<li><a href=""http://www.postgresql.org/docs/devel/static/datatype-json.html"">http://www.postgresql.org/docs/devel/static/datatype-json.html</a></li>
<li><a href=""http://www.postgresql.org/docs/devel/static/functions-json.html"">http://www.postgresql.org/docs/devel/static/functions-json.html</a></li>
</ul>

<h3>Update I</h3>

<p>I've put together a <a href=""https://gist.github.com/2715918"">gist detailing what is currently possible with PostgreSQL 9.2</a>.
Using some custom functions, it is possible to do things like:</p>

<pre><code>SELECT id, json_string(data,'name') FROM things
WHERE json_string(data,'name') LIKE 'G%';
</code></pre>

<h3>Update II</h3>

<p>I've now moved my JSON functions into their own project:</p>

<p><a href=""https://github.com/tobyhede/postsql""><strong>PostSQL</strong></a> - a set of functions for transforming PostgreSQL and PL/v8 into a totally awesome JSON document store</p>
","<p><strong>Update</strong>: <a href=""http://www.postgresql.org/docs/9.5/static/functions-json.html"" rel=""noreferrer"">With PostgreSQL 9.5</a>, there are some <code>jsonb</code> manipulation functionality within PostgreSQL itself (but none for <code>json</code>; casts are required to manipulate <code>json</code> values).</p>

<p>Merging 2 (or more) JSON objects (or concatenating arrays):</p>

<pre><code>SELECT jsonb '{""a"":1}' || jsonb '{""b"":2}', -- will yield jsonb '{""a"":1,""b"":2}'
       jsonb '[""a"",1]' || jsonb '[""b"",2]'  -- will yield jsonb '[""a"",1,""b"",2]'
</code></pre>

<p>So, <strong>setting a simple key</strong> can be done using:</p>

<pre><code>SELECT jsonb '{""a"":1}' || jsonb_build_object('&lt;key&gt;', '&lt;value&gt;')
</code></pre>

<p>Where <code>&lt;key&gt;</code> should be string, and <code>&lt;value&gt;</code> can be whatever type <code>to_jsonb()</code> accepts.</p>

<p>For <strong>setting a value deep in a JSON hierarchy</strong>, the <code>jsonb_set()</code> function can be used:</p>

<pre><code>SELECT jsonb_set('{""a"":[null,{""b"":[]}]}', '{a,1,b,0}', jsonb '{""c"":3}')
-- will yield jsonb '{""a"":[null,{""b"":[{""c"":3}]}]}'
</code></pre>

<p>Full parameter list of <code>jsonb_set()</code>:</p>

<pre><code>jsonb_set(target         jsonb,
          path           text[],
          new_value      jsonb,
          create_missing boolean default true)
</code></pre>

<p><code>path</code> can contain JSON array indexes too &amp; negative integers that appear there count from the end of JSON arrays. However, a non-existing, but positive JSON array index will append the element to the end of the array:</p>

<pre><code>SELECT jsonb_set('{""a"":[null,{""b"":[1,2]}]}', '{a,1,b,1000}', jsonb '3', true)
-- will yield jsonb '{""a"":[null,{""b"":[1,2,3]}]}'
</code></pre>

<p>For <strong>inserting into JSON array (while preserving all of the original values)</strong>, the <code>jsonb_insert()</code> function can be used (<em>in 9.6+; this function only, in this section</em>):</p>

<pre><code>SELECT jsonb_insert('{""a"":[null,{""b"":[1]}]}', '{a,1,b,0}', jsonb '2')
-- will yield jsonb '{""a"":[null,{""b"":[2,1]}]}', and
SELECT jsonb_insert('{""a"":[null,{""b"":[1]}]}', '{a,1,b,0}', jsonb '2', true)
-- will yield jsonb '{""a"":[null,{""b"":[1,2]}]}'
</code></pre>

<p>Full parameter list of <code>jsonb_insert()</code>:</p>

<pre><code>jsonb_insert(target       jsonb,
             path         text[],
             new_value    jsonb,
             insert_after boolean default false)
</code></pre>

<p>Again, negative integers that appear in <code>path</code> count from the end of JSON arrays.</p>

<p>So, f.ex. appending to an end of a JSON array can be done with:</p>

<pre><code>SELECT jsonb_insert('{""a"":[null,{""b"":[1,2]}]}', '{a,1,b,-1}', jsonb '3', true)
-- will yield jsonb '{""a"":[null,{""b"":[1,2,3]}]}', and
</code></pre>

<p>However, this function is working slightly differently (than <code>jsonb_set()</code>) when the <code>path</code> in <code>target</code> is a JSON object's key. In that case, it will only add a new key-value pair for the JSON object when the key is not used. If it's used, it will raise an error:</p>

<pre><code>SELECT jsonb_insert('{""a"":[null,{""b"":[1]}]}', '{a,1,c}', jsonb '[2]')
-- will yield jsonb '{""a"":[null,{""b"":[1],""c"":[2]}]}', but
SELECT jsonb_insert('{""a"":[null,{""b"":[1]}]}', '{a,1,b}', jsonb '[2]')
-- will raise SQLSTATE 22023 (invalid_parameter_value): cannot replace existing key
</code></pre>

<p><strong>Deleting a key (or an index)</strong> from a JSON object (or, from an array) can be done with the <code>-</code> operator:</p>

<pre><code>SELECT jsonb '{""a"":1,""b"":2}' - 'a', -- will yield jsonb '{""b"":2}'
       jsonb '[""a"",1,""b"",2]' - 1    -- will yield jsonb '[""a"",""b"",2]'
</code></pre>

<p><strong>Deleting, from deep in a JSON hierarchy</strong> can be done with the <code>#-</code> operator:</p>

<pre><code>SELECT '{""a"":[null,{""b"":[3.14]}]}' #- '{a,1,b,0}'
-- will yield jsonb '{""a"":[null,{""b"":[]}]}'
</code></pre>

<p><strong>For 9.4</strong>, you can use a modified version of the original answer (below), but instead of aggregating a JSON string, you can aggregate into a json object directly with <code>json_object_agg()</code>.</p>

<p><strong>Original answer</strong>: It is possible (without plpython or plv8) in pure SQL too (but needs 9.3+, will not work with 9.2)</p>

<pre><code>CREATE OR REPLACE FUNCTION ""json_object_set_key""(
  ""json""          json,
  ""key_to_set""    TEXT,
  ""value_to_set""  anyelement
)
  RETURNS json
  LANGUAGE sql
  IMMUTABLE
  STRICT
AS $function$
SELECT concat('{', string_agg(to_json(""key"") || ':' || ""value"", ','), '}')::json
  FROM (SELECT *
          FROM json_each(""json"")
         WHERE ""key"" &lt;&gt; ""key_to_set""
         UNION ALL
        SELECT ""key_to_set"", to_json(""value_to_set"")) AS ""fields""
$function$;
</code></pre>

<p><a href=""http://sqlfiddle.com/#!15/d41d8/2902"" rel=""noreferrer"">SQLFiddle</a></p>

<p><strong>Edit</strong>:</p>

<p>A version, which sets multiple keys &amp; values:</p>

<pre><code>CREATE OR REPLACE FUNCTION ""json_object_set_keys""(
  ""json""          json,
  ""keys_to_set""   TEXT[],
  ""values_to_set"" anyarray
)
  RETURNS json
  LANGUAGE sql
  IMMUTABLE
  STRICT
AS $function$
SELECT concat('{', string_agg(to_json(""key"") || ':' || ""value"", ','), '}')::json
  FROM (SELECT *
          FROM json_each(""json"")
         WHERE ""key"" &lt;&gt; ALL (""keys_to_set"")
         UNION ALL
        SELECT DISTINCT ON (""keys_to_set""[""index""])
               ""keys_to_set""[""index""],
               CASE
                 WHEN ""values_to_set""[""index""] IS NULL THEN 'null'::json
                 ELSE to_json(""values_to_set""[""index""])
               END
          FROM generate_subscripts(""keys_to_set"", 1) AS ""keys""(""index"")
          JOIN generate_subscripts(""values_to_set"", 1) AS ""values""(""index"")
         USING (""index"")) AS ""fields""
$function$;
</code></pre>

<p><strong>Edit 2</strong>: as @ErwinBrandstetter <a href=""https://stackoverflow.com/questions/18209625/how-do-i-modify-fields-inside-the-new-postgresql-json-datatype/23500670?noredirect=1#comment39388159_23500670"">noted</a> these functions above works like a so-called <code>UPSERT</code> (updates a field if it exists, inserts if it does not exist). Here is a variant, which only <code>UPDATE</code>:</p>

<pre><code>CREATE OR REPLACE FUNCTION ""json_object_update_key""(
  ""json""          json,
  ""key_to_set""    TEXT,
  ""value_to_set""  anyelement
)
  RETURNS json
  LANGUAGE sql
  IMMUTABLE
  STRICT
AS $function$
SELECT CASE
  WHEN (""json"" -&gt; ""key_to_set"") IS NULL THEN ""json""
  ELSE (SELECT concat('{', string_agg(to_json(""key"") || ':' || ""value"", ','), '}')
          FROM (SELECT *
                  FROM json_each(""json"")
                 WHERE ""key"" &lt;&gt; ""key_to_set""
                 UNION ALL
                SELECT ""key_to_set"", to_json(""value_to_set"")) AS ""fields"")::json
END
$function$;
</code></pre>

<p><strong>Edit 3</strong>: Here is recursive variant, which can set (<code>UPSERT</code>) a leaf value (and uses the first function from this answer), located at a key-path (where keys can only refer to inner objects, inner arrays not supported):</p>

<pre><code>CREATE OR REPLACE FUNCTION ""json_object_set_path""(
  ""json""          json,
  ""key_path""      TEXT[],
  ""value_to_set""  anyelement
)
  RETURNS json
  LANGUAGE sql
  IMMUTABLE
  STRICT
AS $function$
SELECT CASE COALESCE(array_length(""key_path"", 1), 0)
         WHEN 0 THEN to_json(""value_to_set"")
         WHEN 1 THEN ""json_object_set_key""(""json"", ""key_path""[l], ""value_to_set"")
         ELSE ""json_object_set_key""(
           ""json"",
           ""key_path""[l],
           ""json_object_set_path""(
             COALESCE(NULLIF((""json"" -&gt; ""key_path""[l])::text, 'null'), '{}')::json,
             ""key_path""[l+1:u],
             ""value_to_set""
           )
         )
       END
  FROM array_lower(""key_path"", 1) l,
       array_upper(""key_path"", 1) u
$function$;
</code></pre>

<p><strong>Update</strong>: functions are compacted now.</p>
"
"<p>I'm running a server at my office to process some files and report the results to a remote MySQL server.</p>

<p>The files processing takes some time and the process dies halfway through with the following error: </p>

<pre><code>2006, MySQL server has gone away
</code></pre>

<p>I've heard about the MySQL setting, <strong>wait_timeout</strong>, but do I need to change that on the server at my office or the remote MySQL server?</p>
","<p>I've encountered this a number of times and I've normally found the answer to be a very low default setting of <a href=""https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_allowed_packet"" rel=""nofollow noreferrer""><code>max_allowed_packet</code></a>. </p>

<p>Raising it in <code>/etc/my.cnf</code> (under <code>[mysqld]</code>) to 8 or 16M usually fixes it. (The default in MySql 5.7 is <code>4194304</code>, which is 4MB.)</p>

<pre><code>[mysqld]
max_allowed_packet=16M
</code></pre>

<hr>

<p>Note: Just create the line if it does not exist</p>

<p>Note: This can be set on your server as it's running. </p>

<p>Use <code>set global max_allowed_packet=104857600</code>. This sets it to 100MB.</p>
"
"<p>I'm just wondering what is the difference between an <code>RDD</code> and <code>DataFrame</code> <em>(Spark 2.0.0 DataFrame is a mere type alias for <code>Dataset[Row]</code>)</em> in Apache Spark? </p>

<p>Can you convert one to the other?</p>
","<p>If structure is flat:</p>

<pre><code>val df = Seq((1L, ""a"", ""foo"", 3.0)).toDF
df.printSchema
// root
//  |-- _1: long (nullable = false)
//  |-- _2: string (nullable = true)
//  |-- _3: string (nullable = true)
//  |-- _4: double (nullable = false)
</code></pre>

<p>the simplest thing you can do is to use <code>toDF</code> method:</p>

<pre><code>val newNames = Seq(""id"", ""x1"", ""x2"", ""x3"")
val dfRenamed = df.toDF(newNames: _*)

dfRenamed.printSchema
// root
// |-- id: long (nullable = false)
// |-- x1: string (nullable = true)
// |-- x2: string (nullable = true)
// |-- x3: double (nullable = false)
</code></pre>

<p>If you want to rename individual columns you can use either <code>select</code> with <code>alias</code>:</p>

<pre><code>df.select($""_1"".alias(""x1""))
</code></pre>

<p>which can be easily generalized to multiple columns:</p>

<pre><code>val lookup = Map(""_1"" -&gt; ""foo"", ""_3"" -&gt; ""bar"")

df.select(df.columns.map(c =&gt; col(c).as(lookup.getOrElse(c, c))): _*)
</code></pre>

<p>or <code>withColumnRenamed</code>:</p>

<pre><code>df.withColumnRenamed(""_1"", ""x1"")
</code></pre>

<p>which use with <code>foldLeft</code> to rename multiple columns:</p>

<pre><code>lookup.foldLeft(df)((acc, ca) =&gt; acc.withColumnRenamed(ca._1, ca._2))
</code></pre>

<p>With nested structures (<code>structs</code>) one possible option is renaming by selecting a whole structure:</p>

<pre><code>val nested = spark.read.json(sc.parallelize(Seq(
    """"""{""foobar"": {""foo"": {""bar"": {""first"": 1.0, ""second"": 2.0}}}, ""id"": 1}""""""
)))

nested.printSchema
// root
//  |-- foobar: struct (nullable = true)
//  |    |-- foo: struct (nullable = true)
//  |    |    |-- bar: struct (nullable = true)
//  |    |    |    |-- first: double (nullable = true)
//  |    |    |    |-- second: double (nullable = true)
//  |-- id: long (nullable = true)

@transient val foobarRenamed = struct(
  struct(
    struct(
      $""foobar.foo.bar.first"".as(""x""), $""foobar.foo.bar.first"".as(""y"")
    ).alias(""point"")
  ).alias(""location"")
).alias(""record"")

nested.select(foobarRenamed, $""id"").printSchema
// root
//  |-- record: struct (nullable = false)
//  |    |-- location: struct (nullable = false)
//  |    |    |-- point: struct (nullable = false)
//  |    |    |    |-- x: double (nullable = true)
//  |    |    |    |-- y: double (nullable = true)
//  |-- id: long (nullable = true)
</code></pre>

<p>Note that it may affect <code>nullability</code> metadata. Another possibility is to rename by casting:</p>

<pre><code>nested.select($""foobar"".cast(
  ""struct&lt;location:struct&lt;point:struct&lt;x:double,y:double&gt;&gt;&gt;""
).alias(""record"")).printSchema

// root
//  |-- record: struct (nullable = true)
//  |    |-- location: struct (nullable = true)
//  |    |    |-- point: struct (nullable = true)
//  |    |    |    |-- x: double (nullable = true)
//  |    |    |    |-- y: double (nullable = true)
</code></pre>

<p>or:</p>

<pre><code>import org.apache.spark.sql.types._

nested.select($""foobar"".cast(
  StructType(Seq(
    StructField(""location"", StructType(Seq(
      StructField(""point"", StructType(Seq(
        StructField(""x"", DoubleType), StructField(""y"", DoubleType)))))))))
).alias(""record"")).printSchema

// root
//  |-- record: struct (nullable = true)
//  |    |-- location: struct (nullable = true)
//  |    |    |-- point: struct (nullable = true)
//  |    |    |    |-- x: double (nullable = true)
//  |    |    |    |-- y: double (nullable = true)
</code></pre>
"
"<p>What does it mean for an SqlConnection to be ""enlisted"" in a transaction?  Does it simply mean that commands I execute on the connection will participate in the transaction?</p>

<p>If so, under what circumstances is an SqlConnection <em>automatically</em> enlisted in an ambient TransactionScope Transaction?</p>

<p>See questions in code comments.  My guess to each question's answer follows each question in parenthesis.</p>

<h2>Scenario 1: Opening connections INSIDE a transaction scope</h2>

<pre><code>using (TransactionScope scope = new TransactionScope())
using (SqlConnection conn = ConnectToDB())
{   
    // Q1: Is connection automatically enlisted in transaction? (Yes?)
    //
    // Q2: If I open (and run commands on) a second connection now,
    // with an identical connection string,
    // what, if any, is the relationship of this second connection to the first?
    //
    // Q3: Will this second connection's automatic enlistment
    // in the current transaction scope cause the transaction to be
    // escalated to a distributed transaction? (Yes?)
}
</code></pre>

<h2>Scenario 2: Using connections INSIDE a transaction scope that were opened OUTSIDE of it</h2>

<pre><code>//Assume no ambient transaction active now
SqlConnection new_or_existing_connection = ConnectToDB(); //or passed in as method parameter
using (TransactionScope scope = new TransactionScope())
{
    // Connection was opened before transaction scope was created
    // Q4: If I start executing commands on the connection now,
    // will it automatically become enlisted in the current transaction scope? (No?)
    //
    // Q5: If not enlisted, will commands I execute on the connection now
    // participate in the ambient transaction? (No?)
    //
    // Q6: If commands on this connection are
    // not participating in the current transaction, will they be committed
    // even if rollback the current transaction scope? (Yes?)
    //
    // If my thoughts are correct, all of the above is disturbing,
    // because it would look like I'm executing commands
    // in a transaction scope, when in fact I'm not at all, 
    // until I do the following...
    //
    // Now enlisting existing connection in current transaction
    conn.EnlistTransaction( Transaction.Current );
    //
    // Q7: Does the above method explicitly enlist the pre-existing connection
    // in the current ambient transaction, so that commands I
    // execute on the connection now participate in the
    // ambient transaction? (Yes?)
    //
    // Q8: If the existing connection was already enlisted in a transaction
    // when I called the above method, what would happen?  Might an error be thrown? (Probably?)
    //
    // Q9: If the existing connection was already enlisted in a transaction
    // and I did NOT call the above method to enlist it, would any commands
    // I execute on it participate in it's existing transaction rather than
    // the current transaction scope. (Yes?)
}
</code></pre>
","<p>I've done some tests since asking this question and found most if not all answers on my own, since no one else replied.  Please let me know if I've missed anything.</p>

<p><strong>Q1.</strong> Yes, unless ""enlist=false"" is specified in the connection string.  The connection pool finds a usable connection.  A usable connection is one that's not enlisted in a transaction or one that's enlisted in the same transaction.</p>

<p><strong>Q2.</strong> The second connection is an independent connection, which participates in the same transaction.  I'm not sure about the interaction of commands on these two connections, since they're running against the same database, but I think errors can occur if commands are issued on both at the same time: errors like <a href=""https://stackoverflow.com/questions/2858750/what-is-the-reason-of-transaction-context-in-use-by-another-session/2885059#2885059"">""Transaction context in use by another session""</a></p>

<p><strong>Q3.</strong> Yes, it gets escalated to a distributed transaction, so enlisting more than one connection, even with the same connection string, causes it to become a distributed transaction, which can be confirmed by checking for a non-null GUID at Transaction.Current.TransactionInformation.DistributedIdentifier.
*Update: I read somewhere that this is fixed in SQL Server 2008, so that MSDTC is not used when the same connection string is used for both connections (as long as both connections are not open at the same time).  That allows you to open a connection and close it multiple times within a transaction, which could make better use of the connection pool by opening connections as late as possible and closing them as soon as possible.</p>

<p><strong>Q4.</strong> No. A connection opened when no transaction scope was active, will not be automatically enlisted in a newly created transaction scope.</p>

<p><strong>Q5.</strong> No. Unless you open a connection in the transaction scope, or enlist an existing connection in the scope, there basically is NO TRANSACTION.  Your connection must be automatically or manually enlisted in the transaction scope in order for your commands to participate in the transaction.</p>

<p><strong>Q6.</strong> Yes, commands on a connection not participating in a transaction are committed as issued, even though the code happens to have executed in a transaction scope block that got rolled back.  If the connection is not enlisted in the current transaction scope, it's not participating in the transaction, so committing or rolling back the transaction will have no effect on commands issued on a connection not enlisted in the transaction scope... as <a href=""https://stackoverflow.com/questions/1707566/data-committed-even-though-system-transactions-transactionscope-commit-not-call"">this guy found out</a>.  That's a very hard one to spot unless you understand the automatic enlistment process: it occurs only when a connection is opened <em>inside</em> an active transaction scope.</p>

<p><strong>Q7.</strong> Yes.  An existing connection can be explicitly enlisted in the current transaction scope by calling EnlistTransaction( Transaction.Current ).  You can also enlist a connection on a separate thread in the transaction by using a DependentTransaction, but like before, I'm not sure how two connections involved in the same transaction against the same database may interact... and errors may occur, and of course the second enlisted connection causes the transaction to escalate to a distributed transaction.</p>

<p><strong>Q8.</strong> An error may be thrown.  If TransactionScopeOption.Required was used, and the connection was already enlisted in a transaction scope transaction, then there is no error; in fact, there's no new transaction created for the scope, and the transaction count (@@trancount) does not increase.  If, however, you use TransactionScopeOption.RequiresNew, then you get a helpful error message upon attempting to enlist the connection in the new transaction scope transaction: ""Connection currently has transaction enlisted.  Finish current transaction and retry.""  And yes, if you complete the transaction the connection is enlisted in, you can safely enlist the connection in a new transaction.
<em>Update: If you previously called BeginTransaction on the connection, a slightly different error is thrown when you try to enlist in a new transaction scope transaction: ""Cannot enlist in the transaction because a local transaction is in progress on the connection.  Finish local transaction and retry.""  On the other hand, you can safely call BeginTransaction on the SqlConnection while its enlisted in a transaction scope transaction, and that will actually increase @@trancount by one, unlike using the Required option of a nested transaction scope, which does not cause it to increase.  Interestingly, if you then go on to create another nested transaction scope with the Required option, you will not get an error, because nothing changes as a result of already having an active transaction scope transaction (remember @@trancount is not increased when a transaction scope transaction is already active and the Required option is used).</em></p>

<p><strong>Q9.</strong> Yes. Commands participate in whatever transaction the connection is enlisted in, regardless of what the active transaction scope is in the C# code.</p>
"
"<p>We have an application running locally where we're experiencing the following error:</p>

<blockquote>
  <p>ORA-12514: TNS:listener does not currently know of service requested
  in connect descriptor</p>
</blockquote>

<p>I've tested the connection using <code>TNSPing</code> which resolved correctly and
I tried <code>SQLPlus</code> to try connecting, which failed with the same error as above. I used this syntax for <code>SQLPlus</code>:</p>

<pre><code>sqlplus username/password@addressname[or host name]
</code></pre>

<p>We have verified that:</p>

<ul>
<li>the TNS Listener on the server is running. </li>
<li>Oracle itself on the server is running. </li>
</ul>

<p>We don't know of any changes that were made to this environment. 
Anything else we can test?</p>
","<p>I had this issue and the fix was to make sure in <code>tnsnames.ora</code> the <code>SERVICE_NAME</code> is a valid service name in your database. To find out valid service names, you can use the following query in oracle:</p>

<pre><code>select value from v$parameter where name='service_names'
</code></pre>

<p>Once I updated <code>tnsnames.ora</code> to:</p>

<pre><code>TEST =
   (DESCRIPTION =
    (ADDRESS_LIST =
      (ADDRESS = (PROTOCOL = TCP)(HOST = *&lt;validhost&gt;*)(PORT = *&lt;validport&gt;*))
    )
    (CONNECT_DATA =
      (SERVER = DEDICATED)
      (SERVICE_NAME = *&lt;servicenamefromDB&gt;*)
    )
)
</code></pre>

<p>then I ran:</p>

<pre><code>sqlplus user@TEST
</code></pre>

<p>Success! 
The listener is basically telling you that whatever service_name you are using isn't a valid service according to the DB. </p>

<p>(*I was running sqlplus from Win7 client workstation to remote DB and blame the DBAs ;) *)</p>
"
"<p>I have two columns in table users namely <code>registerDate and lastVisitDate</code> which consist of datetime data type. I would like to do the following.</p>

<ol>
<li>Set registerDate defaults value to MySQL NOW()</li>
<li>Set lastVisitDate default value to <code>0000-00-00 00:00:00</code> Instead of null which it uses by default.</li>
</ol>

<p>Because the table already exists and has existing records, I would like to use Modify table.  I've tried using the two piece of code below, but neither works.  </p>

<pre><code>ALTER TABLE users MODIFY registerDate datetime DEFAULT NOW()
ALTER TABLE users MODIFY registerDate datetime DEFAULT CURRENT_TIMESTAMP;
</code></pre>

<p>It gives me Error : <code>ERROR 1067 (42000): Invalid default value for 'registerDate'</code></p>

<p>Is it possible for me to set the default datetime value to NOW() in MySQL?</p>
","<p>As of MySQL 5.6.5, you can use the <code>DATETIME</code> type with a dynamic default value:</p>

<pre><code>CREATE TABLE foo (
    creation_time      DATETIME DEFAULT   CURRENT_TIMESTAMP,
    modification_time  DATETIME ON UPDATE CURRENT_TIMESTAMP
)
</code></pre>

<p>Or even combine both rules:</p>

<pre><code>modification_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
</code></pre>

<p>Reference:<br>
<a href=""http://dev.mysql.com/doc/refman/5.7/en/timestamp-initialization.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.7/en/timestamp-initialization.html</a><br>
<a href=""http://optimize-this.blogspot.com/2012/04/datetime-default-now-finally-available.html"" rel=""noreferrer"">http://optimize-this.blogspot.com/2012/04/datetime-default-now-finally-available.html</a></p>

<p>Prior to 5.6.5, you need to use the <code>TIMESTAMP</code> data type, which automatically updates whenever the record is modified. Unfortunately, however, only one auto-updated <code>TIMESTAMP</code> field can exist per table.</p>

<pre><code>CREATE TABLE mytable (
  mydate TIMESTAMP
)
</code></pre>

<p>See: <a href=""http://dev.mysql.com/doc/refman/5.1/en/create-table.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.1/en/create-table.html</a></p>

<p>If you want to prevent MySQL from updating the timestamp value on <code>UPDATE</code> (so that it only triggers on <code>INSERT</code>) you can change the definition to:</p>

<pre><code>CREATE TABLE mytable (
  mydate TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
"
"<p>Why there can be only one TIMESTAMP column with CURRENT_TIMESTAMP in DEFAULT or ON UPDATE clause?</p>

<pre><code>CREATE TABLE `foo` (
  `ProductID` INT(10) UNSIGNED NOT NULL,
  `AddedDate` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
  `UpdatedDate` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
) ENGINE=INNODB;
</code></pre>

<p>The error that results:</p>

<blockquote>
  <p>Error Code : 1293</p>
  
  <p>Incorrect table definition; there can
  be only one TIMESTAMP column with
  CURRENT_TIMESTAMP in DEFAULT or ON
  UPDATE clause</p>
</blockquote>
","<p>This limitation, which was only due to historical, code legacy reasons, has been lifted in recent versions of MySQL:</p>

<blockquote>
  <p>Changes in MySQL 5.6.5 (2012-04-10, Milestone 8) </p>
  
  <p>Previously, at most one TIMESTAMP column per table could be
  automatically initialized or updated to the current date and time.
  This restriction has been lifted. Any TIMESTAMP column definition can
  have any combination of DEFAULT CURRENT_TIMESTAMP and ON UPDATE
  CURRENT_TIMESTAMP clauses. In addition, these clauses now can be used
  with DATETIME column definitions. For more information, see Automatic
  Initialization and Updating for TIMESTAMP and DATETIME.</p>
</blockquote>

<p><a href=""http://dev.mysql.com/doc/relnotes/mysql/5.6/en/news-5-6-5.html"" rel=""noreferrer"">http://dev.mysql.com/doc/relnotes/mysql/5.6/en/news-5-6-5.html</a></p>
"
"<p>How do I list all extensions that are already installed in a database or schema from psql?</p>

<p>See also</p>

<ul>
<li><a href=""https://dba.stackexchange.com/questions/166283/where-can-i-find-a-list-of-the-extensions-for-postgresql"">Finding a list of available extensions that PostgreSQL ships with</a></li>
</ul>
","<p>In psql that would be</p>

<pre><code>\dx
</code></pre>

<p>See the manual for details: <a href=""http://www.postgresql.org/docs/current/static/app-psql.html"" rel=""noreferrer"">http://www.postgresql.org/docs/current/static/app-psql.html</a></p>

<p>Doing it in plain SQL it would be a select on <code>pg_extension</code>: </p>

<pre><code>SELECT * 
FROM pg_extension
</code></pre>

<p><a href=""http://www.postgresql.org/docs/current/static/catalog-pg-extension.html"" rel=""noreferrer"">http://www.postgresql.org/docs/current/static/catalog-pg-extension.html</a></p>
"
"<p>The query I'm running is as follows, however I'm getting this error:</p>

<blockquote>
  <p>#1054 - Unknown column 'guaranteed_postcode' in 'IN/ALL/ANY subquery'</p>
</blockquote>

<pre><code>SELECT `users`.`first_name`, `users`.`last_name`, `users`.`email`,
SUBSTRING(`locations`.`raw`,-6,4) AS `guaranteed_postcode`
FROM `users` LEFT OUTER JOIN `locations`
ON `users`.`id` = `locations`.`user_id`
WHERE `guaranteed_postcode` NOT IN #this is where the fake col is being used
(
 SELECT `postcode` FROM `postcodes` WHERE `region` IN
 (
  'australia'
 )
)
</code></pre>

<p>My question is: why am I unable to use a fake column in the where clause of the same DB query?</p>
","<p>You can only use column aliases in GROUP BY, ORDER BY, or HAVING clauses.</p>

<blockquote>
  <p>Standard SQL doesn't allow you to
  refer to a column alias in a WHERE
  clause. This restriction is imposed
  because when the WHERE code is
  executed, the column value may not yet
  be determined.</p>
</blockquote>

<p>Copied from <a href=""https://dev.mysql.com/doc/refman/8.0/en/problems-with-alias.html"" rel=""noreferrer"">MySQL documentation</a></p>

<p>As pointed in the comments, using HAVING instead may do the work. Make sure to give a read at this <a href=""https://stackoverflow.com/questions/2905292/where-vs-having/18710763#18710763"">WHERE vs HAVING</a> though.</p>
"
"<p>How do you execute raw SQL in SQLAlchemy?</p>

<p>I have a python web app that runs on flask and interfaces to the database through SQLAlchemy. </p>

<p>I need a way to run the raw SQL. The query involves multiple table joins along with Inline views. </p>

<p>I've tried:</p>

<pre><code>connection = db.session.connection()
connection.execute( &lt;sql here&gt; )
</code></pre>

<p>But I keep getting gateway errors.</p>
","<p>There are several ways to <code>UPDATE</code> using <code>sqlalchemy</code></p>

<pre><code>1) user.no_of_logins += 1
   session.commit()

2) session.query().\
       filter(User.username == form.username.data).\
       update({""no_of_logins"": (User.no_of_logins +1)})
   session.commit()

3) conn = engine.connect()
   stmt = User.update().\
       values(no_of_logins=(User.no_of_logins + 1)).\
       where(User.username == form.username.data)
   conn.execute(stmt)

4) setattr(user, 'no_of_logins', user.no_of_logins+1)
   session.commit()
</code></pre>
"
"<p>I have the following table schema which maps user_customers to permissions on a live MySQL database:</p>

<pre><code>mysql&gt; describe user_customer_permission;
+------------------+---------+------+-----+---------+----------------+
| Field            | Type    | Null | Key | Default | Extra          |
+------------------+---------+------+-----+---------+----------------+
| id               | int(11) | NO   | PRI | NULL    | auto_increment |
| user_customer_id | int(11) | NO   | PRI | NULL    |                |
| permission_id    | int(11) | NO   | PRI | NULL    |                |
+------------------+---------+------+-----+---------+----------------+
3 rows in set (0.00 sec)
</code></pre>

<p>I would like to remove the primary keys for user_customer_id and permission_id and retain the primary key for id.</p>

<p>When I run the command:</p>

<pre><code>alter table user_customer_permission drop primary key;
</code></pre>

<p>I get the following error:</p>

<pre><code>ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key
</code></pre>

<p>How can I drop a column's primary key?</p>
","<p>Without an index, maintaining an autoincrement column becomes too expensive, that's why <code>MySQL</code> requires an autoincrement column to be a leftmost part of an index.</p>

<p>You should remove the autoincrement property before dropping the key:</p>

<pre><code>ALTER TABLE user_customer_permission MODIFY id INT NOT NULL;
ALTER TABLE user_customer_permission DROP PRIMARY KEY;
</code></pre>

<p>Note that you have a composite <code>PRIMARY KEY</code> which covers all three columns and <code>id</code> is not guaranteed to be unique.</p>

<p>If it happens to be unique, you can make it to be a <code>PRIMARY KEY</code> and <code>AUTO_INCREMENT</code> again:</p>

<pre><code>ALTER TABLE user_customer_permission MODIFY id INT NOT NULL PRIMARY KEY AUTO_INCREMENT;
</code></pre>
"
"<p>How do I declare a variable for use in a PostgreSQL 8.3 query?</p>

<p>In MS SQL Server I can do this:</p>

<pre><code>DECLARE @myvar INT
SET @myvar = 5

SELECT *
FROM somewhere
WHERE something = @myvar
</code></pre>

<p>How do I do the same in PostgreSQL? According to the documentation variables are declared simply as ""name type;"", but this gives me a syntax error:</p>

<pre><code>myvar INTEGER;
</code></pre>

<p>Could someone give me an example of the correct syntax?</p>
","<p>I accomplished the same goal by using a <a href=""https://www.postgresql.org/docs/current/static/queries-with.html"" rel=""noreferrer""><code>WITH</code> clause</a>, it's nowhere near as elegant but can do the same thing. Though for this example it's really overkill. I also don't particularly recommend this.</p>

<pre><code>WITH myconstants (var1, var2) as (
   values (5, 'foo')
)
SELECT *
FROM somewhere, myconstants
WHERE something = var1
   OR something_else = var2;
</code></pre>
"
"<p>I have a MS SQL 2005 database with a table <code>Test</code> with column <code>ID</code>. <code>ID</code> is an identity column. </p>

<p>I have rows in this table and all of them have their corresponding ID auto incremented value.</p>

<p>Now I would like to change every ID in this table like this:</p>

<p><code>ID = ID + 1</code></p>

<p>But when I do this I get an error:</p>

<blockquote>
  <p>Cannot update identity column 'ID'.</p>
</blockquote>

<p>I've tried this:</p>

<pre><code>    ALTER TABLE Test NOCHECK CONSTRAINT ALL 
    set identity_insert ID ON
</code></pre>

<p>But this does not solve the problem.</p>

<p>I need to have identity set to this column, but I need to change values as well from time to time. So my question is how to accomplish this task.</p>
","<p>You need to </p>

<pre><code>set identity_insert YourTable ON
</code></pre>

<p>Then delete your row and reinsert it with different identity.</p>

<p>Once you have done the insert don't forget to turn identity_insert off</p>

<pre><code>set identity_insert YourTable OFF
</code></pre>
"
"<p>The following code gives an error - ""No implicit conversion from DBnull to int.""</p>

<pre><code>SqlParameter[] parameters = new SqlParameter[1];    
SqlParameter planIndexParameter = new SqlParameter(""@AgeIndex"", SqlDbType.Int);
planIndexParameter.Value = (AgeItem.AgeIndex== null) ? DBNull.Value : AgeItem.AgeIndex;
parameters[0] = planIndexParameter;
</code></pre>
","<p>The problem is that the <code>?:</code> operator cannot determine the return type because you are either returning an <code>int</code> value or a DBNull type value, which are not compatible.</p>

<p>You can of course cast the instance of AgeIndex to be type <code>object</code> which would satisfy the <code>?:</code> requirement.</p>

<p>You can use the <code>??</code> null-coalescing operator as follows</p>

<pre><code>SqlParameter[] parameters = new SqlParameter[1];     
SqlParameter planIndexParameter = new SqlParameter(""@AgeIndex"", SqlDbType.Int);
planIndexParameter.Value = (object)AgeItem.AgeIndex ?? DBNull.Value;
parameters[0] = planIndexParameter; 
</code></pre>

<p>Here is a quote from the <a href=""http://msdn.microsoft.com/en-us/library/ty67wk28.aspx"" rel=""noreferrer"">MSDN documentation</a> for the <code>?:</code> operator that explains the problem</p>

<blockquote>
  <p>Either the type of first_expression and second_expression must be the same, or an implicit conversion must exist from one type to the other.</p>
</blockquote>
"
"<p>I am trying to import a .sql file and its failing on creating tables.</p>

<p>Here's the query that fails:</p>

<pre><code>CREATE TABLE `data` (
`id` int(10) unsigned NOT NULL,
`name` varchar(100) NOT NULL,
`value` varchar(15) NOT NULL,
UNIQUE KEY `id` (`id`,`name`),
CONSTRAINT `data_ibfk_1` FOREIGN KEY (`id`) REFERENCES `keywords` (`id`) ON DELETE CASCADE ON UPDATE CASCADE
) ENGINE=InnoDB DEFAULT CHARSET=latin1;    
</code></pre>

<p>I exported the .sql from the the same database, I dropped all the tables and now im trying to import it, why is it failing?</p>

<blockquote>
  <p>MySQL: Can't create table './dbname/data.frm' (errno: 150)</p>
</blockquote>
","<p>I had the same problem with <code>ALTER TABLE ADD FOREIGN KEY</code>.</p>

<p>After an hour, I found that these conditions must be satisfied to not get error 150:</p>

<ol>
<li><p>The Parent table must exist before you define a foreign key to reference it. You must define the tables in the right order: Parent table first, then the Child table. If both tables references each other, you must create one table without FK constraints, then create the second table, then add the FK constraint to the first table with <code>ALTER TABLE</code>. </p></li>
<li><p>The two tables must both support foreign key constraints, i.e. <code>ENGINE=InnoDB</code>. Other storage engines silently ignore foreign key definitions, so they return no error or warning, but the FK constraint is not saved.</p></li>
<li><p>The referenced columns in the Parent table must be the left-most columns of a key. Best if the key in the Parent is <code>PRIMARY KEY</code> or <code>UNIQUE KEY</code>.</p></li>
<li><p>The FK definition must reference the PK column(s) in the same order as the PK definition. For example, if the FK <code>REFERENCES Parent(a,b,c)</code> then the Parent's PK must not be defined on columns in order <code>(a,c,b)</code>.</p></li>
<li><p>The PK column(s) in the Parent table must be the same data type as the FK column(s) in the Child table. For example, if a PK column in the Parent table is <code>UNSIGNED</code>, be sure to define <code>UNSIGNED</code> for the corresponding column in the Child table field. </p>

<p>Exception: length of strings may be different. For example, <code>VARCHAR(10)</code> can reference <code>VARCHAR(20)</code> or vice versa.</p></li>
<li><p>Any string-type FK column(s) must have the same character set and collation as the corresponding PK column(s).</p></li>
<li><p>If there is data already in the Child table, every value in the FK column(s) must match a value in the Parent table PK column(s). Check this with a query like:</p>

<pre><code>SELECT COUNT(*) FROM Child LEFT OUTER JOIN Parent ON Child.FK = Parent.PK 
WHERE Parent.PK IS NULL;
</code></pre>

<p>This must return zero (0) unmatched values. Obviously, this query is an generic example; you must substitute your table names and column names.</p></li>
<li><p>Neither the Parent table nor the Child table can be a <code>TEMPORARY</code> table.</p></li>
<li><p>Neither the Parent table nor the Child table can be a <code>PARTITIONED</code> table.</p></li>
<li><p>If you declare a FK with the <code>ON DELETE SET NULL</code> option, then the FK column(s) must be nullable.</p></li>
<li><p>If you declare a constraint name for a foreign key, the constraint name must be unique in the whole schema, not only in the table in which the constraint is defined. Two tables may not have their own constraint with the same name.</p></li>
</ol>

<p>Hope this helps.</p>
"
"<p>I would like to give an user all the permissions on a database without making it an admin.
The reason why I want to do that is that at the moment DEV and PROD are different DBs on the same cluster so I don't want a user to be able to change production objects but it must be able to change objects on DEV.</p>

<p>I tried:</p>

<pre><code>grant ALL on database MY_DB to group MY_GROUP;
</code></pre>

<p>but it doesn't seem to give any permission.</p>

<p>Then I tried:</p>

<pre><code>grant all privileges on schema MY_SCHEMA to group MY_GROUP;
</code></pre>

<p>and it seems to give me permission to create objects but not to query\delete objects on that schema that belong to other users</p>

<p>I could go on by giving USAGE permission to the user on MY_SCHEMA but then it would complain about not having permissions on the table ...</p>

<p>So I guess my question is: is there any easy way of giving all the permissions to a user on a DB?</p>

<p>I'm working on PostgreSQL 8.1.23.</p>
","<p>The user needs access to the <em>database</em>, obviously:</p>

<pre><code>GRANT CONNECT ON DATABASE my_db TO my_user;
</code></pre>

<p>And (at least) the <code>USAGE</code> privilege on the <em>schema</em>:</p>

<pre><code>GRANT USAGE ON SCHEMA public TO my_user;
</code></pre>

<p>Then, all permissions for all <em>tables</em> (requires Postgres <strong>9.0</strong> or later):</p>

<pre><code>GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO my_user;
</code></pre>

<p>And don't forget <em>sequences</em> (if any):</p>

<pre><code>GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO my_user;
</code></pre>

<p>For <strong>older versions</strong> you could use the ""Grant Wizard"" of pgAdmin III (the default GUI).</p>

<p>More:</p>

<ul>
<li><a href=""https://dba.stackexchange.com/questions/117109/how-to-manage-default-privileges-for-users-on-a-database-vs-schema/117661#117661""><strong>How to manage DEFAULT PRIVILEGES for USERs on a DATABASE vs SCHEMA?</strong></a></li>
<li><a href=""https://stackoverflow.com/questions/24918367/grant-privileges-for-a-particular-database-in-postgresql/24923877#24923877"">Grant privileges for a particular database in PostgreSQL</a></li>
<li><a href=""https://stackoverflow.com/questions/10491208/how-to-grant-all-privileges-on-views-to-arbitrary-user/10491275#10491275"">How to grant all privileges on views to arbitrary user</a></li>
</ul>

<p>But really, you should <a href=""https://www.postgresql.org/support/versioning/"" rel=""noreferrer"">upgrade to a current version</a>.</p>
"
"<p>I have a database schema named: <code>nyummy</code> and a table named <code>cimory</code>:</p>

<pre><code>create table nyummy.cimory (
  id numeric(10,0) not null,
  name character varying(60) not null,
  city character varying(50) not null,
  CONSTRAINT cimory_pkey PRIMARY KEY (id)
);
</code></pre>

<p>I want to export the <code>cimory</code> table's data as insert SQL script file. However, I only want to export records/data where the city is equal to 'tokyo' (assume city data are all lowercase).</p>

<p>How to do it? </p>

<p>It doesn't matter whether the solution is in freeware GUI tools or command line (although GUI tools solution is better). I had tried pgAdmin III, but I can't find an option to do this.</p>
","<p>Create a table with the set you want to export and then use the command line utility pg_dump to export to a file:</p>

<pre><code>create table export_table as 
select id, name, city
from nyummy.cimory
where city = 'tokyo'
</code></pre>

<pre class=""lang-sh prettyprint-override""><code>$ pg_dump --table=export_table --data-only --column-inserts my_database &gt; data.sql
</code></pre>

<p><code>--column-inserts</code> will dump as insert commands with column names.</p>

<p><code>--data-only</code> do not dump schema.</p>

<p>As commented below, creating a view in instead of a table will obviate the table creation whenever a new export is necessary.</p>
"
"<p>I installed mysql server on linux box IP = 192.168.1.100 but when i try to connect to this IP it alway error(111). but use localhost and 127.0.0.1 is OK.</p>

<pre>
beer@beer-laptop# ifconfig | grep ""inet addr""
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet addr:192.168.1.100  Bcast:192.168.1.255  Mask:255.255.255.0

beer@beer-laptop# mysql -ubeer -pbeer -h192.168.1.100
ERROR 2003 (HY000): Can't connect to MySQL server on '192.168.1.100' (111)

beer@beer-laptop# mysql -ubeer -pbeer -hlocalhost
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 160
Server version: 5.1.31-1ubuntu2 (Ubuntu)

Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

mysql> 

beer@beer-laptop# mysql -ubeer -pbeer -h127.0.0.1
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 161
Server version: 5.1.31-1ubuntu2 (Ubuntu)

Type 'help;' or '\h' for help. Type '\c' to clear the buffer.

mysql> 

</pre>

<p>Connect from another machine it also error 111.</p>

<pre>
another@another-laptop# mysql -ubeer -pbeer -h192.168.1.100
ERROR 2003 (HY000): Can't connect to MySQL server on '192.168.1.100' (111)
</pre>

<p>How difference between use localhost/127.0.0.1 and 192.168.1.100 in this case. 
I don't know how to connect to this database from another machine.</p>

<p>Help please.
Thank.</p>
","<p>It probably means that your MySQL server is only listening the localhost interface.</p>

<p>If you have lines like this :</p>

<pre><code>skip-networking
bind-address = 127.0.0.1
</code></pre>

<p>In your <a href=""https://stackoverflow.com/a/2485758/1243247""><code>my.cnf</code> configuration file</a>, you should comment them <em>(add a # at the beginning of the lines)</em>, and restart MySQL.</p>

<pre><code>sudo service mysql restart
</code></pre>

<p>Of course, to do this, you must be the administrator of the server.</p>
"
"<p>In PHP, when accessing MySQL database with PDO with parametrized query, how can you check the final query (after having replaced all tokens)?</p>

<p>Is there a way to check what gets really executed by the database?</p>
","<p>Your EXEC example would NOT be parameterized.  You need parameterized queries (prepared statements in some circles) to prevent input like this from causing damage:</p>

<blockquote>
  <p>';DROP TABLE bar;--</p>
</blockquote>

<p>Try putting that in your fuz variable (or don't, if you value your bar table).  More subtle and damaging queries are possible as well.</p>

<p>Here's an example of how you do parameters with Sql Server:</p>

<pre class=""lang-vb prettyprint-override""><code>Public Function GetBarFooByBaz(ByVal Baz As String) As String
    Dim sql As String = ""SELECT foo FROM bar WHERE baz= @Baz""

    Using cn As New SqlConnection(""Your connection string here""), _
        cmd As New SqlCommand(sql, cn)

        cmd.Parameters.Add(""@Baz"", SqlDbType.VarChar, 50).Value = Baz
        Return cmd.ExecuteScalar().ToString()
    End Using
End Function
</code></pre>

<p>Stored procedures are sometimes credited with preventing SQL injection.  However, most of the time you still have to call them using query parameters or they don't help.  If you use stored procedures <em>exclusively</em>, then you can turn off permissions for SELECT, UPDATE, ALTER, CREATE, DELETE, etc (just about everything but EXEC) for the application user account and get some protection that way.</p>
"
"<p>For several reasons that I don't have the liberty to talk about, we are defining a view on our Sql Server 2005 database like so:</p>

<pre><code>CREATE VIEW [dbo].[MeterProvingStatisticsPoint]
AS
SELECT
    CAST(0 AS BIGINT) AS 'RowNumber',
    CAST(0 AS BIGINT) AS 'ProverTicketId',
    CAST(0 AS INT) AS 'ReportNumber',
    GETDATE() AS 'CompletedDateTime',
    CAST(1.1 AS float) AS 'MeterFactor',
    CAST(1.1 AS float) AS 'Density',
    CAST(1.1 AS float) AS 'FlowRate',
    CAST(1.1 AS float) AS 'Average',
    CAST(1.1 AS float) AS 'StandardDeviation',
    CAST(1.1 AS float) AS 'MeanPlus2XStandardDeviation',
    CAST(1.1 AS float) AS 'MeanMinus2XStandardDeviation'
WHERE 0 = 1
</code></pre>

<p>The idea is that the Entity Framework will create an entity based on this query, which it does, but it generates it with an error that states the following:</p>

<blockquote>
  <p>Warning 6002: The table/view 'Keystone_Local.dbo.MeterProvingStatisticsPoint' does not have a primary key defined. The key has been inferred and the definition was created as a read-only table/view.</p>
</blockquote>

<p>And it decides that the CompletedDateTime field will be this entity primary key. </p>

<p>We are using EdmGen to generate the model. Is there a way not to have the entity framework include any field of this view as a primary key?</p>
","<p>We had the same problem and this is the solution:</p>

<p>To force entity framework to use a column as a primary key, use ISNULL.</p>

<p>To force entity framework not to use a column as a primary key, use NULLIF.</p>

<p>An easy way to apply this is to wrap the select statement of your view in another select.</p>

<p>Example:</p>

<pre><code>SELECT
  ISNULL(MyPrimaryID,-999) MyPrimaryID,
  NULLIF(AnotherProperty,'') AnotherProperty
  FROM ( ... ) AS temp
</code></pre>
"
"<p>I have an SDF file and I would like to retrieve its schema and query it with some UI. How can I do this? I have no Visual Studio installed on the machine and I would like to install as little software as possible.</p>
","<p>To alias the table you'd have to say:</p>

<pre><code>DELETE f FROM dbo.foods AS f WHERE f.name IN (...);
</code></pre>

<p>I fail to see the point of aliasing for this specific <code>DELETE</code> statement, especially since (at least IIRC) this no longer conforms to strict ANSI. But yes, as comments suggest, it may be necessary for other query forms (eg correlation).</p>
"
"<p>I regularly need to delete all the data from my PostgreSQL database before a rebuild. How would I do this directly in SQL?</p>

<p>At the moment I've managed to come up with a SQL statement that returns all the commands I need to execute: </p>

<pre><code>SELECT 'TRUNCATE TABLE ' ||  tablename || ';' FROM pg_tables WHERE tableowner='MYUSER';
</code></pre>

<p>But I can't see a way to execute them programmatically once I have them.</p>
","<p>FrustratedWithFormsDesigner is correct, PL/pgSQL can do this. Here's the script:</p>

<pre><code>CREATE OR REPLACE FUNCTION truncate_tables(username IN VARCHAR) RETURNS void AS $$
DECLARE
    statements CURSOR FOR
        SELECT tablename FROM pg_tables
        WHERE tableowner = username AND schemaname = 'public';
BEGIN
    FOR stmt IN statements LOOP
        EXECUTE 'TRUNCATE TABLE ' || quote_ident(stmt.tablename) || ' CASCADE;';
    END LOOP;
END;
$$ LANGUAGE plpgsql;
</code></pre>

<p>This creates a stored function (you need to do this just once) which you can afterwards use like this:</p>

<pre><code>SELECT truncate_tables('MYUSER');
</code></pre>
"
"<p>When doing:</p>

<pre><code>DELETE FROM `jobs` WHERE `job_id` =1 LIMIT 1 
</code></pre>

<p>It errors:</p>

<pre><code>#1451 - Cannot delete or update a parent row: a foreign key constraint fails 
(paymesomething.advertisers, CONSTRAINT advertisers_ibfk_1 FOREIGN KEY 
(advertiser_id) REFERENCES jobs (advertiser_id))
</code></pre>

<p>Here are my tables:</p>

<pre><code>CREATE TABLE IF NOT EXISTS `advertisers` (
  `advertiser_id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL,
  `password` char(32) NOT NULL,
  `email` varchar(128) NOT NULL,
  `address` varchar(255) NOT NULL,
  `phone` varchar(255) NOT NULL,
  `fax` varchar(255) NOT NULL,
  `session_token` char(30) NOT NULL,
  PRIMARY KEY (`advertiser_id`),
  UNIQUE KEY `email` (`email`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8 AUTO_INCREMENT=2 ;


INSERT INTO `advertisers` (`advertiser_id`, `name`, `password`, `email`, `address`, `phone`, `fax`, `session_token`) VALUES
(1, 'TEST COMPANY', '', '', '', '', '', '');

CREATE TABLE IF NOT EXISTS `jobs` (
  `job_id` int(11) unsigned NOT NULL AUTO_INCREMENT,
  `advertiser_id` int(11) unsigned NOT NULL,
  `name` varchar(255) NOT NULL,
  `shortdesc` varchar(255) NOT NULL,
  `longdesc` text NOT NULL,
  `address` varchar(255) NOT NULL,
  `time_added` int(11) NOT NULL,
  `active` tinyint(1) NOT NULL,
  `moderated` tinyint(1) NOT NULL,
  PRIMARY KEY (`job_id`),
  KEY `advertiser_id` (`advertiser_id`,`active`,`moderated`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8 AUTO_INCREMENT=2 ;


INSERT INTO `jobs` (`job_id`, `advertiser_id`, `name`, `shortdesc`, `longdesc`, `address`, `active`, `moderated`) VALUES
(1, 1, 'TEST', 'TESTTEST', 'TESTTESTES', '', 0, 0);

ALTER TABLE `advertisers`
  ADD CONSTRAINT `advertisers_ibfk_1` FOREIGN KEY (`advertiser_id`) REFERENCES `jobs` (`advertiser_id`);
</code></pre>
","<p>The simple way would be to disable the foreign key check; make the changes then re-enable foreign key check.</p>

<pre><code>SET FOREIGN_KEY_CHECKS=0; -- to disable them
SET FOREIGN_KEY_CHECKS=1; -- to re-enable them
</code></pre>
"
"<p>I have a simple SQL query in PostgreSQL 8.3 that grabs a bunch of comments. I provide a <em>sorted</em> list of values to the <code>IN</code> construct in the <code>WHERE</code> clause:</p>

<pre><code>SELECT * FROM comments WHERE (comments.id IN (1,3,2,4));
</code></pre>

<p>This returns comments in an arbitrary order which in my happens to be ids like <code>1,2,3,4</code>.</p>

<p>I want the resulting rows sorted like the list in the <code>IN</code> construct: <code>(1,3,2,4)</code>.<br>
How to achieve that?</p>
","<p><code>EXISTS</code> will be faster because once the engine has found a hit, it will quit looking as the condition has proved true.<br>
With <code>IN</code> it will collect all the results from the sub-query before further processing.</p>
"
"<p>The following is the simplest possible example, though any solution should be able to scale to however many n top results are needed:</p>

<p>Given a table like that below, with person, group, and age columns, how would you <strong>get the 2 oldest people in each group?</strong> (Ties within groups should not yield more results, but give the first 2 in alphabetical order)</p>

<pre>
+--------+-------+-----+
| Person | Group | Age |
+--------+-------+-----+
| Bob    | 1     | 32  |
| Jill   | 1     | 34  |
| Shawn  | 1     | 42  |
| Jake   | 2     | 29  |
| Paul   | 2     | 36  |
| Laura  | 2     | 39  |
+--------+-------+-----+
</pre>

<p>Desired result set:  </p>

<pre>
+--------+-------+-----+
| Shawn  | 1     | 42  |
| Jill   | 1     | 34  |
| Laura  | 2     | 39  |
| Paul   | 2     | 36  |
+--------+-------+-----+
</pre>

<hr>

<p><strong>NOTE:</strong> This question builds on a previous one- <a href=""https://stackoverflow.com/q/12102200/165673"">Get records with max value for each group of grouped SQL results</a> - for getting a single top row from each group, and which received a great MySQL-specific answer from @Bohemian:</p>

<pre><code>select * 
from (select * from mytable order by `Group`, Age desc, Person) x
group by `Group`
</code></pre>

<p>Would love to be able to build off this, though I don't see how.</p>
","<p>Here is one way to do this, using <code>UNION ALL</code> (See <a href=""http://sqlfiddle.com/#!2/4c0a5/18"" rel=""noreferrer"">SQL Fiddle with Demo</a>). This works with two groups, if you have more than two groups, then you would need to specify the <code>group</code> number and add queries for each <code>group</code>:</p>

<pre><code>(
  select *
  from mytable 
  where `group` = 1
  order by age desc
  LIMIT 2
)
UNION ALL
(
  select *
  from mytable 
  where `group` = 2
  order by age desc
  LIMIT 2
)
</code></pre>

<p>There are a variety of ways to do this, see this article to determine the best route for your situation:</p>

<p><a href=""http://www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per-group-in-sql/"" rel=""noreferrer"">http://www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per-group-in-sql/</a></p>

<p>Edit:</p>

<p>This might work for you too, it generates a row number for each record. Using an example from the link above this will return only those records with a row number of less than or equal to 2:</p>

<pre><code>select person, `group`, age
from 
(
   select person, `group`, age,
      (@num:=if(@group = `group`, @num +1, if(@group := `group`, 1, 1))) row_number 
  from test t
  CROSS JOIN (select @num:=0, @group:=null) c
  order by `Group`, Age desc, person
) as x 
where x.row_number &lt;= 2;
</code></pre>

<p>See <a href=""http://rextester.com/UPG27258"" rel=""noreferrer"">Demo</a></p>
"
"<p>Is there a way to ""limit"" the result with ELOQUENT ORM of Laravel?</p>

<pre><code> SELECT * FROM  `games` LIMIT 30 , 30 
</code></pre>

<p>And with Eloquent ? </p>
","<p>Create a Game model which extends Eloquent and use this:</p>

<pre><code>Game::take(30)-&gt;skip(30)-&gt;get();
</code></pre>

<p><code>take()</code> here will get 30 records and <code>skip()</code> here will offset to 30 records.</p>

<hr>

<p>In recent Laravel versions you can also use:</p>

<pre><code>Game::limit(30)-&gt;offset(30)-&gt;get();
</code></pre>
"
"<p>I have two tables, <code>table1</code> is the parent table with a column <code>ID</code> and <code>table2</code> with a column <code>IDFromTable1</code> (not the actual name) when I put a FK on <code>IDFromTable1</code> to <code>ID</code> in <code>table1</code> I get the error <code>Foreign key constraint is incorrectly formed error</code>. I would like to delete table 2 record if <code>table1</code> record gets deleted. Thanks for any help</p>

<pre><code>ALTER TABLE `table2`  
   ADD CONSTRAINT `FK1` 
      FOREIGN KEY (`IDFromTable1`) REFERENCES `table1` (`ID`) 
      ON UPDATE CASCADE 
      ON DELETE CASCADE;
</code></pre>

<p>Let me know if any other information is needed. I am new to mysql</p>
","<p>I ran into this same problem with HeidiSQL. The error you receive is very cryptic. My problem ended up being that the foreign key column and the referencing column were not of the same type or length.</p>

<p>The foreign key column was <code>SMALLINT(5) UNSIGNED</code> and the referenced column was <code>INT(10) UNSIGNED</code>. Once I made them both the same exact type, the foreign key creation worked perfectly.</p>
"
"<p>I need to dump a <strong>.sql</strong> or <strong>.csv</strong> file into SQLite (I'm using SQLite3 API). I've only found documentation for importing/loading tables, not entire databases. Right now, when I type:</p>

<pre><code>sqlite3prompt&gt; .import FILENAME TABLE 
</code></pre>

<p>I get a syntax error, since it's expecting a table and not an entire DB.</p>
","<p>To import from an SQL file use the following:</p>

<pre><code>sqlite&gt; .read &lt;filename&gt;
</code></pre>

<p>To import from a CSV file you will need to specify the file type and destination table:</p>

<pre><code>sqlite&gt; .mode csv &lt;table&gt;
sqlite&gt; .import &lt;filename&gt; &lt;table&gt;
</code></pre>
"
"<p>I can quite easily dump data into a text file such as:</p>

<pre><code>sqlcmd -S myServer -d myDB -E -Q ""select col1, col2, col3 from SomeTable"" 
     -o ""MyData.txt""
</code></pre>

<p>However, I have looked at the help files for <code>SQLCMD</code> but have not seen an option specifically for CSV. </p>

<p>Is there a way to dump data from a table into a CSV text file using <code>SQLCMD</code>?</p>
","<p>You can run something like this:</p>

<pre><code>sqlcmd -S MyServer -d myDB -E -Q ""select col1, col2, col3 from SomeTable"" 
       -o ""MyData.csv"" -h-1 -s"","" -w 700
</code></pre>

<ul>
<li><code>-h-1</code> removes column name headers from the result<br/></li>
<li><code>-s"",""</code> sets the column seperator to , <br/></li>
<li><code>-w 700</code> sets the row width to 700 chars (this will need to be as wide as the longest row or it will wrap to the next line)</li>
</ul>
"
"<p>I got the following error from a MySQL query.</p>

<p><code>#126 - Incorrect key file for table</code></p>

<p>I have not even declared a key for this table, but I do have indices.  Does anyone know what could be the problem?</p>
","<p>Every Time this has happened, it's been a full disk in my experience.</p>

<p><strong>EDIT</strong></p>

<p>It is also worth noting that this can be caused by a full ramdisk when doing things like altering a large table if you have a ramdisk configured. You can temporarily comment out the ramdisk line to allow such operations if you can't increase the size of it.</p>
"
"<p>I have a table of tags and want to get the highest count tags from the list.</p>

<p>Sample data looks like this</p>

<pre><code>id (1) tag ('night')
id (2) tag ('awesome')
id (3) tag ('night')
</code></pre>

<p>using</p>

<pre><code>SELECT COUNT(*), `Tag` from `images-tags`
GROUP BY `Tag`
</code></pre>

<p>gets me back the data I'm looking for perfectly. However, I would like to organize it, so that the highest tag counts are first, and limit it to only send me the first 20 or so.</p>

<p>I tried this...</p>

<pre><code>SELECT COUNT(id), `Tag` from `images-tags`
GROUP BY `Tag`
ORDER BY COUNT(id) DESC
LIMIT 20
</code></pre>

<p>and I keep getting an ""Invalid use of group function - ErrNr 1111""</p>

<p>What am I doing wrong?</p>

<p>I'm using MySQL 4.1.25-Debian</p>
","<p>In all versions of MySQL, simply alias the aggregate in the SELECT list, and order by the alias:</p>

<pre><code>SELECT COUNT(id) AS theCount, `Tag` from `images-tags`
GROUP BY `Tag`
ORDER BY theCount DESC
LIMIT 20
</code></pre>
"
"<p>I'm getting this strange error while processing a large number of data...</p>

<pre><code>Error Number: 1267

Illegal mix of collations (latin1_swedish_ci,IMPLICIT) and (utf8_general_ci,COERCIBLE) for operation '='

SELECT COUNT(*) as num from keywords WHERE campaignId='12' AND LCASE(keyword)='hello again     '
</code></pre>

<p>What can I do to resolve this? Can I escape the string somehow so this error wouldn't occur, or do I need to change my table encoding somehow, and if so, what should I change it to?</p>
","<pre><code>SET collation_connection = 'utf8_general_ci';
</code></pre>

<p>then for your databases</p>

<pre><code>ALTER DATABASE your_database_name CHARACTER SET utf8 COLLATE utf8_general_ci;

ALTER TABLE your_table_name CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci;
</code></pre>

<p>MySQL sneaks swedish in there sometimes for no sensible reason.</p>
"
"<p>I come from pandas background and am used to reading data from CSV files into a dataframe and then simply changing the column names to something useful using the simple command:</p>

<pre><code>df.columns = new_column_name_list
</code></pre>

<p>However, the same doesn't work in pyspark dataframes created using sqlContext. 
The only solution I could figure out to do this easily is the following:</p>

<pre><code>df = sqlContext.read.format(""com.databricks.spark.csv"").options(header='false', inferschema='true', delimiter='\t').load(""data.txt"")
oldSchema = df.schema
for i,k in enumerate(oldSchema.fields):
  k.name = new_column_name_list[i]
df = sqlContext.read.format(""com.databricks.spark.csv"").options(header='false', delimiter='\t').load(""data.txt"", schema=oldSchema)
</code></pre>

<p>This is basically defining the variable twice and inferring the schema first then renaming the column names and then loading the dataframe again with the updated schema. </p>

<p>Is there a better and more efficient way to do this like we do in pandas ?</p>

<p>My spark version is 1.5.0</p>
","<p>There are many ways to do that:  </p>

<ul>
<li><p>Option 1. Using <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.selectExpr"" rel=""noreferrer"">selectExpr</a>.</p>

<pre><code>data = sqlContext.createDataFrame([(""Alberto"", 2), (""Dakota"", 2)], 
                                  [""Name"", ""askdaosdka""])
data.show()
data.printSchema()

# Output
#+-------+----------+
#|   Name|askdaosdka|
#+-------+----------+
#|Alberto|         2|
#| Dakota|         2|
#+-------+----------+

#root
# |-- Name: string (nullable = true)
# |-- askdaosdka: long (nullable = true)

df = data.selectExpr(""Name as name"", ""askdaosdka as age"")
df.show()
df.printSchema()

# Output
#+-------+---+
#|   name|age|
#+-------+---+
#|Alberto|  2|
#| Dakota|  2|
#+-------+---+

#root
# |-- name: string (nullable = true)
# |-- age: long (nullable = true)
</code></pre></li>
<li><p>Option 2. Using <a href=""https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=selectexpr#pyspark.sql.DataFrame.withColumnRenamed"" rel=""noreferrer"">withColumnRenamed</a>, notice that this method allows you to ""overwrite"" the same column.</p>

<pre><code>oldColumns = data.schema.names
newColumns = [""name"", ""age""]

df = reduce(lambda data, idx: data.withColumnRenamed(oldColumns[idx], newColumns[idx]), xrange(len(oldColumns)), data)
df.printSchema()
df.show()
</code></pre></li>
<li><p>Option 3. using
<a href=""https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=column#pyspark.sql.Column.alias"" rel=""noreferrer"">alias</a>, in Scala you can also use <a href=""http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column"" rel=""noreferrer"">as</a>.</p>

<pre><code>from pyspark.sql.functions import *

data = data.select(col(""Name"").alias(""name""), col(""askdaosdka"").alias(""age""))
data.show()

# Output
#+-------+---+
#|   name|age|
#+-------+---+
#|Alberto|  2|
#| Dakota|  2|
#+-------+---+
</code></pre></li>
<li><p>Option 4. Using <a href=""http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.sql"" rel=""noreferrer"">sqlContext.sql</a>, which lets you use SQL queries on <code>DataFrames</code> registered as tables.</p>

<pre><code>sqlContext.registerDataFrameAsTable(data, ""myTable"")
df2 = sqlContext.sql(""SELECT Name AS name, askdaosdka as age from myTable"")

df2.show()

# Output
#+-------+---+
#|   name|age|
#+-------+---+
#|Alberto|  2|
#| Dakota|  2|
#+-------+---+
</code></pre></li>
</ul>
"
"<p>I have the following UPSERT in PostgreSQL 9.5:</p>

<pre><code>INSERT INTO chats (""user"", ""contact"", ""name"") 
           VALUES ($1, $2, $3), 
                  ($2, $1, NULL) 
ON CONFLICT(""user"", ""contact"") DO NOTHING
RETURNING id;
</code></pre>

<p>If there are no conflicts it returns something like this:</p>



<pre class=""lang-none prettyprint-override""><code>----------
    | id |
----------
  1 | 50 |
----------
  2 | 51 |
----------
</code></pre>

<p>But if there are conflicts it doesn't return any rows:</p>

<pre class=""lang-none prettyprint-override""><code>----------
    | id |
----------
</code></pre>

<p>I want to return the new <code>id</code> columns if there are no conflicts or return the existing <code>id</code> columns of the conflicting columns.<br>
<strong>Can this be done?</strong> If so, <strong>how?</strong></p>
","<p>The <a href=""https://stackoverflow.com/a/37543015/939860"">currently accepted answer</a> seems ok for <em>few</em> conflicts, small tuples and no triggers. And it avoids <strong><em>concurrency issue 1</em></strong> with brute force (see below). The simple solution has its appeal, the side effects may be less important.</p>

<p>For all other cases, though, do <strong><em>not</em></strong> update identical rows without need. Even if you see no difference on the surface, there are <strong>various side effects</strong>:</p>

<ul>
<li><p>It might fire triggers that should not be fired.</p></li>
<li><p>It write-locks ""innocent"" rows, possibly incurring costs for concurrent transactions.</p></li>
<li><p>It might make the row seem new, though it's old (transaction timestamp).</p></li>
<li><p><strong>Most importantly</strong>, with <a href=""https://www.postgresql.org/docs/current/static/mvcc-intro.html"" rel=""noreferrer"">PostgreSQL's MVCC model</a> a new row version is written either way, no matter whether the row data is the same. This incurs a performance penalty for the UPSERT itself, table bloat, index bloat, performance penalty for all subsequent operations on the table, <code>VACUUM</code> cost. A minor effect for few duplicates, but <em>massive</em> for mostly dupes.</p></li>
</ul>

<p>You can achieve (almost) the same without empty updates and side effects.</p>

<h2>Without concurrent write load</h2>

<pre><code>WITH input_rows(usr, contact, name) AS (
   VALUES
      (text 'foo1', text 'bar1', text 'bob1')  -- type casts in first row
    , ('foo2', 'bar2', 'bob2')
    -- more?
   )
, ins AS (
   INSERT INTO chats (usr, contact, name) 
   SELECT * FROM input_rows
   ON CONFLICT (usr, contact) DO NOTHING
   RETURNING id  --, usr, contact              -- return more columns?
   )
SELECT 'i' AS source                           -- 'i' for 'inserted'
     , id  --, usr, contact                    -- return more columns?
FROM   ins
UNION  ALL
SELECT 's' AS source                           -- 's' for 'selected'
     , c.id  --, usr, contact                  -- return more columns?
FROM   input_rows
JOIN   chats c USING (usr, contact);           -- columns of unique index
</code></pre>

<p>The <code>source</code> column is an optional addition to demonstrate how this works. You may actually need it to tell the difference between both cases (another advantage over empty writes).</p>

<p>The final <code>JOIN chats</code> works because newly inserted rows from an attached <a href=""https://www.postgresql.org/docs/current/static/queries-with.html"" rel=""noreferrer"">data-modifying CTE</a> are not yet visible in the underlying table. (All parts of the same SQL statement see the same snapshots of underlying tables.)</p>

<p>Since the <code>VALUES</code> expression is free-standing (not directly attached to an <code>INSERT</code>) Postgres cannot derive data types from the target columns and you may have to add explicit type casts. <a href=""https://www.postgresql.org/docs/current/static/sql-values.html"" rel=""noreferrer"">The manual:</a></p>

<blockquote>
  <p>When <code>VALUES</code> is used in <code>INSERT</code>, the values are all automatically
  coerced to the data type of the corresponding destination column. When
  it's used in other contexts, it might be necessary to specify the
  correct data type. If the entries are all quoted literal constants,
  coercing the first is sufficient to determine the assumed type for all.</p>
</blockquote>

<p>The query itself may be a bit more expensive for <em>few</em> dupes, due to the overhead of the CTE and the additional <code>SELECT</code> (which should be cheap since the perfect index is there by definition - a unique constraint is implemented with an index).</p>

<p>May be (much) faster for <em>many</em> duplicates. The effective cost of additional writes depends on many factors.</p>

<p>But there are <strong>fewer side effects and hidden costs</strong> in any case. It's most probably cheaper overall.</p>

<p>(Attached sequences are still advanced, since default values are filled in <em>before</em> testing for conflicts.)</p>

<p>About CTEs:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/22749253/are-select-type-queries-the-only-type-that-can-be-nested/22750328#22750328"">Are SELECT type queries the only type that can be nested?</a></li>
<li><a href=""https://dba.stackexchange.com/questions/111345/deduplicate-select-statements-in-relational-division/111362#111362"">Deduplicate SELECT statements in relational division</a></li>
</ul>

<h2>With concurrent write load</h2>

<p>Assuming default <code>READ COMMITTED</code> transaction isolation.</p>

<p>Related answer on dba.SE with detailed explanation:</p>

<ul>
<li><a href=""https://dba.stackexchange.com/questions/212580/postgres-concurrent-transactions-resulting-in-race-condition-unique-foreign-key"">Concurrent transactions result in race condition with unique constraint on insert</a></li>
</ul>

<p>The best strategy to defend against race conditions depends on exact requirements, the number and size of rows in the table and in the UPSERTs, the number of concurrent transactions, the likelihood of conflicts, available resources and other factors ...</p>

<h3>Concurrency issue 1</h3>

<p>If a concurrent transaction has written to a row which your transaction now tries to UPSERT, your transaction has to wait for the other one to finish.</p>

<p>If the other transaction ends with <code>ROLLBACK</code> (or any error, i.e. automatic <code>ROLLBACK</code>), your transaction can proceed normally. Minor side effect: gaps in the sequential numbers. But no missing rows.</p>

<p>If the other transaction ends normally (implicit or explicit <code>COMMIT</code>), your <code>INSERT</code> will detect a conflict (the <code>UNIQUE</code> index / constraint is absolute) and <code>DO NOTHING</code>, hence also not return the row. (Also cannot lock the row as demonstrated in <em>concurrency issue 2</em> below, since it's <em>not visible</em>.) The <code>SELECT</code> sees the same snapshot from the start of the query and also cannot return the yet invisible row.</p>

<p><strong><em>Any such rows are missing from the result set (even though they exist in the underlying table)!</em></strong></p>

<p>This <strong>may be ok as is</strong>. Especially if you are not returning rows like in the example and are satisfied knowing the row is there. If that's not good enough, there are various ways around it.  </p>

<p>You could check the row count of the output and repeat the statement if it does not match the row count of the input. May be good enough for the rare case. The point is to start a new query (can be in the same transaction), which will then see the newly committed rows.</p>

<p><strong>Or</strong> check for missing result rows <em>within</em> the same query and <em>overwrite</em> those with the brute force trick demonstrated in <a href=""https://stackoverflow.com/a/37543015/939860"">Alextoni's answer</a>.</p>

<pre><code>WITH input_rows(usr, contact, name) AS ( ... )  -- see above
, ins AS (
   INSERT INTO chats AS c (usr, contact, name) 
   SELECT * FROM input_rows
   ON     CONFLICT (usr, contact) DO NOTHING
   RETURNING id, usr, contact                   -- we need unique columns for later join
   )
, sel AS (
   SELECT 'i'::""char"" AS source                 -- 'i' for 'inserted'
        , id, usr, contact
   FROM   ins
   UNION  ALL
   SELECT 's'::""char"" AS source                 -- 's' for 'selected'
        , c.id, usr, contact
   FROM   input_rows
   JOIN   chats c USING (usr, contact)
   )
, ups AS (                                      -- RARE corner case
   INSERT INTO chats AS c (usr, contact, name)  -- another UPSERT, not just UPDATE
   SELECT i.*
   FROM   input_rows i
   LEFT   JOIN sel   s USING (usr, contact)     -- columns of unique index
   WHERE  s.usr IS NULL                         -- missing!
   ON     CONFLICT (usr, contact) DO UPDATE     -- we've asked nicely the 1st time ...
   SET    name = c.name                         -- ... this time we overwrite with old value
   -- SET name = EXCLUDED.name                  -- alternatively overwrite with *new* value
   RETURNING 'u'::""char"" AS source              -- 'u' for updated
           , id  --, usr, contact               -- return more columns?
   )
SELECT source, id FROM sel
UNION  ALL
TABLE  ups;
</code></pre>

<p>It's like the query above, but we add one more step with the CTE <code>ups</code>, before we return the <strong><em>complete</em></strong> result set. That last CTE will do nothing most of the time. Only if rows go missing from the returned result, we use brute force.</p>

<p>More overhead, yet. The more conflicts with pre-existing rows, the more likely this will outperform the simple approach.</p>

<p>One side effect: the 2nd UPSERT writes rows out of order, so it re-introduces the possibility of deadlocks (see below) if <em>three or more</em> transactions writing to the same rows overlap. If that's a problem, you need a different solution.</p>

<h3>Concurrency issue 2</h3>

<p>If concurrent transactions can write to involved columns of affected rows, and you have to make sure the rows you found are still there at a later stage in the same transaction, you can <strong>lock rows</strong> cheaply with:</p>

<pre><code>...
ON CONFLICT (usr, contact) DO UPDATE
SET name = name WHERE FALSE  -- never executed, but still locks the row
...
</code></pre>

<p>And add a <a href=""https://www.postgresql.org/docs/current/static/sql-select.html#SQL-FOR-UPDATE-SHARE"" rel=""noreferrer"">locking clause to the <code>SELECT</code> as well, like <code>FOR UPDATE</code></a>.</p>

<p>This makes competing write operations wait till the end of the transaction, when all locks are released. So be brief.</p>

<p>More details and explanation:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/35949877/how-to-include-excluded-rows-in-returning-from-insert-on-conflict/35953488#35953488"">How to include excluded rows in RETURNING from INSERT ... ON CONFLICT</a></li>
<li><a href=""https://stackoverflow.com/questions/15939902/is-select-or-insert-in-a-function-prone-to-race-conditions/15950324#15950324"">Is SELECT or INSERT in a function prone to race conditions?</a></li>
</ul>

<h3>Deadlocks?</h3>

<p>Defend against <strong>deadlocks</strong> by inserting rows in <strong>consistent order</strong>. See:</p>

<ul>
<li><a href=""https://dba.stackexchange.com/questions/194756/deadlock-with-multi-row-inserts-despite-on-conflict-do-nothing/195220#195220"">Deadlock with multi-row INSERTs despite ON CONFLICT DO NOTHING</a></li>
</ul>

<h2>Data types and casts</h2>

<h3>Existing table as template for data types ...</h3>

<p>Explicit type casts for the first row of data in the free-standing <code>VALUES</code> expression may be inconvenient. There are ways around it. You can use any existing relation (table, view, ...) as row template. The target table is the obvious choice for the use case. Input data is coerced to appropriate types automatically, like in a <code>VALUES</code> clause of an <code>INSERT</code>:</p>

<pre><code>WITH input_rows AS (
  (SELECT usr, contact, name FROM chats LIMIT 0)  -- only copies column names and types
   UNION ALL
   VALUES
      ('foo1', 'bar1', 'bob1')  -- no type casts needed
    , ('foo2', 'bar2', 'bob2')
   )
   ...
</code></pre>

<p>This does not work for some data types (explanation in the linked answer at the bottom). The next trick works for <em>all</em> data types:</p>

<h3>... and names</h3>

<p>If you insert whole rows (all columns of the table - or at least a set of <em>leading</em> columns), you can omit column names, too. Assuming table <code>chats</code> in the example only has the 3 columns used:</p>

<pre><code>WITH input_rows AS (
   SELECT * FROM (
      VALUES
      ((NULL::chats).*)         -- copies whole row definition
      ('foo1', 'bar1', 'bob1')  -- no type casts needed
    , ('foo2', 'bar2', 'bob2')
      ) sub
   OFFSET 1
   )
   ...
</code></pre>

<p>Detailed explanation and more alternatives:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/12426363/casting-null-type-when-updating-multiple-rows/12427434#12427434"">Casting NULL type when updating multiple rows</a></li>
</ul>

<hr>

<p>Aside: don't use reserved words like <code>""user""</code> as identifier. That's a loaded footgun. Use legal, lower-case, unquoted identifiers. I replaced it with <code>usr</code>.</p>
"
"<p>When I right click on the indexes folder in the table the ""New Index"" menu item is grayed out. I don't understand why. I've deleted all data in the table just in case, and refreshed and restarted SSMS, but no luck. I'm using SQL Server 2012 Business Intelligence SP1 CTP.</p>
","<p><strong>Solution:</strong> Close your table designers and database diagrams and try again. If that doesn't help, close all windows in Management Studio.</p>

<p><strong>Cause:</strong> The ""New Index"" option gets disabled when the table is schema-locked.</p>
"
"<p>At every company I have worked at, I have found that people are still writing their SQL queries in the ANSI-89 standard:</p>

<pre><code>select a.id, b.id, b.address_1
from person a, address b
where a.id = b.id
</code></pre>

<p>rather than the ANSI-92 standard:</p>

<pre><code>select a.id, b.id, b.address_1
from person a
inner join address b
on a.id = b.id
</code></pre>

<p>For an extremely simple query like this, there's not a big difference in readability, but for large queries I find that having my join criteria grouped in with listing out the table makes it much easier to see where I might have issues in my join, and let's me keep all my filtering in my WHERE clause.  Not to mention that I feel that outer joins are much intuitive than the (+) syntax in Oracle.</p>

<p>As I try to evangelize ANSI-92 to people, are there any concrete performance benefits in using ANSI-92 over ANSI-89?  I would try it on my own, but the Oracle setups we have here don't allow us to use EXPLAIN PLAN - wouldn't want people to try to optimize their code, would ya?</p>
","<p>According to ""SQL Performance Tuning"" by Peter Gulutzan and Trudy Pelzer, of the six or eight RDBMS brands they tested, there was no difference in optimization or performance of SQL-89 versus SQL-92 style joins.  One can assume that most RDBMS engines transform the syntax into an internal representation before optimizing or executing the query, so the human-readable syntax makes no difference.</p>

<p>I also try to evangelize the SQL-92 syntax.  Sixteen years after it was approved, it's about time people start using it!  And all brands of SQL database now support it, so there's no reason to continue to use the abhorrent <code>(+)</code> Oracle syntax or <code>*=</code> Microsoft/Sybase syntax.</p>

<p>As for why it's so hard to break the developer community of the SQL-89 habit, I can only assume that there's a large ""base of the pyramid"" of programmers who code by copy &amp; paste, using ancient examples from books, magazine articles, or another code base, and these people don't learn new syntax abstractly.  Some people pattern-match, and some people learn by rote.</p>

<p>I am gradually seeing people using SQL-92 syntax more frequently than I used to, though.  I've been answering SQL questions online since 1994.</p>
"
"<p>I searched for a solution to this problem on internet and checked the SO questions but no solution worked for my case. </p>

<p>I want to create a foreign key from table sira_no to metal_kod. </p>

<pre><code>ALTER TABLE sira_no 
    ADD CONSTRAINT METAL_KODU FOREIGN KEY(METAL_KODU) 
    REFERENCES metal_kod(METAL_KODU) 
    ON DELETE SET NULL 
    ON UPDATE SET NULL ;
</code></pre>

<p>This script returns:</p>

<pre><code>Error Code: 1005. Can't create table 'ebs.#sql-f48_1a3' (errno: 150) 
</code></pre>

<p>I tried adding index to the referenced table:</p>

<pre><code>CREATE INDEX METAL_KODU_INDEX ON metal_kod (METAL_KODU);
</code></pre>

<p>I checked METAL_KODU on both tables (charset and collation). But couldn't find a solution to this problem. Does anyone have any idea? Thanks in advance.</p>

<p>EDIT: Here is the metal_kod table:</p>

<pre><code>METAL_KODU  varchar(4)  NO  PRI     
DURUM   bit(1)  NO          
METAL_ISMI  varchar(30) NO          
AYAR_YOGUNLUK   smallint(6) YES     100 
</code></pre>
","<p>Error Code: 1005 -- there is a wrong primary key reference in your code</p>

<p>usually it's due to a reference FK field not exist. might be you have typo mistake,or check case it should be same, or there's a field-type mismatch. FK-linked fields must match definitions exactly.</p>

<p>Some Known causes may be :</p>

<ol>
<li>The two key fields type and/or size doesnt match exactly. For example, if one is <code>INT(10)</code> the key field needs to be <code>INT(10)</code> as well and not <code>INT(11)</code> or <code>TINYINT</code>. You may want to confirm the field size using <code>SHOW</code> <code>CREATE</code> <code>TABLE</code> because Query Browser will sometimes visually show just <code>INTEGER</code> for both <code>INT(10)</code> and <code>INT(11)</code>. You should also check that one is not <code>SIGNED</code> and the other is <code>UNSIGNED</code>. They both need to be exactly the same.</li>
<li>One of the key field that you are trying to reference does not have an index and/or is not a primary key. If one of the fields in the relationship is not a primary key, you must create an index for that field.</li>
<li>The foreign key name is a duplicate of an already existing key. Check that the name of your foreign key is unique within your database. Just add a few random characters to the end of your key name to test for this.</li>
<li>One or both of your tables is a <code>MyISAM</code> table. In order to use foreign keys, the tables must both be <code>InnoDB</code>. (Actually, if both tables are <code>MyISAM</code> then you wont get an error message - it just wont create the key.) In Query Browser, you can specify the table type.</li>
<li>You have specified a cascade <code>ON</code> <code>DELETE</code> <code>SET</code> <code>NULL</code>, but the relevant key field is set to <code>NOT</code> <code>NULL</code>.  You can fix this by either changing your cascade or setting the field to allow <code>NULL</code> values. </li>
<li>Make sure that the Charset and Collate options are the same both at the table level as well as individual field level for the key columns.</li>
<li>You have a default value (ie default=0) on your foreign key column </li>
<li>One of the fields in the relationship is part of a combination (composite) key and does not have its own individual index. Even though the field has an index as part of the composite key, you must create a separate index for only that key field in order to use it in a constraint.</li>
<li>You have a syntax error in your <code>ALTER</code> statement or you have mistyped one of the field names in the relationship</li>
<li>The name of your foreign key exceeds the max length of 64 chars.</li>
</ol>

<p>for more details refer :  <a href=""http://verysimple.com/2006/10/22/mysql-error-number-1005-cant-create-table-mydbsql-328_45frm-errno-150/"">MySQL Error Number 1005 Cant create table</a></p>
"
"<p>Anyone knows some good SQL builder library for Java like <a href=""http://code.google.com/p/squiggle-sql/"">Squiggle</a> (not maintained anymore it seems). Preferably, a project in active development.</p>

<p>Preferably with syntax like <a href=""http://framework.zend.com/manual/en/zend.db.select.html"">Zend_Db_Select</a>, something that will allow to make a query like </p>

<pre><code>String query = db.select().from('products').order('product_id');
</code></pre>
","<p><a href=""http://www.querydsl.com/"" rel=""noreferrer"">Querydsl</a> and <a href=""http://www.jooq.org"" rel=""noreferrer"">jOOQ</a> are two popular choices.</p>
"
"<p>I am having trouble loading Django fixtures into my MySQL database because of contenttypes conflicts. First I tried dumping the data from only my app like this:</p>

<pre><code>./manage.py dumpdata escola &gt; fixture.json
</code></pre>

<p>but I kept getting missing foreign key problems, because my app ""escola"" uses tables from other applications. I kept adding additional apps until I got to this:</p>

<pre><code>./manage.py dumpdata contenttypes auth escola &gt; fixture.json
</code></pre>

<p>Now the problem is the following constraint violation when I try to load the data as a test fixture:</p>

<pre><code>IntegrityError: (1062, ""Duplicate entry 'escola-t23aluno' for key 2"")
</code></pre>

<p>It seems the problem is that Django is trying to dynamically recreate contenttypes with different primary key values that conflict with the primary key values from the fixture. This appears to be the same as bug documented here: <a href=""http://code.djangoproject.com/ticket/7052"" rel=""noreferrer"">http://code.djangoproject.com/ticket/7052</a></p>

<p>The problem is that the recommended workaround is to dump the contenttypes app which I'm already doing!? What gives? If it makes any difference I do have some custom model permissions as documented here: <a href=""http://docs.djangoproject.com/en/dev/ref/models/options/#permissions"" rel=""noreferrer"">http://docs.djangoproject.com/en/dev/ref/models/options/#permissions</a></p>
","<p><code>manage.py dumpdata --natural</code> will use a more durable representation of foreign keys. In django they are called ""natural keys"". For example:</p>

<ul>
<li><code>Permission.codename</code> is used in favour of <code>Permission.id</code></li>
<li><code>User.username</code> is used in favour of <code>User.id</code></li>
</ul>

<p>Read more: <a href=""http://docs.djangoproject.com/en/dev/topics/serialization/#natural-keys"" rel=""noreferrer"">natural keys section in ""serializing django objects""</a></p>

<p>Some other useful arguments for <code>dumpdata</code>:</p>

<ul>
<li><code>--indent=4</code> make it human readable.</li>
<li><code>-e sessions</code> exclude session data</li>
<li><code>-e admin</code> exclude history of admin actions on admin site</li>
<li><code>-e contenttypes -e auth.Permission</code> exclude objects which are recreated automatically from schema every time during <code>syncdb</code>. Only use it together with <code>--natural</code> or else you might end up with badly aligned id numbers.</li>
</ul>
"
"<p>Whenever I try to drop database I get:</p>

<pre><code>ERROR:  database ""pilot"" is being accessed by other users
DETAIL:  There is 1 other session using the database.
</code></pre>

<p>When I use:</p>

<pre><code>SELECT pg_terminate_backend(pg_stat_activity.pid)
FROM pg_stat_activity
WHERE pg_stat_activity.datname = 'TARGET_DB';
</code></pre>

<p>I terminated the connection from that DB, but if I try to drop database after that somehow someone automatically connects to that database and gives this error. What could be doing that? 
No one uses this database, except me.</p>
","<p>You can prevent future connections:</p>

<pre><code>REVOKE CONNECT ON DATABASE thedb FROM public;
</code></pre>

<p>(and possibly other users/roles; see <code>\l+</code> in <code>psql</code>)</p>

<p>You can then terminate all connections to this db except your own:</p>

<pre><code>SELECT pid, pg_terminate_backend(pid) 
FROM pg_stat_activity 
WHERE datname = current_database() AND pid &lt;&gt; pg_backend_pid();
</code></pre>

<p>On older versions <code>pid</code> was called <code>procpid</code> so you'll have to deal with that.</p>

<p>Since you've revoked <code>CONNECT</code> rights, whatever was trying to auto-connect should no longer be able to do so.</p>

<p>You'll now be able to drop the DB.</p>

<p>This won't work if you're using superuser connections for normal operations, but if you're doing that you need to fix that problem first.</p>
"
"<p>I am new to Docker, and trying to go through this tutorial setting up MemSQL from a Docker image - <a href=""http://docs.memsql.com/4.0/setup/docker/"">http://docs.memsql.com/4.0/setup/docker/</a> .  I am on a Mac, and the tutorial uses <code>boot2docker</code> which seems to have been deprecated.  </p>

<p>The VM needs 4GB memory to run.  The tutorial specifies how to do this with <code>boot2docker</code> but I cannot find a way to do this with the docker-machine/docker toolbox. </p>

<p>Here is the command I am using and the error I am getting just trying to go through the tutorial without altering the boot2docker config.  </p>

<pre><code>docker run --rm --net=host memsql/quickstart check-system
Error: MemSQL requires at least 4 GB of memory to run.
</code></pre>
","<p>You can do this via the command line. For example, to change the machine from the default 1cpu/2048MB RAM run:</p>

<pre><code>docker-machine stop
VBoxManage modifyvm default --cpus 2
VBoxManage modifyvm default --memory 4096
docker-machine start
</code></pre>
"
"<p>I need to know how to make a SQL query run daily using a SQL Server Agent job, with minimum required configuration settings.</p>
","<ol>
<li><p>Expand the SQL Server Agent node and right click the Jobs node in SQL Server Agent and select <code>'New Job'</code></p></li>
<li><p>In the <code>'New Job'</code> window enter the name of the job and a description on the <code>'General'</code> tab.</p></li>
<li><p>Select <code>'Steps'</code> on the left hand side of the window and click <code>'New'</code> at the bottom.</p></li>
<li><p>In the <code>'Steps'</code> window enter a step name and select the database you want the query to run against.</p></li>
<li><p>Paste in the T-SQL command you want to run into the Command window and click <code>'OK'</code>.</p></li>
<li><p>Click on the <code>'Schedule'</code> menu on the left of the New Job window and enter the schedule information (e.g. daily and a time).</p></li>
<li><p>Click <code>'OK'</code> - and that should be it.</p></li>
</ol>

<p>(There are of course other options you can add - but I would say that is the bare minimum you need to get a job set up and scheduled)</p>
"
"<p>I use msyql.data 8.08 and .net core to connect to mysql5.7.18 
but following exception is being thrown:</p>

<p><code>MySql.Data.MySqlClient.MySqlException:The host localhost does not support SSL connections.</code></p>

<p>How to deal with it?</p>
","<p>I had the same problem today when moving from MySql.Data 7.0.7 to 8.0.8. I was able to move forward adding the ""SslMode=none"" in the connection string.</p>

<p>You will endup with something like:</p>

<pre><code>server={0};user id={1};password={2};persistsecurityinfo=True;port={3};database={4};SslMode=none
</code></pre>

<p>(replacing the values with your database details)</p>
"
"<p>I have a large data table.
There are 10 million records in this table.</p>

<p>What is the best way for this query</p>

<pre><code>   Delete LargeTable where readTime &lt; dateadd(MONTH,-7,GETDATE())
</code></pre>
","<ol>
<li><p>If you are Deleting All the rows in that table the simplest option is to Truncate table, something like </p>

<pre><code>TRUNCATE TABLE LargeTable
GO
</code></pre>

<p>Truncate table will simply empty  the table, you cannot use WHERE clause to limit the rows being deleted and no triggers will be fired. </p></li>
<li><p>On the other hand if you are deleting more than 80-90 Percent of the data, say if you have total of 11 Million rows and you want to delete 10 million another way would be to Insert these 1 million rows (records you want to keep) to another staging table. Truncate this Large table and Insert back these 1 Million rows. </p></li>
<li><p>Or if permissions/views or other objects which has this large table as their underlying table doesnt get affected by dropping this table you can get these relatively small amount of the rows into another table drop this table and create another table with same schema and import these rows back into this ex-Large table.</p></li>
<li><p>One last option I can think of is to change your database's <code>Recovery Mode to SIMPLE</code> and then delete rows in smaller batches using a while loop something like this..</p>

<pre><code>DECLARE @Deleted_Rows INT;
SET @Deleted_Rows = 1;


WHILE (@Deleted_Rows &gt; 0)
  BEGIN
   -- Delete some small number of rows at a time
     DELETE TOP (10000)  LargeTable 
     WHERE readTime &lt; dateadd(MONTH,-7,GETDATE())

  SET @Deleted_Rows = @@ROWCOUNT;
END
</code></pre></li>
</ol>

<p>and dont forget to change the Recovery mode back to full and I think you have to take a backup to make it fully affective (the change or recovery modes).</p>
"
"<p>Assuming I have the tables <code>student</code>, <code>club</code>, and <code>student_club</code>:</p>

<pre><code>student {
    id
    name
}
club {
    id
    name
}
student_club {
    student_id
    club_id
}
</code></pre>

<p>I want to know how to find all students in both the soccer (30) and baseball (50) club.<br>
While this query doesn't work, it's the closest thing I have so far:</p>

<pre><code>SELECT student.*
FROM   student
INNER  JOIN student_club sc ON student.id = sc.student_id
LEFT   JOIN club c ON c.id = sc.club_id
WHERE  c.id = 30 AND c.id = 50
</code></pre>
","<p>I was curious. And as we all know, curiosity has a reputation for killing cats.</p>

<h2>So,  which is the fastest way to skin a cat?</h2>

<p>The precise cat-skinning environment for this test:</p>

<ul>
<li><strong>PostgreSQL 9.0</strong> on Debian Squeeze with decent RAM and settings.</li>
<li>6.000 students, 24.000 club memberships (data copied from a similar database with real life data.)</li>
<li>Slight diversion from the naming schema in the question: <code>student.id</code> is <code>student.stud_id</code> and <code>club.id</code> is <code>club.club_id</code> here.</li>
<li>I named the queries after their author in this thread, with an index where there are two.</li>
<li>I ran all queries a couple of times to populate the cache, then I picked the best of 5 with EXPLAIN ANALYZE.</li>
<li><p>Relevant indexes (should be the optimum - as long as we lack fore-knowledge which clubs will be queried):</p>

<pre><code>ALTER TABLE student ADD CONSTRAINT student_pkey PRIMARY KEY(stud_id );
ALTER TABLE student_club ADD CONSTRAINT sc_pkey PRIMARY KEY(stud_id, club_id);
ALTER TABLE club       ADD CONSTRAINT club_pkey PRIMARY KEY(club_id );
CREATE INDEX sc_club_id_idx ON student_club (club_id);
</code></pre>

<p><code>club_pkey</code> is not required by most queries here.<br>
Primary keys implement unique indexes automatically In PostgreSQL.<br>
The last index is to make up for this known shortcoming of <a href=""http://www.postgresql.org/docs/current/interactive/indexes-multicolumn.html"" rel=""noreferrer"">multi-column indexes</a> on PostgreSQL:</p></li>
</ul>

<blockquote>
  <p>A multicolumn B-tree index can be used with query conditions that
  involve any subset of the index's columns, but the index is most
  efficient when there are constraints on the leading (leftmost)
  columns.</p>
</blockquote>

<h1>Results:</h1>

<p>Total runtimes from EXPLAIN ANALYZE.</p>

<h3>1) Martin 2: 44.594 ms</h3>

<pre><code>SELECT s.stud_id, s.name
FROM   student s
JOIN   student_club sc USING (stud_id)
WHERE  sc.club_id IN (30, 50)
GROUP  BY 1,2
HAVING COUNT(*) &gt; 1;
</code></pre>

<hr>

<h3>2) Erwin 1: 33.217 ms</h3>

<pre><code>SELECT s.stud_id, s.name
FROM   student s
JOIN   (
   SELECT stud_id
   FROM   student_club
   WHERE  club_id IN (30, 50)
   GROUP  BY 1
   HAVING COUNT(*) &gt; 1
   ) sc USING (stud_id);
</code></pre>

<hr>

<h3>3) Martin 1: 31.735 ms</h3>

<pre><code>SELECT s.stud_id, s.name
   FROM   student s
   WHERE  student_id IN (
   SELECT student_id
   FROM   student_club
   WHERE  club_id = 30
   INTERSECT
   SELECT stud_id
   FROM   student_club
   WHERE  club_id = 50);
</code></pre>

<hr>

<h3>4) Derek: 2.287 ms</h3>

<pre><code>SELECT s.stud_id,  s.name
FROM   student s
WHERE  s.stud_id IN (SELECT stud_id FROM student_club WHERE club_id = 30)
AND    s.stud_id IN (SELECT stud_id FROM student_club WHERE club_id = 50);
</code></pre>

<hr>

<h3>5) Erwin 2: 2.181 ms</h3>

<pre><code>SELECT s.stud_id,  s.name
FROM   student s
WHERE  EXISTS (SELECT * FROM student_club
               WHERE  stud_id = s.stud_id AND club_id = 30)
AND    EXISTS (SELECT * FROM student_club
               WHERE  stud_id = s.stud_id AND club_id = 50);
</code></pre>

<hr>

<h3>6) Sean: 2.043 ms</h3>

<pre><code>SELECT s.stud_id, s.name
FROM   student s
JOIN   student_club x ON s.stud_id = x.stud_id
JOIN   student_club y ON s.stud_id = y.stud_id
WHERE  x.club_id = 30
AND    y.club_id = 50;
</code></pre>

<p>The last three perform pretty much the same. 4) and 5) result in the same query plan.</p>

<h2>Late Additions:</h2>

<p>Fancy SQL, but the performance can't keep up.</p>

<h3>7) ypercube 1: 148.649 ms</h3>

<pre><code>SELECT s.stud_id,  s.name
FROM   student AS s
WHERE  NOT EXISTS (
   SELECT *
   FROM   club AS c 
   WHERE  c.club_id IN (30, 50)
   AND    NOT EXISTS (
      SELECT *
      FROM   student_club AS sc 
      WHERE  sc.stud_id = s.stud_id
      AND    sc.club_id = c.club_id  
      )
   );
</code></pre>

<hr>

<h3>8) ypercube 2: 147.497 ms</h3>

<pre><code>SELECT s.stud_id,  s.name
FROM   student AS s
WHERE  NOT EXISTS (
   SELECT *
   FROM  (
      SELECT 30 AS club_id  
      UNION  ALL
      SELECT 50
      ) AS c
   WHERE NOT EXISTS (
      SELECT *
      FROM   student_club AS sc 
      WHERE  sc.stud_id = s.stud_id
      AND    sc.club_id = c.club_id  
      )
   );
</code></pre>

<p>As expected, those two perform almost the same. Query plan results in table scans, the planner doesn't find a way to use the indexes here.</p>

<hr>

<h3>9) wildplasser 1: 49.849 ms</h3>

<pre><code>WITH RECURSIVE two AS (
   SELECT 1::int AS level
        , stud_id
   FROM   student_club sc1
   WHERE  sc1.club_id = 30
   UNION
   SELECT two.level + 1 AS level
        , sc2.stud_id
   FROM   student_club sc2
   JOIN   two USING (stud_id)
   WHERE  sc2.club_id = 50
   AND    two.level = 1
   )
SELECT s.stud_id, s.student
FROM   student s
JOIN   two USING (studid)
WHERE  two.level &gt; 1;
</code></pre>

<p>Fancy SQL, decent performance for a CTE. Very exotic query plan.<br>
Again, would be interesting how 9.1 handles this. I am going to upgrade the db cluster used here to 9.1 soon. Maybe I'll rerun the whole shebang ...</p>

<hr>

<h3>10) wildplasser 2: 36.986 ms</h3>

<pre><code>WITH sc AS (
   SELECT stud_id
   FROM   student_club
   WHERE  club_id IN (30,50)
   GROUP  BY stud_id
   HAVING COUNT(*) &gt; 1
   )
SELECT s.*
FROM   student s
JOIN   sc USING (stud_id);
</code></pre>

<p>CTE variant of query 2). Surprisingly, it can result in a slightly different query plan with the exact same data. I found a sequential scan on <code>student</code>, where the subquery-variant used the index.</p>

<hr>

<h3>11) ypercube 3: 101.482 ms</h3>

<p>Another late addition @ypercube. It is positively amazing, how many ways there are.</p>

<pre><code>SELECT s.stud_id, s.student
FROM   student s
JOIN   student_club sc USING (stud_id)
WHERE  sc.club_id = 10                 -- member in 1st club ...
AND    NOT EXISTS (
   SELECT *
   FROM  (SELECT 14 AS club_id) AS c  -- can't be excluded for missing the 2nd
   WHERE  NOT EXISTS (
      SELECT *
      FROM   student_club AS d
      WHERE  d.stud_id = sc.stud_id
      AND    d.club_id = c.club_id
      )
   )
</code></pre>

<hr>

<h3>12) erwin 3: 2.377 ms</h3>

<p>@ypercube's 11) is actually just the mind-twisting reverse approach of this simpler variant, that was also still missing. Performs almost as fast as the top cats.</p>

<pre><code>SELECT s.*
FROM   student s
JOIN   student_club x USING (stud_id)
WHERE  sc.club_id = 10                 -- member in 1st club ...
AND    EXISTS (                        -- ... and membership in 2nd exists
   SELECT *
   FROM   student_club AS y
   WHERE  y.stud_id = s.stud_id
   AND    y.club_id = 14
   )
</code></pre>

<h3>13) erwin 4: 2.375 ms</h3>

<p>Hard to believe, but here's another, genuinely new variant. I see potential for more than two memberships, but it also ranks among the top cats with just two.</p>

<pre><code>SELECT s.*
FROM   student AS s
WHERE  EXISTS (
   SELECT *
   FROM   student_club AS x
   JOIN   student_club AS y USING (stud_id)
   WHERE  x.stud_id = s.stud_id
   AND    x.club_id = 14
   AND    y.club_id = 10
   )
</code></pre>

<h2>Dynamic number of club memberships</h2>

<p>In other words: varying number of filters. This question asked for exactly <strong><em>two</em></strong> club memberships. But many use cases have to prepare for a varying number.</p>

<p>Detailed discussion in this related later answer:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/47351766/using-same-column-multiple-times-in-where-clause/47512013#47512013"">Using same column multiple times in WHERE clause</a></li>
</ul>
"
"<p>I'm new to SqlServer, right now I have <code>SqlLocalDb</code> installed to work locally. Good, but I can see two connection strings typically and both works:</p>

<pre><code>Data Source=(localdb)\v11.0;Integrated Security=true;
</code></pre>

<p>and </p>

<pre><code>Server=(localdb)\v11.0;Integrated Security=true;
</code></pre>

<p>What exact difference is there between the two?</p>
","<p>For the full list of all of the connection string keywords, including those that are entirely synonymous, please refer to the <a href=""http://msdn.microsoft.com/en-gb/library/system.data.sqlclient.sqlconnection.connectionstring.aspx""><code>SqlConnection.ConnectionString</code> documentation</a>:</p>

<p>These are all entirely equivalent:</p>

<blockquote>
  <ul>
  <li>Data Source </li>
  <li>Server </li>
  <li>Address </li>
  <li>Addr </li>
  <li>Network Address</li>
  </ul>
</blockquote>
"
"<p>I'm running a data import (using C#/Linq), and naturally I'm trying to optimize my queries as much as possible.  To this end I'm running a trace on the DB using SQL Server Profiler, with my trace filtered by my SQL login name (it's a name that can uniquely be attributed to my data import process).</p>

<p>Strangely enough, most of my SQL statements are really quick :) - very few queries even break over the 1ms mark.  But spaced in between all my queries are several rows where the EventClass is ""Audit Login"" or ""Audit Logout"" - and the duration of an ""Audit Logout"" can be up to a minute!</p>

<p>Has this got something to do with the fact that I'm using transactions in my import?  If so, is there any way to find which are the big-hitting queries so I can clean those up?</p>
","<p>If I remember correct, the duration of an Audit Logout is the amount of time the connection was open. E.g. nothing to do with speed of the command - just the amount of time the login was 'logged in'.</p>
"
"<p>Looking through the documentation for the Postgres 9.4 datatype JSONB, it is not immediately obvious to me how to do updates on JSONB columns. </p>

<p>Documentation for JSONB types and functions: </p>

<p><a href=""http://www.postgresql.org/docs/9.4/static/functions-json.html"" rel=""noreferrer"">http://www.postgresql.org/docs/9.4/static/functions-json.html</a>
<a href=""http://www.postgresql.org/docs/9.4/static/datatype-json.html"" rel=""noreferrer"">http://www.postgresql.org/docs/9.4/static/datatype-json.html</a></p>

<p>As an examples, I have this basic table structure:</p>

<pre><code>CREATE TABLE test(id serial, data jsonb);
</code></pre>

<p>Inserting is easy, as in:</p>

<pre><code>INSERT INTO test(data) values ('{""name"": ""my-name"", ""tags"": [""tag1"", ""tag2""]}');
</code></pre>

<p>Now, how would I update the 'data' column? This is invalid syntax:</p>

<pre><code>UPDATE test SET data-&gt;'name' = 'my-other-name' WHERE id = 1;
</code></pre>

<p>Is this documented somewhere obvious that I missed? Thanks. </p>
","<p>If you're able to upgrade to Postgresql 9.5, the <code>jsonb_set</code> command is available, as others have mentioned.</p>

<p>In each of the following SQL statements, I've omitted the <code>where</code> clause for brevity; obviously, you'd want to add that back.</p>

<p>Update name:</p>

<pre><code>UPDATE test SET data = jsonb_set(data, '{name}', '""my-other-name""');
</code></pre>

<p>Replace the tags (as oppose to adding or removing tags):</p>

<pre><code>UPDATE test SET data = jsonb_set(data, '{tags}', '[""tag3"", ""tag4""]');
</code></pre>

<p>Replacing the second tag (0-indexed):</p>

<pre><code>UPDATE test SET data = jsonb_set(data, '{tags,1}', '""tag5""');
</code></pre>

<p>Append a tag (<strike>this will work as long as there are fewer than 999 tags; changing argument 999 to 1000 or above generates an error</strike>. This no longer appears to be the case in Postgres 9.5.3; a much larger index can be used):</p>

<pre><code>UPDATE test SET data = jsonb_set(data, '{tags,999999999}', '""tag6""', true);
</code></pre>

<p>Remove the last tag:</p>

<pre><code>UPDATE test SET data = data #- '{tags,-1}'
</code></pre>

<p>Complex update (delete the last tag, insert a new tag, and change the name):  </p>

<pre><code>UPDATE test SET data = jsonb_set(
    jsonb_set(data #- '{tags,-1}', '{tags,999999999}', '""tag3""', true), 
    '{name}', '""my-other-name""');
</code></pre>

<p>It's important to note that in each of these examples, you're not actually updating a single field of the JSON data. Instead, you're creating a temporary, modified version of the data, and assigning that modified version back to the column. In practice, the result should be the same, but keeping this in mind should make complex updates, like the last example, more understandable. </p>

<p>In the complex example, there are three transformations and three temporary versions: First, the last tag is removed. Then, that version is transformed by adding a new tag. Next, the second version is transformed by changing the <code>name</code> field. The value in the <code>data</code> column is replaced with the final version.</p>
"
"<p>I want to execute one update raw sql like below:</p>

<pre><code>update table set f1=? where f2=? and f3=?
</code></pre>

<p>This SQL will be executed by <code>ActiveRecord::Base.connection.execute</code>, but I don't know how to pass the dynamic parameter values into the method.</p>

<p>Could someone give me any help on it? </p>
","<p>It doesn't look like the Rails API exposes methods to do this generically. You could try accessing the underlying connection and using it's methods, e.g. for MySQL:</p>

<pre><code>st = ActiveRecord::Base.connection.raw_connection.prepare(""update table set f1=? where f2=? and f3=?"")
st.execute(f1, f2, f3)
st.close
</code></pre>

<p>I'm not sure if there are other ramifications to doing this (connections left open, etc). I would trace the Rails code for a normal update to see what it's doing aside from the actual query.</p>

<p>Using prepared queries can save you a small amount of time in the database, but unless you're doing this a million times in a row, you'd probably be better off just building the update with normal Ruby substitution, e.g. </p>

<pre><code>ActiveRecord::Base.connection.execute(""update table set f1=#{ActiveRecord::Base.sanitize(f1)}"")
</code></pre>

<p>or using ActiveRecord like the commenters said.</p>
"
"<p>I'm confused how to import a SQL dump file.  I can't seem to import the database without creating the database first in MySQL.</p>

<p>This is the error displayed when <code>database_name</code> has not yet been created: </p>

<p><code>username</code> = username of someone with access to the database on the original server.<br>
<code>database_name</code> = name of database from the original server </p>

<pre class=""lang-none prettyprint-override""><code>$ mysql -u username -p -h localhost database_name &lt; dumpfile.sql   
Enter password:  
ERROR 1049 (42000): Unknown database 'database_name' 
</code></pre>

<p>If I log into MySQL as root and create the database, <code>database_name</code> </p>

<pre><code>mysql -u root  
create database database_name;  
create user username;# same username as the user from the database I got the dump from.  
grant all privileges on database_name.* to username@""localhost"" identified by 'password';  
exit mysql
</code></pre>

<p>then attempt to import the sql dump again:  </p>

<pre><code>$ mysql -u username -p database_name &lt; dumpfile.sql  
Enter password:  
ERROR 1007 (HY000) at line 21: Can't create database 'database_name'; database exists
</code></pre>

<p>How am I supposed to import the SQL dumpfile?</p>
","<p>This is a <strong>known bug</strong> at MySQL.</p>

<p><a href=""http://bugs.mysql.com/bug.php?id=42996"" rel=""noreferrer"">bug 42996</a><br>
<a href=""http://bugs.mysql.com/bug.php?id=40477"" rel=""noreferrer"">bug 40477</a></p>

<p>As you can see this has been a known issue since 2008 and they have not fixed it yet!!!</p>

<p><strong>WORK AROUND</strong><br>
You first need to create the database to import. It doesn't need any tables. Then you can import your database.</p>

<ol>
<li><p>first start your MySQL command line (apply username and password if you need to)<br>
<code>C:\&gt;mysql -u user -p</code></p></li>
<li><p>Create your database and exit  </p>

<pre><code>mysql&gt; DROP DATABASE database;  
mysql&gt; CREATE DATABASE database;  
mysql&gt; Exit  
</code></pre></li>
<li><p>Import your selected database from the dump file<br>
<code>C:\&gt;mysql -u user -p -h localhost -D database -o &lt; dumpfile.sql</code></p></li>
</ol>

<p>You can replace localhost with an IP or domain for any MySQL server you want to import to.
The reason for the DROP command in the mysql prompt is to be sure we start with an empty and clean database.</p>
"
"<p>I'd like to have a function behaving as mysql_real_escape_string without connecting to database as at times I need to do dry testing without DB connection. mysql_escape_string is deprecated and therefore is undesirable. Some of my findings:</p>

<p><a href=""http://www.gamedev.net/community/forums/topic.asp?topic_id=448909"" rel=""noreferrer"">http://www.gamedev.net/community/forums/topic.asp?topic_id=448909</a></p>

<p><a href=""http://w3schools.invisionzone.com/index.php?showtopic=20064"" rel=""noreferrer"">http://w3schools.invisionzone.com/index.php?showtopic=20064</a></p>
","<p>The difference is that <code>mysql_escape_string</code> just treats the string as raw bytes, and adds escaping where it believes it's appropriate.</p>

<p><code>mysql_real_escape_string</code>, on the other hand, uses the information about the character set used for the MySQL connection. This means the string is escaped while treating multi-byte characters properly; i.e., it won't insert escaping characters in the middle of a character. This is why you need a connection for <code>mysql_real_escape_string</code>; it's necessary in order to know how the string should be treated.</p>

<p>However, instead of escaping, it's a better idea to use parameterized queries from the MySQLi library; there has previously been bugs in the escaping routine, and it's possible that some could appear again. Parameterizing the query is much, much harder to mess up, so it's less likely that you can get compromised by a MySQL bug.</p>
"
"<p>I'm confused how to import a SQL dump file.  I can't seem to import the database without creating the database first in MySQL.</p>

<p>This is the error displayed when <code>database_name</code> has not yet been created: </p>

<p><code>username</code> = username of someone with access to the database on the original server.<br>
<code>database_name</code> = name of database from the original server </p>

<pre class=""lang-none prettyprint-override""><code>$ mysql -u username -p -h localhost database_name &lt; dumpfile.sql   
Enter password:  
ERROR 1049 (42000): Unknown database 'database_name' 
</code></pre>

<p>If I log into MySQL as root and create the database, <code>database_name</code> </p>

<pre><code>mysql -u root  
create database database_name;  
create user username;# same username as the user from the database I got the dump from.  
grant all privileges on database_name.* to username@""localhost"" identified by 'password';  
exit mysql
</code></pre>

<p>then attempt to import the sql dump again:  </p>

<pre><code>$ mysql -u username -p database_name &lt; dumpfile.sql  
Enter password:  
ERROR 1007 (HY000) at line 21: Can't create database 'database_name'; database exists
</code></pre>

<p>How am I supposed to import the SQL dumpfile?</p>
","<p>This is a <strong>known bug</strong> at MySQL.</p>

<p><a href=""http://bugs.mysql.com/bug.php?id=42996"" rel=""noreferrer"">bug 42996</a><br>
<a href=""http://bugs.mysql.com/bug.php?id=40477"" rel=""noreferrer"">bug 40477</a></p>

<p>As you can see this has been a known issue since 2008 and they have not fixed it yet!!!</p>

<p><strong>WORK AROUND</strong><br>
You first need to create the database to import. It doesn't need any tables. Then you can import your database.</p>

<ol>
<li><p>first start your MySQL command line (apply username and password if you need to)<br>
<code>C:\&gt;mysql -u user -p</code></p></li>
<li><p>Create your database and exit  </p>

<pre><code>mysql&gt; DROP DATABASE database;  
mysql&gt; CREATE DATABASE database;  
mysql&gt; Exit  
</code></pre></li>
<li><p>Import your selected database from the dump file<br>
<code>C:\&gt;mysql -u user -p -h localhost -D database -o &lt; dumpfile.sql</code></p></li>
</ol>

<p>You can replace localhost with an IP or domain for any MySQL server you want to import to.
The reason for the DROP command in the mysql prompt is to be sure we start with an empty and clean database.</p>
"
"<p>I just downloaded the latest version of SQL Express 2012 but I cannot connect to localhost. I tried localhost\SQLExpress and Windows authentication but it gives me an error message saying cannot connect. Am I missing something here? I've used SQL Server 2008 before and I've never had issues connecting to localhost. It seems that it can't even find it. Also in the Services I only see a SQL Server VSS Writer. Is this the way it should be? Or am I missing something? Thanks</p>
","<p><a href=""https://stackoverflow.com/questions/10624287/cant-connect-to-sql-server-2005-localhost/10625298#10625298"">According to Aaron Bertand</a>:</p>

<ol>
<li><p>You need to verify  that the SQL Server service is running. You can do this by going to <code>Start &gt; Control Panel &gt; Administrative Tools &gt; Services</code>, and checking that the service SQL Server (<code>SQLEXPRESS</code>) is running. If not, start it.</p></li>
<li><p>While you're in the services applet, also make sure that the service <a href=""https://technet.microsoft.com/en-us/library/ms181087(v=sql.105).aspx"" rel=""noreferrer"">SQL Browser</a> is started. If not, <a href=""https://www.tenforums.com/tutorials/4499-start-stop-disable-services-windows-10-a.html"" rel=""noreferrer"">start it</a>.</p></li>
<li><p>You need to make sure that SQL Server is allowed to use TCP/IP or named pipes. You can turn these on by opening the SQL Server Configuration Manager in <code>Start &gt; Programs &gt; Microsoft SQL Server 2012 &gt; Configuration Tools</code> (or <code>SQL Server Configuration Manager</code>), and <a href=""https://www.blackbaud.com/files/support/infinityinstaller/content/installermaster/tkenablenamedpipesandtcpipconnections.htm"" rel=""noreferrer"">make sure that TCP/IP and Named Pipes are enabled</a>.</p>

<p><a href=""https://i.stack.imgur.com/dPM5j.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/dPM5j.png"" alt=""SQL Server Configuration Manager""></a></p></li>
<li><p>Verify your SQL Server connection authentication mode matches your connection string:</p>

<ul>
<li><p>If you're connecting using a username and password, you need to configure SQL Server to accept ""SQL Server Authentication Mode"":</p>

<pre><code>-- YOU MUST RESTART YOUR SQL SERVER AFTER RUNNING THIS!
USE [master]
GO
DECLARE @SqlServerAndWindowsAuthenticationMode INT = 2;
EXEC xp_instance_regwrite
  N'HKEY_LOCAL_MACHINE',
  N'Software\Microsoft\MSSQLServer\MSSQLServer',
  N'LoginMode',
  REG_DWORD,
  @SqlServerAndWindowsAuthenticationMode;
GO
</code></pre></li>
<li>If you're connecting using ""Integrated Security=true"" (Windows Mode), and this error only comes up when debugging in web applications, then you need to <a href=""https://blogs.msdn.microsoft.com/ericparvin/2015/04/14/how-to-add-the-applicationpoolidentity-to-a-sql-server-login/"" rel=""noreferrer"">add the ApplicationPoolIdentity as a SQL Server login</a>:</li>
</ul></li>
<li><p>otherwise, run <code>Start -&gt; Run -&gt; Services.msc</code> If so, is it running?</p></li>
</ol>

<p>If it's not running then</p>

<p>It sounds like you didn't get everything installed. Launch the install file and chose the option ""New installation or add features to an existing installation"". From there you should be able to make sure the database engine service gets installed.</p>
"
"<p>HSQLDB 2.0 is soon to be released. I wonder if it will outperform H2 since, as far as I know, most users prefer H2 than HSQLDB. I am interested in the MVCC support of HSQLDB 2.0. I have learned that MVCC on H2 is still experimental. With regards to support/documentation, concurrency, performance, which is better between the two?</p>
","<ol>
<li><p>You can clear the data by dropping the schema. The default schema is called PUBLIC. If you execute the SQL satement below, it will clear all data and drop all tables.</p>

<p>DROP SCHEMA PUBLIC CASCADE</p></li>
<li><p>Alternatively, if you need the table and schema object definitions, you can create a file: database containing the objects but no data, and add the property below to the .properties file. Using this type of database for tests, the changes to data are not persisted</p>

<p>files_read_only=true </p></li>
<li><p>The latest alternative, available in HSQLDB 2.2.6 and later allows you to clear all the data in a schema while keeping the tables. In the example below, the PUBLIC schema is cleared.</p>

<p>TRUNCATE SCHEMA public AND COMMIT</p>

<p>This statement has been enhanced in the latest versions of HSQLDB. See <a href=""http://hsqldb.org/doc/2.0/guide/dataaccess-chapt.html#dac_truncate_statement"" rel=""noreferrer"">http://hsqldb.org/doc/2.0/guide/dataaccess-chapt.html#dac_truncate_statement</a> under <strong>Truncate Statement</strong></p></li>
</ol>
"
"<p>Ok, here is my dilemma I have a database set up with about 5 tables all with the exact same data structure. The data is separated in this manner for localization purposes and to split up a total of about 4.5 million records.</p>

<p>A majority of the time only one table is needed and all is well. However, sometimes data is needed from 2 or more of the tables and it needs to be sorted by a user defined column. This is where I am having problems.</p>

<p>data columns:</p>

<pre><code>id, band_name, song_name, album_name, genre
</code></pre>

<p>MySQL statment:</p>

<pre><code>SELECT * from us_music, de_music where `genre` = 'punk'
</code></pre>

<p>MySQL spits out this error:</p>

<pre><code>#1052 - Column 'genre' in where clause is ambiguous
</code></pre>

<p>Obviously, I am doing this wrong. Anyone care to shed some light on this for me?</p>
","<p>I think you're looking for the <a href=""http://dev.mysql.com/doc/refman/5.1/en/union.html"" rel=""noreferrer"">UNION</a> clause, a la</p>

<pre><code>(SELECT * from us_music where `genre` = 'punk')
UNION
(SELECT * from de_music where `genre` = 'punk')
</code></pre>
"
"<p>I'm just learning Laravel, and have a working migration file creating a users table. I am trying to populate a user record as part of the migration:</p>

<pre><code>public function up()
{
    Schema::create('users', function($table){

        $table-&gt;increments('id');
        $table-&gt;string('email', 255);
        $table-&gt;string('password', 64);
        $table-&gt;boolean('verified');
        $table-&gt;string('token', 255);
        $table-&gt;timestamps();

        DB::table('users')-&gt;insert(
            array(
                'email' =&gt; 'name@domain.com',
                'verified' =&gt; true
            )
        );

    });
}
</code></pre>

<p>But I'm getting the following error when running <code>php artisan migrate</code>:</p>

<pre><code>SQLSTATE[42S02]: Base table or view not found: 1146 Table 'vantage.users' doesn't exist
</code></pre>

<p>This is obviously because Artisan hasn't yet created the table, but all the documentation seems to say that there is a way of using Fluent Query to populate data as part of a migration.</p>

<p>Anyone know how? Thanks!</p>
","<p>Don't put the DB::insert() inside of the Schema::create(), because the create method has to finish making the table before you can insert stuff. Try this instead:</p>

<pre><code>public function up()
{
    // Create the table
    Schema::create('users', function($table){
        $table-&gt;increments('id');
        $table-&gt;string('email', 255);
        $table-&gt;string('password', 64);
        $table-&gt;boolean('verified');
        $table-&gt;string('token', 255);
        $table-&gt;timestamps();
    });

    // Insert some stuff
    DB::table('users')-&gt;insert(
        array(
            'email' =&gt; 'name@domain.com',
            'verified' =&gt; true
        )
    );
}
</code></pre>
"
"<p>Is it possible to prevent SQL injections in Node.js (preferably with a module) in the same way that PHP had Prepared Statements that protected against them.</p>

<p>If so, how? If not, <strong>what are some examples</strong> that might bypass the code I've provided (see below).</p>

<hr>

<p>Some Context:</p>

<p>I'm making a web application with a back-end stack consisting of Node.js + MySql using the <a href=""https://github.com/felixge/node-mysql"" rel=""noreferrer"">node-mysql</a> module. From a usability perspective, the module is great, but it has not yet implemented something akin to PHP's <a href=""http://php.net/manual/en/pdo.prepared-statements.php"" rel=""noreferrer"">Prepared Statements</a> (though I'm aware it is on the <a href=""https://github.com/felixge/node-mysql#todo"" rel=""noreferrer"">todo</a>).</p>

<p>From my understanding, PHP's implementation of prepared statements, among other things, <a href=""https://stackoverflow.com/questions/5741187/sql-injection-that-gets-around-mysql-real-escape-string/12118602#12118602"">helped greatly</a> in the prevention of SQL injections. I'm worried, though, that my node.js app may be open to similar attacks, <a href=""https://github.com/felixge/node-mysql#escaping-query-values"" rel=""noreferrer"">even with the string escaping provided by default</a> (as in the code snippet below).</p>

<p>node-mysql seems to be the most popular mysql connector for node.js, so I was wondering what other people might be doing (if anything) to account for this issue - or if it is even an issue with node.js to begin with (not sure how this wouldn't be, since user/client-side input is involved).</p>

<p><strong>Should I switch to <a href=""https://github.com/sidorares/nodejs-mysql-native"" rel=""noreferrer"">node-mysql-native</a> for the time being, since it does provide prepared statements?</strong> I'm hesitant to do this, because it does not seem to be as active as node-mysql (though that may just mean that it is complete).</p>

<p>Here is a snippet of user registration code, which uses the <a href=""https://github.com/theSmaw/Caja-HTML-Sanitizer"" rel=""noreferrer"">sanitizer</a> module, along with node-mysql's prepared statement-like syntax (which, as I mentioned above, does character escaping), to prevent cross site scripting and sql injections, respectively:</p>

<pre><code>// Prevent xss
var clean_user = sanitizer.sanitize(username);

// assume password is hashed already
var post = {Username: clean_user, Password: hash};

// This just uses connection.escape() underneath
var query = connection.query('INSERT INTO users SET ?', post,
   function(err, results)
   {
       // Can a Sql injection happen here?
   });
</code></pre>
","<p>Pay attention to the <a href=""https://github.com/felixge/node-mysql#escaping-query-values"" rel=""noreferrer"">documentation of <code>node-mysql</code></a>:</p>

<blockquote>
  <p>If you paid attention, you may have noticed that this escaping allows you to do neat things like this:</p>

<pre><code>var post  = {id: 1, title: 'Hello MySQL'};
var query = connection.query('INSERT INTO posts SET ?', post, function(err, result) {
  // Neat!
});
console.log(query.sql); // INSERT INTO posts SET `id` = 1, `title` = 'Hello MySQL'
</code></pre>
</blockquote>

<p>Notice that they use <code>SET</code> instead of <code>VALUES</code>. <code>INSERT INTO ... SET x = y</code> is a valid MySQL query, while <code>INSERT INTO ... VALUES x = y</code> is not.</p>
"
"<p>What is wrong with the code there are lots of error while debugging. I am writing a code for a singleton class to connect with the database mysql.</p>

<p>Here is my code</p>

<pre><code>package com.glomindz.mercuri.util;
import java.sql.Connection;
import java.sql.Driver;
import java.sql.DriverManager;
import java.sql.SQLException;

public class MySingleTon {
    String url = ""jdbc:mysql://localhost:3306/"";
    String dbName = ""test"";
    String driver = ""com.mysql.jdbc.Driver"";
    String userName = ""root"";
    String password = """";

    private static MySingleTon myObj;   
    private Connection Con ;
    private MySingleTon() {
        System.out.println(""Hello"");
        Con= createConnection();
    }

    @SuppressWarnings(""rawtypes"")
    public Connection createConnection() {
        Connection connection = null;
        try {
            // Load the JDBC driver
            Class driver_class = Class.forName(driver);
            Driver driver = (Driver) driver_class.newInstance();
            DriverManager.registerDriver(driver);
            connection = DriverManager.getConnection(url + dbName);
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        } catch (SQLException e) {
            e.printStackTrace();
        } catch (IllegalAccessException e) {
            e.printStackTrace();
        } catch (InstantiationException e) {
            e.printStackTrace();
        }
        return connection;
    }

    /**
     * Create a static method to get instance.
     */
    public static MySingleTon getInstance() {
        if (myObj == null) {
            myObj = new MySingleTon();
        }
        return myObj;
    }

    public static void main(String a[]) {
        MySingleTon st = MySingleTon.getInstance();
    }
}
</code></pre>

<p>I am new to java. Please help.</p>
","<p>It seems the <strong>mysql connectivity library is not included</strong> in the project. Solve the problem following one of the proposed solutions:</p>

<ul>
<li><strong>MAVEN PROJECTS SOLUTION</strong></li>
</ul>

<p>Add the mysql-connector dependency to the pom.xml project file:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;mysql&lt;/groupId&gt;
    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
    &lt;version&gt;5.1.39&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Here you are all the versions: <a href=""https://mvnrepository.com/artifact/mysql/mysql-connector-java"">https://mvnrepository.com/artifact/mysql/mysql-connector-java</a></p>

<ul>
<li><strong>ALL PROJECTS SOLUTION</strong></li>
</ul>

<p>Add the jar library manually to the project.</p>

<p><strong>Right Click the project -- > build path -- > configure build path</strong></p>

<p>In <code>Libraries Tab</code> <strong>press</strong> <code>Add External Jar</code> and <code>Select</code> your jar.</p>

<p>You can find zip for mysql-connector <a href=""http://dev.mysql.com/downloads/connector/j/5.0.html"">here</a></p>

<ul>
<li><strong><em>Explanation:</em></strong></li>
</ul>

<p>When building the project, java throws you an exception because a file (the com.mysql.jdbc.Driver class) from the mysql connectivity library is not found. The solution is adding the library to the project, and java will find the com.mysql.jdbc.Driver </p>
"
"<p><strong>The problem:</strong></p>

<p>I need a device agnostic (e.g. HTML5) solution for storing and querying 250,000+ rows of data offline on a phone or tablet type device (e.g. iOS/Android). The idea being I have people working in remote areas without any cellular data connection and they need to run queries on this data and edit it while offline. Partly it will be geo-location based so if there are assets in the area they are in (uses GPS) then it will show those assets and let them be edited. When they return to the office they can sync the data back to the office server.</p>

<p>The reason that I'm approaching this from a web standard point of view is basically to save money and time by writing it once in HTML5 and then it works across multiple platforms rather than writing it twice in Objective C and Java. Also if you write something that's platform agnostic then you're not locked in and don't go down with the ship when everyone moves to a newer one. We had a similar app written for Windows Mobile 5, now it's useless as that platform is dead.</p>

<p>The offline database on the device needs to be:</p>

<ul>
<li>fast (responses under 2 seconds)</li>
<li>potentially perform joins and have relationships with other tables able to query the database</li>
<li>select data within a certain range or criteria e.g. by x &amp; y co-ordinate based on the GPS reading.</li>
</ul>

<p><strong>Options:</strong></p>

<p><em><strong>HTML5 local storage:</em></strong></p>

<p>Fine for small amounts of data &lt;5,000 key/values, you can even store arrays/objects in it if you convert it to JSON.</p>

<p><em>Cons:</em></p>

<ul>
<li>For more than 10,000 rows even on a high end machine the browser will
slow to a crawl.</li>
<li>Can't do complex queries on the data to pull out the data you want as you have to iterate through the whole storage and manually search for it.</li>
<li>Limitations with the amount of storage that can be stored</li>
</ul>

<p><em><strong>Web SQL Database:</em></strong></p>

<ul>
<li>Meets the requirements.</li>
<li>Fast to run a query on 250,000 rows (1-2secs)</li>
<li>Can create complex queries, joins etc</li>
<li>Supported by Safari, Android and Opera so will work on iOS and Android devices</li>
</ul>

<p><em>Cons:</em></p>

<ul>
<li>Deprecated as of November 2010</li>
<li>Security flaw with cross-directory attacks. Not really an issue as we won't be on shared hosting</li>
</ul>

<p><em><strong>IndexedDB:</em></strong></p>

<p>Key/value object store similar to local storage except with indexes.</p>

<p><em>Cons:</em></p>

<ul>
<li>Slow to run a query on 200,000 rows (15-18secs)   </li>
<li>Can't run complex queries      </li>
<li>Can't do joins with other tables   </li>
<li>Not supported by main phone or tablet devices e.g. iPad/Android</li>
<li>Standard not complete</li>
</ul>

<p>This leaves the only option of implementing the deprecated Web SQL method which may only work for another year or so. IndexedDB and local storage are unusable at present.</p>

<p>I'm not sure how Mozilla and Microsoft got the Web SQL Database standard deprecated and why the W3C let it happen. Supposedly between them they have 77% of the desktop browser market. On advanced mobile devices Mozilla and Microsoft have nearly zero influence as <a href=""http://www.netmarketshare.com/"">Safari, Opera and Android have over 90% of the market share</a>. How Mozilla &amp; Microsoft can dictate which standard should be used in the mobile market which is where offline storage is most likely to be used doesn't make any sense.</p>

<p>In the <a href=""http://hacks.mozilla.org/2010/06/beyond-html5-database-apis-and-the-road-to-indexeddb/"">comments from Mozilla</a> about why they wanted to go with IndexedDB instead are mainly about 'developer aesthetics' and they don't like the idea of running SQL in JavaScript. I'm not buying it.</p>

<ol>
<li><p>Currently the proposed standard is inferior and an extremely basic NoSQL implementation that is slow and doesn't even support the advanced features people need in a database. There is a lot of boilerplate code to establish the database and get data out but they claim people will write some nice abstraction libraries over the top of it that will provide more advanced features. As of Oct 2011 they're nowhere to be seen.</p></li>
<li><p>They've deprecated the existing Web SQL standard which actually works and is implemented in the main mobile/tablet browsers. Whereas their 'new' and 'better' standard is not available in the major mobile browsers.</p></li>
<li><p>What are we as developers supposed to use for the next 3-5 years which is when the IndexedDB specification might get around to being standardised, have more features, implemented in the main mobile/tablet browsers and there's some nice libraries to make things easier?</p></li>
</ol>

<p>The W3C should keep the Web SQL Database standard running in parallel and just fix the issues. It already has support for the major mobile platforms and it works pretty well. The fact that Mozilla and Microsoft as the two players with the most desktop browser share were able to get this standard scrapped is pretty dubious and could be seen as an attempt to hinder progress on the mobile web platforms until they are able to catch up and offer competing solutions against iOS/Safari and Android.</p>

<p>In conclusion does anyone have a solution for my problem that will work for iOS/Android for phone/tablet devices. Maybe a nice wrapper API that can use multiple database implementations in the background with querying capability and it lets you choose which database has priority. I've seen things like <a href=""http://westcoastlogic.com/lawnchair/"">lawnchair</a> but I'm pretty sure it only lets you use local storage by default and falls back to the to the others. I think I'd rather it used Web SQL (by default) then the slower options.</p>

<p>Any help for a solution much appreciated, thanks!</p>
","<p>Let me answer step by step</p>

<blockquote>
  <p>if the app stores data with localStorage or web SQL, and the user
  switches to a different standard browser on their Android, will the
  app be opened with the new browser and does that mean that the stored
  data is unavailable?</p>
</blockquote>

<p>The data is saved in the 'cache'(its not exactly the cache) of the browser, so if you change the browser, or set the settings so that the default browser is removed or changed, the data will go.</p>

<blockquote>
  <p>If the user doesn't use the app for a year (which in my case is a
  realistic and not necessarily a bad scenario), will the data have
  expired like a cookie, or maybe been pushed out of the browser's
  storage by a deluge of data from other apps?</p>
</blockquote>

<p>No, the data will stay there no matter for how long it is not used. So even if you clear the browser cache, it will still be there.</p>

<blockquote>
  <p>Or will the data be destroyed even earlier, such as when: - user
  visits another site in the browser - browser is manually closed -
  browser process is killed or dies - etc</p>
</blockquote>

<p>No, the data stays all right. :-)</p>

<blockquote>
  <p>Or are localStorage and web SQL the kind of storage that you only
  delete when (in Android) you go to Settings > Apps and actively remove
  the user data associated with the app?</p>
</blockquote>

<p>Yes, the data only goes if you either manually delete it from your app or you uninstall your app. It will stay in all other cases.</p>

<p>EDIT:
In the case of iOS, the OS will remove the data in the local storage when there is a shortage of memory in the device. </p>
"
"<p>I want to run a small PostgreSQL database which runs in memory only, for each unit test I write. For instance:</p>

<pre><code>@Before
void setUp() {
    String port = runPostgresOnRandomPort();
    connectTo(""postgres://localhost:""+port+""/in_memory_db"");
    // ...
}
</code></pre>

<p>Ideally I'll have a single postgres executable checked into the version control, which the unit test will use.</p>

<p>Something like <code>HSQL</code>, but for postgres. How can I do that?</p>

<p>Were can I get such a Postgres version? How can I instruct it not to use the disk?</p>
","<p>For best read performance you need a <a href=""https://www.postgresql.org/docs/current/static/indexes-multicolumn.html"" rel=""noreferrer"">multicolumn index</a>:</p>

<pre><code>CREATE INDEX user_msg_log_combo_idx
ON user_msg_log (user_id, aggr_date DESC NULLS LAST)
</code></pre>

<p>To make <strong><a href=""https://wiki.postgresql.org/wiki/Index-only_scans"" rel=""noreferrer"">index only scans</a></strong> possible, add the otherwise not needed column <code>running_total</code>:</p>

<pre><code>CREATE INDEX user_msg_log_combo_covering_idx
ON user_msg_log (user_id, aggr_date DESC NULLS LAST, running_total)
</code></pre>

<p>Why <code>DESC NULLS LAST</code>? </p>

<ul>
<li><a href=""https://dba.stackexchange.com/a/90183/3684"">Unused index in range of dates query</a></li>
</ul>

<p>For <strong><em>few</em></strong> rows per <code>user_id</code> or small tables a simple <code>DISTINCT ON</code> is among the fastest and simplest solutions:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/3800551/select-first-row-in-each-group-by-group/7630564#7630564"">Select first row in each GROUP BY group?</a></li>
</ul>

<p>For <strong><em>many</em></strong> rows per <code>user_id</code> a <a href=""https://wiki.postgresql.org/wiki/Loose_indexscan"" rel=""noreferrer""><strong>loose index scan</strong></a> would be (much) more efficient. That's not implemented in Postgres (at least up to Postgres 10), but there are ways to emulate it:</p>

<h2>1. No separate table with unique users</h2>

<p>The following solutions go beyond what's covered in the <a href=""https://wiki.postgresql.org/wiki/Loose_indexscan"" rel=""noreferrer""><strong>Postgres Wiki</strong></a>.<br>
<sup>With a separate <code>users</code> table, solutions in <strong><em>2.</em></strong> below are typically simpler and faster.</sup>  </p>

<h3>1a. Recursive CTE with <code>LATERAL</code> join</h3>

<p><a href=""https://www.postgresql.org/docs/current/static/queries-with.html"" rel=""noreferrer"">Common Table Expressions</a> require Postgres <strong>8.4+</strong>.<br>
<a href=""https://www.postgresql.org/docs/current/static/queries-table-expressions.html#QUERIES-LATERAL"" rel=""noreferrer""><code>LATERAL</code></a> requires Postgres <strong>9.3+</strong>.</p>

<pre><code>WITH RECURSIVE cte AS (
   (  -- parentheses required
   SELECT user_id, aggr_date, running_total
   FROM   user_msg_log
   WHERE  aggr_date &lt;= :mydate
   ORDER  BY user_id, aggr_date DESC NULLS LAST
   LIMIT  1
   )
   UNION ALL
   SELECT u.user_id, u.aggr_date, u.running_total
   FROM   cte c
   ,      LATERAL (
      SELECT user_id, aggr_date, running_total
      FROM   user_msg_log
      WHERE  user_id &gt; c.user_id   -- lateral reference
      AND    aggr_date &lt;= :mydate  -- repeat condition
      ORDER  BY user_id, aggr_date DESC NULLS LAST
      LIMIT  1
      ) u
   )
SELECT user_id, aggr_date, running_total
FROM   cte
ORDER  BY user_id;
</code></pre>

<p>This is preferable in current versions of Postgres and it's simple to retrieve arbitrary columns. More explanation in chapter <em>2a.</em> below.</p>

<h3>1b. Recursive CTE with correlated subqueries</h3>

<p>Convenient to retrieve either a <em>single column</em> or the <em>whole row</em>. The example uses the whole row type of the table. Other variants are possible.</p>

<pre><code>WITH RECURSIVE cte AS (
   (
   SELECT u  -- whole row
   FROM   user_msg_log u
   WHERE  aggr_date &lt;= :mydate
   ORDER  BY user_id, aggr_date DESC NULLS LAST
   LIMIT  1
   )
   UNION ALL
   SELECT (SELECT u1  -- again, whole row
           FROM   user_msg_log u1
           WHERE  user_id &gt; (c.u).user_id  -- parentheses to access row type
           AND    aggr_date &lt;= :mydate     -- repeat predicate
           ORDER  BY user_id, aggr_date DESC NULLS LAST
           LIMIT  1)
   FROM   cte c
   WHERE  (c.u).user_id IS NOT NULL        -- any NOT NULL column of the row
   )
SELECT (u).*                               -- finally decompose row
FROM   cte
WHERE  (u).user_id IS NOT NULL             -- any column defined NOT NULL
ORDER  BY (u).user_id;
</code></pre>

<p>It could be misleading to test the row value with <code>c.u IS NOT NULL</code>. This only returns <code>true</code> if every single column of the tested row is <code>NOT NULL</code> and would fail if a single <code>NULL</code> value is contained. (I had this bug in my answer for some time.) Instead, to assert a row was found in the previous iteration, test a single column of the row that is defined <code>NOT NULL</code> (like the primary key). More:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/21021102/not-null-constraint-over-a-set-of-columns/21026085#21026085"">NOT NULL constraint over a set of columns</a></li>
<li><a href=""https://stackoverflow.com/questions/21765198/is-not-null-test-for-a-record-does-not-return-true-when-variable-is-set/21770324#21770324"">IS NOT NULL test for a record does not return TRUE when variable is set</a></li>
</ul>

<p>More explanation for this query in chapter <em>2b.</em> below.<br>
Related answers:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/25957558/querying-last-n-related-records-in-postgres/25965393#25965393"">Query last N related rows per row</a></li>
<li><a href=""https://dba.stackexchange.com/a/74811/3684"">GROUP BY one column, while sorting by another in PostgreSQL</a></li>
</ul>

<h2>2. With separate <code>users</code> table</h2>

<p>Table layout hardly matters as long as we have exactly one row per relevant <code>user_id</code>. Example:</p>

<pre><code>CREATE TABLE users (
   user_id  serial PRIMARY KEY
 , username text NOT NULL
);
</code></pre>

<p>Ideally, the table is physically sorted. See:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/13998139/optimize-postgres-timestamp-query-range/14007963#14007963"">Optimize Postgres timestamp query range</a></li>
</ul>

<p>Or it's small enough (low cardinality) that it hardly matters.<br>
Else, sorting rows in the query can help to further optimize performance. <a href=""https://stackoverflow.com/a/36223670/939860"">See Gang Liang's addition.</a></p>

<h3>2a. <code>LATERAL</code> join</h3>

<pre><code>SELECT u.user_id, l.aggr_date, l.running_total
FROM   users u
CROSS  JOIN LATERAL (
   SELECT aggr_date, running_total
   FROM   user_msg_log
   WHERE  user_id = u.user_id  -- lateral reference
   AND    aggr_date &lt;= :mydate
   ORDER  BY aggr_date DESC NULLS LAST
   LIMIT  1
   ) l;
</code></pre>

<p><a href=""https://www.postgresql.org/docs/current/static/sql-select.html"" rel=""noreferrer""><code>JOIN LATERAL</code></a> allows to reference preceding <code>FROM</code> items on the same query level. You get one index (-only) look-up per user.</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/28550679/what-is-the-difference-between-lateral-and-a-subquery-in-postgresql/28557803#28557803"">What is the difference between LATERAL and a subquery in PostgreSQL?</a></li>
</ul>

<p>Consider the possible improvement by sorting the <code>users</code> table <a href=""https://stackoverflow.com/a/36223670/939860"">suggested by Gang Liang in another answer</a>. If the physical sort order of the <code>users</code> table happens to match the index on <code>user_msg_log</code>, you don't need this.</p>

<p>You don't get results for users missing in the <code>users</code> table, even if you have entries in <code>user_msg_log</code>. Typically, you would have a <strong>foreign key</strong> constraint enforcing referential integrity to rule that out.</p>

<p>You also don't get a row for any user that has no matching entry in <code>user_msg_log</code>. That conforms to your original question. If you need to include those rows in the result use <strong><code>LEFT JOIN LATERAL ... ON true</code></strong> instead of <code>CROSS JOIN LATERAL</code>:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/26107915/call-a-set-returning-function-with-an-array-argument-multiple-times/26514968#26514968"">Call a set-returning function with an array argument multiple times</a></li>
</ul>

<p>This form is also best to retrieve <strong>more than one rows</strong> (but not all) per user. Just use <strong><code>LIMIT n</code></strong> instead of <code>LIMIT 1</code>.</p>

<p>Effectively, all of these would do the same:</p>

<pre><code>JOIN LATERAL ... ON true
CROSS JOIN LATERAL ...
, LATERAL ...
</code></pre>

<p>The latter has a lower priority, though. Explicit <code>JOIN</code> binds before comma.</p>

<h3>2b. Correlated subquery</h3>

<p>Good choice to retrieve a <strong>single column</strong> from a <strong>single row</strong>. Code example:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/24244026/optimize-groupwise-maximum-query/24377356#24377356"">Optimize groupwise maximum query</a></li>
</ul>

<p>The same is possible for <strong>multiple columns</strong>, but you need more smarts:</p>

<pre><code>CREATE TEMP TABLE combo (aggr_date date, running_total int);

SELECT user_id, (my_combo).*  -- note the parentheses
FROM (
   SELECT u.user_id
        , (SELECT (aggr_date, running_total)::combo
           FROM   user_msg_log
           WHERE  user_id = u.user_id
           AND    aggr_date &lt;= :mydate
           ORDER  BY aggr_date DESC NULLS LAST
           LIMIT  1) AS my_combo
   FROM   users u
   ) sub;
</code></pre>

<ul>
<li><p>Like <code>LEFT JOIN LATERAL</code> above, this variant includes <em>all</em> users, even without entries in <code>user_msg_log</code>. You get <code>NULL</code> for <code>my_combo</code>, which you can easily filter with a <code>WHERE</code> clause in the outer query if need be.<br>
<sub>Nitpick: in the outer query you can't distinguish whether the subquery didn't find a row or all values returned happen to be NULL - same result. You would have to include a <code>NOT NULL</code> column in the subquery to be sure.</sub></p></li>
<li><p>A correlated subquery can only return a <strong>single value</strong>. You can wrap multiple columns into a composite type. But to decompose it later, Postgres demands a well-known composite type. Anonymous records can only be decomposed providing a column definition list.</p></li>
<li><p>Use a registered type like the row type of an existing table, or create a type. Register a composite type explicitly (and permanently) with <code>CREATE TYPE</code>, or create a temporary table (dropped automatically at end of session) to provide a row type temporarily. Cast to that type: <code>(aggr_date, running_total)::combo</code></p></li>
<li><p>Finally, we do not want to decompose <code>combo</code> on the same query level. Due to a weakness in the query planner this would evaluate the subquery once for each column (up to Postgres 9.6 - improvements are planned for Postgres 10). Instead, make it a subquery and decompose in the outer query.</p></li>
</ul>

<p>Related:</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/25170215/get-values-from-first-and-last-row-per-group/25173081#25173081"">Get values from first and last row per group</a></li>
</ul>

<p>Demonstrating all 4 queries with 100k log entries and 1k users:<br>
<a href=""http://sqlfiddle.com/#!15/56cd3/30"" rel=""noreferrer""><strong>SQL Fiddle</strong></a> - pg 9.6<br>
<em>db&lt;>fiddle <a href=""https://dbfiddle.uk/?rdbms=postgres_10&amp;fiddle=05b6e1d34b05ff5b29996e4444488d12"" rel=""noreferrer"">here</a></em> - pg 10</p>
"
"<p>I have already seen 
<a href=""http://dev.mysql.com/doc/refman/4.1/en/mysql-config-wizard-file-location.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/4.1/en/mysql-config-wizard-file-location.html</a></p>

<p><a href=""https://stackoverflow.com/questions/2482234/how-to-know-mysql-my-cnf-location"">how to know mysql my.cnf location</a></p>

<p>and </p>

<p><a href=""http://dev.mysql.com/doc/refman/5.1/en/option-files.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.1/en/option-files.html</a></p>

<p>But I am still stuck with the ages old question!
""Where is my my.ini""
I am using windows server 2008 with mysql 5.5.28. I installed the service using mysqld --install and I am able to use the mysql server using sqlyog. But unfortunately I am not able to find my.ini in <code>installation directory</code> or not in <code>c:\</code> neither in <code>c:\windows</code> nor in <code>data_dir</code> query <code>show variables like ""mysql_home""</code> returned nothing as well.</p>

<p>Any suggestions?</p>
","<p><strong>my.ini LOCATION ON WINDOWS MYSQL 5.6 MSI (USING THE INSTALL WIZARD)</strong></p>

<p>Open a Windows command shell and type: <code>echo %PROGRAMDATA%</code>. On Windows Vista this results in: <code>C:\ProgramData</code>.</p>

<p>According to <a href=""http://dev.mysql.com/doc/refman/5.6/en/option-files.html"">http://dev.mysql.com/doc/refman/5.6/en/option-files.html</a>, the first location MySQL will look under is in <code>%PROGRAMDATA%\MySQL\MySQL Server 5.6\my.ini</code>. In your Windows shell if you do <code>ls ""%PROGRAMDATA%\MySQL\MySQL Server 5.6\my.ini""</code>, you will see that the file is there.</p>

<p><strong>Unlike most suggestions you will find in Stackoverflow and around the web, putting the file in <code>C:\Program Files\MySQL\MySQL Server 5.6\my.ini</code> WILL NOT WORK. Neither will <code>C:\Program Files (x86)\MySQL\MySQL Server 5.1</code>.</strong> The reason being quoted on the MySQL link posted above:</p>

<blockquote>
  <p>On Windows, MySQL programs read startup options from the following
  files, in the specified order (top items are used first).</p>
</blockquote>

<p>The 5.6 MSI installer <b>does</b> create a my.ini in the <b>highest</b> priority location, meaning no other file will ever be found/used, except for the one created by the installer.</p>

<p>The solution accepted above will not work for 5.6 MSI-based installs.</p>
"
"<p>There is a MySQL <code>table</code> which has this definition taken from <code>SQLYog Enterprise</code> :</p>

<pre><code>Table              Create Table                                             
-----------------  ---------------------------------------------------------
etape_prospection  CREATE TABLE `etape_prospection` (                       
                     `etape_prosp_id` int(10) NOT NULL AUTO_INCREMENT,      
                     `type_prosp_id` int(10) NOT NULL DEFAULT '0',          
                     `prosp_id` int(10) NOT NULL DEFAULT '0',               
                     `etape_prosp_date` datetime DEFAULT NULL,              
                     `etape_prosp_comment` text,                            
                     PRIMARY KEY (`etape_prosp_id`),                        
                     KEY `concerne_fk` (`prosp_id`),                        
                     KEY `de_type_fk` (`type_prosp_id`)                     
                   ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=latin1  
</code></pre>

<p>I want to change the <code>default charset</code> of this table from <code>latin1</code> to <code>utf8</code>. How to do that ?</p>
","<p>If you want to change the table <code>default character set</code> and all character columns  to a new character set, use a statement like this: </p>

<pre><code>ALTER TABLE tbl_name CONVERT TO CHARACTER SET charset_name;
</code></pre>

<p>So  query will be:</p>

<pre><code>ALTER TABLE etape_prospection CONVERT TO CHARACTER SET utf8;
</code></pre>
"
"<p>When I try to install tSQLt onto an existing database i get the following error:</p>

<blockquote>
  <p>The database owner SID recorded in the master database differs from
  the database owner SID recorded in database ''. You should correct
  this situation by resetting the owner of database '' using the ALTER
  AUTHORIZATION statement.</p>
</blockquote>
","<p>This problem can arise when a database restored from a backup and the SID of the database owner does not match the owners SID listed in the master database.  Here is a solution that uses the ""ALTER AUTHORIZATION"" statement recommended in the error message: </p>

<pre><code>DECLARE @Command VARCHAR(MAX) = 'ALTER AUTHORIZATION ON DATABASE::[&lt;&lt;DatabaseName&gt;&gt;] TO 
[&lt;&lt;LoginName&gt;&gt;]' 

SELECT @Command = REPLACE(REPLACE(@Command 
            , '&lt;&lt;DatabaseName&gt;&gt;', SD.Name)
            , '&lt;&lt;LoginName&gt;&gt;', SL.Name)
FROM master..sysdatabases SD 
JOIN master..syslogins SL ON  SD.SID = SL.SID
WHERE  SD.Name = DB_NAME()

PRINT @Command
EXEC(@Command)
</code></pre>
"
"<p>When I try to install the mysql2 gem, it fails with no apparent errors. Does anyone know what to do to work around this so mysql2 installs?</p>

<pre><code>$ sudo gem install mysql2
Building native extensions.  This could take a while...
ERROR:  Error installing mysql2:
    ERROR: Failed to build gem native extension.

/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/bin/ruby extconf.rb
checking for rb_thread_blocking_region()... no
checking for mysql_query() in -lmysqlclient... no
checking for main() in -lm... yes
checking for mysql_query() in -lmysqlclient... no
checking for main() in -lz... yes
checking for mysql_query() in -lmysqlclient... no
checking for main() in -lsocket... no
checking for mysql_query() in -lmysqlclient... no
checking for main() in -lnsl... no
checking for mysql_query() in -lmysqlclient... no
checking for main() in -lmygcc... no
checking for mysql_query() in -lmysqlclient... no
*** extconf.rb failed ***
Could not create Makefile due to some reason, probably lack of
necessary libraries and/or headers.  Check the mkmf.log file for more
details.  You may need configuration options.

Provided configuration options:
    --with-opt-dir
    --without-opt-dir
    --with-opt-include
    --without-opt-include=${opt-dir}/include
    --with-opt-lib
    --without-opt-lib=${opt-dir}/lib
    --with-make-prog
    --without-make-prog
    --srcdir=.
    --curdir
    --ruby=/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/bin/ruby
    --with-mysql-config
    --without-mysql-config
    --with-mysql-dir
    --without-mysql-dir
    --with-mysql-include
    --without-mysql-include=${mysql-dir}/include
    --with-mysql-lib
    --without-mysql-lib=${mysql-dir}/lib
    --with-mysqlclientlib
    --without-mysqlclientlib
    --with-mlib
    --without-mlib
    --with-mysqlclientlib
    --without-mysqlclientlib
    --with-zlib
    --without-zlib
    --with-mysqlclientlib
    --without-mysqlclientlib
    --with-socketlib
    --without-socketlib
    --with-mysqlclientlib
    --without-mysqlclientlib
    --with-nsllib
    --without-nsllib
    --with-mysqlclientlib
    --without-mysqlclientlib
    --with-mygcclib
    --without-mygcclib
    --with-mysqlclientlib
    --without-mysqlclientlib


Gem files will remain installed in /Library/Ruby/Gems/1.8/gems/mysql2-0.2.6 for inspection.
Results logged to /Library/Ruby/Gems/1.8/gems/mysql2-0.2.6/ext/mysql2/gem_make.out
</code></pre>
","<p>Ubuntu: </p>

<pre><code>sudo apt-get install libmysqlclient-dev  #(mysql development headers)
sudo gem install mysql2 -- --with-mysql-dir=/etc/mysql/
</code></pre>

<p>That's it! </p>

<p>Result:</p>

<pre><code>Building native extensions. This could take a while...
Successfully installed mysql2-0.2.6
1 gem installed
Installing ri documentation for mysql2-0.2.6...
Enclosing class/module 'mMysql2' for class Result not known
Enclosing class/module 'mMysql2' for class Client not known
Installing RDoc documentation for mysql2-0.2.6...
Enclosing class/module 'mMysql2' for class Result not known
Enclosing class/module 'mMysql2' for class Client not known
</code></pre>
"
"<p>Is there any way I can get my Node.js app to communicate with Microsoft SQL?
I haven't seen any MS SQL drivers out there in the wild?</p>

<p>I'm putting a very simple app together and need to be able to communicate with an existing MS SQL database (otherwise I would have gone with mongoDB or Redis)</p>
","<p>The solution is to enable TCP connections which are disabled by default.</p>

<p><img src=""https://i.stack.imgur.com/7ElnG.png"" alt=""enter image description here""></p>
"
"<p>How can I get the SQL Server server and instance name of the current connection, using a T-SQL script?</p>
","<p>Just found the answer, in <a href=""https://stackoverflow.com/questions/129861/how-can-i-query-the-name-of-the-current-sql-server-database-instance"">this SO question</a> (literally, inside the question, not any answer):</p>

<pre><code>SELECT @@servername
</code></pre>

<p>returns servername\instance as far as this is not the default instance</p>

<pre><code>SELECT @@servicename
</code></pre>

<p>returns instance name, even if this is the default (MSSQLSERVER)</p>
"
"<p>How do I convert a LocalDate to a <code>java.sql.Date</code>?</p>

<p>Attempt:</p>

<pre><code>Record r = new Record();
LocalDate date = new Date(1967, 06, 22);
r.setDateOfBirth(new Date(date));
</code></pre>

<p>This fails <em>(won't compile)</em> and all I can find is Joda time stuff.</p>

<p>I'm using Java 8</p>
","<p>The answer is really simple;</p>

<pre><code>import java.sql.Date;
...
LocalDate locald = LocalDate.of(1967, 06, 22);
<strong>Date date = Date.valueOf(locald); // Magic happens here!</strong>
r.setDateOfBirth(date);
</code></pre>

<p>If you would want to convert it the other way around, you do it like this:</p>

<pre><code>Date date = r.getDate();
LocalDate localD = date.toLocalDate();
</code></pre>

<p><code>r</code> is the record you're using in JOOQ and <code>.getDate()</code> is the method for getting the date out of your record; let's say you have a date column called date_of_birth, then your get method should be called <code>getDateOfBirth()</code>.</p>
"
"<p>What are the advantages and disadvantages of storing JSON data in MySQL database vs. serialized array?</p>
","<ol>
<li>JSON <a href=""http://www.php.net/manual/en/function.json-encode.php"" rel=""noreferrer"" title=""json_encode"">encode</a>() &amp; <a href=""http://www.php.net/manual/en/function.json-decode.php"" rel=""noreferrer"" title=""json_decode"">decode</a>()

<ul>
<li>PHP Version >= 5.0.0

<ul>
<li>Nesting Limit of 20.</li>
</ul></li>
<li>PHP Version >= 5.2.3

<ul>
<li>Nesting Limit of 128.</li>
</ul></li>
<li>PHP Version >= 5.3.0

<ul>
<li>Nesting Limit of 512.</li>
</ul></li>
<li>Small footprint vs PHP's serialize'd string.</li>
</ul></li>
<li><a href=""http://www.php.net/manual/en/function.serialize.php"" rel=""noreferrer"" title=""serialize"">serialize</a>() &amp; <a href=""http://www.php.net/manual/en/function.unserialize.php"" rel=""noreferrer"" title=""unserialize"">unserialize</a>()

<ul>
<li>PHP Version >= 4.0.0

<ul>
<li>Methods are not lost on PHP Datatype Object.</li>
<li>__wakeup() magic method called on any object being unserialize. (VERY POWERFUL)</li>
<li>It has been noted that it is some times best the <a href=""http://php.net/manual/en/function.base64-encode.php"" rel=""noreferrer"" title=""base64_encode"">base64 encode</a> strings put into the database, and <a href=""http://php.net/manual/en/function.base64-decode.php"" rel=""noreferrer"" title=""base64_decode"">base64 decode</a> strings taken out of the database with this function, as there are some issues with the handling of some white space characters.</li>
</ul></li>
</ul></li>
</ol>

<p>The choice is yours.</p>
"
"<p>I currently use the following but it ALWAYS prompts me to manually type the password. Is there any way to pass it in on the command line when launching the executable?</p>

<pre><code>mysqladmin processlist -u root -p
</code></pre>
","<p>Just found out the answer....</p>

<pre><code>mysqladmin processlist -u root -pYOURPASSWORDHERE
</code></pre>

<p>No space between your password and the -p</p>
"
"<p>While working on a system I'm creating, I attempted to use the following query in my project:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT
topics.id,
topics.name,
topics.post_count,
topics.view_count,
COUNT( posts.solved_post ) AS solved_post,
(SELECT users.username AS posted_by,
    users.id AS posted_by_id
    FROM users
    WHERE users.id = posts.posted_by)
FROM topics
LEFT OUTER JOIN posts ON posts.topic_id = topics.id
WHERE topics.cat_id = :cat
GROUP BY topics.id
</code></pre>

<p>"":cat"" is bound by my PHP code as I'm using PDO. 2 is a valid value for "":cat"".</p>

<p>That query though gives me an error: ""#1241 - Operand should contain 1 column(s)""</p>

<p>What stumps me is that I would think that this query would work no problem. Selecting columns, then selecting two more from another table, and continuing on from there. I just can't figure out what the problem is.</p>

<p>Is there a simple fix to this, or another way to write my query?</p>
","<p>Syntax error, remove the <code>( )</code> from <code>select</code>.</p>

<pre><code>insert into table2 (name, subject, student_id, result)
select name, subject, student_id, result
from table1;
</code></pre>
"
"<p>How can ALTER be used to drop a column in a MySQL table if that column exists? </p>

<p>I know I can use <code>ALTER my_table DROP COLUMN my_column</code>, but that will throw an error if <code>my_column</code> does not exist. Is there alternative syntax for dropping the column conditionally?</p>

<p>I'm using MySQL version 4.0.18.</p>
","<p><strong>For MySQL, there is none:</strong> <a href=""http://bugs.mysql.com/bug.php?id=10789"" rel=""noreferrer"">MySQL Feature Request</a>. </p>

<p>Allowing this is arguably a really bad idea, anyway: <code>IF EXISTS</code> indicates that you're running destructive operations on a database with (to you) unknown structure. There may be situations where this is acceptable for quick-and-dirty local work, but if you're tempted to run such a statement against production data (in a migration etc.), you're playing with fire.</p>

<p>But if you insist, it's not difficult to simply check for existence first in the client, or to catch the error.</p>

<p>MariaDB also supports the following starting with 10.0.2:</p>

<blockquote>
  <p>DROP [COLUMN] [IF EXISTS] col_name </p>
</blockquote>

<p>i. e.</p>

<blockquote>
  <p>ALTER TABLE my_table DROP IF EXISTS my_column;</p>
</blockquote>

<p>But it's arguably a bad idea to rely on a non-standard feature supported by only one of several forks of MySQL. </p>
"
"<p>I have a table that contains a <code>Xml</code> column:</p>

<pre><code>SELECT * 
FROM Sqm
</code></pre>

<p><img src=""https://i.stack.imgur.com/nVgUG.png"" alt=""enter image description here""></p>

<p>A sample of the <code>xml</code> data of a row would be:</p>

<pre><code>&lt;Sqm version=""1.2""&gt;
  &lt;Metrics&gt;
    &lt;Metric id=""TransactionCleanupThread.RecordUsedTransactionShift"" type=""timer"" unit=""s"" count=""1"" sum=""21490""   average=""21490""   minValue=""73701""    maxValue=""73701""                               &gt;73701&lt;/Metric&gt;
    &lt;Metric id=""TransactionCleanupThread.RefundOldTrans""             type=""timer"" unit=""s"" count=""1"" sum=""184487""  average=""184487""  minValue=""632704""   maxValue=""632704""                              &gt;632704&lt;/Metric&gt;
    &lt;Metric id=""Database.CreateConnection_SaveContextUserGUID""       type=""timer"" unit=""s"" count=""2"" sum=""7562""    average=""3781""    minValue=""12928""    maxValue=""13006""    standardDeviation=""16""     &gt;12967&lt;/Metric&gt;
    &lt;Metric id=""Global.CurrentUser""                                  type=""timer"" unit=""s"" count=""6"" sum=""4022464"" average=""670411""  minValue=""15""       maxValue=""13794345"" standardDeviation=""1642047""&gt;2299194&lt;/Metric&gt;
    &lt;Metric id=""Global.CurrentUser_FetchIdentityFromDatabase""        type=""timer"" unit=""s"" count=""1"" sum=""4010057"" average=""4010057"" minValue=""13752614"" maxValue=""13752614""                            &gt;13752614&lt;/Metric&gt;
  &lt;/Metrics&gt;
&lt;/Sqm&gt;
</code></pre>

<p>In the case of this data, I would want:</p>

<pre><code>SqmId  id                                                   type   unit  count  sum      minValue  maxValue  standardDeviation  Value
=====  ===================================================  =====  ====  =====  ======   ========  ========  =================  ======
1      TransactionCleanupThread.RecordUsedTransactionShift  timer  s    1      21490    73701     73701     NULL               73701
1      TransactionCleanupThread.RefundOldTrans              timer  s    1      184487   632704    632704    NULL               632704
1      Database.CreateConnection_SaveContextUserGUID        timer  s    2      7562     12928     13006     16                 12967
1      Global.CurrentUser                                   timer  s    6      4022464  15        13794345  1642047            2299194
1      Global.CurrentUser_FetchIdentityFromDatabase         timer  s    1      4010057  13752614  13752614  NULL               13752614
2      ...
</code></pre>

<p>In the end I'll actually be performing <code>SUM()</code>, <code>MIN()</code>, <code>MAX()</code> aggregation. But for now I'm just trying to <em>query</em> an xml column.</p>

<p>In pseudo-code, I would try something like:</p>

<pre><code>SELECT
    SqmId,
    Data.query('/Sqm/Metrics/Metric/@id') AS id,
    Data.query('/Sqm/Metrics/Metric/@type') AS type,
    Data.query('/Sqm/Metrics/Metric/@unit') AS unit,
    Data.query('/Sqm/Metrics/Metric/@sum') AS sum,
    Data.query('/Sqm/Metrics/Metric/@count') AS count,
    Data.query('/Sqm/Metrics/Metric/@minValue') AS minValue,
    Data.query('/Sqm/Metrics/Metric/@maxValue') AS maxValue,
    Data.query('/Sqm/Metrics/Metric/@standardDeviation') AS standardDeviation,
    Data.query('/Sqm/Metrics/Metric') AS value
FROM Sqm
</code></pre>

<p>But that SQL query doesn't work:</p>

<blockquote>
  <p>Msg 2396, Level 16, State 1, Line 2<br>
  XQuery [Sqm.data.query()]: Attribute may not appear outside of an element</p>
</blockquote>

<p>I've hunted, and it's amazing how poorly documented, or exampled, Xml querying is. Most resources rather than querying a <strong>table</strong>, query a <strong>variable</strong>; which I'm not doing. Most resources only use xml querying for filtering and selection, rather than reading values. Most resources read hard-coded child nodes (by index), rather than actual values.</p>

<h2>Related resources that I read</h2>

<ul>
<li><a href=""https://stackoverflow.com/questions/966441/xml-query-in-sql-server-2008"">https://stackoverflow.com/questions/966441/xml-query-in-sql-server-2008</a></li>
<li><a href=""https://stackoverflow.com/questions/12913724/sql-server-query-xml-attribute-for-an-element-value"">SQL Server query xml attribute for an element value</a></li>
<li><a href=""https://stackoverflow.com/questions/14343263/sql-querying-xml-attributes"">SQL querying XML attributes</a></li>
<li><a href=""http://www.codeguru.com/csharp/.net/net_data/article.php/c19491/SQL-Server-2005-XQuery-and-XMLDML--Part-1.htm"" rel=""noreferrer"">SQL Server 2005 XQuery and XML-DML - Part 1</a></li>
<li><a href=""http://msdn.microsoft.com/en-us/library/ms345117.aspx"" rel=""noreferrer"">BOL: XML Support in Microsoft SQL Server 2005</a></li>
<li><a href=""http://blog.jontav.com/post/6811942997/querying-xml-in-sql-server"" rel=""noreferrer"">Querying XML in SQL Server</a></li>
<li><a href=""http://www.mssqltips.com/sqlservertip/2889/basic-sql-server-xml-querying/"" rel=""noreferrer"">Basic SQL Server XML Querying</a></li>
<li><a href=""http://technet.microsoft.com/en-us/library/ms191474.aspx"" rel=""noreferrer"">BOL: query() Method (xml Data Type)</a></li>
<li><a href=""http://jacobsebastian.blogspot.ca/2007/11/xml-workshop-v-reading-values-from-xml.html"" rel=""noreferrer"">XML Workshop V - Reading Values from XML Columns</a></li>
<li><a href=""http://blog.sqlauthority.com/2012/04/27/sql-server-introduction-to-discovering-xml-data-type-methods-a-primer/"" rel=""noreferrer"">SQL SERVER  Introduction to Discovering XML Data Type Methods  A Primer</a></li>
</ul>

<h1>Update: .value rather than .query</h1>

<p>I tried randomly using <code>.value</code>, in place of <code>.query</code>:</p>

<pre><code>SELECT
    Sqm.SqmId,
    Data.value('/Sqm/Metrics/Metric/@id', 'varchar(max)') AS id,
    Data.value('/Sqm/Metrics/Metric/@type', 'varchar(max)') AS type,
    Data.value('/Sqm/Metrics/Metric/@unit', 'varchar(max)') AS unit,
    Data.value('/Sqm/Metrics/Metric/@sum', 'varchar(max)') AS sum,
    Data.value('/Sqm/Metrics/Metric/@count', 'varchar(max)') AS count,
    Data.value('/Sqm/Metrics/Metric/@minValue', 'varchar(max)') AS minValue,
    Data.value('/Sqm/Metrics/Metric/@maxValue', 'varchar(max)') AS maxValue,
    Data.value('/Sqm/Metrics/Metric/@standardDeviation', 'varchar(max)') AS standardDeviation,
    Data.value('/Sqm/Metrics/Metric', 'varchar(max)') AS value
FROM Sqm
</code></pre>

<p>But that also doesn't work: </p>

<blockquote>
  <p>Msg 2389, Level 16, State 1, Line 3 XQuery [Sqm.data.value()]:<br>
  'value()' requires a singleton (or empty sequence), found operand of
  type 'xdt:untypedAtomic *'</p>
</blockquote>
","<p>Actually you're close to your goal, you just need to use <a href=""http://technet.microsoft.com/en-us/library/ms188282.aspx"">nodes()</a> method to split your rows and then get values:</p>

<pre><code>select
    s.SqmId,
    m.c.value('@id', 'varchar(max)') as id,
    m.c.value('@type', 'varchar(max)') as type,
    m.c.value('@unit', 'varchar(max)') as unit,
    m.c.value('@sum', 'varchar(max)') as [sum],
    m.c.value('@count', 'varchar(max)') as [count],
    m.c.value('@minValue', 'varchar(max)') as minValue,
    m.c.value('@maxValue', 'varchar(max)') as maxValue,
    m.c.value('.', 'nvarchar(max)') as Value,
    m.c.value('(text())[1]', 'nvarchar(max)') as Value2
from sqm as s
    outer apply s.data.nodes('Sqm/Metrics/Metric') as m(c)
</code></pre>

<p><strong><kbd><a href=""http://sqlfiddle.com/#!3/263bc/5"">sql fiddle demo</a></kbd></strong></p>
"
"<p>After reading <a href=""https://www.gamasutra.com/view/news/170502/Indepth_SQL_Server__High_performance_inserts.php"" rel=""noreferrer"">this article</a> I decided to take a closer look at the way I was using Dapper.</p>

<p>I ran this code on an empty database</p>

<pre><code>var members = new List&lt;Member&gt;();
for (int i = 0; i &lt; 50000; i++)
{
    members.Add(new Member()
    {
        Username = i.toString(),
        IsActive = true
    });
}

using (var scope = new TransactionScope())
{
    connection.Execute(@""
insert Member(Username, IsActive)
values(@Username, @IsActive)"", members);

    scope.Complete();
}
</code></pre>

<p>it took about 20 seconds. That's 2500 inserts/second. Not bad, but not great either considering the blog was achieving 45k inserts/second. Is there a more efficient way to do this in Dapper?</p>

<p>Also, as a side note, running this code through the Visual Studio debugger took <strong>over 3 minutes!</strong> I figured the debugger would slow it down a little, but I was really surprised to see that much.</p>

<p><strong>UPDATE</strong></p>

<p>So this</p>

<pre><code>using (var scope = new TransactionScope())
{
    connection.Execute(@""
insert Member(Username, IsActive)
values(@Username, @IsActive)"", members);

    scope.Complete();
}
</code></pre>

<p>and this</p>

<pre><code>    connection.Execute(@""
insert Member(Username, IsActive)
values(@Username, @IsActive)"", members);
</code></pre>

<p>both took 20 seconds.</p>

<p>But this took 4 seconds!</p>

<pre><code>SqlTransaction trans = connection.BeginTransaction();

connection.Execute(@""
insert Member(Username, IsActive)
values(@Username, @IsActive)"", members, transaction: trans);

trans.Commit();
</code></pre>
","<p>The best I was able to achieve was 50k records in 4 seconds using this approach</p>

<pre><code>SqlTransaction trans = connection.BeginTransaction();

connection.Execute(@""
insert Member(Username, IsActive)
values(@Username, @IsActive)"", members, transaction: trans);

trans.Commit();
</code></pre>
"
"<p>From a dataframe like this</p>

<pre><code>test &lt;- data.frame('id'= rep(1:5,2), 'string'= LETTERS[1:10])
test &lt;- test[order(test$id), ]
rownames(test) &lt;- 1:10

&gt; test
    id string
 1   1      A
 2   1      F
 3   2      B
 4   2      G
 5   3      C
 6   3      H
 7   4      D
 8   4      I
 9   5      E
 10  5      J
</code></pre>

<p>I want to create a new one with the first row of each id / string pair. If sqldf accepted R code within it, the query could look like this:</p>

<pre><code>res &lt;- sqldf(""select id, min(rownames(test)), string 
              from test 
              group by id, string"")

&gt; res
    id string
 1   1      A
 3   2      B
 5   3      C
 7   4      D
 9   5      E
</code></pre>

<p>Is there a solution short of creating a new column like</p>

<pre><code>test$row &lt;- rownames(test)
</code></pre>

<p>and running the same sqldf query with min(row)?</p>
","<p>You can use <code>duplicated</code> to do this very quickly.</p>

<pre><code>test[!duplicated(test$id),]
</code></pre>

<p>Benchmarks, for the speed freaks:</p>

<pre><code>ju &lt;- function() test[!duplicated(test$id),]
gs1 &lt;- function() do.call(rbind, lapply(split(test, test$id), head, 1))
gs2 &lt;- function() do.call(rbind, lapply(split(test, test$id), `[`, 1, ))
jply &lt;- function() ddply(test,.(id),function(x) head(x,1))
jdt &lt;- function() {
  testd &lt;- as.data.table(test)
  setkey(testd,id)
  # Initial solution (slow)
  # testd[,lapply(.SD,function(x) head(x,1)),by = key(testd)]
  # Faster options :
  testd[!duplicated(id)]               # (1)
  # testd[, .SD[1L], by=key(testd)]    # (2)
  # testd[J(unique(id)),mult=""first""]  # (3)
  # testd[ testd[,.I[1L],by=id] ]      # (4) needs v1.8.3. Allows 2nd, 3rd etc
}

library(plyr)
library(data.table)
library(rbenchmark)

# sample data
set.seed(21)
test &lt;- data.frame(id=sample(1e3, 1e5, TRUE), string=sample(LETTERS, 1e5, TRUE))
test &lt;- test[order(test$id), ]

benchmark(ju(), gs1(), gs2(), jply(), jdt(),
    replications=5, order=""relative"")[,1:6]
#     test replications elapsed relative user.self sys.self
# 1   ju()            5    0.03    1.000      0.03     0.00
# 5  jdt()            5    0.03    1.000      0.03     0.00
# 3  gs2()            5    3.49  116.333      2.87     0.58
# 2  gs1()            5    3.58  119.333      3.00     0.58
# 4 jply()            5    3.69  123.000      3.11     0.51
</code></pre>

<p>Let's try that again, but with just the contenders from the first heat and with more data and more replications.</p>

<pre><code>set.seed(21)
test &lt;- data.frame(id=sample(1e4, 1e6, TRUE), string=sample(LETTERS, 1e6, TRUE))
test &lt;- test[order(test$id), ]
benchmark(ju(), jdt(), order=""relative"")[,1:6]
#    test replications elapsed relative user.self sys.self
# 1  ju()          100    5.48    1.000      4.44     1.00
# 2 jdt()          100    6.92    1.263      5.70     1.15
</code></pre>
"
"<p>Here's what I've done so far on my x64 OS:</p>

<ul>
<li>Installed Python (v2.7 --specifically 2.7.6) and added it to the system path (C:\Python27)</li>
<li>Installed MS VS C++ 2010 Express Version (I already had VS 2012 but without the C++ component)</li>
<li>Installed the compiler update for Windows SDK 7.1</li>
<li><p>Successfully executed node-gyp configure (from my add-on directory under nodejs\node_modules where binding.gyp is located)</p></li>
<li><p>ran node-gyp build (as administrator)** This is what crashed, leaving me with:</p></li>
</ul>

<p>this error:</p>

<pre><code>C:\Program Files\nodejs\node_modules\msnodesql&gt;node-gyp build
gyp info it worked if it ends with ok
gyp info using node-gyp@0.12.2
gyp info using node@0.10.25 | win32 | x64
gyp info spawn C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe
gyp info spawn args [ 'build/binding.sln',
gyp info spawn args   '/clp:Verbosity=minimal',
gyp info spawn args   '/nologo',
gyp info spawn args   '/p:Configuration=Release;Platform=x64' ]
Building the projects in this solution one at a time. To enable parallel build, please add the   ""/m"" switch.
</code></pre>

<p><strong>LINK : fatal error LNK1181: cannot open input file 'kernel32.lib' [C:\Program  Files\nodejs\node_modules\msnodesql\build\sqlserver.vcxproj]</strong></p>

<pre><code>gyp ERR! build error
gyp ERR! stack Error: `C:\Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe` failed with exit code: 1
gyp ERR! stack     at ChildProcess.onExit (C:\Users\RNelson\AppData\Roaming\npm\node_modules\node-gyp\lib\build.js:267:23)
gyp ERR! stack     at ChildProcess.EventEmitter.emit (events.js:98:17)
gyp ERR! stack     at Process.ChildProcess._handle.onexit (child_process.js:797:12)
gyp ERR! System Windows_NT 6.1.7601
gyp ERR! command ""node"" ""C:\\Users\\RNelson\\AppData\\Roaming\\npm\\node_modules\\node-      gyp\\bin\\node-gyp.js"" ""build""
gyp ERR! cwd C:\Program Files\nodejs\node_modules\msnodesql
gyp ERR! node -v v0.10.25
gyp ERR! node-gyp -v v0.12.2
gyp ERR! not ok
</code></pre>

<p>Any ideas as to what is going on? Thanks in advance! Just trying to use my node-sqlserver MS driver for Node.js</p>
","<p>I had a similar problem. I found that this switch helped me</p>

<pre><code>--msvs_version=2012
</code></pre>

<p>so for example</p>

<pre><code>npm install --msvs_version=2012 &lt;package&gt;
</code></pre>
"
"<p>I'm adding this table:</p>

<pre><code>CREATE TABLE contenttype (
        contenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT,
        class VARBINARY(50) NOT NULL,
        packageid INT UNSIGNED NOT NULL,
        canplace ENUM('0','1') NOT NULL DEFAULT '0',
        cansearch ENUM('0','1') NOT NULL DEFAULT '0',
        cantag ENUM('0','1') DEFAULT '0',
        canattach ENUM('0','1') DEFAULT '0',
        isaggregator ENUM('0', '1') NOT NULL DEFAULT '0',
        PRIMARY KEY (contenttypeid),
        UNIQUE KEY packageclass (packageid, class)
);
</code></pre>

<p>And I get a 1050 ""table already exists""</p>

<p>But the table does NOT exist. Any ideas?</p>

<p>EDIT: more details because everyone seems to not believe me :)</p>

<pre><code>DESCRIBE contenttype
</code></pre>

<p>yields:</p>

<blockquote>
  <p>1146 - Table 'gunzfact_vbforumdb.contenttype' doesn't exist</p>
</blockquote>

<p>and</p>

<pre><code>CREATE TABLE gunzfact_vbforumdb.contenttype(
contenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT ,
class VARBINARY( 50 ) NOT NULL ,
packageid INT UNSIGNED NOT NULL ,
canplace ENUM( '0', '1' ) NOT NULL DEFAULT '0',
cansearch ENUM( '0', '1' ) NOT NULL DEFAULT '0',
cantag ENUM( '0', '1' ) DEFAULT '0',
canattach ENUM( '0', '1' ) DEFAULT '0',
isaggregator ENUM( '0', '1' ) NOT NULL DEFAULT '0',
PRIMARY KEY ( contenttypeid ) ,
</code></pre>

<p>Yields:</p>

<blockquote>
  <p>1050 - Table 'contenttype' already exists</p>
</blockquote>
","<p>Sounds like you have <a href=""http://en.wikipedia.org/wiki/Schr%C3%B6dinger&#39;s_cat"" rel=""noreferrer"">Schroedinger's table</a>... </p>

<p>Seriously now, you probably have a broken table. Try:</p>

<ul>
<li><code>DROP TABLE IF EXISTS contenttype</code></li>
<li><code>REPAIR TABLE contenttype</code></li>
<li>If you have sufficient permissions, delete the data files (in /mysql/data/db_name)</li>
</ul>
"
"<p>In what seems to be a very odd turn, I am trying to create a Sql Database project in Visual Studio 2012, and am getting this nutty error:</p>

<p><img src=""https://i.stack.imgur.com/CRDtb.png"" alt=""enter image description here""></p>

<p>And clicking on either link leads to a Page Not Found at Microsoft.com, which is odd in itself.</p>

<p>From the bare text of the error message, which is (for search purposes):</p>

<blockquote>
  <p><strong>Unable to open Database Project</strong></p>
  
  <p>This version of SQL Server Data Tools is not compatible with the
  database runtime components installed on this computer.</p>
</blockquote>

<p>Considering that I have Sql Server 2012 Developer Edition installed on the workstation, this seems incredible.  I can open or create a database project in VS2008 with Sql Server 2012 DE installed, so why not VS 2012?</p>
","<p>I Installed SQL Server 2012 Service pack 1 yesterday and then I started getting the problem you describe in Visual Studio 2012. Not only with database projects; I could not use the SQL Server Object Explorer, not open sql-scripts and lots of other weird database related errors. Always with the same message:</p>

<p>""This version of SQL Server Data Tools is not compatible with ... bla, bla, bla ...""</p>

<p>This solution helped me: <a href=""https://web.archive.org/web/20150315060703/http://blog.wharton.com.au/2012/11/16/sql-server-2012-sp-1-breaks-sql-server-database-projects/"" rel=""nofollow noreferrer"">https://web.archive.org/web/20150315060703/http://blog.wharton.com.au/2012/11/16/sql-server-2012-sp-1-breaks-sql-server-database-projects/</a>
Hope this can help you too</p>

<p><strong>Update:</strong> 
With the <em>March 2013 Release</em> it seems you have to also update the SQL Server Data Tools available <a href=""http://msdn.microsoft.com/en-us/jj650015"" rel=""nofollow noreferrer"">here</a> See the comments in <a href=""http://blogs.msdn.com/b/ssdt/archive/2013/03/06/sql-server-data-tier-application-framework-march-2013-available.aspx"" rel=""nofollow noreferrer"">this post</a> for more details.</p>
"
"<p>If you schedule a SQL Server job to run every X number of minutes, and it does not finish the previous call before the # of minutes is up, will it skip the run since it is already running, or will it run two instances of the job doing the same steps?</p>
","<p>Try something like this:</p>

<pre><code>DECLARE @jobId binary(16)

SELECT @jobId = job_id FROM msdb.dbo.sysjobs WHERE (name = N'Name of Your Job')
IF (@jobId IS NOT NULL)
BEGIN
    EXEC msdb.dbo.sp_delete_job @jobId
END

DECLARE @ReturnCode int
EXEC @ReturnCode =  msdb.dbo.sp_add_job @job_name=N'Name of Your Job'
</code></pre>

<p>Best to read the docs on all the parameters required for <a href=""http://msdn.microsoft.com/en-us/library/ms182079.aspx"" rel=""noreferrer"">'sp_add_job'</a> and <a href=""http://msdn.microsoft.com/en-us/library/ms188376.aspx"" rel=""noreferrer"">'sp_delete_job'</a></p>
"
"<p>In MySQL 5.0 why does the following error occur when trying to create a view with a subquery in the FROM clause?</p>

<blockquote>
  <p>ERROR 1349 (HY000): View's SELECT contains a subquery in the FROM clause</p>
</blockquote>

<p>If this is a limitation of the MySQL engine, then why haven't they implemented this feature yet?</p>

<p>Also, what are some good workarounds for this limitation?</p>

<p>Are there any workarounds that work for any subquery in the FROM clause or are there some queries that can not be expressed without using a subquery in the FROM clause?</p>

<hr>

<p>An example query (was buried in a comment):</p>

<pre><code>SELECT temp.UserName 
FROM (SELECT u1.name as UserName, COUNT(m1.UserFromId) as SentCount 
      FROM Message m1, User u1 
      WHERE u1.uid = m1.UserFromId 
      Group BY u1.name HAVING SentCount &gt; 3 ) as temp
</code></pre>
","<p>I had the same problem.  I wanted to create a view to show information of the most recent year, from a table with records from 2009 to 2011.  Here's the original query:</p>

<pre><code>SELECT a.* 
FROM a 
JOIN ( 
  SELECT a.alias, MAX(a.year) as max_year 
  FROM a 
  GROUP BY a.alias
) b 
ON a.alias=b.alias and a.year=b.max_year
</code></pre>

<p>Outline of solution:</p>

<ol>
<li>create a view for each subquery</li>
<li>replace subqueries with those views</li>
</ol>

<p>Here's the solution query:</p>

<pre><code>CREATE VIEW v_max_year AS 
  SELECT alias, MAX(year) as max_year 
  FROM a 
  GROUP BY a.alias;

CREATE VIEW v_latest_info AS 
  SELECT a.* 
  FROM a 
  JOIN v_max_year b 
  ON a.alias=b.alias and a.year=b.max_year;
</code></pre>

<p>It works fine on mysql 5.0.45, without much of a speed penalty (compared to executing
the original sub-query select without any views).</p>
"
"<p>I am trying to install pymssql on ubuntu 12.04 using pip. This is the error I am getting. Any help would be greatly appreciated as I am completely lost!</p>

<p>Tried googling this but unfortunately to no avail...</p>

<pre><code>  Downloading pymssql-2.0.0b1-dev-20130403.tar.gz (2.8Mb): 2.8Mb downloaded
  Running setup.py egg_info for package pymssql
    warning: no files found matching '*.pyx' under directory 'Cython/Debugger/Tests'
    warning: no files found matching '*.pxd' under directory 'Cython/Debugger/Tests'
    warning: no files found matching '*.h' under directory 'Cython/Debugger/Tests'
    warning: no files found matching '*.pxd' under directory 'Cython/Utility'
    Compiling module Cython.Plex.Scanners ...
    Compiling module Cython.Plex.Actions ...
    Compiling module Cython.Compiler.Lexicon ...
    Compiling module Cython.Compiler.Scanning ...
    Compiling module Cython.Compiler.Parsing ...
    Compiling module Cython.Compiler.Visitor ...
    Compiling module Cython.Compiler.FlowControl ...
    Compiling module Cython.Compiler.Code ...
    Compiling module Cython.Runtime.refnanny ...

    Installed /home/radek/build/pymssql/Cython-0.19.1-py2.7-linux-x86_64.egg
    cc -c /tmp/clock_gettimeh7sDgX.c -o tmp/clock_gettimeh7sDgX.o
    cc tmp/clock_gettimeh7sDgX.o -lrt -o a.out

    warning: no files found matching 'win32/freetds.zip'
Installing collected packages: pymssql
  Running setup.py install for pymssql
    skipping '_mssql.c' Cython extension (up-to-date)
    building '_mssql' extension
    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/radek/build/pymssql/freetds/nix_64/include -I/usr/include/python2.7 -c _mssql.c -o build/temp.linux-x86_64-2.7/_mssql.o -Wno-parentheses-equality -DMSDBLIB
    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/_mssql.o -L/home/radek/build/pymssql/freetds/nix_64/lib -lsybdb -lct -lrt -o build/lib.linux-x86_64-2.7/_mssql.so
    /usr/bin/ld: cannot find -lct
    collect2: ld returned 1 exit status
    error: command 'gcc' failed with exit status 1
    Complete output from command /usr/bin/python -c ""import setuptools;__file__='/home/radek/build/pymssql/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --single-version-externally-managed --record /tmp/pip-Et_P1_-record/install-record.txt:
    running install

running build

running build_ext

skipping '_mssql.c' Cython extension (up-to-date)

building '_mssql' extension

creating build

creating build/temp.linux-x86_64-2.7

gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/radek/build/pymssql/freetds/nix_64/include -I/usr/include/python2.7 -c _mssql.c -o build/temp.linux-x86_64-2.7/_mssql.o -Wno-parentheses-equality -DMSDBLIB

creating build/lib.linux-x86_64-2.7

gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/_mssql.o -L/home/radek/build/pymssql/freetds/nix_64/lib -lsybdb -lct -lrt -o build/lib.linux-x86_64-2.7/_mssql.so

/usr/bin/ld: cannot find -lct

collect2: ld returned 1 exit status

error: command 'gcc' failed with exit status 1

----------------------------------------
Command /usr/bin/python -c ""import setuptools;__file__='/home/radek/build/pymssql/setup.py';exec(compile(open(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --single-version-externally-managed --record /tmp/pip-Et_P1_-record/install-record.txt failed with error code 1
</code></pre>
","<p>Make sure you have the <code>python-dev</code> package installed (needed to compile packages with native bindings (thanks to @ravihemnani).</p>

<p>Then you'll need to install the FreeTDS development package (<code>freetds-dev</code>) before trying to install <code>pymssql</code> with pip:</p>

<pre><code>$ sudo apt-get install freetds-dev
</code></pre>

<p>and then, in your <em>virtualenv</em> or wherever you wish to install it:</p>

<pre><code>$ pip install pymssql
</code></pre>
"
"<p>So this is something we all should know about, and played on my mind when I first seen it..</p>

<p>I know that <code>mysql_escape_string</code> is deprecated from 5.3 but what was the actual difference in <code>mysql_real_escape_string</code>.</p>

<p>What I thought was that <code>mysql_real_escape_string</code> is the exact same as <code>mysql_escape_string</code> apart from <code>mysql_real_escape_string</code> takes a second argument for the mysql resource.</p>

<p>so then I thought well surly there must be some difference as to how strings are handled because there would not be a need for 2 functions.</p>

<p>So then I thought that the difference was purely down to locale and character encodings. ?</p>

<p>can anyone clear this up for me ?</p>
","<p>The difference is that <code>mysql_escape_string</code> just treats the string as raw bytes, and adds escaping where it believes it's appropriate.</p>

<p><code>mysql_real_escape_string</code>, on the other hand, uses the information about the character set used for the MySQL connection. This means the string is escaped while treating multi-byte characters properly; i.e., it won't insert escaping characters in the middle of a character. This is why you need a connection for <code>mysql_real_escape_string</code>; it's necessary in order to know how the string should be treated.</p>

<p>However, instead of escaping, it's a better idea to use parameterized queries from the MySQLi library; there has previously been bugs in the escaping routine, and it's possible that some could appear again. Parameterizing the query is much, much harder to mess up, so it's less likely that you can get compromised by a MySQL bug.</p>
"
"<p>In my website, I am using MySQL database. I am using a webservice where in I do all my database related manipulations.</p>

<p>Now In one of the methods of that webservice, I get the following Error.</p>

<blockquote>
  <p>select command denied to user ''@'' for table ''</p>
</blockquote>

<p>What could be wrong?</p>

<p>Below is the code where I get that error. I tried debugging and found that it fails at the line</p>

<p><code>MySqlDataReader result1 = command1.ExecuteReader();</code></p>

<p>Here is my code:</p>

<pre><code>        String addSQL = ""Select Max(`TradeID`) from `jsontest`.`tbl_Positions"";
        MySqlConnection objMyCon = new MySqlConnection(strProvider);
        objMyCon.Open();
        MySqlCommand command = objMyCon.CreateCommand();

        command.CommandText = addSQL;
         MySqlDataReader result = command.ExecuteReader();
        //int j = command.ExecuteNonQuery();
         while (result.Read())
         {
             MaxTradeID = Convert.ToInt32(result[0]);
         }
        objMyCon.Close();
        for (i = 1; i &lt;= MaxTradeID; i++)
        {
            String newSQL = ""Select `Strike`,`LongShort`,`Current`,`TPLevel`,`SLLevel` from `json`.`tbl_Position` where `TradeID` = '"" + i + ""'"";
            MySqlConnection objMyCon1 = new MySqlConnection(strProvider);
            objMyCon1.Open();
            MySqlCommand command1 = objMyCon1.CreateCommand();

            command1.CommandText = newSQL;
            MySqlDataReader result1 = command1.ExecuteReader();
           objMyCon2.Close();
</code></pre>
","<p>I'm sure the original poster's issue has long since been resolved.  However, I had this same issue, so I thought I'd explain what was causing this problem for me.</p>

<p>I was doing a union query with two tables -- 'foo' and 'foo_bar'.  However, in my SQL statement, I had a typo: 'foo.bar'</p>

<p>So, instead of telling me that the 'foo.bar' table doesn't exist, the error message indicates that the command was denied -- as though I don't have permissions.</p>

<p>Hope this helps someone.</p>
"
"<p>Maybe it's an obvious question, but I want to be sure.</p>

<p>How can i know if it MySQLnd is the active driver?</p>

<p>I'm runing PHP 5.3 and MySQL 5.1.37.
In phpinfo() mysqlnd is listed but only with this I can't be sure if I'm using MySQLnd or the old driver...</p>

<p>Extract of phpinfo() output</p>

<pre><code>mysql
MySQL Support   enabled
Active Persistent Links     0
Active Links    0
Client API version  mysqlnd 5.0.5-dev - 081106 - $Revision: 1.3.2.27 $ 

mysqli
MysqlI Support  enabled
Client API library version  mysqlnd 5.0.5-dev - 081106 - $Revision: 1.3.2.27 $
Active Persistent Links     0
Inactive Persistent Links   0
Active Links    26 

mysqlnd
mysqlnd enabled
Version     mysqlnd 5.0.5-dev - 081106 - $Revision: 1.3.2.27 $ 

PDO
PDO support enabled
PDO drivers     mysql

pdo_mysql
PDO Driver for MySQL    enabled
Client API version  mysqlnd 5.0.5-dev - 081106 - $Revision: 1.3.2.27 $ 
</code></pre>

<p>I'm using PDO, and PDO driver says mysql...</p>
","<p>This should do the trick:</p>

<pre><code>&lt;?php
$mysqlnd = function_exists('mysqli_fetch_all');

if ($mysqlnd) {
    echo 'mysqlnd enabled!';
}
</code></pre>

<p>To detect if its the active PDO driver, create your MySQL PDO object then:</p>

<pre><code>if (strpos($pdo-&gt;getAttribute(PDO::ATTR_CLIENT_VERSION), 'mysqlnd') !== false) {
    echo 'PDO MySQLnd enabled!';
}
</code></pre>
"
"<p>I have ran <code>aptitude install php5-mysql</code> (and restarted MySQL/Apache 2), but I am still getting this error:</p>

<blockquote>
  <p>Fatal error: Call to undefined function mysql_connect() in /home/validate.php on line 21 </p>
</blockquote>

<p><code>phpinfo()</code> says the /etc/php5/apache2/conf.d/pdo_mysql.ini file has been parsed.</p>
","<p>I see that you tagged this with Ubuntu. Most likely the MySQL driver (and possibly MySQL) is not installed. Assuming you have <a href=""http://en.wikipedia.org/wiki/Secure_Shell"" rel=""noreferrer"">SSH</a> or terminal access and sudo permissions, log into the server and run this:</p>

<pre><code>sudo apt-get install mysql-server mysql-client php5-mysql
</code></pre>

<p>If the MySQL packages or the php5-mysql package are already installed, this will update them.</p>

<hr>

<p><strong>UPDATE</strong></p>

<p>Since this answer still gets the occasional click I am going to update it to include <strong>PHP 7</strong>. PHP 7 requires a different package for MySQL so you will want to use a different argument for the apt-get command.</p>

<pre><code>sudo apt-get install mysql-server mysql-common php7.0 php7.0-mysql
</code></pre>

<p>And importantly, <code>mysql_connect()</code> has been deprecated since PHP v5.5.0. Refer the official documentation here: <a href=""http://php.net/manual/en/function.mysql-connect.php"" rel=""noreferrer"">PHP: mysql_connect()</a></p>
"
"<p>I'm using a ResultSet in Java, and am not sure how to properly close it. I'm considering using the ResultSet to construct a HashMap and then closing the ResultSet after that. Is this HashMap technique efficient, or are there more efficient ways of handling this situation? I need both keys and values, so using a HashMap seemed like a logical choice.</p>

<p>If using a HashMap is the most efficient method, how do I construct and use the HashMap in my code?</p>

<p>Here's what I've tried:</p>

<pre><code>public HashMap resultSetToHashMap(ResultSet rs) throws SQLException {

  ResultSetMetaData md = rs.getMetaData();
  int columns = md.getColumnCount();
  HashMap row = new HashMap();
  while (rs.next()) {
     for (int i = 1; i &lt;= columns; i++) {
       row.put(md.getColumnName(i), rs.getObject(i));
     }
  }
  return row;
}
</code></pre>
","<ol>
<li>Iterate over the ResultSet </li>
<li>Create a new Object for each row, to store the fields you need</li>
<li>Add this new object to ArrayList or Hashmap or whatever you fancy</li>
<li>Close the ResultSet, Statement and the DB connection</li>
</ol>

<p>Done</p>

<p>EDIT: now that you have posted code, I have made a few changes to it.</p>

<pre><code>public List resultSetToArrayList(ResultSet rs) throws SQLException{
  ResultSetMetaData md = rs.getMetaData();
  int columns = md.getColumnCount();
  ArrayList list = new ArrayList(50);
  while (rs.next()){
     HashMap row = new HashMap(columns);
     for(int i=1; i&lt;=columns; ++i){           
      row.put(md.getColumnName(i),rs.getObject(i));
     }
      list.add(row);
  }

 return list;
}
</code></pre>
"
"<p>What are the differences between a query, a native query, a named query and a typed query? Does the 'alone-standing' query even exist, or is it just an abbreviation? In my mind, a native Query is a query written in simple sql, whereas a named query relates to entities (hibernate-mapping). Can someone explain this briefly?</p>
","<p>You could maybe use a result transformer. Quoting <a href=""http://in.relation.to/2006/03/17/hibernate-32-transformers-for-hql-and-sql/"" rel=""noreferrer"">Hibernate 3.2: Transformers for HQL and SQL</a>:</p>

<blockquote>
  <h3>SQL Transformers</h3>
  
  <p>With native sql returning non-entity
  beans or Map's is often more useful
  instead of basic <code>Object[]</code>. With
  result transformers that is now
  possible.</p>

<pre><code>List resultWithAliasedBean = s.createSQLQuery(
  ""SELECT st.name as studentName, co.description as courseDescription "" +
  ""FROM Enrolment e "" +
  ""INNER JOIN Student st on e.studentId=st.studentId "" +
  ""INNER JOIN Course co on e.courseCode=co.courseCode"")
  .addScalar(""studentName"")
  .addScalar(""courseDescription"")
  .setResultTransformer( Transformers.aliasToBean(StudentDTO.class))
  .list();

StudentDTO dto =(StudentDTO) resultWithAliasedBean.get(0);
</code></pre>
  
  <p>Tip: the <code>addScalar()</code> calls were
  required on HSQLDB to make it match a
  property name since it returns column
  names in all uppercase (e.g.
  ""STUDENTNAME""). This could also be
  solved with a custom transformer that
  search the property names instead of
  using exact match - maybe we should
  provide a fuzzyAliasToBean() method ;)</p>
</blockquote>

<h3>References</h3>

<ul>
<li>Hibernate Reference Guide

<ul>
<li><a href=""http://docs.jboss.org/hibernate/core/3.3/reference/en/html/querysql.html#d0e13904"" rel=""noreferrer"">16.1.5. Returning non-managed entities</a></li>
</ul></li>
<li>Hibernate's Blog

<ul>
<li><a href=""http://in.relation.to/2006/03/17/hibernate-32-transformers-for-hql-and-sql/"" rel=""noreferrer"">Hibernate 3.2: Transformers for HQL and SQL</a> </li>
</ul></li>
</ul>
"
"<p>I am trying to use SQLClient library in the ASP.net Core but cant seem to get it working.  I found this article online advising how to setup but its not working for me: <a href=""http://blog.developers.ba/using-classic-ado-net-in-asp-net-vnext/"" rel=""noreferrer"">http://blog.developers.ba/using-classic-ado-net-in-asp-net-vnext/</a></p>

<p>I have a simple console application package.  My project.json looks like this:</p>

<pre><code>{
  ""version"": ""1.0.0-*"",
  ""description"": ""DBTest Console Application"",
  ""authors"": [ """" ],
  ""tags"": [ """" ],
  ""projectUrl"": """",
  ""licenseUrl"": """",

  ""compilationOptions"": {
    ""emitEntryPoint"": true
  },

  ""dependencies"": {
    ""System.Data.Common"": ""4.0.1-beta-23516"",
    ""System.Data.SqlClient"" :  ""4.0.0-beta-23516""
  },

  ""commands"": {
    ""DBTest"": ""DBTest""
  },

  ""frameworks"": {
    ""dnx451"": { },
    ""dnxcore50"": {
      ""dependencies"": {
        ""Microsoft.CSharp"": ""4.0.1-beta-23516"",
        ""System.Collections"": ""4.0.11-beta-23516"",
        ""System.Console"": ""4.0.0-beta-23516"",
        ""System.Linq"": ""4.0.1-beta-23516"",
        ""System.Threading"": ""4.0.11-beta-23516""
      }
    }
  }
}
</code></pre>

<p>And I try the following code:</p>

<pre><code>using System;
using System.Data.SqlClient;

namespace DBTest
{
    public class Program
    {
        public static void Main(string[] args)
        {
            using (SqlConnection con = new SqlConnection(ConnStr)) {
                con.Open();
                try {
                    using (SqlCommand command = new SqlCommand(""SELECT * FROM SAMPLETABLE"", con)) {
                        command.ExecuteNonQuery();
                    }
                }
                catch {
                    Console.WriteLine(""Something went wrong"");
                }
            }

            Console.Read();
        }
    }
}
</code></pre>

<p>But get the following errors:</p>

<p><a href=""https://i.stack.imgur.com/3psE9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3psE9.png"" alt=""enter image description here""></a></p>

<p>Anyone else got this working?</p>
","<p>I think you may have missed this part in the tutorial:</p>



<blockquote>
  <p>Instead of referencing System.Data and System.Data.SqlClient you need
  to grab from Nuget:</p>
  
  <p>System.Data.Common and System.Data.SqlClient.</p>
  
  <p>Currently this creates dependency in project.json > aspnetcore50
  section to these two libraries.</p>

<pre class=""lang-json prettyprint-override""><code>""aspnetcore50"": {
       ""dependencies"": {
           ""System.Runtime"": ""4.0.20-beta-22523"",
           ""System.Data.Common"": ""4.0.0.0-beta-22605"",
           ""System.Data.SqlClient"": ""4.0.0.0-beta-22605""
       }
}
</code></pre>
</blockquote>

<p>Try getting System.Data.Common and System.Data.SqlClient <strong>via Nuget</strong> and see if this adds the above dependencies for you, but in a nutshell you are missing <strong>System.Runtime.</strong></p>
"
"<p>I'm running this from PyDev in Eclipse...</p>

<pre><code>import pymysql
conn = pymysql.connect(host='localhost', port=3306, user='userid', passwd='password', db='fan')
cur = conn.cursor()
print ""writing to db""
cur.execute(""INSERT INTO cbs_transactions(leagueID) VALUES ('test val')"")
print ""wrote to db""
</code></pre>

<p>The result is, at the top of the Console it says  C:...test.py, and in the Console:</p>

<p>writing to db
wrote to db</p>

<p>So it's not terminating until after the execute command.  But when I look in the table in MySQL it's empty.  A record did not get inserted.</p>

<p>First off, why isn't it writing the record.  Second, how can I see a log or error to see what happened.  Usually there should be some kind of error in red if the code fails.</p>
","<p>Did you commit it? <code>conn.commit()</code> </p>
"
"<p>I am integrating SqlCacheDependency to use in my LinqToSQL datacontext.</p>

<p>I am using an extension class for Linq querys found here - <a href=""http://code.msdn.microsoft.com/linqtosqlcache"" rel=""noreferrer"">http://code.msdn.microsoft.com/linqtosqlcache</a></p>

<p>I have wired up the code and when I open the page I get this exception - </p>

<p>""The SQL Server Service Broker for the current database is not enabled, and as a result query notifications are not supported.  Please enable the Service Broker for this database if you wish to use notifications.""</p>

<p>its coming from this event in the global.asax</p>

<pre><code>        protected void Application_Start()
    {
        RegisterRoutes(RouteTable.Routes);
        //In Application Start Event
        System.Data.SqlClient.SqlDependency.Start(new dataContextDataContext().Connection.ConnectionString);

    }
</code></pre>

<p>my question is...</p>

<ol>
<li><p>how do i enable Service Broker in my SQL server 2008 database? I have tried to run this query.. ALTER DATABASE tablename SET ENABLE_BROKER but it never ends and runs for ever, I have to manually stop it.</p></li>
<li><p>once I have this set in SQL server 2008, will it filter down to my DataContext, or do I need to configure something there too ?</p></li>
</ol>

<p>thanks for any help</p>

<p>Truegilly</p>
","<p>In case anyone else is looking for a solution to this problem, the following command worked great for me.  It releases all other connections to the database instead of waiting.</p>

<pre><code>ALTER DATABASE [DBNAME] SET ENABLE_BROKER WITH ROLLBACK IMMEDIATE
</code></pre>
"
"<p>After installing the release version of VS2012, I am unable to find SQL CLR proect template.
How can I go about creating a project of this type.</p>

<p>Thanks!</p>
","<p>Try</p>

<pre><code>CREATE FUNCTION NormalizeString(@s nvarchar(max), 
                                @normalizationForm nvarchar(50)) 
RETURNS nvarchar(max)
AS EXTERNAL NAME CLRFunctions.[CLRFunctions.T].NormalizeString
</code></pre>
"
"<p>Cannot login to MySQL database after fresh install with root ID and empty/no password like other older MySQL versions do</p>
","<p>After you installed MySQL-community-server 5.7 from fresh on linux, you will need to find the temporary password from /var/log/mysqld.log to login as root. </p>

<ol>
<li><code>grep 'temporary password' /var/log/mysqld.log</code></li>
<li>Run <code>mysql_secure_installation</code> to change new password</li>
</ol>

<p>ref: <a href=""http://dev.mysql.com/doc/refman/5.7/en/linux-installation-yum-repo.html"">http://dev.mysql.com/doc/refman/5.7/en/linux-installation-yum-repo.html</a></p>
"
"<p>I have a backup script for my MySQL database, using <code>mysqldump</code> with the <code>--tab</code> option so it produces a <code>.sql</code> file for the structure and a <code>.txt</code> file (pipe-separated) for the content.</p>

<p>Some tables have foreign keys, so when I import it I'm getting the error:</p>

<blockquote>
  <p>ERROR 1217 (23000) at line 8: Cannot delete or update a parent row: a foreign key constraint fails</p>
</blockquote>

<p>I know about using <code>SET FOREIGN_KEY_CHECKS=0</code> (and <code>SET FOREIGN_KEY_CHECKS=1</code> afterward). If I add those to each <code>.sql</code> file then the import works. But then obviously on the next <code>mysqldump</code> those get overwritten.</p>

<p>I also tried running it as a separate command, like below but the error comes back:</p>

<pre><code>echo ""SET FOREIGN_KEY_CHECKS=0"" | mysql [user/pass/database] 
[all the imports]
echo ""SET FOREIGN_KEY_CHECKS=1"" | mysql [user/pass/database] 
</code></pre>

<p>Is there some other way to disable FK checks on the command line?</p>
","<p>The accepted answer by RandomSeed could take a long time!  Importing the table (just to drop it later) could be very wasteful depending on size.</p>

<p>For a file created using</p>

<pre><code>mysqldump -u user -ppasswd --opt --routines DBname &gt; DBdump.sql
</code></pre>

<p>I currently get a file about 7GB, 6GB of which is data for a log table that I don't 'need' to be there; reloading this file takes a couple of hours.  If I need to reload (for development purposes, or if ever required for a live recovery) I skim the file thus:</p>

<pre><code>sed '/INSERT INTO `TABLE_TO_SKIP`/d' DBdump.sql &gt; reduced.sql
</code></pre>

<p>And reload with:</p>

<pre><code>mysql -u user -ppasswd DBname &lt; reduced.sql
</code></pre>

<p>This gives me a complete database, with the ""unwanted"" table created but empty.  If you really don't want the tables at all, simply drop the empty tables after the load finishes.</p>

<p>For multiple tables you could do something like this:</p>

<pre><code>sed '/INSERT INTO `TABLE1_TO_SKIP`/d' DBdump.sql | \
sed '/INSERT INTO `TABLE2_TO_SKIP`/d' | \
sed '/INSERT INTO `TABLE3_TO_SKIP`/d' &gt; reduced.sql
</code></pre>

<p>There IS a 'gotcha' - watch out for procedures in your dump that might contain ""INSERT INTO TABLE_TO_SKIP"".</p>
"
"<p>I have a web application using sqlalchemy (within Pylons). I need to effiently change the schema to be able to change the production version at least on a daily basis, maybe more, without losing the data.</p>

<p>I have played a little bit with sqlalchemy-migrate over the week-end and I would say that it gave me a bad impression. First <strong>I think it cannot help with migration between two databases engines</strong>; that's something that could probably be done with sqlalchemy alone. 
Second the docs do not seem up to date. I had to change some command-line options, like giving the repository path at each command, this could be a bug of migrate. </p>

<p>But the worst thing it the ""manage.py <strong>test</strong>"" command. Not only it actually <strong>modifies the database</strong> (this point is clearly indicated in the documentation so I can't blame migrate) but my first migration script just made plain stupid schema migration, leaving the upgraded-downgraded db with a <strong>different schema than the original</strong>. But the ""manage.py test"" just answered something like</p>

<pre><code> success !
</code></pre>

<p>That is, it did not even check if the schema was left in a coherent state. 
So <strong>is it worth using migrate?</strong> Is there any advantage compared to the Do It Yourself method associated with good practices <a href=""https://stackoverflow.com/questions/4165452/how-to-efficiently-manage-frequent-schema-changes-using-sqlalchemy/4165496#4165496"">as proposed by S.Lott</a> ?
Are there alternatives to sqlalchemy-migrate actually simplifying the migration process or am I just trying to use migrate with a bad a <em>priori</em> (then please show me why is't clearly superior to creating CSV columns as proposed in the link above)?</p>

<p>Many Thanks!</p>
","<p>Use Alembic instead:</p>

<p><a href=""http://pypi.python.org/pypi/alembic"">http://pypi.python.org/pypi/alembic</a></p>

<p>Thanks for comments, edited to add some reasoning --</p>

<p>It's developed by the author of SQLAlchemy, and it's brand new and well supported.  I don't know enough about sqlalchemy-migrate to give a good comparison.  But I took a quick read through the clear and concise Alembic docs, then got my own autogenerated migration working in a very short time.</p>

<p>Autogeneration: Not its only mode of operation, but if you choose, Alembic will read your application's sqlalchemy configuration (for instance, your declarative model classes that set up all your tables, constraints, and mappings) and compare to the actual current state of your database, and output a Python script that represents the delta between the two.  You then pass that script to Alembic's upgrade command and there you go, the differences are resolved.  A small amount of editing the migration script by hand is usually needed, and that's (a) just the nature of migrations, and (b) something you want to do anyway to make sure you were fully aware of the exact steps that the migration is going to perform before you run it.</p>

<p>Alembic brings a DVCS-like ability to the way your migrations are tracked, too.  It makes it really easy to return to any past state of your db schema.</p>
"
"<p>My current situation is that I have an application that needs to be notified when new data arrives in a database table.  The data is coming from an external source (that I have no control over--this is this only integration option).  When new data arrives, my application needs to take certain actions--basically query for the new data, handle it, insert the result into a local table, etc.</p>

<p>I want to avoid polling if possible, as the data is expected to be handled in real time.  That said, making sure no data ever gets missed is the #1 priority.</p>

<p>My questions:</p>

<ol>
<li>Is SqlDependency generally considered reliable?</li>
<li>Do I need to be concerned about race conditions, e.g. I am handling one change when another arrives?</li>
<li>What happens when the database gets rebooted?  Will my app recover and start receiving changes again, or will I need a fail-safe timer of some sort that will resubscribe to notifications periodically?</li>
<li>Most of the articles I have read on the topic address SQL Server 2005.  I am using SQL Server 2008 R2.  Is there a newer technique that is preferred over SqlDependency?</li>
<li>(Edit)Also, What if the application goes down? I guess I would have to query for missed data on start up?</li>
</ol>
","<p>The most complete list I can find (<a href=""http://msdn.microsoft.com/en-us/library/ms181122.aspx"" rel=""noreferrer"">from here</a>) is as follows:</p>

<ul>
<li>The projected columns in the SELECT statement must be explicitly stated, and table names must be qualified with two-part names. Notice that this means that all tables referenced in the statement must be in the same database.</li>
<li>The statement may not use the asterisk (<em>) or table_name.</em> syntax to specify columns.</li>
<li>The statement may not use unnamed columns or duplicate column names.</li>
<li>The statement must reference a base table.</li>
<li>The statement must not reference tables with computed columns.</li>
<li>The projected columns in the SELECT statement may not contain aggregate expressions unless the statement uses a GROUP BY expression. When a GROUP BY expression is provided, the select list may contain the aggregate functions COUNT_BIG() or SUM(). However, SUM() may not be specified for a nullable column. The statement may not specify HAVING, CUBE, or ROLLUP.</li>
<li>A projected column in the SELECT statement that is used as a simple expression must not appear more than once.</li>
<li>The statement must not include PIVOT or UNPIVOT operators.</li>
<li>The statement must not include the UNION, INTERSECT, or EXCEPT operators.</li>
<li>The statement must not reference a view.</li>
<li>The statement must not contain any of the following: DISTINCT, COMPUTE or COMPUTE BY, or INTO.</li>
<li>The statement must not reference server global variables (@@variable_name).</li>
<li>The statement must not reference derived tables, temporary tables, or table variables.</li>
<li>The statement must not reference tables or views from other databases or servers.</li>
<li>The statement must not contain subqueries, outer joins, or self-joins.</li>
<li>The statement must not reference the large object types: text, ntext, and image.</li>
<li>The statement must not use the CONTAINS or FREETEXT full-text predicates.</li>
<li>The statement must not use rowset functions, including OPENROWSET and OPENQUERY.</li>
<li>The statement must not use any of the following aggregate functions: AVG, COUNT(*), MAX, MIN, STDEV, STDEVP, VAR, or VARP.</li>
<li>The statement must not use any nondeterministic functions, including ranking and windowing functions.</li>
<li>The statement must not contain user-defined aggregates.</li>
<li>The statement must not reference system tables or views, including catalog views and dynamic management views.</li>
<li>The statement must not include FOR BROWSE information.</li>
<li>The statement must not reference a queue.</li>
<li>The statement must not contain conditional statements that cannot change and cannot return results (for example, WHERE 1=0).</li>
<li>The statement can not specify READPAST locking hint.</li>
<li>The statement must not reference any Service Broker QUEUE.</li>
<li>The statement must not reference synonyms.</li>
<li>The statement must not have comparison or expression based on double/real data types.</li>
<li>The statement must not use the TOP expression.</li>
</ul>

<p>Additional reference(s):</p>

<ul>
<li><a href=""http://msdn.microsoft.com/en-us/library/ms130764.aspx"" rel=""noreferrer"">Working with Query Notifications</a></li>
</ul>
"
"<p>This has been asked a few times but I cannot find a resolution to my problem. Basically when using mysqldump, which is the built in tool for the MySQL Workbench administration tool, when I dump a database using extended inserts, I get massive long lines of data. I understand why it does this, as it speeds inserts by inserting the data as one command (especially on InnoDB), but the formatting makes it REALLY difficult to actually look at the data in a dump file, or compare two files with a diff tool if you are storing them in version control etc. In my case I am storing them in version control as we use the dump files to keep track of our integration test database.</p>

<p>Now I know I can turn off extended inserts, so I will get one insert per line, which works, but any time you do a restore with the dump file it will be slower.</p>

<p>My core problem is that in the OLD tool we used to use (MySQL Administrator) when I dump a file, it does basically the same thing but it FORMATS that INSERT statement to put one insert per line, while still doing bulk inserts. So instead of this:</p>

<pre><code>INSERT INTO `coupon_gv_customer` (`customer_id`,`amount`) VALUES (887,'0.0000'),191607,'1.0300');
</code></pre>

<p>you get this:</p>

<pre><code>INSERT INTO `coupon_gv_customer` (`customer_id`,`amount`) VALUES 
 (887,'0.0000'),
 (191607,'1.0300');
</code></pre>

<p>No matter what options I try, there does not seem to be any way of being able to get a dump like this, which is really the best of both worlds. Yes, it take a little more space, but in situations where you need a human to read the files, it makes it MUCH more useful.</p>

<p>Am I missing something and there is a way to do this with MySQLDump, or have we all gone backwards and this feature in the old (now deprecated) MySQL Administrator tool is no longer available?</p>
","<p>Try use the following option:
 <strong>--skip-extended-insert</strong></p>

<p>It worked for me.</p>
"
"<p>I have this stored procedure. How can I run this for example with intervals of 5 seconds? Like a routine for eliminate data with a time-stamp older than one day?</p>

<pre><code>DROP PROCEDURE IF EXISTS `delete_rows_links` 
GO

CREATE PROCEDURE delete_rows_links
BEGIN 

    DELETE activation_link
    FROM activation_link_password_reset
    WHERE  TIMESTAMPDIFF(DAY, `time`, NOW()) &lt; 1 ; 

END 

GO
</code></pre>
","<p>Events are run by the scheduler, which is not started by default.
Using <code>SHOW PROCESSLIST</code> is possible to check whether it is started. If not, run the command</p>

<pre><code> SET GLOBAL event_scheduler = ON;
</code></pre>

<p>to run it.</p>
"
"<p>I'm trying to create a simple table in <a href=""http://www.sqlfiddle.com"">SQLFiddle</a> for Oracle database and keep getting the cryptic error:</p>

<blockquote>
  <p>Create script error.</p>
</blockquote>

<p>Here is a simple script it failed on:</p>

<pre><code>create table t1 (f1 number, f2 number, f3 number);
</code></pre>

<p>Could anyone can help with that?</p>
","<p><strong>UPDATE:</strong> All seems to be working again. See <a href=""https://stackoverflow.com/questions/35735405/is-sqlfiddle-broken-errors-for-oracle-sql-server/37646270?noredirect=1#44610587"">the answer from Jake Feasel</a> (the creator of SQL Fiddle).</p>

<p><s>To summarise info from the comments as an answer and bring up to date as of writing:</p>

<ol>
<li>SQLFiddle was broken for about a year but as of 2017-07-07 it is working for <a href=""http://sqlfiddle.com/#!4"" rel=""nofollow noreferrer"">Oracle 11g R2</a>.</li>
<li>It also seems to be broken for <a href=""http://sqlfiddle.com/#!3/d227f"" rel=""nofollow noreferrer"">MS SQL Server 2008</a>, <a href=""http://sqlfiddle.com/#!6/d227f"" rel=""nofollow noreferrer"">MS SQL Server 2014</a> and <a href=""http://sqlfiddle.com/#!2"" rel=""nofollow noreferrer"">MySQL 5.5</a> but <a href=""http://sqlfiddle.com/#!9/dafdd"" rel=""nofollow noreferrer"">MySQL 5.6</a> is working.</li>
</ol>

<p><em>(Please comment if any of the above changes and I'll update the answer accordingly).</em></s></p>

<p><strong>SQLFiddle Alternatives</strong></p>

<ol>
<li><a href=""http://rextester.com"" rel=""nofollow noreferrer"">http://rextester.com</a> has options for MySQL, Oracle, PostgreSQL and SQL Server in the ""Language"" dropdown (as well as the ability to save work online in a similar way to SQLFiddle).</li>
<li><a href=""http://db-fiddle.com"" rel=""nofollow noreferrer"">http://db-fiddle.com</a> has options for MySQL 5.5 / 5.6 / 5.7 / 8.0, PostgreSQL 9.4 / 9.5 / 9.6 / 10.0 and SQLite 3.16 / 3.17 / 3.18.</li>
<li><a href=""https://dbfiddle.uk"" rel=""nofollow noreferrer"">https://dbfiddle.uk</a> has options for MariaDB 10.2 / 10.3, MySQL 8.0, Oracle 11g Release 2, Postgres 8.4 / 9.4 / 9.5 / 9.6 / 10 / 11, SQL Server 2012 / 2014 / 2016 / 2017 / 2017 (Linux) and SQLite 3.8 / 3.16 (as well as the ability to save work online in a similar way to SQLFiddle). See further discussion <a href=""https://dba.meta.stackexchange.com/questions/2686"">here</a>.</li>
<li>Suggested alternatives for Oracle are <a href=""https://apex.oracle.com/en/"" rel=""nofollow noreferrer"">Oracle Application Express</a> and <a href=""https://livesql.oracle.com/apex/livesql/file/index.html"" rel=""nofollow noreferrer"">Oracle Live SQL</a>.</li>
<li>A MySQL alternative with more limited functionality is <a href=""http://sqlize.com/"" rel=""nofollow noreferrer"">sqlize.com</a>.</li>
<li>A clunky MySQL alternative is <a href=""https://sqltest.net/"" rel=""nofollow noreferrer"">sqltest.net</a> (if you can find your way around the adverts).</li>
<li>As a last resort, I've occasionally seen answers that use <a href=""https://data.stackexchange.com/stackoverflow/query/new"" rel=""nofollow noreferrer"">Stack Exchange Data Explorer</a> to create custom queries (in SQL Server 2016) that aren't based on the built-in tables. [Disclaimer: I'm not sure it's really supposed to be used for this kind of purpose.]</li>
</ol>
"
"<p>I am not sure what is this error!</p>

<pre><code>#1292 - Truncated incorrect DOUBLE value: 
</code></pre>

<p>I don't have double value field or data!</p>

<p>I have wasted a whole hour trying to figure this out!</p>

<p>here is my query</p>

<pre><code>INSERT INTO call_managment_system.contact_numbers 
    (account_id, contact_number, contact_extension, main_number, created_by)
SELECT
    ac.account_id,
    REPLACE(REPLACE(REPLACE(REPLACE(ta.phone_number, '-', ''), ' ', ''), ')', ''),'(','') AS Phone,
    IFNULL(ta.ext, '') AS extention,
    '1' AS MainNumber,
    '2' AS created_by
FROM 
    cvsnumbers AS ta
    INNER JOIN accounts AS ac ON ac.company_code = ta.company_code
WHERE 
    LENGTH(REPLACE(REPLACE(REPLACE(REPLACE(ta.phone_number, '-', ''), ' ', ''), ')', ''),'(','') ) = 10
</code></pre>

<p>here is my show create table for the table which the results are going into</p>

<pre><code>CREATE TABLE `contact_numbers` ( 
    `number_id` int(10) unsigned NOT NULL AUTO_INCREMENT, 
    `account_id` int(10) unsigned NOT NULL DEFAULT '0', 
    `person_id` int(11) NOT NULL DEFAULT '0', 
    `contact_number` char(15) NOT NULL, 
    `contact_extension` char(10) NOT NULL DEFAULT '', 
    `contact_type` enum('Primary','Direct','Cell','Fax','Home','Reception','Office','TollFree') NOT NULL DEFAULT 'Primary', 
    `contact_link` enum('Account','PDM','Other') NOT NULL DEFAULT 'Account', 
    `status` tinyint(1) NOT NULL DEFAULT '1' COMMENT '0 = inactive, 1=active', 
   `main_number` tinyint(1) NOT NULL DEFAULT '0' COMMENT '1 = main phone number', 
    `created_on` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP, 
    `created_by` int(11) NOT NULL, 
    `modified_on` datetime DEFAULT NULL, 
    `modified_by` int(11) NOT NULL DEFAULT '0', 
    PRIMARY KEY (`number_id`), 
    KEY `account_id` (`account_id`), 
    KEY `person_id` (`person_id`)
) ENGINE=InnoDB AUTO_INCREMENT=534 DEFAULT CHARSET=utf8
</code></pre>
","<p>This message means you're trying to compare a number and a string in a <code>WHERE</code> or <code>ON</code> clause. In your query, the only potential place where that could be occurring is <code>ON ac.company_code = ta.company_code</code>; either make sure they have similar declarations, or use an explicit <code>CAST</code> to convert the number to a string.</p>

<p>If you turn off <code>strict</code> mode, the error should turn into a warning.</p>
"
"<p>I've got a short piece of code that originally created an <strong>SqlDataAdapter</strong> object over and over.</p>

<p>Trying to streamline my calls a little bit, I replaced the <strong>SqlDataAdapter</strong> with an <strong>SqlCommand</strong> and moved the <strong>SqlConnection</strong> outside of the loop.</p>

<p>Now, whenever I try to edit rows of data returned to my <strong>DataTable</strong>, I get a <strong>ReadOnlyException</strong> thrown that was not thrown before.</p>

<p>NOTE: I have a custom function that retrieves the employee's full name based on their ID. For simplicity here, I used ""John Doe"" in my example code below to demonstrate my point.</p>

<p><em>ExampleQueryOld</em> works with the <strong>SqlDataAdapter</strong>; <em>ExampleQueryNew</em> fails with the <strong>ReadOnlyException</strong> whenever I try to write to an element of the <strong>DataRow</strong>:</p>

<ul>
<li><em>ExampleQueryOld</em></li>
</ul>

<p>This works and has no issues:</p>

<pre><code>public static DataTable ExampleQueryOld(string targetItem, string[] sqlQueryStrings) {
  DataTable bigTable = new DataTable();
  for (int i = 0; i &lt; sqlQueryStrings.Length; i++) {
    string sqlText = sqlQueryStrings[i];
    DataTable data = new DataTable(targetItem);
    using (SqlDataAdapter da = new SqlDataAdapter(sqlText, Global.Data.Connection)) {
      try {
        da.Fill(data);
      } catch (Exception err) {
        Global.LogError(_CODEFILE, err);
      }
    }
    int rowCount = data.Rows.Count;
    if (0 &lt; rowCount) {
      int index = data.Columns.IndexOf(GSTR.Employee);
      for (int j = 0; j &lt; rowCount; j++) {
        DataRow row = data.Rows[j];
        row[index] = ""John Doe""; // This Version Works
      }
      bigTable.Merge(data);
    }
  }
  return bigTable;
}
</code></pre>

<ul>
<li><em>ExampleQueryNew</em></li>
</ul>

<p>This example throws the ReadOnlyException:</p>

<pre><code>public static DataTable ExampleQueryNew(string targetItem, string[] sqlQueryStrings) {
  DataTable bigTable = new DataTable();
  using (SqlConnection conn = Global.Data.Connection) {
    for (int i = 0; i &lt; sqlQueryStrings.Length; i++) {
      string sqlText = sqlQueryStrings[i];
      using (SqlCommand cmd = new SqlCommand(sqlText, conn)) {
        DataTable data = new DataTable(targetItem);
        try {
          if (cmd.Connection.State == ConnectionState.Closed) {
            cmd.Connection.Open();
          }
          using (SqlDataReader reader = cmd.ExecuteReader()) {
            data.Load(reader);
          }
        } catch (Exception err) {
          Global.LogError(_CODEFILE, err);
        } finally {
          if ((cmd.Connection.State &amp; ConnectionState.Open) != 0) {
            cmd.Connection.Close();
          }
        }
        int rowCount = data.Rows.Count;
        if (0 &lt; rowCount) {
          int index = data.Columns.IndexOf(GSTR.Employee);
          for (int j = 0; j &lt; rowCount; j++) {
            DataRow row = data.Rows[j];
            try {
              // ReadOnlyException thrown below: ""Column 'index'  is read only.""
              row[index] = ""John Doe"";
            } catch (ReadOnlyException roErr) {
              Console.WriteLine(roErr.Message);
            }
          }
          bigTable.Merge(data);
        }
      }
    }
  }
  return bigTable;
}
</code></pre>

<p>Why can I write to the <strong>DataRow</strong> element in one case, but not in the other?</p>

<p>Is it because the <strong>SqlConnection</strong> is still open or is the <strong>SqlDataAdapter</strong> doing something behind the scene?</p>
","<p>using <code>DataAdapter.Fill</code> does not load the database schema, which includes whether a column is a primary key or not, and whether a column is read-only or not. To load the database schema, use <code>DataAdapter.FillSchema</code>, but then that's not your questions.</p>

<p>using <code>DataReader</code> to fill a table loads the schema. So, the <code>index</code> column is read-only (probably because it's the primary key) and that information is loaded into the <code>DataTable</code>. Thereby preventing you from modifying the data in the table.</p>

<p>I think @k3b got it right; by setting <code>ReadOnly = false</code>, you should be able to write to the data table.</p>

<pre><code>foreach (System.Data.DataColumn col in tab.Columns) col.ReadOnly = false; 
</code></pre>
"
"<p>I need to look up rows falling under the particular time frame.</p>

<pre><code>select * 
from TableA 
where startdate &gt;= '12-01-2012 21:24:00' 
  and startdate &lt;= '12-01-2012 21:25:33'
</code></pre>

<p>i.e. look up rows with timestamp precision of seconds, how do I achieve this?</p>
","<p>You need to use to_timestamp() to convert your string to a proper timestamp value:</p>

<pre><code>to_timestamp('12-01-2012 21:24:00', 'dd-mm-yyyy hh24:mi:ss')
</code></pre>

<p>If your column is of type <code>DATE</code> (which also supports seconds), you need to use to_date()</p>

<pre><code>to_date('12-01-2012 21:24:00', 'dd-mm-yyyy hh24:mi:ss')
</code></pre>

<p>To get this into a where condition use the following:</p>

<pre><code>select * 
from TableA 
where startdate &gt;= to_timestamp('12-01-2012 21:24:00', 'dd-mm-yyyy hh24:mi:ss')
  and startdate &lt;= to_timestamp('12-01-2012 21:25:33', 'dd-mm-yyyy hh24:mi:ss')
</code></pre>

<p>You never need to use <code>to_timestamp()</code> on a column that is of type `timestamp'</p>

<p><em>Edit corrected typo</em></p>
"
"<p>I have receipt and logs model in android app. Receipt hasMany logs. </p>

<p>I made query for group_by logs by sortID and grade. </p>

<pre><code>//recieve RecepitID and query to group logs
final long forwardedId = (long) getIntent().getExtras().get(String.valueOf(""recepitID""));
List&lt;Logs&gt; logsList = new Select().from(Logs.class).where(""Receipt = "" + forwardedId).groupBy(""SortID, Grade"").execute();
</code></pre>

<p>This grouping works fine. Problem is next. I need to have output like this (part in red circle): 
<a href=""https://i.stack.imgur.com/l0Y8T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l0Y8T.png"" alt=""enter image description here""></a>
but I get it like this:</p>

<p><a href=""https://i.stack.imgur.com/d0OzX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d0OzX.png"" alt=""enter image description here""></a></p>

<p>And this is part of code how I done this.</p>

<pre><code>public class LogsRecapitulation extends AppCompatActivity {

    private ListView mainListView;
    private BaseAdapter listAdapter;
    private TextView logsCount;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.recapitulation_listview);
        mainListView = (ListView) findViewById(R.id.ListViewItem);

        //recieve RecepitID and query to group logs
        final long forwardedId = (long) getIntent().getExtras().get(String.valueOf(""recepitID""));
        List&lt;Logs&gt; logsList = new Select().from(Logs.class).where(""Receipt = "" + forwardedId).groupBy(""SortID, Grade"").execute();

        TextView result = (TextView) findViewById(R.id.LogMassResult);
        double sum = 0.0;
        for (int i = 0; i &lt; logsList.size(); i++) {
            sum += logsList.get(i).getM3();
        }
        result.setText(String.format(""%.2f m3"", sum));

        for (int i = 0; i &lt; logsList.size(); i++) {
            if (logsList.get(i).receipt.priceType.equals(""Na panju"")) {
                TextView stumpPriceKN = (TextView) findViewById(R.id.sumPriceKN);
                double sumPricekn = 0.0;

                for (int j = 0; j &lt; logsList.size(); j++) {
                    sumPricekn += logsList.get(j).price.stumpPrice_kn * logsList.get(j).getM3();
                }
                stumpPriceKN.setText(String.format(""%.2f KN"", sumPricekn));

            } else {
                TextView roadKN = (TextView) findViewById(R.id.sumPriceKN);
                double roadPrKn = 0.0;
                for (int j = 0; j &lt; logsList.size(); j++) {
                    roadPrKn += logsList.get(j).price.roadPrice_kn * logsList.get(j).getM3();
                }
                roadKN.setText(String.format(""%.2f KN"", roadPrKn));
            }
        }

        for (int i = 0; i &lt; logsList.size(); i++) {
            if (logsList.get(i).receipt.priceCorrection &gt; 0 &amp;&amp; logsList.get(i).receipt.priceType.equals(""Na panju"")) {
                TextView corecctionPriceKn = (TextView) findViewById(R.id.correctionPriceKN);
                double correcSumKN = 0.0;
                for (int j = 0; j &lt; logsList.size(); j++) {
                    correcSumKN += (logsList.get(j).price.stumpPrice_kn * logsList.get(j).getM3()) + ((logsList.get(j).price.stumpPrice_kn * logsList.get(j).getM3()) * logsList.get(j).receipt.priceCorrection / 100);
                }
                corecctionPriceKn.setText(String.format(""%.2f KN"", correcSumKN));
            } else if (logsList.get(i).receipt.priceCorrection &gt; 0 &amp;&amp; logsList.get(i).receipt.priceType.equals(""umska cesta"")) {
                TextView corecctionPriceKn = (TextView) findViewById(R.id.correctionPriceKN);
                double correcSumKN = 0.0;
                for (int j = 0; j &lt; logsList.size(); j++) {
                    correcSumKN += (logsList.get(j).price.roadPrice_kn * logsList.get(j).getM3()) + ((logsList.get(j).price.roadPrice_kn * logsList.get(j).getM3()) * logsList.get(j).receipt.priceCorrection / 100);
                }
                corecctionPriceKn.setText(String.format(""%.2f KN"", correcSumKN));
            } else {
                TextView priceHolder = (TextView) findViewById(R.id.KorekcijaCijene);
                TextView corecctionPriceKn = (TextView) findViewById(R.id.correctionPriceKN);
                priceHolder.setText("""");
                corecctionPriceKn.setText("""");
            }
        }

        listAdapter = new RecapitulationArrayAdapter(logsList);
        mainListView.setAdapter(listAdapter);

        //display logs count
        logsCount = (TextView) findViewById(R.id.logsCount);
        logsCount.setText(String.valueOf(logsList.size()));
    }

    private class RecapitulationArrayAdapter extends BaseAdapter {
        private LayoutInflater inflater;
        private List&lt;Logs&gt; logsList;

        public RecapitulationArrayAdapter(List&lt;Logs&gt; logsList) {
            inflater = LayoutInflater.from(LogsRecapitulation.this);
            this.logsList = logsList;
        }

        @Override
        public int getCount() {
            return logsList.size();
        }

        @Override
        public Object getItem(int position) {
            return logsList.get(position);
        }

        @Override
        public long getItemId(int position) {
            return logsList.get(position).getId();
        }

        @Override
        public View getView(int position, View convertView, ViewGroup parent) {
            if (convertView == null) {
                convertView = inflater.inflate(R.layout.logs_recapitulation, parent, false);
            }
            Logs log = logsList.get(position);
            ((TextView) convertView.findViewById(R.id.rec_log_sort)).setText(log.sort_id);
            ((TextView) convertView.findViewById(R.id.rec_log_class)).setText(log.grade);
            ((TextView) convertView.findViewById(R.id.rec_log_count)).setText(String.valueOf(logsList.size()));
            ((TextView) convertView.findViewById(R.id.rec_logs_mass)).setText(String.format(""%.2f m3"", log.getM3()));

            if (log.receipt.priceType.equals(""Na panju"")) {
                ((TextView) convertView.findViewById(R.id.rec_log_price_default)).setText(String.valueOf(log.price.stumpPrice_kn));
            } else {
                ((TextView) convertView.findViewById(R.id.rec_log_price_default)).setText(String.valueOf(log.price.roadPrice_kn));
            }

            if (log.receipt.priceType.equals(""Na panju"")) {
                ((TextView) convertView.findViewById(R.id.rec_calculated_price)).setText(String.format(""%.2f KN"", log.price.stumpPrice_kn * log.getM3()));
            } else {
                ((TextView) convertView.findViewById(R.id.rec_calculated_price)).setText(String.format(""%.2f KN"", log.price.roadPrice_kn * log.getM3()));
            }

            return convertView;
        }
    }

}
</code></pre>

<p>I'm using ActiveAndroid and displaying this in listview.</p>

<p><strong><em>PROBLEM is with displaying these grouped items. Im displaying them in BaseAdapter listView and it shows me only one item per group but it needs to show me multiple items (because it has more than 4 items in each group).</em></strong></p>

<p><strong>Can anyone give me advice what should I do to get output like on first image?</strong> </p>
","<p>Correct code here would be:</p>

<pre><code>query.prepare(QString(""UPDATE table SET %1 = :value WHERE id = :id "").arg(column));
query.bindValue("":value"", value);
</code></pre>

<p>You bind values, not fields names.</p>

<p>P.S.: answered before reading whole question. Yes, you are right - there is no way to use bindValues to bind columns, not only for sqlite, but for each db.</p>
"
"<p>I have downloaded and installed SQL Server 2014 Express
(from this site: <a href=""http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx#Installation_Options"" rel=""noreferrer"">http://www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql-server-express.aspx#Installation_Options</a>).</p>

<p>The problem is that I can't connect/find my local DB server, and I can't develop DB on my local PC. How can I reach my local server?</p>

<p>My system consists of Windows 8.1 (no Pro or Enterprise editions) 64 bits</p>

<p>Checking the configuration of SQL Server with <code>SQL Server 2014 Configuration Manager</code> tool, I see an empty list selecting ""SQL Server Services"" from the tree at the left. Below you can find a screenshot.</p>

<p><img src=""https://i.stack.imgur.com/UBKCY.jpg"" alt=""enter image description here""></p>

<p>In the Windows Services list, there is just only one service: <strong>""SQL Server VSS Writer""</strong></p>

<p><strong>EDIT</strong>
My installation window of SQL Server 2014 is the following:
<img src=""https://i.stack.imgur.com/HcVJd.jpg"" alt=""enter image description here""></p>
","<p>Most probably, you didn't install any SQL Server Engine service. If no SQL Server engine is installed, no service will appear in the SQL Server Configuration Manager tool. Consider that the packages <code>SQLManagementStudio_Architecture_Language.exe</code> and <code>SQLEXPR_Architecture_Language.exe</code>, available in the <a href=""http://www.microsoft.com/en-US/download/details.aspx?id=42299"" rel=""noreferrer"">Microsoft site</a> contain, respectively only the Management Studio GUI Tools and the SQL Server engine.</p>

<p>If you want to have a full featured SQL Server installation, with the database engine and Management Studio, <a href=""http://www.microsoft.com/en-US/download/details.aspx?id=42299"" rel=""noreferrer"">download the installer file of <strong>SQL Server with Advanced Services</strong></a>.
Moreover, to have a sample database in order to perform some local tests, use the <a href=""https://msftdbprodsamples.codeplex.com/releases/view/125550"" rel=""noreferrer"">Adventure Works database</a>.</p>

<p>Considering the package of <strong>SQL Server with Advanced Services</strong>, at the beginning at the installation you should see something like this (the screenshot below is about SQL Server 2008 Express, but the feature selection is very similar). The checkbox next to ""Database Engine Services"" must be checked. In the next steps, you will be able to configure the instance settings and other options.</p>

<p>Execute again the installation process and select the database engine services in the feature selection step. At the end of the installation, you should be able to see the SQL Server services in the SQL Server Configuration Manager.</p>

<p><img src=""https://i.stack.imgur.com/Kyaiu.png"" alt=""enter image description here""></p>
"
"<p>I need to know if a given Job is currently running on Ms SQL 2008 server. So as to not to invoke same job again that may lead to concurrency issues.</p>
","<p>It looks like you can use <code>msdb.dbo.sysjobactivity</code>, checking for a record with a non-null start_execution_date and a null stop_execution_date, meaning the job was started, but has not yet completed.</p>

<p>This would give you currently running jobs:</p>

<pre><code>SELECT sj.name
   , sja.*
FROM msdb.dbo.sysjobactivity AS sja
INNER JOIN msdb.dbo.sysjobs AS sj ON sja.job_id = sj.job_id
WHERE sja.start_execution_date IS NOT NULL
   AND sja.stop_execution_date IS NULL
</code></pre>
"
"<p>I'm using the MySQL Query Browser (part of the <a href=""http://dev.mysql.com/downloads/gui-tools/"" rel=""noreferrer"">MySQL GUI Tools</a>) and need to change a field to NULL, but I can't figure out how to do it - if I delete the value it tries to update it to <code>''</code>. Typing ""NULL"" makes it try to update to <code>'NULL'</code> (a string).</p>

<p>I know I could just write a query to do it, but that defeats the entire purpose of the tool, no?</p>
","<p>In MySQL Query Browser, right click on the cell and select 'Clear field content' while the focus is in another cell.</p>

<p>In MySQL Workbench, right click on the cell and select 'Set Field to NULL'.</p>
"
"<p>I pulled <code>mysql-connector-python</code> code and when I run <code>python ./setup.py build</code> I get the following error:</p>

<pre class=""lang-none prettyprint-override""><code>Unable to find Protobuf include directory.
</code></pre>

<p><code>pip install Protobuf</code> was useless</p>

<p>How can I solve this problem?</p>
","<p>I found that this error occurs since version 2.2.3.
You can avoid this issue using version 2.1.6.</p>

<p><code>pip install mysql-connector==2.1.6</code></p>

<p>try above.</p>
"
"<p>I'm trying this code:</p>

<pre><code>import sqlite

connection = sqlite.connect('cache.db')
cur = connection.cursor()
cur.execute('''create table item
  (id integer primary key, itemno text unique,
        scancode text, descr text, price real)''')

connection.commit()
cur.close()
</code></pre>

<p>I'm catching this exception:</p>

<pre><code>Traceback (most recent call last):
  File ""cache_storage.py"", line 7, in &lt;module&gt;
    scancode text, descr text, price real)''')
  File ""/usr/lib/python2.6/dist-packages/sqlite/main.py"", line 237, in execute
    self.con._begin()
  File ""/usr/lib/python2.6/dist-packages/sqlite/main.py"", line 503, in _begin
    self.db.execute(""BEGIN"")
_sqlite.OperationalError: database is locked
</code></pre>

<p>Permissions for cache.db are ok. Any ideas?</p>
","<p>I'm presuming you are actually using sqlite3 even though your code says otherwise. Here are some things to check:</p>

<ol>
<li>That you don't have a hung process sitting on the file (unix: <code>$ fuser cache.db</code> should say nothing)</li>
<li>There isn't a cache.db-journal file in the directory with cache.db; this would indicate a crashed session that hasn't been cleaned up properly.</li>
<li>Ask the database shell to check itself: <code>$ sqlite3 cache.db ""pragma integrity_check;""</code></li>
<li>Backup the database <code>$ sqlite3 cache.db "".backup cache.db.bak""</code></li>
<li>Remove cache.db as you probably have nothing in it (if you are just learning) and try your code again</li>
<li>See if the backup works <code>$ sqlite3 cache.db.bak "".schema""</code></li>
</ol>

<p>Failing that, read <a href=""http://www.sqlite.org/atomiccommit.html#sect_9_0"" rel=""noreferrer"">Things That Can Go Wrong</a> and <a href=""http://www.sqlite.org/lockingv3.html#how_to_corrupt"" rel=""noreferrer"">How to Corrupt Your Database Files</a></p>
"
"<p>Whats the best browser tool for HSQLDB databases?</p>
","<p>HSQL has built-in GUI query tool called <a href=""http://hsqldb.org/doc/2.0/util-guide/dbm-chapt.html"" rel=""noreferrer"">Database manager</a>. In the directory of the database files run:</p>

<pre><code>java -cp ../lib/hsqldb.jar org.hsqldb.util.DatabaseManager 
</code></pre>

<p>and then setting the config to:</p>

<pre><code>jdbc:hsqldb:file:databaseName
</code></pre>

<p>Allowed me to edit what I needed in a really convenient way. </p>
"
"<p>I have added the following simple test event on my mysql database via phpmyadmin:</p>

<pre><code>CREATE DEFINER=`root`@`localhost` EVENT `my_event` 
ON SCHEDULE EVERY 1 MINUTE STARTS '2013-05-27 00:00:00' 
ON COMPLETION NOT PRESERVE ENABLE DO 
BEGIN
    UPDATE `test` SET `name`=""z"";
END
</code></pre>

<p>My environment is mac + MAMP Pro. I am expecting to change all rows on my 'test' table with name 'z' within a minute. But not happening so.</p>

<p>Do I have to something additional to get my events start working? </p>

<p>Output of ""SHOW PROCESSLIST"":
<img src=""https://i.stack.imgur.com/Xj2j9.png"" alt=""enter image description here""></p>

<p>Thanks.</p>
","<p>Events are run by the scheduler, which is not started by default.
Using <code>SHOW PROCESSLIST</code> is possible to check whether it is started. If not, run the command</p>

<pre><code> SET GLOBAL event_scheduler = ON;
</code></pre>

<p>to run it.</p>
"
"<p>I'd like to use RazorSQL to connect to my database which is running on a remote server. I create a SSH tunnel on my localhost with the following command:</p>

<pre><code>ssh -L 1111:remote.server.com:5432 myuser@remote.server.com
</code></pre>

<p>I configure my connection via RazorSQL's GUI, specifying <code>localhost</code> as the host and <code>1111</code> as the port. When I click on ""Connect"", the following error message appears:</p>

<pre><code>ERROR: An error occurred while trying to make a connection to
the database: 

JDBC URL: jdbc:postgresql://localhost:1111/myuser

FATAL:
no pg_hba.conf entry for host ""aaa.bbb.ccc.ddd"",
user ""myuser"", database ""mydatabase"", SSL off
</code></pre>

<p>where <code>aaa.bbb.ccc.ddd</code> is a remote server's IP address.</p>

<p>What is more, I am not allowed to change the contents of my <code>pg_hba.conf</code> file. That's how it look like at the moment:</p>

<pre><code># TYPE  DATABASE    USER        CIDR-ADDRESS          METHOD

@remove-line-for-nolocal@# ""local"" is for Unix domain socket connections only
@remove-line-for-nolocal@local   all         all                               @authmethod@
# IPv4 local connections:
host    all         all         127.0.0.1/32          @authmethod@
# IPv6 local connections:
host    all         all         ::1/128               @authmethod@
</code></pre>

<p>Is it possible to connect to the database server via SSH tunnel using my current setup and without modifying the server's configuration?</p>
","<p>Your pg_hba.conf appears to permit connections from localhost. The easiest way of causing your SSH tunnel connections to appear from localhost is to make them <strong>to</strong> localhost.</p>

<p>The following SSH command connects to remote.example.com as user ""user"", and causes your ssh client to listen on localhost, port 1111/tcp. Any connections made to that port will be forwarded over the ssh tunnel, and on the ssh server side the connections will be made to localhost, port 5432/tcp. Since we're connecting to localhost, the connections will appear to be from localhost also, and should match your existing pg_hba.conf line.</p>

<pre><code>ssh -L 1111:localhost:5432 user@remote.example.com
</code></pre>

<p>If this is expected to be a long-running tunnel, I would recommend using <a href=""http://www.harding.motd.ca/autossh/"" rel=""noreferrer"" title=""autossh"">autossh</a></p>

<p>To connect using the psql client on the host where you are running the ssh client, use something like this:</p>

<pre><code>psql -h localhost -p 1111 -U your-db-username database-name
</code></pre>

<p>You should then be prompted for your database user's password.</p>

<p>Alternately, you can add a line line the following to a file called <code>.pgpass</code> in your home directory on the client where you're running psql:</p>

<pre><code>localhost:1111:database-name:your-db-user:your-db-password
</code></pre>
"
"<p>I'm looking for a good explanation on how to test an Oracle stored procedure in SQL Developer or Embarcardero Rapid XE2.  Thank you.</p>
","<p>Something like</p>

<pre><code>create or replace procedure my_proc( p_rc OUT SYS_REFCURSOR )
as
begin
  open p_rc
   for select 1 col1
         from dual;
end;
/

variable rc refcursor;
exec my_proc( :rc );
print rc;
</code></pre>

<p>will work in SQL*Plus or SQL Developer.  I don't have any experience with Embarcardero Rapid XE2 so I have no idea whether it supports SQL*Plus commands like this.</p>
"
"<p>I'm looking for a good explanation on how to test an Oracle stored procedure in SQL Developer or Embarcardero Rapid XE2.  Thank you.</p>
","<p>Something like</p>

<pre><code>create or replace procedure my_proc( p_rc OUT SYS_REFCURSOR )
as
begin
  open p_rc
   for select 1 col1
         from dual;
end;
/

variable rc refcursor;
exec my_proc( :rc );
print rc;
</code></pre>

<p>will work in SQL*Plus or SQL Developer.  I don't have any experience with Embarcardero Rapid XE2 so I have no idea whether it supports SQL*Plus commands like this.</p>
"
"<p>I can get Python to work with Postgresql but I cannot get it to work with MySQL. The main problem is that on the shared hosting account I have I do not have the ability to install things such as Django or PySQL, I generally fail when installing them on my computer so maybe it's good I can't install on the host.</p>

<p>I found <a href=""http://barryp.org/software/bpgsql/"" rel=""noreferrer"">bpgsql</a> really good because it does not require an install, it's a single file that I can look at, read and then call the functions of. Does anybody know of something like this for MySQL?</p>
","<p>MySQLdb is what I have used before.</p>

<p>If you host is using Python version 2.5 or higher, support for sqlite3 databases is built in (sqlite allows you to have a relational database that is simply a file in your filesystem).  But buyer beware, sqlite is not suited for production, so it may depend what you are trying to do with it.</p>

<p>Another option may be to call your host and complain, or change hosts.  Honestly these days, any self respecting web host that supports python and mysql ought to have MySQLdb pre installed.</p>
"
"<p>there are lots of questions about this problem, but I couldn't solve my case.
can some one please take a look at this:</p>

<p>I have an <code>Office</code> table which has one-many relationship with <code>Doctor</code> and <code>Secretary</code> tables. Both of the last tables, are derived from <code>Employee</code> table which has a shared-primary-key relationship with predefined <code>Users</code> table created by <code>sqlmembershipprovider</code>. It seems there is a many-many relationship between <code>Users</code> table and <code>Roles</code> table which I haven't any hand in it.</p>

<p>My problem was in creating a (zero,one) - (one) relationship between my <code>Employee</code> table and that <code>Users</code> table which I ended with a shared primary key relationship between them and the error raised, then. (Is there a better solution for that problem?)</p>

<p><strong>here is the error:</strong></p>

<blockquote>
  <p>Introducing FOREIGN KEY constraint
  'FK_dbo.aspnet_UsersInRoles_dbo.aspnet_Users_UserId' on table
  'aspnet_UsersInRoles' may cause cycles or multiple cascade paths.
  Specify ON DELETE NO ACTION or ON UPDATE NO ACTION, or modify other
  FOREIGN KEY constraints. Could not create constraint. See previous
  errors.</p>
</blockquote>

<p><strong>here are my codes and membership codes after reverse engineering:</strong></p>

<pre><code>public class Office
{
    public Office()
    {
        this.Doctors = new HashSet&lt;Doctor&gt;();
        this.Secretaries = new HashSet&lt;Secretary&gt;();
    }

    [Key]
    public System.Guid OfficeId { get; set; }
    public virtual ICollection&lt;Doctor&gt; Doctors { get; set; }
    public virtual ICollection&lt;Secretary&gt; Secretaries { get; set; }
}

public class Employee
{
    [Key, ForeignKey(""User"")]
    [DatabaseGenerated(DatabaseGeneratedOption.None)]
    public System.Guid Id { get; set; }
    public string Name { get; set; }

    [ForeignKey(""Office"")]
    public System.Guid OfficeId { get; set; }

    // shared primary key 
    public virtual aspnet_Users User { get; set; }

    public virtual Office Office { get; set; }
}

public class Doctor :Employee
{
    public Doctor()
    {
        this.Expertises = new HashSet&lt;Expertise&gt;();
    }
    //the rest..    
    public virtual ICollection&lt;Expertise&gt; Expertises { get; set; }
}

public class Secretary : Employee
{
    // blah blah
}

public class aspnet_Users
{
    public aspnet_Users()
    {
        this.aspnet_Roles = new List&lt;aspnet_Roles&gt;();
    }

    public System.Guid ApplicationId { get; set; }
    public System.Guid UserId { get; set; }
    //the rest..
    public virtual aspnet_Applications aspnet_Applications { get; set; }
    public virtual ICollection&lt;aspnet_Roles&gt; aspnet_Roles { get; set; }
}

public class aspnet_Roles
{
    public aspnet_Roles()
    {
        this.aspnet_Users = new List&lt;aspnet_Users&gt;();
    }

    public System.Guid ApplicationId { get; set; }
    public System.Guid RoleId { get; set; }
    //the rest..
    public virtual aspnet_Applications aspnet_Applications { get; set; }
    public virtual ICollection&lt;aspnet_Users&gt; aspnet_Users { get; set; }
}
</code></pre>

<p><strong>EDIT:</strong> and the relationships go deeper, there is a many-one relationship between <code>Users</code> table and <code>Applications</code> table, also between <code>Roles</code> and <code>Applications</code> too.</p>
","<p>You can use the fluent api to specify the actions the error message suggests.</p>

<p>In your Context:</p>

<pre><code>protected override void OnModelCreating( DbModelBuilder modelBuilder )
{
        base.OnModelCreating(modelBuilder);
        modelBuilder.Entity&lt;aspnet_UsersInRoles&gt;().HasMany(i =&gt; i.Users).WithRequired().WillCascadeOnDelete(false);
}
</code></pre>

<p>Note that you have not included the definition for the table aspnet_UsersInRoles so this code may not work.</p>

<p>Another option is to remove all CASCADE DELETES by adding this </p>

<pre><code>modelBuilder.Conventions.Remove&lt;OneToManyCascadeDeleteConvention&gt;();
</code></pre>

<p>If you need more info about configuring relationships with the fluent api I suggest <a href=""http://msdn.microsoft.com/en-US/data/jj591620"" rel=""noreferrer"">http://msdn.microsoft.com/en-US/data/jj591620</a></p>
"
"<p>I have a MySQL query in which I want to include a list of ID's from another table. On the website, people are able to add certain items, and people can then add those items to their favourites. I basically want to get the list of ID's of people who have favourited that item (this is a bit simplified, but this is what it boils down to).</p>

<p>Basically, I do something like this:</p>

<pre><code>SELECT *,
GROUP_CONCAT((SELECT userid FROM favourites WHERE itemid = items.id) SEPARATOR ',') AS idlist
FROM items
WHERE id = $someid
</code></pre>

<p>This way, I would be able to show who favourited some item, by splitting the idlist later on to an array in PHP further on in my code, however I am getting the following MySQL error:</p>

<blockquote>
  <p><em>1242 - Subquery returns more than 1 row</em> </p>
</blockquote>

<p>I thought that was kind of the point of using <code>GROUP_CONCAT</code> instead of, for example, <code>CONCAT</code>? Am I going about this the wrong way?</p>

<hr>

<p>Ok, thanks for the answers so far, that seems to work. However, there is a catch. Items are also considered to be a favourite if it was added by that user. So I would need an additional check to check if creator = userid. Can someone help me come up with a smart (and hopefully efficient) way to do this?</p>

<p>Thank you!</p>

<p>Edit: I just tried to do this:</p>

<pre><code>SELECT [...] LEFT JOIN favourites ON (userid = itemid OR creator = userid)
</code></pre>

<p>And <strong>idlist</strong> is empty. Note that if I use <code>INNER JOIN</code> instead of <code>LEFT JOIN</code> I get an empty result. Even though I am sure there are rows that meet the ON requirement.</p>
","<p>OP almost got it right. <code>GROUP_CONCAT</code> should be wrapping the columns in the subquery and not the <a href=""https://stackoverflow.com/a/4455991/838733"">complete subquery</a> (I'm dismissing the separator because comma is the default):</p>

<pre><code>SELECT i.*,
(SELECT GROUP_CONCAT(userid) FROM favourites f WHERE f.itemid = i.id) AS idlist
FROM items i
WHERE i.id = $someid
</code></pre>

<p>This will yield the desired result and also means that the accepted answer is partially wrong, because you can access outer scope variables in a subquery.</p>
"
"<p>If I create a Stored Procedure in SQL and call it (<code>EXEC spStoredProcedure</code>) within the BEGIN/END TRANSACTION, does this other stored procedure also fall into the transaction?</p>

<p>I didn't know if it worked like try/catches in C#.</p>
","<p>Yes, <em>everything</em> that you do between the Begin Transaction and Commit (or Rollback) is part of the transaction.</p>
"
"<p>I'm trying to use <code>mysqldump</code> to dump a schema, and it mostly works but I ran into one curiosity: the <code>-p</code> or <code>--password</code> option seems like it is doing something other than setting the password (as the <code>man</code> page and <code>--help</code> output say it should).</p>

<p>Specifically, it looks like it's doing what is indicated here: <a href=""http://snippets.dzone.com/posts/show/360"" rel=""noreferrer"">http://snippets.dzone.com/posts/show/360</a> - that is, setting the database to dump.</p>

<p>To support my somewhat outlandish claim, I can tell you that if I do not specify the <code>--password</code> (or <code>-p</code>) option, the command prints the usage statement and exits with an error.  If I do specify it, I am immediately prompted to enter a password (!), and then the database specified in the <code>--password</code> option is dumped (or an error is given in the usual case that a password not matching any database name was specified).</p>

<p>Here's a transcript:</p>

<pre>
    $ mysqldump -u test -h myhost --no-data --tables --password lose
    Enter password: 
    -- MySQL dump 10.10
    mysqldump: Got error: 1044: Access denied for user 'test'@'%' to
    database 'lose' when selecting the database
</pre>

<p>So, what gives?  Is this the way this is supposed to work?  It surely does not appear to make sense nor does it match the official documentation.  And finally, if this just the way it works, how am I meant to specify the password to be used in an automated job?  Using <code>expect</code>???</p>

<p>I'm using <code>mysqldump  Ver 10.10 Distrib 5.0.22, for pc-linux-gnu (i486)</code>.</p>
","<p>From man mysqldump:</p>

<blockquote>
  <p>--password[=password], -p[password]</p>
  
  <p>The password to use when connecting to the server. If you use
  the short option form (-p), you cannot have a space between the option
            and the password. If you omit the password value following the
            --password or -p option on the command line, you are prompted for
            one.<br>
            Specifying a password on the command line should be considered
            insecure. See Section 6.6, ""Keeping Your Password Secure"".</p>
</blockquote>

<p>Syntactically, you are not using the --password switch correctly.  As such, the command line parser is seeing your use of ""lose"" as a stand-alone argument which mysqldump interprets as the database name as it would if you were to attempt a simpler command like <code>mysqldump lose</code></p>

<p>To correct this, try using <code>--password=lose</code> or <code>-plose</code> or simply use <code>-p</code> or <code>--password</code> and type the password when prompted.</p>
"
"<p>I have the following code (Database is SQL Server Compact 4.0):</p>

<pre><code>Dim competitor=context.Competitors.Find(id)
</code></pre>

<p>When I profile this the Find method takes 300+ms to retrieve the competitor from a table of just 60 records.</p>

<p>When I change the code to:</p>

<pre><code>Dim competitor=context.Competitors.SingleOrDefault(function(c) c.ID=id)
</code></pre>

<p>Then the competitor is found in just 3 ms.</p>

<p>The Competitor class:</p>

<pre><code>Public Class Competitor
    Implements IEquatable(Of Competitor)

    Public Sub New()
        CompetitionSubscriptions = New List(Of CompetitionSubscription)
        OpponentMeetings = New List(Of Meeting)
        GUID = GUID.NewGuid
    End Sub

    Public Sub New(name As String)
        Me.New()
        Me.Name = name
    End Sub

    'ID'
    Public Property ID As Long
    Public Property GUID As Guid

    'NATIVE PROPERTIES'
    Public Property Name As String

    'NAVIGATION PROPERTIES'
    Public Overridable Property CompetitionSubscriptions As ICollection(Of CompetitionSubscription)
    Public Overridable Property OpponentMeetings As ICollection(Of Meeting)
End Class
</code></pre>

<p>I defined the many to many relations for <code>CompetitionSubscriptions</code> and <code>OpponentMeetings</code> using the fluent API. </p>

<p>The ID property of the <code>Competitor</code> class is a Long which is translated by Code First to an Identity column with a primary key in the datatable (SQL Server Compact 4.0)</p>

<p>What is going on here??</p>
","<p><code>Find</code> calls <code>DetectChanges</code> internally, <code>SingleOrDefault</code> (or generally any query) doesn't. <code>DetectChanges</code> is an expensive operation, so that's the reason why <code>Find</code> is slower (but it might become faster if the entity is already loaded into the context because <code>Find</code> would not run a query but just return the loaded entity).</p>

<p>If you want to use <code>Find</code> for a lot of entities - in a loop for example - you can disable automatic change detection like so (can't write it in VB, so a C# example):</p>

<pre class=""lang-cs prettyprint-override""><code>try
{
    context.Configuration.AutoDetectChangesEnabled = false;
    foreach (var id in someIdCollection)
    {
        var competitor = context.Competitors.Find(id);
        // ...
    }
}
finally
{
    context.Configuration.AutoDetectChangesEnabled = true;
}
</code></pre>

<p>Now, <code>Find</code> won't call <code>DetectChanges</code> with every call and it should be as fast as <code>SingleOrDefault</code> (and faster if the entity is already attached to the context).</p>

<p>Automatic change detection is a complex and somewhat mysterious subject. A great detailed discussion can be found in this four-part series:</p>

<p>(Link to part 1, the links to parts 2, 3 and 4 are at the beginning of that article)</p>

<p><a href=""http://blog.oneunicorn.com/2012/03/10/secrets-of-detectchanges-part-1-what-does-detectchanges-do/"">http://blog.oneunicorn.com/2012/03/10/secrets-of-detectchanges-part-1-what-does-detectchanges-do/</a></p>
"
"<p>I'm using mongo 2.2.3 and the java driver.
My dilemma, I have to $push a field and value into an array, but I cant seem to figure out how to do this.  A sample of my data:</p>

<pre><code>""_id"" : 1,
""scores"" : [
    {
        ""type"" : ""homework"",
        ""score"" : 78.97979
    },
    {
        ""type"" : ""homework"",
        ""score"" : 6.99
    },
    {
        ""type"" : ""quiz"",
        ""score"" : 99
    }
]
</code></pre>

<p>I can $push in the shell:</p>

<pre><code>db.collection.update({_id:1},{$push:{scores:{type:""quiz"", score:99}}})
</code></pre>

<p>but it's when I translate this into java I confuse my self and chuck my keyboard at a wall.</p>

<p>my java code (incomplete and wrong) so far:</p>

<pre><code>DBObject find = new BasicDBObject(""_id"", 1);
DBObject push = new BasicDBObject(""$push"", new BasicDBObject(
                        ""scores"", new BasicDBObject()));
</code></pre>
","<pre><code>DBObject listItem = new BasicDBObject(""scores"", new BasicDBObject(""type"",""quiz"").append(""score"",99));
DBObject updateQuery = new BasicDBObject(""$push"", listItem);
myCol.update(findQuery, updateQuery);
</code></pre>
"
"<p>I am trying to run mysql client on my terminal. I have installed the latest mysql gem. </p>

<pre><code>   ~ git:(master)  ruby -v
    ruby 1.8.7 (2010-01-10 patchlevel 249) [universal-darwin11.0]
      ~ git:(master)  rails -v
    Rails 2.3.14
      ~ git:(master)  which mysql
    mysql: aliased to nocorrect mysql
      ~ git:(master)  which ruby
    /usr/bin/ruby
      ~ git:(master)  which rails
    /usr/bin/rails
      ~ git:(master)  gem list

*** LOCAL GEMS ***

actionmailer (2.3.14)
actionpack (2.3.14)
activerecord (2.3.14)
activeresource (2.3.14)
activesupport (2.3.14)
builder (2.1.2)
bundler (1.0.21)
capistrano (2.9.0)
capybara (0.3.9)
cgi_multipart_eof_fix (2.5.0)
childprocess (0.2.2)
columnize (0.3.4, 0.3.3)
cucumber (0.9.4)
cucumber-rails (0.3.2)
culerity (0.2.15)
daemons (1.1.4)
database_cleaner (0.6.7)
diff-lcs (1.1.3)
expertiza-authlogic (2.1.6.1)
fastercsv (1.5.4)
fastthread (1.0.7)
ffi (1.0.10, 1.0.9)
gdata (1.1.2)
gem_plugin (0.2.3)
gherkin (2.2.9)
highline (1.6.2)
hoptoad_notifier (2.4.11)
json (1.4.6)
json_pure (1.6.1)
linecache (0.46)
mime-types (1.16)
mongrel (1.1.5)
mysql (2.8.1)
mysql2 (0.3.7)
net-scp (1.0.4)
net-sftp (2.0.5)
net-ssh (2.2.1)
net-ssh-gateway (1.1.0)
nokogiri (1.5.0)
rack (1.1.2)
rack-test (0.6.1)
rails (2.3.14)
rake (0.9.2)
rbx-require-relative (0.0.5)
rdoc (3.11)
RedCloth (4.2.8)
rgl (0.4.0)
ruby-debug (0.10.4)
ruby-debug-base (0.10.4)
rubyzip (0.9.4)
selenium-webdriver (2.8.0, 2.7.0)
stream (0.5)
term-ansicolor (1.0.7, 1.0.6)

  expertiza git:(master)  sudo su
Password:
sh-3.2# mysql -u root -p
Enter password: 
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/tmp/mysql.sock' (2)
sh-3.2# 
</code></pre>

<p>I am not able to get rid of the above error. I have created a <code>mysql.sock file</code> in <code>/Users/HPV/expertiza/tmp/sockets</code>. In the file I have written <code>mysql.default_socket =/expertiza/tmp/sockets/mysql.sock</code>. </p>

<p>What am I doing wrong?</p>

<p>Thanks!</p>
","<p>You need to <a href=""https://dev.mysql.com/doc/refman/5.7/en/osx-installation-pkg.html"" rel=""noreferrer"">follow the directions</a> to install and start the server. </p>

<p>The command varies depending on how you installed MySQL. Try this first:</p>

<blockquote>
  <p>sudo /Library/StartupItems/MySQLCOM/MySQLCOM start</p>
</blockquote>

<p>If that fails:</p>

<blockquote>
  <p>cd /usr/local/mysql<br>
  sudo ./bin/mysqld_safe<br>
  (Enter your password, if necessary)<br>
  (Press Control-Z)<br>
  bg</p>
</blockquote>
"
"<p>I'm trying to select a column from a single table (no joins) and I need the count of the number of rows, ideally before I begin retrieving the rows.  I have come to two approaches that provide the information I need.</p>

<p><strong>Approach 1:</strong></p>

<pre><code>SELECT COUNT( my_table.my_col ) AS row_count
  FROM my_table
 WHERE my_table.foo = 'bar'
</code></pre>

<p>Then</p>

<pre><code>SELECT my_table.my_col
  FROM my_table
 WHERE my_table.foo = 'bar'
</code></pre>

<p>Or <strong>Approach 2</strong></p>

<pre><code>SELECT my_table.my_col, ( SELECT COUNT ( my_table.my_col )
                            FROM my_table
                           WHERE my_table.foo = 'bar' ) AS row_count
  FROM my_table
 WHERE my_table.foo = 'bar'
</code></pre>

<p>I am doing this because my SQL driver (SQL Native Client 9.0) does not allow me to use SQLRowCount on a SELECT statement but I need to know the number of rows in my result in order to allocate an array before assigning information to it.  The use of a dynamically allocated container is, unfortunately, not an option in this area of my program.</p>

<p>I am concerned that the following scenario might occur:</p>

<ul>
<li>SELECT for count occurs</li>
<li>Another instruction occurs, adding or removing a row</li>
<li>SELECT for data occurs and suddenly the array is the wrong size.<br>
  -In the worse case, this will attempt to write data beyond the arrays limits and crash my program.</li>
</ul>

<p>Does Approach 2 prohibit this issue?</p>

<p>Also, Will one of the two approaches be faster?  If so, which?</p>

<p>Finally, is there a better approach that I should consider (perhaps a way to instruct the driver to return the number of rows in a SELECT result using SQLRowCount?)</p>

<p>For those that asked, I am using Native C++ with the aforementioned SQL driver (provided by Microsoft.)</p>
","<p>If you're using SQL Server, after your query you can select the <a href=""https://docs.microsoft.com/en-us/sql/t-sql/functions/rowcount-transact-sql"" rel=""nofollow noreferrer"">@@RowCount</a> function (or if your result set might have more than 2 billion rows use the <a href=""https://docs.microsoft.com/en-us/sql/t-sql/functions/rowcount-big-transact-sql"" rel=""nofollow noreferrer"">RowCount_Big()</a> function). This will return the number of rows selected by the previous statement or number of rows affected by an insert/update/delete statement.</p>

<pre><code>SELECT my_table.my_col
  FROM my_table
 WHERE my_table.foo = 'bar'

SELECT @@Rowcount
</code></pre>

<p>Or if you want to row count included in the result sent similar to Approach #2, you can use the the <a href=""https://docs.microsoft.com/en-us/sql/t-sql/queries/select-over-clause-transact-sql"" rel=""nofollow noreferrer"">OVER clause</a>.  </p>

<pre><code>SELECT my_table.my_col,
    count(*) OVER(PARTITION BY my_table.foo) AS 'Count'
  FROM my_table
 WHERE my_table.foo = 'bar'
</code></pre>

<p>Using the OVER clause will have much better performance than using a subquery to get the row count.  Using the @@RowCount will have the best performance because the there won't be any query cost for the select @@RowCount statement</p>

<p>Update in response to comment:  The example I gave would give the # of rows in partition - defined in this case by ""PARTITION BY my_table.foo"". The value of the column in each row is the # of rows with the same value of my_table.foo. Since your example query had the clause ""WHERE my_table.foo = 'bar'"", all rows in the resultset will have the same value of my_table.foo and therefore the value in the column will be the same for all rows and equal (in this case) this the # of rows in the query.  </p>

<p>Here is a better/simpler example of how to include a column in each row that is the total # of rows in the resultset. Simply remove the optional Partition By clause.</p>

<pre><code>SELECT my_table.my_col, count(*) OVER() AS 'Count'
  FROM my_table
 WHERE my_table.foo = 'bar'
</code></pre>
"
"<p>I am starting a project involving a small database and I am considering to use SQLite. 
If I create a table and I defined one of the columns as text, but all the values stored are integers - is there a way to change the data type of a column? I am using SQLite Manager and I can't find a function that allows me to do that. Can I do it using a SQL command, or is it a restriction of SQLite?
Probably, even with this restriction, one can find some way around and create a new table with the required column types and then import data into it.</p>

<p>Regards,
Nick</p>
","<p>Are you sure <a href=""http://www.sqlite.org/foreignkeys.html#fk_enable"" rel=""noreferrer"">foreign key support</a> is enabled?</p>

<blockquote>
  <p>Assuming the library is compiled with foreign key constraints enabled,
  it must still be enabled by the application at runtime, using the
  PRAGMA foreign_keys command. For example:</p>
</blockquote>

<pre><code>sqlite&gt; PRAGMA foreign_keys = ON;
</code></pre>
"
"<p>Is there an easy way to determine what traces have been set up by <code>sp_trace_create</code> on SQL Server 2000? How about for SQL Server 2005, 2008, 2012, or 2014?</p>
","<p><strong>SQL Server 2005</strong> (onwards):</p>

<pre><code>    SELECT * FROM sys.traces
</code></pre>

<p><strong>SQL Server 2000</strong> :</p>

<pre><code>    USE msdb
    SELECT * FROM fn_trace_getinfo(default);
</code></pre>

<p>Ref: <a href=""http://msdn.microsoft.com/en-us/library/ms173875.aspx"" rel=""nofollow noreferrer""><code>fn_trace_getinfo</code></a></p>

<p>Column descriptions for <code>sys.traces</code> DMV can be found here: <a href=""https://docs.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-traces-transact-sql"" rel=""nofollow noreferrer""><code>sys.traces</code></a></p>
"
"<p>Here's a strange one:</p>

<p>I can filter on <code>NOT NULLS</code> from SQLite, but not <code>NULLS</code>:</p>

<p><strong>This works:</strong></p>

<pre><code>SELECT * FROM project WHERE parent_id NOT NULL;
</code></pre>

<p><strong>These don't:</strong></p>

<pre><code>SELECT * FROM project WHERE parent_id IS NULL; 
SELECT * FROM project WHERE parent_id ISNULL; 
SELECT * FROM project WHERE parent_id NULL;
</code></pre>

<p>All return:</p>

<blockquote>
  <p>There is a problem with the syntax of your query (Query was not
  executed) ...</p>
</blockquote>

<p><strong>UPDATE</strong>:</p>

<p>I am doing this with PHP- through my code with ezSQl and using the <a href=""http://www.danedesigns.com/phpliteadmin.php"" rel=""noreferrer"">PHPLiteAdmin</a> interface </p>

<p>Using the <a href=""http://www.danedesigns.com/phpliteadmin.php?table=Customers&amp;action=table_sql"" rel=""noreferrer"">PHPLiteAdmin demo</a>, this expression works- so now I'm suspecting a version issue with my PHP's SQLite?  Could that be?  Wasn't this expression always valid?</p>

<p><strong>UPDATE 2</strong>:</p>

<p>When I run the code from PHP using ezSQL, the PHP warning is:</p>

<blockquote>
  <p>PHP Warning:  SQL logic error or missing database</p>
</blockquote>

<p>Is there a way to get more information out of PHP? This is maddeningly opaque and weird, especially because the same statement in the CLI works fine...</p>

<p><strong>UPDATE 3</strong></p>

<p>The only other possible clue I have is that the databases that I create with PHP cannot be read by the CLI, and vice versa.  I get:</p>

<blockquote>
  <p>Error: file is encrypted or is not a database</p>
</blockquote>

<p>So there's definitly two SQlite flavors butting heads here. (<a href=""https://stackoverflow.com/questions/1513849/error-file-is-encrypted-or-is-not-a-database"">See this</a>)  Still, why the invalid statment??</p>

<p><strong>UPDATE 4</strong></p>

<p>OK I think I've traced the problem to the culprit, if not the reason- The DB I created with PHP ezSQL is the one where the IS NULL statement fails.  If I create the DB using PHP's SQLite3 class, the statement works fine, and moreover, I can access the DB from the CLI, whereas ezSQL created DB gave the <code>file is encrypted</code> error.</p>

<p>So I did a little digging into ezSQL code- Off the bat I see it uses PDO methods, not the newer SQLite3 class.  Maybe that's something- I'm not gonna waste further time on it...</p>

<p>In any case, I've found my solution, which is to steer clear of ezSQL, and just use PHPs SQLite3 class.</p>
","<p><code>a IS b</code> and <code>a IS NOT b</code> is the general form where <code>a</code> and <code>b</code> are expressions.</p>

<p>This is generally only seen in <code>a IS NULL</code> and <code>a IS NOT NULL</code> cases. There are also <code>ISNULL</code> and <code>NOTNULL</code> (also <code>NOT NULL</code>) operators which are short-hands for the previous expressions, respectively (they only take in a single operand).</p>

<p>The SQL understood in SQLite expressions is covered in <a href=""http://www.sqlite.org/lang_expr.html"" rel=""noreferrer"">SQLite Query Language: Expressions</a>.</p>

<p>Make sure that (previous) statements have been terminated with a <code>;</code> first if using the CLI.</p>

<p>These are all valid to negate a ""null match"":</p>

<pre><code>expr NOT NULL
expr NOTNULL
expr IS NOT NULL
</code></pre>

<p>These are all valid to ""match null"":</p>

<pre><code>expr ISNULL
expr IS NULL
</code></pre>

<p>Since all of the above constructs are themselves expressions the negations are also valid (e.g. <code>NOT (expr NOT NULL)</code> is equivalent to <code>expr IS NULL</code>).</p>

<p>Happy coding.</p>

<hr>

<p>The proof in the pudding:</p>

<pre><code>SQLite version 3.7.7.1 2011-06-28 17:39:05
Enter "".help"" for instructions
Enter SQL statements terminated with a "";""
sqlite&gt; create table x (y int null);
sqlite&gt; select * from x where y isnull;
sqlite&gt; select * from x where y notnull;
sqlite&gt; select * from x where y not null;
sqlite&gt; select * from x where y is null;
sqlite&gt; select * from x where y is not null;
sqlite&gt;
</code></pre>
"
"<p>Was trying to select...into a temp Table #TempTable in sp_Executedsql.
Not its successfully inserted or not but there Messages there written 
(359 row(s) affected) that mean successful inserted?
Script below</p>

<pre><code>DECLARE @Sql NVARCHAR(MAX);
SET @Sql = 'select distinct Coloum1,Coloum2 into #TempTable 
            from SPCTable with(nolock)
            where Convert(varchar(10), Date_Tm, 120) Between @Date_From And @Date_To';

SET @Sql = 'DECLARE @Date_From VARCHAR(10);
            DECLARE @Date_To VARCHAR(10);
            SET @Date_From = '''+CONVERT(VARCHAR(10),DATEADD(d,DATEDIFF(d,0,GETDATE()),0)-1,120)+''';
            SET @Date_To = '''+CONVERT(VARCHAR(10),DATEADD(d,DATEDIFF(d,0,GETDATE()),0)-1,120)+''';
            '+ @Sql;

EXECUTE sp_executesql @Sql;
</code></pre>

<p>After executed,its return me on messages (359 row(s) affected).
Next when trying to select out the data from #TempTable.</p>

<pre><code>Select * From #TempTable;
</code></pre>

<p>Its return me:</p>

<pre><code>Msg 208, Level 16, State 0, Line 2
Invalid object name '#TempTable'.
</code></pre>

<p>Suspected its working only the 'select' section only. The insert is not working.
how fix it?</p>
","<p>Here is a simple example:</p>

<pre><code>EXEC sp_executesql @sql, N'@p1 INT, @p2 INT, @p3 INT', @p1, @p2, @p3;
</code></pre>

<p>Your call will be something like this</p>

<pre><code>EXEC sp_executesql @statement, N'@LabID int, @BeginDate date, @EndDate date, @RequestTypeID varchar', @LabID, @BeginDate, @EndDate, @RequestTypeID
</code></pre>
"
"<p>Im trying to create a FULLTEXT index on an attribute of a table. Mysql returns </p>

<blockquote>
  <p>ERROR 1214: The used table type doesnt support FULLTEXT indexes.</p>
</blockquote>

<p>Any idea what Im doing wrong? </p>
","<p>Youre using the wrong type of table. Mysql supports a few different types of tables, but the most commonly used are MyISAM and InnoDB. <a href=""http://dev.mysql.com/doc/refman/5.1/en/fulltext-restrictions.html"" rel=""noreferrer"">MyISAM (in MySQL 5.6+also InnoDB tables) are the types of tables that Mysql supports for Full-text indexes.</a> </p>

<p>To check your tables type issue the following sql query:</p>

<pre><code>SHOW TABLE STATUS
</code></pre>

<p>Looking at the result returned by the query, find your table and corresponding value in the Engine column. If this value is anything except MyISAM or InnoDB then Mysql will throw an error if your trying to add FULLTEXT indexes.</p>

<p>To correct this, you can use the sql query below to change the engine type:</p>

<pre><code>ALTER TABLE &lt;table name&gt; ENGINE = [MYISAM | INNODB]
</code></pre>

<p>Additional information (thought it might be useful):
Mysql using different engine storage types to optimize for the needed functionality of specific tables. Example MyISAM is the default type for operating systems (besides windows), preforms SELECTs and INSERTs quickly; but does not handle transactions. InnoDB is the default for windows, can be used for transactions. But InnoDB does require more disk space on the server.</p>
"
"<p>I've been successfully publishing DACPACs to SQL Server 2008-2012 instances using SqlPackage.exe, as installed by SQL Server Data Tools (and typically found in <code>C:\Program Files (x86)\Microsoft SQL Server\110\DAC\bin</code>). However, in attempting to publish a 2014-targeted DACPAC to a SQL Server 2014 instance using this same SqlPackage.exe, I get the following:</p>

<pre><code>*** Could not deploy package.
Internal Error. The database platform service with type Microsoft.Data.Tools.
Schema.Sql.Sql120DatabaseSchemaProvider is not valid. You must make sure the
service is loaded, or you must provide the full type name of a valid database
platform service.
</code></pre>

<p>I've found minimal info regarding this; the closest I have found is was a <a href=""http://social.msdn.microsoft.com/Forums/windowsazure/en-US/8ac0ae34-e614-4d35-8a50-46931cdad20a/internal-error-import-bacpac-file?forum=ssdsgetstarted"" rel=""noreferrer"">problem</a> publishing to Azure.</p>

<p>I've kept up to date with SSDT patches but would guess that the SqlPackage.exe I have (which shows an 11.0.2902.0 version) is simply incompatible. I am able to publish to this same instance using Visual Studio 2012's Publish command so the instance itself does not seem to be the issue.</p>

<p>Is there a newer version of SqlPackage available that would support publishing a 2014 DACPAC to a 2014 server? Or another scriptable way to do this?</p>
","<p>Yes, there is a new version supporting SQL Server 2005-2016 available and it installs into a different location than the previous (SQL Server 2012 and lower) version. In fact, you'll have different install locations depending on if you just use SSDT or if you install it as part of SSMS or the standalone installer.</p>

<ul>
<li><p>SSDT installs the Dac DLLs inside Visual Studio in the latest releases. This is to avoid side by side issues (Visual Studio 2012 vs 2013 vs SSMS) that required all to be updated to use the latest code. </p>

<ul>
<li>If you have <a href=""https://msdn.microsoft.com/en-us/library/mt204009.aspx"">updated to the latest SSDT</a>, you'll find SqlPackage.exe and the related DLLs in the <strong>VS Install Directory\Common7\IDE\Extensions\Microsoft\SQLDB\DAC\130</strong>. For VS2013 the VS install directory is <strong>C:\Program Files (x86)\Microsoft Visual Studio 12.0</strong>, and it's 14.0 for VS2015.</li>
</ul></li>
<li><p><a href=""https://msdn.microsoft.com/en-us/library/mt238290.aspx"">SQL Server Management Studio (SSMS)</a> and the standalone <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=53013"">Dac Framework MSI</a> both install to the system-wide location. This is <strong>C:\Program Files (x86)\Microsoft SQL Server\130\Dac\bin</strong>.</p></li>
</ul>
"
"<p>I ran a mysql import mysql dummyctrad &lt; dumpfile.sql on server and its taking too long to complete. The dump file is about 5G. The server is a Centos 6, memory=16G and 8core processors, mysql v 5.7 x64-</p>

<p>Are these normal messages/status ""waiting for table flush"" and the message <code>InnoDB: page_cleaner: 1000ms intended loop took 4013ms. The settings might not be optimal</code> </p>

<p>mysql log contents</p>

<pre><code>2016-12-13T10:51:39.909382Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4013ms. The settings might not be optimal. (flushed=1438 and evicted=0, during the time.)
2016-12-13T10:53:01.170388Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4055ms. The settings might not be optimal. (flushed=1412 and evicted=0, during the time.)
2016-12-13T11:07:11.728812Z 0 [Note] InnoDB: page_cleaner: 1000ms intended loop took 4008ms. The settings might not be optimal. (flushed=1414 and evicted=0, during the time.)
2016-12-13T11:39:54.257618Z 3274915 [Note] Aborted connection 3274915 to db: 'dummyctrad' user: 'root' host: 'localhost' (Got an error writing communication packets)
</code></pre>

<p>Processlist:</p>

<pre><code>mysql&gt; show processlist \G;
*************************** 1. row ***************************
     Id: 3273081
   User: root
   Host: localhost
     db: dummyctrad
Command: Field List
   Time: 7580
  State: Waiting for table flush
   Info: 
*************************** 2. row ***************************
     Id: 3274915
   User: root
   Host: localhost
     db: dummyctrad
Command: Query
   Time: 2
  State: update
   Info: INSERT INTO `radacct` VALUES (351318325,'kxid ge:7186','abcxyz5976c','user100
*************************** 3. row ***************************
     Id: 3291591
   User: root
   Host: localhost
     db: NULL
Command: Query
   Time: 0
  State: starting
   Info: show processlist
*************************** 4. row ***************************
     Id: 3291657
   User: remoteuser
   Host: portal.example.com:32800
     db: ctradius
Command: Sleep
   Time: 2
  State: 
   Info: NULL
4 rows in set (0.00 sec)
</code></pre>

<p><strong>Update-1</strong></p>

<p><a href=""https://bugs.mysql.com/bug.php?id=81899"" rel=""noreferrer"">mysqlforum</a> ,<a href=""http://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_lru_scan_depth"" rel=""noreferrer"">innodb_lru_scan_depth</a></p>

<p>changing innodb_lru_scan_depth value to 256 have improved the insert queries execution time + no warning message in log, the default was innodb_lru_scan_depth=1024;</p>

<p><code>SET GLOBAL innodb_lru_scan_depth=256;</code></p>
","<p><strong>On Windows</strong>:</p>

<p>0) shut down service <code>mysql56</code></p>

<p>1) go to <code>C:\ProgramData\MySQL\MySQL Server 5.6</code>, note that <code>ProgramData</code> is a hidden folder</p>

<p>2) looking for file <code>my.ini</code>, open it and add one line <code>skip-grant-tables</code> below <code>[mysqld]</code>,save</p>

<pre><code>[mysqld]

skip-grant-tables
</code></pre>

<p>3) start service <code>mysql56</code></p>

<p>4) by right, you can access the database, run <code>mysql</code></p>

<p>5) and use the query below to update the password</p>

<pre><code>update mysql.user set password=PASSWORD('NEW PASSWORD') where user='root';
</code></pre>

<p><strong>note</strong>: for newer version, use <code>authentication_string</code> instead of <code>password</code></p>

<p>6) shut down the service again, remove the line <code>skip-grant-tables</code> save it, and start the service again. try to use the password you set to login.</p>

<hr>

<p><strong>On Mac</strong>:</p>

<p>0) stop the service</p>

<pre><code>sudo /usr/local/mysql/support-files/mysql.server stop
</code></pre>

<p>1) skip grant table</p>

<pre><code>sudo /usr/local/mysql/bin/mysqld_safe --skip-grant-tables
</code></pre>

<p>once it's running, don't close it, and open a new terminal window</p>

<p>2) go into mysql terminal</p>

<pre><code>/usr/local/mysql/bin/mysql -u root
</code></pre>

<p>3) update the password</p>

<pre><code>UPDATE mysql.user SET Password=PASSWORD('password') WHERE User='root';
</code></pre>

<p>for newer version like 5.7, use</p>

<pre><code>UPDATE mysql.user SET authentication_string=PASSWORD('password') WHERE User='root';
</code></pre>

<p>4) run <code>FLUSH PRIVILEGES;</code></p>

<p>5) run <code>\q</code> to quit</p>

<p>6) start the mysql server</p>

<pre><code>sudo /usr/local/mysql/support-files/mysql.server start
</code></pre>
"
"<p>I need to connect to a SQL 2008 R2 Server from my Linux box that's not registered in my company's domain. I'm trying to use SQuirreL SQL version 3.2.1.
I downloaded <a href=""http://www.microsoft.com/download/en/details.aspx?displaylang=en&amp;id=21599"">Microsoft SQL Server JDBC Driver 3.0</a> and assigned it to SQuirreL on the Drivers tab.</p>

<p>Now, when I try to create an Alias on SQuirreL, I select the SQL Server driver and adjust the URL. For credentials I use my domain registered username and password.</p>

<p>When I try to test the connection, I always get the same error:</p>

<p>&lt;""database-name"">: Logon failure for user '&lt;""domain"">\&lt;""domain-user"">'.</p>

<p>How can I get this working? Thanks in advance!</p>
","<p>I ran into similar issue and the following change fixed the issue. Open the Application Folder in finder and open the App Package Contents and navigate to Contents/MacOS/. Open the squirrel-sql.sh file and update the value of ""SQUIRREL_SQL_HOME"" around line 56. </p>

<p>Out of box, the value would be </p>

<blockquote>
  <p>SQUIRREL_SQL_HOME=`dirname ""$0""`/Contents/Resources/Java</p>
</blockquote>

<p>Update this to </p>

<blockquote>
  <p>SQUIRREL_SQL_HOME='/Applications/SQuirreLSQL.app/Contents/Resources/Java'</p>
</blockquote>

<p>Thanks to <a href=""https://sourceforge.net/p/squirrel-sql/bugs/1232/#6bc6"" rel=""noreferrer"">https://sourceforge.net/p/squirrel-sql/bugs/1232/#6bc6</a></p>
"
"<p>My standard disclaimer: I haven't worked with Java in about 10 years, so it's very probable I'm doing something elementary wrong here.</p>

<p>I am writing a ""server-side extension"" for <a href=""http://smartfoxserver.com"" rel=""nofollow noreferrer"">SmartFoxServer</a> (SFS).  In my login script, I need to make a connection to MS SQL Server, which I am attempting to do using JDBC.  I have tested the JDBC code in my debug environment, and it works fine.</p>

<p>BUT</p>

<p>When I put the server-side extension in the SFS ""extensions"" folder (as per spec), I'm getting a <code>com.microsoft.sqlserver.jdbc.SQLServerException</code>:</p>

<blockquote>
  <p>""This driver is not configured for integrated authentication."".</p>
</blockquote>

<p>I Googled for this error, and found that it's usually because the file <code>sqljdbc_auth.dll</code> is not in the system path; I have copied this file into a folder in my system path, and still it does not work!</p>

<p>Any other suggestions?</p>
","<p>There are different versions of sqljdbc_auth.dll for different processor architectures (x86/x64/ia64).  Which one are you using on your SFS server?</p>

<p>You must choose the one to match the architecture of the JVM under which SFS is running.  So, if you're running 32-bit Java on a 64-bit machine, you'll need the x86 version, not the x64 version.</p>

<p>I've not used SFS before, so I don't know whether it writes any logs anywhere.  If it does, it might be worth taking a look at these logs to see if anything helpful has been written to them.</p>

<p><strong>EDIT</strong>:  I can't be 100% sure that SFS is using 64-bit Java just because it runs out of C:\Program Files as opposed to C:\Program Files (x86).</p>

<p>I found the following line in the <a href=""http://smartfoxserver.com/docs/"" rel=""noreferrer"">SFS docs</a> under Introduction > Requirements and Installation.  Whilst this line applies only to Linux as opposed to Windows, it might suggest that SFS on Windows also uses 32-bit Java:</p>

<blockquote>
  <p>Since version 1.5 SmartFoxServer comes with its own x86 32-bit Sun Java Runtime. </p>
</blockquote>

<p>One quick way to determine which version(s) of Java you have installed is to see whether either or both of the folders <code>C:\Program Files\Java</code> or <code>C:\Program Files (x86)\Java</code> exist.  Of course, if both folders exist, that's not much help to you.</p>

<p>Does your application work if you use the x86 version of sqljdbc_auth.dll instead of the x64 version?  If it suddenly starts working with the x86 DLL, then SFS must be using 32-bit Java.</p>

<p>Is there a batch-file used to start SFS?  If so, reading through that might help point out where SFS is running Java from.  Also look out for any changes to the <code>PATH</code>.  Java can only load DLLs in the <code>java.library.path</code> system property, and on Windows, this is set to the value of the <code>PATH</code> environment variable.</p>

<p>If you still can't determine whether SFS is using 32-bit or 64-bit Java, try using Process Explorer to look at the environment that the java.exe process running SFS was started with.</p>
"
"<p>I got confused with the manual , should i work like this:</p>

<pre><code>{
 QSqlDatabase db = QSqlDatabase::addDatabase (...);
 QSqlQuery query (db);
 query.exec (...);
}

QSqlDatabase::removeDatabase (...);
</code></pre>

<p>As the document points out, <code>query</code> or <code>db</code> will be deconstructed automatically.
But is that efficient ?</p>

<p>Well , if i cache <code>db</code> inside a class , like the following:</p>

<pre><code>class Dummy {
  Dummy() { 
    db = QSqlDatabase::addDatabase (...);
  }
  ~Dummy() {
    db.close();
  }

  bool run() {
    QSqlQuery query (db);
    bool retval = query.exec (...);
    blabla ...
  }

  private:
    QSqlDatabase db;
};
</code></pre>

<p>Sometimes i could see warnings like: </p>

<pre><code>QSqlDatabasePrivate::removeDatabase: connection 'BLABLA' is still in use, all queries will cease to work.
</code></pre>

<p>Even if i didn't call <code>run()</code>.</p>
","<p>When you create a <code>QSqlDatabase</code> object with <code>addDatabase</code> or when you call <code>removeDatabase</code>, you are merely associating or disassociating a tuple <em>(driver, hostname:port, database name, username/password)</em> to a name (or to the default connection name if you don't specify a connection name). <br>The SQL driver is instantiated, but the database will only be opened when you call <code>QSqlDatabase::open</code>.</p>

<p>That connection name is defined application-wide. So if you call <code>addDatabase</code> in each of the objects that use it, you are changing all <code>QSqlDatabase</code> objects that uses the same connection name and invalidating all queries that were active on them. </p>

<p>The first code example you cited shows how to correctly disassociate the connection name, by ensuring that:</p>

<ul>
<li>all <code>QSqlQuery</code> are detached from the <code>QSqlDatabase</code> before closing the database by calling <code>QSqlQuery::finish()</code>, which is automatic when the <code>QSqlQuery</code> object goes out of scope,</li>
<li>all <code>QSqlDatabase</code> with the same connection name are <code>close()</code>d when you call <code>QSqlDatabase::removeDatabase</code> (<code>close()</code> is also called automatically when the <code>QSqlDatabase</code> object goes out of scope).</li>
</ul>

<p>When you create the QSqlDatabase, depending on whether you want the connection to stay open for the application lifetime (1) or just when needed (2), you can:</p>

<ol>
<li><p>keep a single <code>QSqlDatabase</code> instance in one single class (for example, in your mainwindow), and use it in other objects that needs it either by passing the <code>QSqlDatabase</code> directly or just the connection name that you pass to <code>QSqlDatabase::database</code> to get the <code>QSqlDatabase</code> instance back. <code>QSqlDatabase::database</code> uses <code>QHash</code> to retrieve a <code>QSqlDatabase</code> from its name, so it is probably negligibly slower than passing the <code>QSqlDatabase</code> object directly between objects and functions, and if you you use the default connection, you don't even have to pass anything anywhere, just call <code>QSqlDatabase::database()</code> without any parameter.</p>

<pre><code>// In an object that has the same lifetime as your application
// (or as a global variable, since it has almost the same goal here)
QSqlDatabase db;

// In the constructor or initialization function of that object       
db = QSqlDatabase::addDatabase(""QSQLDRIVER"", ""connection-name""); 
db.setHostname(...);
// ...
if(!this-&gt;db.open())  // open it and keep it opened
{
    // Error handling...
}

// --------
// Anywhere you need it, you can use the ""global"" db object 
// or get the database connection from the connection name        
QSqlDatabase db = QSqlDatabase::database(""connection-name""); 
QSqlQuery query(db);             
</code></pre></li>
<li><p>configure the <code>QSqlDatabase</code> once, open it to test that the parameters are correct, and ditch the instance. The connection name, will still be accessible anywhere, but the database will have to be reopened:</p>

<pre><code>{
    // Allocated on the stack
    QSqlDatabase db = QSqlDatabase::addDatabase(""QSQLDRIVER"", ""connection-name""); 
    db.setHostname(...);
    // ...
    if(!this-&gt;db.open()) // test the connection
    {
       // Error handling
    }
// db is closed when it goes out of scope
} 

{
    // Same thing as for (1), but by default database() opens 
    // the connection if it isn't already opened 
    QSqlDatabase db = QSqlDatabase::database(""connection-name""); 
    QSqlQuery query(db);

// if there is no other connection open with that connection name,
// the connection is closed when db goes out of scope
} 
</code></pre>

<p>In that case, note that you shouldn't close the database explicitly, because you can have multiple objects using the same database connection in a reentrant manner (for example, if a function A use the connection and calls B which also use the connection. If B closes the connection before returning control to A, the connection will also be closed for A, which is probably a bad thing).</p></li>
</ol>
"
"<p>I am working on a social network, one of my procedures returns a VARCHAR output.
So this is what I wrote:</p>

<pre><code>SqlParameter job1 = cmd2.Parameters.Add(""@job"", SqlDbType.VarChar);
job1.Direction = ParameterDirection.Output;
</code></pre>

<p>However this error comes up:</p>

<blockquote>
  <p>String[1]: the Size property has an invalid size of 0.</p>
</blockquote>
","<p>You need to <strong>define a length</strong> when specifying the <code>varchar</code> parameter:</p>

<pre><code>SqlParameter job1 = cmd2.Parameters.Add(""@job"", SqlDbType.VarChar, 50);
</code></pre>

<p>You should use the same length as defined in your SQL Server stored procedure.</p>

<p>And btw: if your stored procedure also has no length defined (something like <code>@job VARCHAR OUTPUT</code>) - then you've defined a <code>varchar</code> string of 1 character length ...... </p>
"
"<p>Long time listener, first time caller (finally made an account here!)...</p>

<p>I am using <strong>Visual Studio 2013</strong> with <strong>.NET 4.5.1</strong> and <strong>Entity Framework 6</strong> (final releases, not RC or beta).</p>

<p>When trying to add a DbGeography property to my entity, I get this error upon execution:</p>

<pre><code>    One or more validation errors were detected during model generation:
    Geocoder.DbGeography: : EntityType 'DbGeography' has no key defined.
    Define the key for this EntityType.
    DbGeographies: EntityType: EntitySet 'DbGeographies' is based on type 'DbGeography' that has no keys defined.
</code></pre>

<p>I have already confirmed I have no references to older versions of Entity Framework (discussed <a href=""http://entityframework.codeplex.com/workitem/1535"" rel=""nofollow noreferrer"">here</a>). I have been using <a href=""https://stackoverflow.com/questions/7409051/why-use-the-sql-server-2008-geography-data-type/7409237"">this post</a> and <a href=""http://msdn.microsoft.com/en-us/data/hh859721.aspx"" rel=""nofollow noreferrer"">this MSDN article</a> for examples/information as this is my first foray into spatial types in .NET (and SQL Server, for that matter).</p>

<p>Here is my entity:</p>

<pre><code>public class Location
{
    public int Id { get; set; }
    public string Name { get; set; }
    public string Address1 { get; set; }
    public string Address2 { get; set; }
    public string City { get; set; }
    public virtual State State { get; private set; }
    public string ZipCode { get; set; }
    public string ZipCodePlus4 { get; set; }
    public DbGeography Geocode { get; set; }
    public Hours Hours { get; set; }
    public virtual ICollection&lt;Language&gt; Languages { get; private set; }
    public virtual OfficeType OfficeType { get; private set; }

    [JsonIgnore]
    public virtual ICollection&lt;ProviderLocation&gt; Providers { get; private set; }
}
</code></pre>

<p>What am I doing wrong?</p>
","<p>This turned out to be the opposite of what I read from Microsoft's own response about a similar issue at Codeplex <a href=""http://entityframework.codeplex.com/workitem/1535"" rel=""noreferrer"">here</a>, and even their <a href=""http://msdn.microsoft.com/en-us/library/system.data.spatial(v=vs.110).aspx"" rel=""noreferrer"">documentation here</a>. Did I interpret it wrong? Both of those links indicate that in EF 6, the DbGeography datatype was moved from System.Data.Entity.Spatial to just System.Data.Spatial, but the reverse seems true.</p>

<p>I changed</p>

<pre><code>using System.Data.Spatial;
</code></pre>

<p>to</p>

<pre><code>using System.Data.Entity.Spatial;
</code></pre>

<p>and it works.</p>
"
"<p>I want to perform a query which would look like this in native SQL:</p>

<pre><code>SELECT
    AVG(t.column) AS average_value
FROM
    table t
WHERE
    YEAR(t.timestamp) = 2013 AND
    MONTH(t.timestamp) = 09 AND
    DAY(t.timestamp) = 16 AND
    t.somethingelse LIKE 'somethingelse'
GROUP BY
    t.somethingelse;
</code></pre>

<p>If I am trying to implement this in Doctrine's query builder like this:</p>

<pre><code>$qb = $this-&gt;getDoctrine()-&gt;createQueryBuilder();
$qb-&gt;select('e.column AS average_value')
   -&gt;from('MyBundle:MyEntity', 'e')
   -&gt;where('YEAR(e.timestamp) = 2013')
   -&gt;andWhere('MONTH(e.timestamp) = 09')
   -&gt;andWhere('DAY(e.timestamp) = 16')
   -&gt;andWhere('u.somethingelse LIKE somethingelse')
   -&gt;groupBy('somethingelse');
</code></pre>

<p>I get the error exception</p>

<blockquote>
  <p>[Syntax Error] line 0, col 63: Error: Expected known function, got 'YEAR'</p>
</blockquote>

<p>How can I implement my query with Doctrines query builder?</p>

<p><strong>Notes:</strong></p>

<ul>
<li>I know about <a href=""http://docs.doctrine-project.org/en/latest/reference/native-sql.html"">Doctrine's Native SQL</a>. I've tried this, but it leads to the problem that my productive and my development database tables have different names. I want to work database agnostic, so this is no option.</li>
<li>Although I want to work db agnostic: FYI, I am using MySQL.</li>
<li>There is way to extend Doctrine to ""learn"" the <code>YEAR()</code> etc. statements, e.g. as <a href=""http://www.simukti.net/blog/2012/04/05/how-to-select-year-month-day-in-doctrine2/"">seen here</a>. But I am looking for a way to avoid including third party plugins.</li>
</ul>
","<p>First day of next month:</p>

<p>sql-server 2012+</p>

<pre><code>DATEADD(d, 1, EOMONTH(current_timestamp))
</code></pre>

<p>sql-server 2008 and older:</p>

<pre><code>DATEADD(m, DATEDIFF(m, -1, current_timestamp), 0)
</code></pre>
"
"<p>I have this doubt, I've searched the web and the answers seem to be diversified. Is it better to use mysql_pconnect over mysql_connect when connecting to a database via PHP? I read that pconnect scales much better, but on the other hand, being a persistent connection... having 10 000 connections at the same time, all persistent, doesn't seem scalable to me.</p>

<p>Thanks in advance.</p>
","<p>Persistent connections should be unnecessary for MySQL.  In other databases (such as Oracle), making a connection is expensive and time-consuming, so if you can re-use a connection it's a big win.  But those brands of database offer connection pooling, which solves the problem in a better way.</p>

<p>Making a connection to a MySQL database is quick compared to those other brands, so using persistent connections gives proportionally less benefit for MySQL than it would for another brand of database.</p>

<p>Persistent connections have a downside too.  The database server allocates resources to each connection, whether the connections are needed or not.  So you see a lot of wasted resources for no purpose if the connections are idle.  I don't know if you'll reach 10,000 idle connections, but even a couple of hundred is costly.</p>

<p>Connections have state, and it would be inappropriate for a PHP request to ""inherit"" information from a session previously used by another PHP request.  For example, temporary tables and user variables are normally cleaned up as a connection closes, but not if you use persistent connections.  Likewise session-based settings like character set and collation.  Also, <code>LAST_INSERT_ID()</code> would report the id last generated during the session -- even if that was during a prior PHP request.</p>

<p>For MySQL at least, the downside of persistent connections probably outweighs their benefits.  And there are other, better techniques to achieve high scalability.</p>

<hr>

<p>Update March 2014:</p>

<p>MySQL connection speed was always low compared to other brands of RDBMS, but it's getting even better.</p>

<p>See <a href=""http://mysqlserverteam.com/improving-connectdisconnect-performance/"" rel=""noreferrer"">http://mysqlserverteam.com/improving-connectdisconnect-performance/</a></p>

<blockquote>
  <p>In MySQL 5.6 we started working on optimizing the code handling connects and disconnects. And this work has accelerated in MySQL 5.7. In this blog post I will first show the results we have achieved and then describe what we have done to get them.</p>
</blockquote>

<p>Read the blog for more details and speed comparisons.</p>
"
"<p>I get the error <code>ERROR 1066 (42000): Not unique table/alias:</code> </p>

<p>I cant figure out whats wrong with it.</p>

<pre><code>SELECT Project_Assigned.ProjectID, Project_Title, Account.Account_ID, Username, Access_Type
FROM Project_Assigned 
JOIN Account 
 ON Project_Assigned.AccountID = Account.Account_ID
JOIN Project
 ON Project_Assigned.ProjectID = Project.Project_ID
where Access_Type = 'Client';
</code></pre>
","<p>You need to give the user table an alias the second time you join to it</p>

<p>e.g.</p>

<pre><code>SELECT article . * , section.title, category.title, user.name, u2.name 
FROM article 
INNER JOIN section ON article.section_id = section.id 
INNER JOIN category ON article.category_id = category.id 
INNER JOIN user ON article.author_id = user.id 
LEFT JOIN user u2 ON article.modified_by = u2.id 
WHERE article.id = '1'
</code></pre>
"
"<p>I connected to a Google Cloud SQL database from eclipse using Data Source explorer. But when I generate DDL of that database using its option <code>Generate DDL</code>, I can't get the <code>AUTO_INCREMENT</code> in my script but get the corresponding primary key. </p>

<p>How would i go about getting the <code>AUTO_INCREMENT</code> in my script?</p>
","<p><strong>Update</strong></p>

<p>Google Cloud SQL now supports direct access, so the <code>MySQLdb</code> dialect can now be used.  The recommended connection via the mysql dialect is using the URL format:</p>

<pre><code>mysql+mysqldb://root@/&lt;dbname&gt;?unix_socket=/cloudsql/&lt;projectid&gt;:&lt;instancename&gt;
</code></pre>

<p><code>mysql+gaerdbms</code> has been deprecated in SQLAlchemy since version 1.0</p>

<p>I'm leaving the original answer below in case others still find it helpful.</p>

<hr>

<p>For those who visit this question later (and don't want to read through all the comments), SQLAlchemy now supports Google Cloud SQL as of version 0.7.8 using the connection string / dialect (see: <a href=""http://docs.sqlalchemy.org/en/rel_0_7/dialects/mysql.html#module-sqlalchemy.dialects.mysql.gaerdbms"" rel=""noreferrer"">docs</a>):</p>

<pre><code>mysql+gaerdbms:///&lt;dbname&gt;
</code></pre>

<p>E.g.:</p>

<pre><code>create_engine('mysql+gaerdbms:///mydb', connect_args={""instance"":""myinstance""})
</code></pre>

<hr>

<p>I have proposed an <a href=""http://www.sqlalchemy.org/trac/ticket/2649"" rel=""noreferrer"">update</a> to the <code>mysql+gaerdmbs://</code> dialect to support both of Google Cloud SQL APIs (<code>rdbms_apiproxy</code> and <code>rdbms_googleapi</code>) for connecting to Cloud SQL from a non-Google App Engine production instance (ex. your development workstation).  The change  will also modify the connection string slightly by including the project and instance as part of the string, and not require being passed separately via <code>connect_args</code>.</p>

<p>E.g.</p>

<pre><code>mysql+gaerdbms:///&lt;dbname&gt;?instance=&lt;project:instance&gt;
</code></pre>

<p>This will also make it easier to use Cloud SQL with Flask-SQLAlchemy or other extension where you don't explicitly make the <code>create_engine()</code> call.</p>

<p>If you are having trouble connecting to Google Cloud SQL from your development workstation, you might want to take a look at my answer here - <a href=""https://stackoverflow.com/a/14287158/191902"">https://stackoverflow.com/a/14287158/191902</a>.</p>
"
"<p>Via the MySQL command line client, I am trying to set the global mysql_mode:</p>

<pre><code>SET GLOBAL sql_mode = TRADITIONAL;
</code></pre>

<p>This works for the current session, but after I restart the server, the sql_mode goes back to its default: '', an empty string. </p>

<p>How can I permanently set sql_mode to TRADITIONAL?</p>

<p>If relevant, the MySQL is part of the WAMP package.</p>

<p>Thank you.</p>
","<p>This problem scuppered me for a while as well.  None of the answers so far addressed the original problem but I believe mine does so I'll post it in case it helps anyone else.</p>

<p>I have MySQL (from mysql.com) Community Edition 5.7.10 installed on OS X 10.10.3 </p>

<p>In the end I created a <code>/etc/mysql/my.cnf</code> with the following contents:-</p>

<pre><code>[mysqld]

sql_mode=NO_ENGINE_SUBSTITUTION
</code></pre>

<p>After restarting the server a <code>SHOW VARIABLES LIKE 'sql_mode';</code> gave me:-</p>

<pre><code>+---------------+------------------------+
| Variable_name | Value                  |
+---------------+------------------------+
| sql_mode      | NO_ENGINE_SUBSTITUTION |
+---------------+------------------------+
1 row in set (0.00 sec)
</code></pre>

<p>Finally, no strict mode!</p>
"
"<p>After several days passed to investigate about the issue, I decided to submit this question because there is no sense apparently in what is happening.</p>

<p><strong>The Case</strong></p>

<p>My computer is configured with a local Oracle Express database.
I have a JAVA project with several JUnit Tests that extend a parent class (I know that it is not a ""best practice"") which opens an OJDBC Connection (using a static Hikari connection pool of 10 Connections) in the @Before method and rolled Back it in the @After.</p>

<pre><code>public class BaseLocalRollbackableConnectorTest {
private static Logger logger = LoggerFactory.getLogger(BaseLocalRollbackableConnectorTest.class);
protected Connection connection;

@Before
public void setup() throws SQLException{
    logger.debug(""Getting connection and setting autocommit to FALSE"");
    connection = StaticConnectionPool.getPooledConnection();
}

@After
public void teardown() throws SQLException{ 
    logger.debug(""Rollback connection"");
    connection.rollback();
    logger.debug(""Close connection"");
    connection.close();
}
</code></pre>

<p>StacicConnectionPool</p>

<pre><code>public class StaticConnectionPool {

private static HikariDataSource ds;

private static final Logger log = LoggerFactory.getLogger(StaticConnectionPool.class);

public static Connection getPooledConnection() throws SQLException {

    if (ds == null) {
        log.debug(""Initializing ConnectionPool"");
        HikariConfig config = new HikariConfig();
        config.setMaximumPoolSize(10);
        config.setDataSourceClassName(""oracle.jdbc.pool.OracleDataSource"");
        config.addDataSourceProperty(""url"", ""jdbc:oracle:thin:@localhost:1521:XE"");
        config.addDataSourceProperty(""user"", ""MyUser"");
        config.addDataSourceProperty(""password"", ""MyPsw"");
        config.setAutoCommit(false);
        ds = new HikariDataSource(config);

    }
    return ds.getConnection();

}
</code></pre>

<p>}</p>

<p>This project has hundreds tests (not in parallel) that use this connection (on localhost) to execute queries (insert/update and select) using Sql2o but transaction and clousure of connection is managed only externally (by the test above). 
The database is completely empty to have ACID tests.</p>

<p>So the expected result is to insert something into DB, makes the assertions and then rollback. in this way the second test will not find any data added by previous test in order to maintain the isolation level.</p>

<p><strong>The Problem</strong>
Running all tests together (sequentially), 90% of times they work properly. the 10% one or two tests, randomly, fail, because there is dirty data in the database (duplicated unique for example) by previous tests. looking the logs, rollbacks of previous tests were done properly. In fact, if I check the database, it is empty) 
If I execute this tests in a server with higher performance but the same JDK, same Oracle DB XE, this failure ratio is increased to 50%. </p>

<p>This is very strange and I have no idea because the connections are different between tests and the rollback is called each time. The JDBC Isolation level is READ COMMITTED    so even if we used the same connection, this should not create any problem even using the same connection.
So my question is:
Why it happen? do you have any idea? Is the JDBC rollback synchronous as I know or there could be some cases where it can go forward even though it is not fully completed?</p>

<p>These are my main DB params:
processes   100
sessions    172
transactions    189</p>
","<p>Try storing it as bytes:</p>

<pre><code>UUID uuid = UUID.randomUUID();
byte[] uuidBytes = new byte[16];
ByteBuffer.wrap(uuidBytes)
        .order(ByteOrder.BIG_ENDIAN)
        .putLong(uuid.getMostSignificantBits())
        .putLong(uuid.getLeastSignificantBits());

con.createQuery(""INSERT INTO TestTable(ID, Name) VALUES(:id, :name)"")
    .addParameter(""id"", uuidBytes)
    .addParameter(""name"", ""test1"").executeUpdate();
</code></pre>

<p>A bit of an explanation: your table is using BINARY(16), so serializing UUID as its raw bytes is a really straightforward approach. UUIDs are essentially 128-bit ints with a few reserved bits, so this code writes it out as a big-endian 128-bit int. The ByteBuffer is just an easy way to turn two longs into a byte array.</p>

<p>Now in practice, all the conversion effort and headaches won't be worth the 20 bytes you save per row.</p>
"
"<p>I've been searching for a while now but can't seem to find answers so here goes...</p>

<p>I've got a CSV file that I want to import into a table in Oracle (9i/10i).</p>

<p>Later on I plan to use this table as a lookup for another use.</p>

<p>This is actually a workaround I'm working on since the fact that querying using the IN clause with more that 1000 values is not possible.</p>

<p>How is this done using SQLPLUS?</p>

<p>Thanks for your time! :)</p>
","<p><strong>SQL Loader</strong> helps load csv files into tables: <a href=""http://www.orafaq.com/wiki/SQL*Loader_FAQ"" rel=""noreferrer"">SQL*Loader</a></p>

<p>If you want sqlplus only, then it gets a bit complicated. You need to locate your sqlloader script and csv file, then run the sqlldr command.</p>
"
"<p>I am using MySQL version 5.1.66.  I saw that the <a href=""http://dev.mysql.com/doc/refman/5.1/en/server-system-variables.html#sysvar_long_query_time"">long_query_time</a> variable is dynamic, but when I tried </p>

<pre><code>set GLOBAL long_query_time=1; 
</code></pre>

<p>After the above operation again I tried</p>

<pre><code>mysql&gt; show variables like 'long_query_time';
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| long_query_time | 10.000000 |
+-----------------+-----------+
1 row in set (0.00 sec)
</code></pre>

<p>From the mysql console it is  not getting altered , why?</p>
","<p>You are setting a GLOBAL system variable, but you querying for the SESSION variable. For the GLOBAL variable setting to take effect for the current session, you need to reconnect, or set the @@SESSION.long_query_time variable. (Note that SHOW VARIABLES by default shows the session variables.)</p>

<p>Here is an example:</p>

<pre><code>mysql&gt; SHOW SESSION VARIABLES LIKE ""long_query_time"";
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| long_query_time | 10.000000 |
+-----------------+-----------+

mysql&gt; SET @@GLOBAL.long_query_time = 1;

mysql&gt; SHOW GLOBAL VARIABLES LIKE ""long_query_time"";
+-----------------+----------+
| Variable_name   | Value    |
+-----------------+----------+
| long_query_time | 1.000000 |
+-----------------+----------+

mysql&gt; SHOW VARIABLES LIKE ""long_query_time"";
+-----------------+-----------+
| Variable_name   | Value     |
+-----------------+-----------+
| long_query_time | 10.000000 |
+-----------------+-----------+
</code></pre>
"
"<p>I have access to command line isql and I like to get Meta-Data of all the tables of a given database, possibly in a formatted file. How I can achieve that?</p>

<p>Thanks.</p>
","<p>Check <a href=""http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.infocenter.dc36274.1570/html/tables/X14933.htm"" rel=""noreferrer"">sysobjects</a> and <a href=""http://infocenter.sybase.com/help/index.jsp?topic=/com.sybase.help.ase_15.0.tables/html/tables/tables46.htm"" rel=""noreferrer"">syscolumns</a> tables.</p>

<p><a href=""http://download.sybase.com/pdfdocs/asg1250e/poster.pdf"" rel=""noreferrer"">Here</a> is a diagram of Sybase system tables.</p>

<p>List of all user tables:</p>

<pre><code>SELECT * FROM sysobjects WHERE type = 'U'
</code></pre>

<p>You can change 'U' to other objects:</p>

<ul>
<li>C  computed column</li>
<li>D  default</li>
<li>F  SQLJ function</li>
<li>L  log</li>
<li>N  partition condition</li>
<li>P  Transact-SQL or SQLJ procedure</li>
<li>PR  prepare objects (created by Dynamic SQL)</li>
<li>R  rule</li>
<li>RI  referential constraint</li>
<li>S  system table</li>
<li>TR  trigger</li>
<li>U  user table</li>
<li>V  view</li>
<li>XP  extended stored procedure</li>
</ul>

<p>List of columns in a table:</p>

<pre><code>SELECT sc.* 
FROM syscolumns sc
INNER JOIN sysobjects so ON sc.id = so.id
WHERE so.name = 'my_table_name'
</code></pre>
"
"<p>I try to avoid doing Count(<em>) because of performance issue.  (i.e. SELECT COUNT(</em>) FROM Users)</p>

<p>If I run the followings in phpMyAdmin, it is ok:</p>

<pre><code>  SELECT SQL_CALC_FOUND_ROWS * FROM Users;
  SELECT FOUND_ROWS();
</code></pre>

<p>It will return # of rows. i.e. # of Users.  </p>

<p>However, if I run in in PHP, I cannot do this:</p>

<pre><code>  $query = 'SELECT SQL_CALC_FOUND_ROWS * FROM Users;
    SELECT FOUND_ROWS(); ';
  mysql_query($query);
</code></pre>

<p>It seems like PHP doesn't like to have two queries passing in.  So, how can I do that?</p>
","<p><code>SQL_CALC_FOUND_ROWS</code> is only useful if you're using a <code>LIMIT</code> clause, but still want to know how many rows would've been found without the <code>LIMIT</code>.</p>

<p>Think of how this works:</p>

<pre><code>SELECT SQL_CALC_FOUND_ROWS * FROM Users;
</code></pre>

<p>You're forcing the database to retrieve/parse ALL the data in the table, and then you throw it away. Even if you aren't going to retrieve any of the rows, the DB server will still start pulling actual data from the disk on the assumption that you will want that data.</p>

<p>In human terms, you bought the entire contents of the super grocery store, but threw away everything except the pack of gum from the stand by the cashier.</p>

<p>Whereas, doing:</p>

<pre><code>SELECT count(*) FROM users;
</code></pre>

<p>lets the DB engine know that while you want to know how many rows there are, you couldn't care less about the actual data. On most any intelligent DBMS, the engine can retrieve this count from the table's metadata, or a simple run through the table's primary key index, without ever touching the on-disk row data.</p>
"
"<p>I've found a few answers for this using mySQL alone, but I was hoping someone could show me a way to get the ID of the last inserted or updated row of a mysql DB when using PHP to handle the inserts/updates.</p>

<p>Currently I have something like this, where column3 is a unique key, and there's also an id column that's an autoincremented primary key:</p>

<pre><code>$query =""INSERT INTO TABLE (column1, column2, column3) VALUES (value1, value2, value3) ON DUPLICATE KEY UPDATE SET column1=value1, column2=value2, column3=value3"";
mysql_query($query);

$my_id = mysql_insert_id();
</code></pre>

<p>$my_id is correct on INSERT, but incorrect when it's updating a row (ON DUPLICATE KEY UPDATE).</p>

<p>I have seen several posts with people advising that you use something like</p>

<pre><code>INSERT INTO table (a) VALUES (0) ON DUPLICATE KEY UPDATE id=LAST_INSERT_ID(id) 
</code></pre>

<p>to get a valid ID value when the ON DUPLICATE KEY is invoked-- but will this return that valid ID to the PHP <code>mysql_insert_id()</code> function?</p>
","<p>Here's the answer, as suggested by Alexandre:</p>

<p>when you use the id=LAST_INSERT_ID(id) it sets the value of mysql_insert_id = the updated ID-- so your final code should look like:</p>

<pre><code>&lt;?
    $query = mysql_query(""
        INSERT INTO table (column1, column2, column3) 
        VALUES (value1, value2, value3) 
        ON DUPLICATE KEY UPDATE
            column1 = value1, 
            column2 = value2, 
            column3 = value3, 
            id=LAST_INSERT_ID(id)
    "");
    $my_id = mysql_insert_id();
</code></pre>

<p>This will return the right value for $my_id regardless of update or insert.</p>
"
"<p>I would like to send a large <code>pandas.DataFrame</code> to a remote server running MS SQL. The way I do it now is by converting a <code>data_frame</code> object to a list of tuples and then send it away with pyODBC's <code>executemany()</code> function. It goes something like this:</p>

<pre><code> import pyodbc as pdb

 list_of_tuples = convert_df(data_frame)

 connection = pdb.connect(cnxn_str)

 cursor = connection.cursor()
 cursor.fast_executemany = True
 cursor.executemany(sql_statement, list_of_tuples)
 connection.commit()

 cursor.close()
 connection.close()
</code></pre>

<p>I then started to wonder if things can be sped up (or at least more readable) by using <code>data_frame.to_sql()</code> method. I have came up with the following solution:</p>

<pre><code> import sqlalchemy as sa

 engine = sa.create_engine(""mssql+pyodbc:///?odbc_connect=%s"" % cnxn_str)
 data_frame.to_sql(table_name, engine, index=False)
</code></pre>

<p>Now the code is more readable, but the upload is <strong>at least 150 times slower</strong>...</p>

<p>Is there a way to flip the <code>fast_executemany</code> when using SQLAlchemy?</p>

<p>I am using pandas-0.20.3, pyODBC-4.0.21 and sqlalchemy-1.1.13.</p>
","<p><strong>EDIT (08/03/2019):</strong> Gord Thompson commented below with good news from the update logs of sqlalchemy: <em>Since SQLAlchemy 1.3.0, released 2019-03-04, sqlalchemy now supports <code>engine = create_engine(sqlalchemy_url, fast_executemany=True)</code> for the <code>mssql+pyodbc</code> dialect. I.e., it is no longer necessary to define a function and use <code>@event.listens_for(engine, 'before_cursor_execute')</code></em> Meaning the below function can be removed and only the flag needs to be set in the create_engine statement - and still retaining the speed-up.</p>

<p><strong>Original Post:</strong></p>

<p>Just made an account to post this. I wanted to comment beneath the above thread as it's a followup on the already provided answer. The solution above worked for me with the Version 17 SQL driver on a Microsft SQL storage writing from a Ubuntu based install.</p>

<p>The complete code I used to speed things up significantly (talking >100x speed-up) is below. This is a turn-key snippet provided that you alter the connection string with your relevant details. To the poster above, thank you very much for the solution as I was looking quite some time for this already.</p>

<pre><code>import pandas as pd
import numpy as np
import time
from sqlalchemy import create_engine, event
from urllib.parse import quote_plus


conn =  ""DRIVER={ODBC Driver 17 for SQL Server};SERVER=IP_ADDRESS;DATABASE=DataLake;UID=USER;PWD=PASS""
quoted = quote_plus(conn)
new_con = 'mssql+pyodbc:///?odbc_connect={}'.format(quoted)
engine = create_engine(new_con)


@event.listens_for(engine, 'before_cursor_execute')
def receive_before_cursor_execute(conn, cursor, statement, params, context, executemany):
    print(""FUNC call"")
    if executemany:
        cursor.fast_executemany = True


table_name = 'fast_executemany_test'
df = pd.DataFrame(np.random.random((10**4, 100)))


s = time.time()
df.to_sql(table_name, engine, if_exists = 'replace', chunksize = None)
print(time.time() - s)
</code></pre>

<p>Based on the comments below I wanted to take some time to explain some limitations about the pandas <code>to_sql</code> implementation and the way the query is handled. There are 2 things that might cause the <code>MemoryError</code> being raised afaik: </p>

<p>1) Assuming you're writing to a remote SQL storage. When you try to write a large pandas DataFrame with the <code>to_sql</code> method it converts the entire dataframe into a list of values. This transformation takes up way more RAM than the original DataFrame does (on top of it, as the old DataFrame still remains present in RAM). This list is provided to the final <code>executemany</code> call for your ODBC connector. I think the ODBC connector has some troubles handling such large queries. A way to solve this is to provide the <code>to_sql</code> method a chunksize argument (10**5 seems to be around optimal giving about 600 mbit/s (!) write speeds on a 2 CPU 7GB ram MSSQL Storage application from Azure - can't recommend Azure btw). So the first limitation, being the query size, can be circumvented by providing a <code>chunksize</code> argument. However, this won't enable you to write a dataframe the size of 10**7 or larger, (at least not on the VM I am working with which has ~55GB RAM), being issue nr 2.       </p>

<p>This can be circumvented by breaking up the DataFrame with <code>np.split</code> (being 10**6 size DataFrame chunks) These can be written away iteratively. I will try to make a pull request when I have a solution ready for the <code>to_sql</code> method in the core of pandas itself so you won't have to do this pre-breaking up every time. Anyhow I ended up writing a function similar (not turn-key) to the following:       </p>

<pre><code>import pandas as pd
import numpy as np

def write_df_to_sql(df, **kwargs):
    chunks = np.split(df, df.shape()[0] / 10**6)
    for chunk in chunks:
        chunk.to_sql(**kwargs)
    return True
</code></pre>

<p>A more complete example of the above snippet can be viewed here: <a href=""https://gitlab.com/timelord/timelord/blob/master/timelord/utils/connector.py"" rel=""nofollow noreferrer"">https://gitlab.com/timelord/timelord/blob/master/timelord/utils/connector.py</a></p>

<p>It's a class I wrote that incorporates the patch and eases some of the necessary overhead that comes with setting up connections with SQL. Still have to write some documentation. Also I was planning on contributing the patch to pandas itself but haven't found a nice way yet on how to do so.</p>

<p>I hope this helps.</p>
"
"<p>I'm using SQL Server Profiler to figure out what process are consuming SQL process and I found that the event class <code>Audit Logout</code> is causing a huge number of reads and consume cpu process.</p>

<p>Is it normal? Or do I have something wrong in the SQL Server configuration?</p>
","<p>The audit logout event aggregates a lot of its values like reads/writes, connection times, etc. from the time the connection was opened. </p>

<p>See <a href=""http://msdn.microsoft.com/en-us/library/ms175827.aspx"">http://msdn.microsoft.com/en-us/library/ms175827.aspx</a> - the definition for your specific question added here:</p>

<p><code>Reads</code> <code>Number of logical read I/Os issued by the user during the connection.</code></p>

<p>So basically, the number you are seeing is not for the audit event itself, it is for all actions done by the connection that is logging out.</p>
"
"<p>When I try to execute the following <code>SQL</code> in <code>MySQL</code>, I'm getting error:</p>

<p>SQL:</p>

<pre><code>        SQL = ""CREATE TABLE Ranges ("";
        SQL += ""ID varchar(20) NOT NULL, "";
        SQL += ""Descriptions longtext NULL, "";
        SQL += ""Version_Number int NULL, "";
        SQL += ""Row_Updated bigint NULL, "";
        SQL += ""Last_Updated datetime NULL, "";
        SQL += ""XML longtext NULL, "";
        SQL += ""PRIMARY KEY (ID)"";
        SQL += "") "" + ""TYPE = InnoDB"";
</code></pre>

<p>Error:</p>

<p><strong>You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ""TYPE = InnoDB""</strong></p>

<p>But if I remove <code>""TYPE = InnoDB""</code>, then the query works fine.</p>

<p>Previously the query worked fine, ie in <code>MySQL 5.0</code>. But when I upgraded to <code>MySQL 5.6</code>, I'm getting the above error.</p>

<p>Any Suggestions / Alternatives ... ??</p>
","<p>Use <code>ENGINE = Innodb</code> instead of <code>TYPE = InnoDB</code>. <code>TYPE</code> was removed in 5.1.</p>
"
"<p>I am getting following error, when I try to import MYSQL database:</p>

<pre><code>Error Code: 2013 - Lost connection to MySQL server during queryQuery:
Error Code: 2006 - MySQL server has gone away
</code></pre>

<p>Can someone let me know what is wrong?</p>
","<p>Investigation shows many solutions correctly talking about setting the max_allowed_packet and wait_timeout for mysql in my.cnf; small addendum that the default install of mysql on mac osx doesn't appear to include this file.  You may first need to create it at /etc/my.cnf (this is only an issue if you're using the default install of mysql instead of a mamp stack or similar)</p>

<p>contents of /etc/my.cnf that corrected this issue for me below:</p>

<pre><code>[mysqld]
max_allowed_packet= 64M
wait_timeout= 6000
</code></pre>
"
"<p>I have a SQL Server Database on Azure Cloud and I want to get a report server up and runnning using SSRS that would accesses the data on that database. </p>

<p>Does anyone have any experience with this scenario, or could provide me some guidance on how to go about this. </p>

<p>I'm reading about how SSRS would run on a Azure VM and that the SQL Server and it's respective Database(s) would be installed on that Azure VM. That's not my situation as I have existing SQL Database that exists outside of any VM I spin up.</p>

<p>Thanks,</p>
","<p>You will need to host SSRS either on an Azure VM or on premise. </p>

<p>In <a href=""https://web.archive.org/web/20140523110804/http://msdn.microsoft.com/en-us/library/azure/jj992719.aspx"">this link</a> deployment topologies for SSRS on Azure VM are discussed. </p>

<p>One strategy is to deploy SSRS to a VM and use Azure SQL Database as the data source. </p>

<p>Once you deploy the VM containing SSRS,  you can then connect SSRS to an Azure SQL DB. This artcle discusses <a href=""https://msdn.microsoft.com/en-us/library/ff519560.aspx"">connecting Azure SQL Database to SSRS</a>.</p>

<p>Hope this helps!</p>
"
"<p>I'm ussing the Appache Jackrabbit JCA 2.7.5, the problem is that files .docx and .xlsx is not indexed.</p>

<p>My steps : </p>

<ul>
<li>Deploy the <a href=""https://archive.apache.org/dist/jackrabbit/2.7.5/jackrabbit-jca-2.7.5.rar"">Jackrabbit JCA</a> as <code>resource adapter</code> on glassfish</li>
<li>create a <code>Connector Connection Pool</code> for the <code>resource adapter</code> indicating the <code>ConfigFile=path/to/the/repository.xml</code> and <code>HomeDir=path/to/the //miss the repository.xml</code></li>
<li>create a <code>Connector Resources</code> for the connector pool (the jndi)</li>
<li>create web application</li>
<li><p>create class to get session from the connector ressources (code below)</p>

<pre><code>import java.io.Serializable;
import java.net.MalformedURLException;
import javax.annotation.Resource;
import javax.ejb.Stateless;
import javax.jcr.LoginException;
import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.naming.InitialContext;
import javax.naming.NamingException;
@Stateless
public class OcmRepository implements Serializable {

    public Repository repository;
    public Session session;

    public OcmRepository() {
    }

    public Session getSession(String log, String mdp) throws LoginException, RepositoryException, NamingException, MalformedURLException {
        InitialContext initalContext = new InitialContext();
        repository = (Repository) initalContext.lookup(""jndi/jca"");
        session = repository.login(new SimpleCredentials(log, mdp.toCharArray()), null);
        return session;
    }
}
</code></pre></li>
<li><p>Create custom filetype</p>

<pre><code>import javax.jcr.PropertyType;
import javax.jcr.Session;
import javax.jcr.nodetype.NodeType;
import javax.jcr.nodetype.NodeTypeManager;
import javax.jcr.nodetype.NodeTypeTemplate;
import javax.jcr.nodetype.PropertyDefinitionTemplate;

/**
 *
 * @author nathan
 */
public class FileType {
    public static void RegisterFileType(Session session) throws Exception {        
        NodeTypeManager nodeTypeManager = session.getWorkspace().getNodeTypeManager();

        NodeTypeTemplate nodeType = nodeTypeManager.createNodeTypeTemplate();
        nodeType.setName(""FileType"");
        String[] str = {""nt:resource""};        
        nodeType.setDeclaredSuperTypeNames(str);
        nodeType.setMixin(false);
        nodeType.setQueryable(true);


        PropertyDefinitionTemplate path = nodeTypeManager.createPropertyDefinitionTemplate();
        path.setName(""jcr:path"");
        path.setRequiredType(PropertyType.PATH);
        path.setQueryOrderable(false);
        path.setFullTextSearchable(false);
        nodeType.getPropertyDefinitionTemplates().add(path);

        PropertyDefinitionTemplate nom = nodeTypeManager.createPropertyDefinitionTemplate();
        nom.setName(""jcr:nom"");
        nom.setRequiredType(PropertyType.STRING);
        nom.setQueryOrderable(true);
        nom.setFullTextSearchable(true);
        nodeType.getPropertyDefinitionTemplates().add(nom);

        PropertyDefinitionTemplate description = nodeTypeManager.createPropertyDefinitionTemplate();
        description.setName(""jcr:description"");
        description.setRequiredType(PropertyType.STRING);
        description.setQueryOrderable(true);
        description.setFullTextSearchable(true);
        nodeType.getPropertyDefinitionTemplates().add(description);

        PropertyDefinitionTemplate motsCles = nodeTypeManager.createPropertyDefinitionTemplate();
        motsCles.setName(""jcr:motsCles"");
        motsCles.setRequiredType(PropertyType.STRING);
        motsCles.setQueryOrderable(true);
        motsCles.setFullTextSearchable(true);
        nodeType.getPropertyDefinitionTemplates().add(motsCles);

        PropertyDefinitionTemplate size = nodeTypeManager.createPropertyDefinitionTemplate();
        size.setName(""jcr:size"");
        size.setRequiredType(PropertyType.STRING);
        size.setQueryOrderable(true);
        size.setFullTextSearchable(false);
        nodeType.getPropertyDefinitionTemplates().add(size);

        PropertyDefinitionTemplate users = nodeTypeManager.createPropertyDefinitionTemplate();
        users.setName(""jcr:users"");
        users.setRequiredType(PropertyType.STRING);
        users.setQueryOrderable(true);
        users.setFullTextSearchable(false);
        nodeType.getPropertyDefinitionTemplates().add(users);

        PropertyDefinitionTemplate groupe = nodeTypeManager.createPropertyDefinitionTemplate();
        groupe.setName(""jcr:groupe"");
        groupe.setRequiredType(PropertyType.STRING);
        groupe.setQueryOrderable(true);
        groupe.setFullTextSearchable(false);
        nodeType.getPropertyDefinitionTemplates().add(groupe);

        NodeType newnodetype = nodeTypeManager.registerNodeType(nodeType, true);             
        session.save();        
    }

}
</code></pre></li>
<li><p>Create the abstract class for persistence</p>

<pre><code>import java.util.ArrayList;
import java.util.List;
import java.util.Map;

import javax.jcr.Session;

import org.apache.jackrabbit.ocm.query.Filter;
import org.apache.jackrabbit.ocm.query.impl.FilterImpl;
import org.apache.jackrabbit.ocm.query.impl.QueryImpl;
import org.apache.jackrabbit.ocm.query.Query;
import org.apache.jackrabbit.ocm.query.QueryManager;

import org.apache.jackrabbit.ocm.manager.ObjectContentManager;
import org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl;

import org.apache.jackrabbit.ocm.mapper.Mapper;
import org.apache.jackrabbit.ocm.mapper.impl.annotation.AnnotationMapperImpl;

import org.apache.jackrabbit.ocm.reflection.ReflectionUtils;


/**
 *
 * @author nathan
 */
public abstract class AbstractBean&lt;T&gt; {

    private Class&lt;T&gt; entityClass;
    private ObjectContentManager ocm;
    private Mapper mapper;

    public AbstractBean(Class&lt;T&gt; entityClass){
        this.entityClass = entityClass;
    }

    /**
     * Construct the Bean according to the extended class
     * This will be also construct the ObjectContentManager nammed ocm with the default Mapper
     * @param session javax.jcr.Session attached to the Bean
     * @return The mapping class found for the desired java bean class
     */
    public AbstractBean(Class&lt;T&gt; entityClass,Session session){
        this.entityClass = entityClass;
        ocm = new ObjectContentManagerImpl(session, this.getDefaultMapper());
    }

    /**
     * @return ObjectContentManager of the Bean
     */
    public ObjectContentManager getOcm() throws Exception{
        return ocm;
    }

    /**
     * Construct the Bean according to the extended class
     * This will be also construct the ObjectContentManager nammed ocm with the param Mapper given
     * @param session from ""javax.jcr.Session"" attached to the Bean
     * @param map from ""org.apache.jackrabbit.ocm.mapper.Mapper"" which 
     * is the use to map entity between apllication and The repository
     * @return ObjectContentManager of the Bean
     */   
    public ObjectContentManager getOcm(Session session, Mapper map) throws Exception{
        return new ObjectContentManagerImpl(session, map);
    }

    public void setOcm(ObjectContentManager ocm) {
        this.ocm = ocm;
    }

    private Mapper getDefaultMapper(){
        ReflectionUtils.setClassLoader(com.ged.ocm.entity.Groupe.class.getClassLoader());
        List&lt;Class&gt; classes = new ArrayList&lt;Class&gt;();
        classes.add(com.ged.ocm.entity.Fichier.class);
        classes.add(com.ged.ocm.entity.Dossier.class);
        classes.add(com.ged.ocm.entity.Groupe.class);
        classes.add(com.ged.ocm.entity.SimpleNode.class);
        return new AnnotationMapperImpl(classes);
    }

    public Mapper getMapper() {
        return mapper;
    }

    public void setMapper(Mapper mapper) {
        this.mapper = mapper;
    }

    public void setLoader(Class classe){        
        ReflectionUtils.setClassLoader(classe.getClassLoader());
    }

    public void create(T entity) {
        ocm.insert(entity);
        ocm.save();
    }

    public void edit(T entity) {
        ocm.update(entity);
        ocm.save();
    }

    public void remove(T entity) {
        ocm.remove(entity);
        ocm.save();
    }

    public void refresh(){
        ocm.refresh(true);
        ocm.save();
    }

    public void copy(String orgPath, String destPath){
        ocm.copy(orgPath, destPath);
        ocm.save();
    }

    public void move(String orgPath, String destPath){
        ocm.move(orgPath, destPath);
        ocm.save();
    }
    public void removeByPath(String path) {
        ocm.remove(path);
        ocm.save();
    }

    public void removeAllByEqual(Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        ocm.remove(query);
        ocm.save();
    }

    public void removeAllByEqual(String nodePath,Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        ocm.remove(query);
        ocm.save();
    }

    public boolean isPathExist(String path){
        return ocm.objectExists(path);
    }

    public T findByPath(String path) {
        try {            
            return (T)ocm.getObject(path);
        } catch (Exception e) {
            return null;
        }
    }

    public T findOneByEqual(Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);

        T result = null;
        try {            
            result = results.get(0);
        } catch (Exception e) {
        }

        return result;
    }

    public List&lt;T&gt; findAllByEqual(Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(""//"");
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;
    }


    public List&lt;T&gt; findAllByLike(Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(""//"");
        for (String key : filters.keySet())filter.addLike(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;
    }

    public List&lt;T&gt; findAllByLikeScoped(String scope,Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(scope);
        for (String key : filters.keySet())filter.addLike(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;
    }

    public List&lt;T&gt; findAllByOrLike(String attr,String[] val){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(""//"");
        filter.addOrFilter(attr, val);

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;
    }

    public T findOneByEqual(String nodePath, Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        T result = results.get(0);
        return result;
    }

    public List&lt;T&gt; findAllByEqual(String nodePath, Map&lt;String,String&gt; filters){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);
        for (String key : filters.keySet())filter.addEqualTo(key, filters.get(key));

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;
    }

    public List&lt;T&gt; findAllByString(String query){        
        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query,javax.jcr.query.Query.JCR_SQL2);
        return results;
    } 


    public List&lt;T&gt; findAllByParentPath(String nodePath){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);

        Query query = queryManager.createQuery(filter);
        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;

    }
    public List&lt;T&gt; findAllByParentPathOrder(String nodePath, String ordering){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);

        Query query = queryManager.createQuery(filter);
//        query.addOrderByDescending(ordering);
        query.addOrderByAscending(ordering);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results;

    }

    public int coutChild(String nodePath){
        QueryManager queryManager = ocm.getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(entityClass);
        filter.setScope(nodePath);

        Query query = queryManager.createQuery(filter);

        List&lt;T&gt; results = (List&lt;T&gt;) ocm.getObjects(query);
        return results.size();
    }

    public boolean ifExistByPath(String path){
        return ocm.objectExists(path);
    }

    public String getParentPath(String path){
        String parent="""";
        String[] tmp=path.split(""/"");
        for (int i = 1; i &lt; (tmp.length-1); i++) {
            parent+=""/""+tmp[i];
        }
        return parent;                
    }
}
</code></pre></li>
<li><p>Create the bean</p>

<pre><code>import javax.ejb.Stateless;
import com.ged.ocm.entity.Fichier;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import javax.jcr.Node;
import javax.jcr.NodeIterator;
import javax.jcr.Session;
import javax.jcr.Workspace;
import javax.jcr.query.QueryResult;
import javax.jcr.query.qom.FullTextSearch;
import javax.jcr.query.qom.StaticOperand;
import org.apache.jackrabbit.ocm.query.Filter;
import org.apache.jackrabbit.ocm.query.Query;
import org.apache.jackrabbit.ocm.query.QueryManager;

@Stateless
public class FichierBean extends AbstractBean&lt;Fichier&gt;{    
    public FichierBean() {
        super(Fichier.class);
    }
    public FichierBean(Session session) {
        super(Fichier.class,session);
    }

    public List&lt;Fichier&gt; findAllByContains(String motCles) throws Exception {
        String requette = ""SELECT * FROM FileType AS Res WHERE CONTAINS (Res.*, '*""+motCles+""*')"";
        List&lt;Fichier&gt; results = (List&lt;Fichier&gt;) this.getOcm().getObjects(requette, javax.jcr.query.Query.JCR_SQL2);
        return results;                
    }
    public List&lt;Fichier&gt; findAllByContains(String path,String motCles) throws Exception {
        String requette = ""SELECT * FROM FileType AS Res WHERE CONTAINS (Res.*, '*""+motCles+""*') ORDER BY Res.nom"";
        List&lt;Fichier&gt; tmp = (List&lt;Fichier&gt;) this.getOcm().getObjects(requette, javax.jcr.query.Query.JCR_SQL2);

        List&lt;Fichier&gt; results = new ArrayList&lt;Fichier&gt;();
        for (Fichier fichier : tmp) {
            if(fichier.getPath().startsWith(path))results.add(fichier);
        }
        return results;                
    }


    public List&lt;Fichier&gt; fulltextByOCM(String motCles) throws Exception {
        QueryManager queryManager = this.getOcm().getQueryManager();

        Filter filter;
        filter = queryManager.createFilter(com.ged.ocm.entity.Fichier.class);
        filter.addContains(""."", ""*""+motCles+""*"");

        Query query = queryManager.createQuery(filter);

        List&lt;Fichier&gt; results = (List&lt;Fichier&gt;) this.getOcm().getObjects(query);
        return results;
    }

}
</code></pre></li>
</ul>

<p>My configuration files :</p>

<ul>
<li><p>repository.xml</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;!DOCTYPE Repository PUBLIC ""-//The Apache Software Foundation//DTD Jackrabbit 1.6//EN""
                        ""http://jackrabbit.apache.org/dtd/repository-1.6.dtd""&gt;
&lt;Repository&gt;        
&lt;FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem""&gt;
    &lt;param name=""path"" value=""${rep.home}/repository""/&gt;
&lt;/FileSystem&gt;
--&gt;

&lt;FileSystem class=""org.apache.jackrabbit.core.fs.db.DbFileSystem""&gt;
    &lt;param name=""driver"" value=""com.mysql.jdbc.jdbc2.optional.MysqlDataSource""/&gt;
    &lt;param name=""url"" value=""jdbc:mysql://:3306/db_ged_mysql"" /&gt;
    &lt;param name=""user"" value=""root"" /&gt;
    &lt;param name=""password"" value=""root"" /&gt;
    &lt;param name=""schema"" value=""mysql""/&gt;
    &lt;param name=""schemaObjectPrefix"" value=""J_R_FS_""/&gt;
&lt;/FileSystem&gt;

&lt;!--
    security configuration
--&gt;
&lt;Security appName=""Jackrabbit""&gt;
    &lt;AccessManager class=""org.apache.jackrabbit.core.security.SimpleAccessManager"" /&gt;
    &lt;LoginModule class=""org.apache.jackrabbit.core.security.SimpleLoginModule""&gt;
        &lt;param name=""anonymousId"" value=""anonymous"" /&gt;
    &lt;/LoginModule&gt;
&lt;/Security&gt;

&lt;!--
    location of workspaces root directory and name of default workspace
--&gt;
&lt;Workspaces rootPath=""${rep.home}/workspaces"" defaultWorkspace=""default""/&gt;
&lt;!--
    workspace configuration template:
    used to create the initial workspace if there's no workspace yet
--&gt;
&lt;Workspace name=""${wsp.name}""&gt;

    &lt;PersistenceManager class=""org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager""&gt;
        &lt;param name=""driver"" value=""com.mysql.jdbc.jdbc2.optional.MysqlDataSource""/&gt;
        &lt;param name=""url"" value=""jdbc:mysql://:3306/db_ged_mysql"" /&gt;
        &lt;param name=""user"" value=""root"" /&gt;
        &lt;param name=""password"" value=""root"" /&gt;
        &lt;param name=""schema"" value=""mysql"" /&gt;
        &lt;param name=""schemaObjectPrefix"" value=""J_PM_${wsp.name}_"" /&gt;
        &lt;param name=""externalBLOBs"" value=""false"" /&gt;
    &lt;/PersistenceManager&gt;
    &lt;FileSystem class=""org.apache.jackrabbit.core.fs.db.DbFileSystem""&gt;
        &lt;param name=""driver"" value=""com.mysql.jdbc.jdbc2.optional.MysqlDataSource""/&gt;
        &lt;param name=""url"" value=""jdbc:mysql://:3306/db_ged_mysql"" /&gt;
        &lt;param name=""user"" value=""root"" /&gt;
        &lt;param name=""password"" value=""root"" /&gt;
        &lt;param name=""schema"" value=""mysql""/&gt;
        &lt;param name=""schemaObjectPrefix"" value=""J_FS_${wsp.name}_""/&gt;
    &lt;/FileSystem&gt;

    &lt;!--
        Search index and the file system it uses.
        class: FQN of class implementing the QueryHandler interface
    --&gt;
    &lt;SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex""&gt;
        &lt;param name=""path"" value=""${rep.home}/workspaces/${wsp.name}/index""/&gt;
        &lt;param name=""tikaConfigPath"" value=""${rep.home}/tika-config.xml""/&gt;
        &lt;param name=""useCompoundFile"" value=""true""/&gt;
        &lt;param name=""minMergeDocs"" value=""100""/&gt;
        &lt;param name=""volatileIdleTime"" value=""3""/&gt;
        &lt;param name=""maxMergeDocs"" value=""2147483647""/&gt;
        &lt;param name=""mergeFactor"" value=""10""/&gt;
        &lt;param name=""maxFieldLength"" value=""10000""/&gt;
        &lt;param name=""bufferSize"" value=""10""/&gt;
        &lt;param name=""cacheSize"" value=""1000""/&gt;
        &lt;param name=""forceConsistencyCheck"" value=""false""/&gt;
        &lt;param name=""enableConsistencyCheck"" value=""false""/&gt;
        &lt;param name=""autoRepair"" value=""true""/&gt;
        &lt;param name=""analyzer"" value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/&gt;
        &lt;param name=""queryClass"" value=""org.apache.jackrabbit.core.query.QueryImpl""/&gt;
        &lt;param name=""respectDocumentOrder"" value=""true""/&gt;
        &lt;param name=""resultFetchSize"" value=""2147483647""/&gt;
        &lt;param name=""extractorPoolSize"" value=""0""/&gt;
        &lt;param name=""extractorTimeout"" value=""100""/&gt;
        &lt;param name=""extractorBackLogSize"" value=""100""/&gt;
        &lt;param name=""supportHighlighting"" value=""true""/&gt;
        &lt;param name=""excerptProviderClass"" value=""org.apache.jackrabbit.core.query.lucene.DefaultXMLExcerpt""/&gt;
    &lt;/SearchIndex&gt;
&lt;/Workspace&gt;

&lt;!--
    Configures the versioning
--&gt;
&lt;Versioning rootPath=""${rep.home}/version""&gt;
    &lt;FileSystem class=""org.apache.jackrabbit.core.fs.db.DbFileSystem""&gt;
        &lt;param name=""driver"" value=""com.mysql.jdbc.jdbc2.optional.MysqlDataSource""/&gt;
        &lt;param name=""url"" value=""jdbc:mysql://:3306/db_ged_mysql"" /&gt;
        &lt;param name=""user"" value=""root"" /&gt;
        &lt;param name=""password"" value=""root"" /&gt;
        &lt;param name=""schema"" value=""mysql""/&gt;
        &lt;param name=""schemaObjectPrefix"" value=""J_V_FS_""/&gt;
    &lt;/FileSystem&gt;
    &lt;PersistenceManager class=""org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager""&gt;
        &lt;param name=""driver"" value=""com.mysql.jdbc.jdbc2.optional.MysqlDataSource""/&gt;
        &lt;param name=""url"" value=""jdbc:mysql://:3306/db_ged_mysql"" /&gt;
        &lt;param name=""user"" value=""root"" /&gt;
        &lt;param name=""password"" value=""root"" /&gt;
        &lt;param name=""schema"" value=""mysql"" /&gt;
        &lt;param name=""schemaObjectPrefix"" value=""J_V_PM_"" /&gt;
        &lt;param name=""externalBLOBs"" value=""false"" /&gt;
    &lt;/PersistenceManager&gt;
&lt;/Versioning&gt;

&lt;!--
    Search index for content that is shared repository wide
    (/jcr:system tree, contains mainly versions)

&lt;SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex""&gt;
    &lt;param name=""path"" value=""${rep.home}/repository/index""/&gt;
    &lt;param name=""extractorPoolSize"" value=""2""/&gt;
    &lt;param name=""supportHighlighting"" value=""true""/&gt;
&lt;/SearchIndex&gt;
--&gt;

&lt;!--
    Cluster configuration with system variables.

--&gt;

&lt;RepositoryLockMechanism class=""org.apache.jackrabbit.core.util.CooperativeFileLock"" /&gt;

&lt;/Repository&gt;
</code></pre></li>
<li><p>tika-config.xml</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;properties&gt;

&lt;mimeTypeRepository resource=""/org/apache/tika/mime/tika-mimetypes.xml"" magic=""false""/&gt;

&lt;parsers&gt;

&lt;parser name=""parse-dcxml"" class=""org.apache.tika.parser.xml.DcXMLParser""&gt;
  &lt;mime&gt;application/xml&lt;/mime&gt;
  &lt;mime&gt;image/svg+xml&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-office"" class=""org.apache.tika.parser.microsoft.OfficeParser""&gt;
  &lt;mime&gt;application/x-tika-msoffice&lt;/mime&gt;
  &lt;mime&gt;application/msword&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-excel&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-excel.sheet.binary.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-powerpoint&lt;/mime&gt;
  &lt;mime&gt;application/vnd.visio&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-outlook&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-ooxml"" class=""org.apache.tika.parser.microsoft.ooxml.OOXMLParser""&gt;
  &lt;mime&gt;application/x-tika-ooxml&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-package.core-properties+xml&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.spreadsheetml.template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-excel.sheet.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-excel.template.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-excel.addin.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.presentationml.presentation&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.presentationml.template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.presentationml.slideshow&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-powerpoint.presentation.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-powerpoint.slideshow.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-powerpoint.addin.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.wordprocessingml.document&lt;/mime&gt;
  &lt;mime&gt;application/vnd.openxmlformats-officedocument.wordprocessingml.template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-word.document.macroenabled.12&lt;/mime&gt;
  &lt;mime&gt;application/vnd.ms-word.template.macroenabled.12&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-html"" class=""org.apache.tika.parser.html.HtmlParser""&gt;
  &lt;mime&gt;text/html&lt;/mime&gt;
  &lt;mime&gt;application/xhtml+xml&lt;/mime&gt;
  &lt;mime&gt;application/x-asp&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser mame=""parse-rtf"" class=""org.apache.tika.parser.rtf.RTFParser""&gt;
  &lt;mime&gt;application/rtf&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-pdf"" class=""org.apache.tika.parser.pdf.PDFParser""&gt;
  &lt;mime&gt;application/pdf&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-txt"" class=""org.apache.tika.parser.txt.TXTParser""&gt;
  &lt;mime&gt;text/plain&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-openoffice"" class=""org.apache.tika.parser.opendocument.OpenOfficeParser""&gt;
  &lt;mime&gt;application/vnd.sun.xml.writer&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.text&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.graphics&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.presentation&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.spreadsheet&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.chart&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.image&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.formula&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.text-master&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.text-web&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.text-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.graphics-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.presentation-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.spreadsheet-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.chart-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.image-template&lt;/mime&gt;
  &lt;mime&gt;application/vnd.oasis.opendocument.formula-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.text&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.graphics&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.presentation&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.spreadsheet&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.chart&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.image&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.formula&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.text-master&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.text-web&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.text-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.graphics-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.presentation-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.spreadsheet-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.chart-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.image-template&lt;/mime&gt;
  &lt;mime&gt;application/x-vnd.oasis.opendocument.formula-template&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-image"" class=""org.apache.tika.parser.image.ImageParser""&gt;
  &lt;mime&gt;image/bmp&lt;/mime&gt;
  &lt;mime&gt;image/gif&lt;/mime&gt;
  &lt;mime&gt;image/jpeg&lt;/mime&gt;
  &lt;mime&gt;image/png&lt;/mime&gt;
  &lt;mime&gt;image/tiff&lt;/mime&gt;
  &lt;mime&gt;image/vnd.wap.wbmp&lt;/mime&gt;
  &lt;mime&gt;image/x-icon&lt;/mime&gt;
  &lt;mime&gt;image/x-psd&lt;/mime&gt;
  &lt;mime&gt;image/x-xcf&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-class"" class=""org.apache.tika.parser.asm.ClassParser""&gt;
  &lt;mime&gt;application/x-tika-java-class&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-mp3"" class=""org.apache.tika.parser.mp3.Mp3Parser""&gt;
  &lt;mime&gt;audio/mpeg&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-midi"" class=""org.apache.tika.parser.audio.MidiParser""&gt;
  &lt;mime&gt;application/x-midi&lt;/mime&gt;
  &lt;mime&gt;audio/midi&lt;/mime&gt;
&lt;/parser&gt;

&lt;parser name=""parse-audio"" class=""org.apache.tika.parser.audio.AudioParser""&gt;
  &lt;mime&gt;audio/basic&lt;/mime&gt;
  &lt;mime&gt;audio/x-wav&lt;/mime&gt;
  &lt;mime&gt;audio/x-aiff&lt;/mime&gt;
&lt;/parser&gt;

&lt;/parsers&gt;

&lt;/properties&gt;
</code></pre></li>
</ul>

<p>All query from the bean work except when I call the function <code>public List&lt;Fichier&gt; findAllByContains(String path,String motCles)</code> to fulltext search into .docx and .xslx document. Fulltext search on .pdf, .txt, .xml, .xls, .doc, ... work perfectly.</p>
","<p>You can treat the criteria on multi-valued properties just like other criteria. For example, the following query will find all nodes that have a value of 'white dog' on the 'someProp' property:</p>

<pre><code>SELECT * FROM [nt:unstructured] WHERE someProp = 'white dog'
</code></pre>

<p>If the 'someProp' property has multiple values, then a node with at least one value that satisfies the criteria will be included in the results.</p>

<p>To find nodes that have multiple values of a multi-valued property, simply AND together multiple criteria. For example, the following query will return all nodes that have both of the specified values:</p>

<pre><code>SELECT * FROM [nt:unstructured] WHERE someProp = 'white dog' 
                                  AND someProp = 'black dog'
</code></pre>

<p>Any of the operators will work, including 'LIKE':</p>

<pre><code>SELECT * FROM [nt:unstructured] WHERE someProp LIKE '%white%'  
                                  AND someProp LIKE '%black%'
</code></pre>

<p>Other combinations are possible, of course.</p>
"
"<p>Ive actually had this problem for a while but I've finally decided to take it on. Postgres was initially installed using Brew. </p>

<p>After my upgrade to OSX 10.8.2 to receive a</p>

<pre><code>psql: could not connect to server: No such file or directory
    Is the server running locally and accepting
    connections on Unix domain socket ""/tmp/.s.PGSQL.5432""?
</code></pre>

<p>error when I typed the </p>

<pre><code>$ psql
</code></pre>

<p>command. </p>

<p>I see the following processes:</p>

<pre><code>$ ps auxw | grep post
Tulsa           59222   0.0  0.2  2483256  14808   ??  Ss   Thu03PM   0:02.61 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid 470DA5CC-1602-4D69-855F-F365A6512F90 -post-exec 4
Tulsa           57940   0.0  0.2  2498852  13648   ??  Ss   Wed10PM   0:00.61 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid FAFAAAB4-0F67-42C8-864E-EF8C31A42EE3 -post-exec 4
root            24447   0.0  0.1  2476468  10080   ??  Ss    8Feb13   0:03.40 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid CC768720-12C2-436C-9020-548C275A6B0C -post-exec 4
Tulsa           74224   0.0  0.0  2432768    596 s002  R+    7:24PM   0:00.00 grep post


$ which psql
/usr/local/bin/psql

$ pg_ctl start 
pg_ctl: no database directory specified and environment variable PGDATA unset
Try ""pg_ctl --help"" for more information. 
</code></pre>
","<p>You haven't started the Postgres server. Some of the dmg packages for Postgres set it to run as a service on startup. But not however you did the install.</p>

<p>You need to init a data directory, start postgres, and then go from there.</p>

<pre><code>initdb /some/directory # just do this ONCE
pg_ctl -D /some/directory start # many other options, e.g. logging, available here
psql postgres
</code></pre>

<p>You can set an environment variable for the data directory and you won't need the <code>-D</code> flag later. You can look that up later.</p>
"
"<p>I'm an R user, and I frequently find that I need to write functions that require subsetting large datasets (10s of millions of rows).  When I apply such functions over a large number of observations, it can get very time consuming if I'm not careful about how I implement it.</p>

<p>To do this, I have sometimes used the data.table package, and this provides much faster speeds than subsetting using data frames.  Recently, I've started experimenting with packages like RMySQL, pushing some tables to mysql, and using the package to run sql queries and return results.</p>

<p>I have found mixed performance improvements.  For smaller datasets (millions), it seems that loading up the data into a data.table and setting the right keys makes for faster subsetting.  For larger datasets (10s to 100s of millions), it appears the sending out a query to mysql moves faster.  </p>

<p>Was wondering if anyone has any insight into which technique should return simple subsetting or aggregation queries faster, and whether or not this should depend on the size of the data?  I understand that setting keys in data.table is somewhat analogous to creating an index, but I don't have much more intuition beyond that.</p>
","<p>Maybe you open a new DB connection with <code>obj &lt;- dbConnect(...)</code> every time you send a query in your code. You can simply call <code>dbDisconnect(obj)</code> on the object you created to kill the respective connection everytime after your query executed.</p>

<p>Also you can use this function kill all open connections at once:</p>

<pre><code>library(RMySQL)  

killDbConnections &lt;- function () {

  all_cons &lt;- dbListConnections(MySQL())

  print(all_cons)

  for(con in all_cons)
    +  dbDisconnect(con)

  print(paste(length(all_cons), "" connections killed.""))

}
</code></pre>

<p>I'd recommed to write a small function outside shiny that handles the whole opening and closing thing:</p>

<pre><code>library(RMySQL)

sqlQuery &lt;- function (query) {

  # creating DB connection object with RMysql package
  DB &lt;- dbConnect(MySQL(), user=""youruser"", password='yourpassword', dbname='yourdb', host='192.168.178.1')

  # close db connection after function call exits
  on.exit(dbDisconnect(DB))

  # send Query to btain result set
  rs &lt;- dbSendQuery(DB, query)

  # get elements from result sets and convert to dataframe
  result &lt;- fetch(rs, -1)

  # return the dataframe
  return(result)
}
</code></pre>

<p>Hope that helps!</p>
"
"<p>Our database seems to be broken, normally it uses about 1-2% of cpu, but if we run some additional backend services making UPDATE and INSERT queries for 10M rows table (about 1 query per 3 second) everything is going to hell (including CPU increase from 2% to 98% usage).</p>

<p>We have decided to debug what's going on, run VACUUM and ANALYZE to learn what's wrong with db but...</p>

<pre><code>production=# ANALYZE VERBOSE users_user;
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
INFO:  analyzing ""public.users_user""
INFO:  ""users_user"": scanned 280 of 280 pages, containing 23889 live rows and 57 dead rows; 23889 rows in sample, 23889 estimated total rows
ERROR:  tuple already updated by self
</code></pre>

<p>we are not able to finish ANALYZE on ANY of the tables and could not find any information about this issue. Any suggestions what can be wrong?</p>

<pre><code> PostgreSQL 9.6.8 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-16), 64-bit
</code></pre>

<p><strong>Additional info as requested in comments:</strong></p>

<blockquote>
  <p>Maybe you have a corrupted pg_class</p>
</blockquote>

<pre><code>SELECT * FROM pg_class WHERE relname = 'users_user';
</code></pre>

<p>Output: <a href=""https://pastebin.com/WhmkH34U"" rel=""noreferrer"">https://pastebin.com/WhmkH34U</a></p>

<blockquote>
  <p>So the first thing to do would be to kick out all other sessions and
  try again</p>
</blockquote>

<p>There are no additional sessions, we have dumped the whole DB on the new testing server, issue still occur, there are no clients connected to this DB</p>
","<p><code>query.on</code> has been removed from node-pg 7.</p>

<p>See <a href=""https://node-postgres.com/guides/upgrading"" rel=""noreferrer"">https://node-postgres.com/guides/upgrading</a> for how to properly handle rows.</p>

<p>The usual way is to use promises or async/await (using promises in a clearer way):</p>

<pre><code>await client.connect();
var res = await client.query(""SELECT * FROM json_test"");
res.rows.forEach(row=&gt;{
    console.log(row);
});
await client.end();
</code></pre>
"
"<p>I have a certain table in mySQL which has a field called ""image"" with a datatype of ""BLOB"". I was wondering if it is possible to upload an image in that field directly from the Command Line Client rather than doing it through php...If it is possible, then where exactly should I place my image files?</p>
","<p>Please try below code</p>

<pre><code>INSERT INTO xx_BLOB(ID,IMAGE) VALUES(1,LOAD_FILE('E:/Images/jack.jpg'));
</code></pre>
"
"<p>I have a ASP.NET MVC workflow configured as two websites managed by a load balancer. The websites use Sql Server as the session state provider and have authentication switch off (its not required). </p>

<p>Now, sporadically I appear to be losing session state, and I believe this is because the request is being handled by the alternative server, so essentially the user is jumping from server to server, depending how the load balancer sees fit. I am not always ""losing session state"" at the same stage in the workflow, so I believe it is something related to the web farm configuration + sql server session state.</p>

<p>Both applications use the same machine key to encrypt and decrypt session state stored in sql server.</p>

<p>The configuration on both servers is as follows:</p>

<pre><code>&lt;authentication mode=""None"" /&gt;
&lt;sessionState mode=""SQLServer"" sqlConnectionString=""{connection-string}"" /&gt;
&lt;machineKey decryptionKey=""777CB456774AF02F7F1AC8570FAF31545B156354D9E2DAAD"" 
            validationKey=""89B5B536D5D17B8FE6A53CBB3CA8B8695289BA3DF0B1370BC47D362D375CF91525DDB5307D8A288230DCD4B3931D23AED4E223955C45CFF2AF66BCC422EC7ECD"" /&gt;
</code></pre>

<p>I've confirmed that this is identical on both servers, is there something I am missing? </p>

<p>This does not occur in my development environment when I am using a single server.</p>

<p>I fear I am suffering from the Friday blues, and no doubt will figure out the answer next week, sadly I don't want to wait!</p>

<p>Any ideas?</p>
","<p>Found the issue.  </p>

<p>When you create applications that you expect to share session state using Sql Server, they need the same ID configured in IIS. This is because the session ID that is generated is generated based on the application ID. (Internally the application ID is something like <em>LM/W3SVC/1</em></p>

<p>The two servers had different IDs for each application in IIS. The resolution is to change the ID under `Manage Website -> Advanced Settings' on each server.</p>

<p><img src=""https://i.stack.imgur.com/KsaLW.png"" alt=""enter image description here""></p>
"
"<p>I've made a web application using SQLite (2.8.17), I've only now discovered that there's an SQLite3. It somehow eluded my attention when making the web application, probably due to the lack of documentation for the php functions.</p>

<p>I'm wondering, what are the benefits of using SQLite3 over SQLite? Is it considerably faster?</p>
","<p>SQLite2 internally stores every value as a string, regardless of its type. 
<br>Upgrading to SQLite3 will certainly shrink the database size since numbers and BLOBS get stored in their native formats, which could make things run faster. <br>Another big advantage in my opinion is that recent versions of sqlite, (starting from 3.6.23) support foreign keys.
<p>
Since you were using PHP, I would suggest that you look into <a href=""http://www.php.net/manual/en/book.pdo.php"" rel=""noreferrer"">PDO</a>. It could prove helpful in case you need to change the DBMS for the application</p>
"
"<p>I have a database that is updated with datasets from time to time. Here it may happen that a dataset is delivered that already exists in database.</p>

<p>Currently I'm first doing a</p>

<pre><code>SELECT FROM ... WHERE val1=... AND val2=...
</code></pre>

<p>to check, if a dataset with these data already exists (using the data in WHERE-statement). If this does not return any value, I'm doing my INSERT.</p>

<p>But this seems to be a bit complicated for me. So my question: is there some kind of conditional INSERT that adds a new dataset only in case it does not exist?</p>

<p>I'm using <strong><a href=""http://www.smallsql.de/"">SmallSQL</a></strong></p>
","<p>You can do that with a single statement and a subquery in nearly all relational databases.</p>

<pre><code>INSERT INTO targetTable(field1) 
SELECT field1
FROM myTable
WHERE NOT(field1 IN (SELECT field1 FROM targetTable))
</code></pre>

<p>Certain relational databases have improved syntax for the above, since what you describe is a fairly common task.  SQL Server has a <code>MERGE</code> syntax with all kinds of options, and MySQL has optional <code>INSERT OR IGNORE</code> syntax.</p>

<p><strong>Edit:</strong> <a href=""http://www.smallsql.de/doc/sqlsyntax.html"" rel=""noreferrer"">SmallSQL's documentation</a> is fairly sparse as to which parts of the SQL standard it implements.  It may not implement subqueries, and as such you may be unable to follow the advice above, or anywhere else, if you need to stick with SmallSQL.  </p>
"
"<p>Why I get #1060 - Duplicate column name 'id'</p>

<pre><code>SELECT COUNT(*) FROM (SELECT * FROM `tips` `t` LEFT JOIN
tip_usage ON tip_usage.tip_id=t.id GROUP BY t.id) sq
</code></pre>
","<p>Probably because the * in <code>select *</code> selects two columns with the same name from <code>tip_usage</code> and <code>tips</code>.</p>
"
"<p>I have one entity class as </p>

<pre><code>  public class someclass
  {
      public string property1 {get; set;}
      public string property2 {get; set;}
      public string property3 {get; set;}
  }
</code></pre>

<p>and using sqlite connection class obj DB I am creating the table</p>

<pre><code>  Db.CreateTableAsync&lt;someclass&gt;().GetAwaiter().GetResult();
</code></pre>

<p>What I want to achieve is, I don't want sqlite to create column in the table for <code>property3</code>. Is there any way to achieve this?</p>

<p>I am using SQLiteAsync library for windows store apps.</p>
","<p>You can use the <code>Ignore</code> attribute:</p>

<pre><code>public class someclass
{
    public string property1 { get; set; }
    public string property2 { get; set; }
    [Ignore]
    public string property3 { get; set; }
}
</code></pre>
"
"<p>I've been using SQLite.swift lately to build my app database.
And I'm defining all my <code>INTEGER</code> columns with a <code>Int64</code> type, like the documentation explains.</p>

<p>But every once in a while I need that <code>Int64</code> to be just <code>Int</code>.
So my question is, if I do this:</p>

<pre><code>//Create a table with Int instead of Int64
let test_id = Expression&lt;Int&gt;(""test_id"")
let tests = db[""tests""]

db.create(table: tests, ifNotExists: true){ t in
    t.column(test_id)
}


class func insertTest(t: Int) -&gt; Int{
    //insert.rowid returns an Int64 type
    let insert = tests.insert(test_id &lt;- t)
    if let rowid = insert.rowid{
        //directly cast Int64 into Int
        return Int(rowid)
    }
    return 0
}
</code></pre>

<p>Will it be correct?</p>

<p>Of course I tested it. And it does works, but I was reading <a href=""https://stackoverflow.com/questions/29055820/using-variables-in-filters-in-sqlite-swift/29064444#29064444"">this question in Stackoverflow</a> </p>

<p>And it seems that I could have a problem with 32 bits devices...</p>

<p>If this is wrong, how can I cast <code>Int64</code> into <code>Int</code>?</p>
","<p>Converting an <code>Int64</code> to <code>Int</code> by passing the <code>Int64</code> value to the <code>Int</code> initializer will always work on a 64-bit machine, and it will crash on a 32-bit machine if the integer is outside of the range <code>Int32.min ... Int32.max</code>.</p>

<p>For safety use the <code>init(truncatingIfNeeded:)</code> initializer (formerly known as <code>init(truncatingBitPattern:)</code> in earlier Swift versions) to convert the value:</p>

<pre><code>return Int(truncatingIfNeeded: rowid)
</code></pre>

<p>On a 64-bit machine, the <code>truncatingIfNeeded</code> will do nothing; you will just get an <code>Int</code> (which is the same size as an <code>Int64</code> anyway).</p>

<p>On a 32-bit machine, this will throw away the top 32 bits, but it they are all zeroes, then you haven't lost any data.  So as long as your value will fit into a 32-bit <code>Int</code>, you can do this without losing data.  If your value is outside of the range <code>Int32.min ... Int32.max</code>, this will change the value of the <code>Int64</code> into something that fits in a 32-bit <code>Int</code>, but it will not crash.</p>

<hr>

<p>You can see how this works in a Playground.  Since <code>Int</code> in a Playground is a 64-bit <code>Int</code>, you can explicitly use an <code>Int32</code> to simulate the behavior of a 32-bit system.</p>

<pre><code>let i: Int64 = 12345678901  // value bigger than maximum 32-bit Int

let j = Int32(truncatingIfNeeded: i)  // j = -539,222,987
let k = Int32(i)                        // crash!
</code></pre>

<hr>

<p><strong>Update for Swift 3/4</strong></p>

<p>In addition to <code>init(truncatingIfNeeded:)</code> which still works, Swift 3 introduces failable initializers to safely convert one integer type to another.  By using <code>init?(exactly:)</code> you can pass one type to initialize another, and it returns <code>nil</code> if the initialization fails.  The value returned is an optional which must be unwrapped in the usual ways.</p>

<p>For example:</p>

<pre><code>let i: Int64 = 12345678901

if let j = Int32(exactly: i) {
    print(""\(j) fits into an Int32"")
} else {
    // the initialization returned nil
    print(""\(i) is too large for Int32"")
}
</code></pre>

<p>This allows you to apply the <em>nil coalescing operator</em> to supply a default value if the conversion fails:</p>

<pre><code>// return 0 if rowid is too big to fit into an Int on this device
return Int(exactly: rowid) ?? 0
</code></pre>
"
"<p><strong>When using the Entity Framework, does ESQL perform better than Linq to Entities?</strong> </p>

<p>I'd prefer to use Linq to Entities (mainly because of the strong-type checking), but some of my other team members are citing performance as a reason to use ESQL. I would like to get a full idea of the pro's/con's of using either method.</p>
","<p>I find that ESQL to be good for edge cases, for example:</p>

<ol>
<li>Where it's just really hard to express something in LINQ.</li>
<li>For building <em>very</em> dynamic searches.</li>
<li>If you want to use a database specific function that is exposed by the provider you are using.</li>
</ol>

<p>Also, if you know Entity SQL, you will be able to express QueryViews and Model-Defined Queries.</p>
"
"<p>I've installed MySql 5.6 with Workbench and MySql Notifier.</p>

<p>I can start/stop the MySql Service ( Service name : MySql56 ) from Services.msc. But I'm not able to start/stop it from MySql Notifier. I'm having the following error.</p>

<p><img src=""https://i.stack.imgur.com/RA40I.png"" alt=""enter image description here""></p>

<p>I don't what's going on there. I can confirm that a service named ""MySql56"" is present and it starts/stops successfully from services.msc</p>

<p>My system is Windows 7 Professional 64 bit</p>
","<p>You must access to location: %APPDATA%\Oracle\MySQL Notifier\settings.config (C:\Users\YourUsername\AppData\Roaming\Oracle\MySQL Notifier\settings.config) 
and change ServiceName to MySQL56 in settings.config file</p>

<p>See this: <a href=""http://i.stack.imgur.com/eSXKl.png"">http://i.stack.imgur.com/eSXKl.png</a></p>
"
"<p>I'm in a situation where I would to generate a script for a database that I could run on another server and get a database identical to the original one, but without any of the data.  In essence, I want to end up with a big create script that captures the database schema. </p>

<p>I am working in an environment that has SQL Server 2000 installed, and I am unable to install the 2005 client tools (in the event that they would help).  I can't afford RedGate, but I really would like to have a database with identical schema on another server.</p>

<p>Any suggestions?  Any simple .exe (no installation required) tools, tips, or T-SQL tricks would be much appreciated.</p>

<p><strong>Update:</strong> The database I'm working with has 200+ tables and several foreign-key relationships and constraints, so manually scripting each table and pasting together the script is not a viable option.  I'm looking for something better than this manual solution</p>

<p><strong>Additional Update</strong> Unless I'm completely missing something, this is not a viable solution using the SQL 2000 tools.  When I select the option to generate a create script on a database.  I end up with a script that contains a CREATE DATABASE command, and creates none of the objects - the tables, the constraints, etc.  SQL 2005's Management studio may handle the objects as well, but the database is in an environment where there is no way for me to connect an installation of Management Studio to it.</p>
","<p>You'd use <code>REGEXP_REPLACE</code> in order to remove all non-digit characters from a string:</p>

<pre><code>select regexp_replace(column_name, '[^0-9]', '')
from mytable;
</code></pre>

<p>or</p>

<pre><code>select regexp_replace(column_name, '[^[:digit:]]', '')
from mytable;
</code></pre>

<p>Of course you can write a function <code>extract_number</code>. It seems a bit like overkill though, to write a funtion that consists of only one function call itself.</p>

<pre><code>create function extract_number(in_number varchar2) return varchar2 is
begin
  return regexp_replace(in_number, '[^[:digit:]]', '');
end; 
</code></pre>
"
"<p>Background:</p>

<p>The <a href=""https://stackoverflow.com/questions/36452626/selecting-a-subset-of-rows-that-exceed-a-percentage-of-total-values"">original case</a> was very simple. Calculate running total per user from highest revenue to lowest:</p>

<pre><code>CREATE TABLE t(Customer INTEGER  NOT NULL PRIMARY KEY 
              ,""User""   VARCHAR(5) NOT NULL
              ,Revenue  INTEGER  NOT NULL);

INSERT INTO t(Customer,""User"",Revenue) VALUES
(001,'James',500),(002,'James',750),(003,'James',450),
(004,'Sarah',100),(005,'Sarah',500),(006,'Sarah',150),
(007,'Sarah',600),(008,'James',150),(009,'James',100);
</code></pre>

<p>Query:</p>

<pre><code>SELECT *,
    1.0 * Revenue/SUM(Revenue) OVER(PARTITION BY ""User"") AS percentage,
    1.0 * SUM(Revenue) OVER(PARTITION BY ""User"" ORDER BY Revenue DESC)
         /SUM(Revenue) OVER(PARTITION BY ""User"") AS running_percentage
FROM t;
</code></pre>

<p><kbd><strong><a href=""http://rextester.com/ZMUJW38298"" rel=""nofollow noreferrer""><code>LiveDemo</code></a></strong></kbd>    </p>

<p>Output:    </p>

<pre><code>
 ID  User   Revenue  percentage  running_percentage 

  2  James      750  0.38        0.38               
  1  James      500  0.26        0.64               
  3  James      450  0.23        0.87               
  8  James      150  0.08        0.95               
  9  James      100  0.05        1                  
  7  Sarah      600  0.44        0.44               
  5  Sarah      500  0.37        0.81               
  6  Sarah      150  0.11        0.93               
  4  Sarah      100  0.07        1                  

</code></pre>

<p>It could be calculated differently using specific windowed functions.    </p>

<hr>

<p>Now let's assume that we cannot use windowed <code>SUM</code> and rewrite it:</p>

<pre><code>SELECT c.Customer, c.""User"", c.""Revenue""
    ,1.0 * Revenue / NULLIF(c3.s,0) AS percentage
    ,1.0 * c2.s    / NULLIF(c3.s,0) AS running_percentage
FROM t c
CROSS APPLY
        (SELECT SUM(Revenue) AS s
        FROM t c2
        WHERE c.""User"" = c2.""User""
            AND c2.Revenue &gt;= c.Revenue) AS c2
CROSS APPLY
        (SELECT SUM(Revenue) AS s
        FROM t c2
        WHERE c.""User"" = c2.""User"") AS c3
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p><kbd><strong><a href=""http://rextester.com/KPEK4632"" rel=""nofollow noreferrer""><code>LiveDemo</code></a></strong></kbd>   </p>

<p>I have used <code>CROSS APPLY</code> because I do not like correlated subqueries in <code>SELECT</code> colums list and <code>c3</code> is used twice.       </p>

<p>Everything work as it should. But when we look closer <code>c2</code> and <code>c3</code> are very similiar. So why not combine them and use simple conditional aggregation:</p>

<pre><code>SELECT c.Customer, c.""User"", c.""Revenue""
    ,1.0 * Revenue        / NULLIF(c2.sum_total,0) AS percentage
    ,1.0 * c2.sum_running / NULLIF(c2.sum_total,0) AS running_percentage
FROM t c
CROSS APPLY
        (SELECT SUM(Revenue) AS sum_total,
                SUM(CASE WHEN c2.Revenue &gt;= c.Revenue THEN Revenue ELSE 0 END) 
                AS sum_running
        FROM t c2
        WHERE c.""User"" = c2.""User"") AS c2
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p>Unfortunately it is not possible. </p>

<blockquote>
  <p>Multiple columns are specified in an aggregated expression containing an outer reference. If an expression being aggregated contains an outer reference, then that outer reference must be the only column referenced in the expression.</p>
</blockquote>

<p>Of course I could circumvent it wrapping with another subquery, but it becomes a bit ""ugly"":</p>

<pre><code>SELECT c.Customer, c.""User"", c.""Revenue""
    ,1.0 * Revenue        / NULLIF(c2.sum_total,0) AS percentage
    ,1.0 * c2.sum_running / NULLIF(c2.sum_total,0) AS running_percentage
FROM t c
CROSS APPLY
(   SELECT SUM(Revenue) AS sum_total,
           SUM(running_revenue) AS sum_running
     FROM (SELECT Revenue,
                  CASE WHEN c2.Revenue &gt;= c.Revenue THEN Revenue ELSE 0 END 
                  AS running_revenue
           FROM t c2
           WHERE c.""User"" = c2.""User"") AS sub
) AS c2
ORDER BY ""User"", Revenue DESC
</code></pre>

<p><kbd><strong><a href=""http://rextester.com/YRKO76442"" rel=""nofollow noreferrer""><code>LiveDemo</code></a></strong></kbd>   </p>

<hr>

<p><code>Postgresql</code> version. The only difference is <code>LATERAL</code> instead of <code>CROSS APPLY</code>.</p>

<pre><code>SELECT c.Customer, c.""User"", c.Revenue
    ,1.0 * Revenue        / NULLIF(c2.sum_total,0) AS percentage 
    ,1.0 * c2.running_sum / NULLIF(c2.sum_total,0) AS running_percentage 
FROM t c
,LATERAL (SELECT SUM(Revenue) AS sum_total,
                 SUM(CASE WHEN c2.Revenue &gt;= c.Revenue THEN c2.Revenue ELSE 0 END) 
                 AS running_sum
        FROM t c2
        WHERE c.""User"" = c2.""User"") c2
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p><kbd><strong><a href=""http://sqlfiddle.com/#!15/8cab1/2/0"" rel=""nofollow noreferrer""><code>SqlFiddleDemo</code></a></strong></kbd></p>

<p>It works very nice.</p>

<hr>

<p><code>SQLite</code>/<code>MySQL</code> version (that is why I prefer<code>LATERAL/CROSS APPLY</code>):</p>

<pre><code>SELECT c.Customer, c.""User"", c.Revenue,
    1.0 * Revenue / (SELECT SUM(Revenue)
                     FROM t c2
                     WHERE c.""User"" = c2.""User"") AS percentage,
    1.0 * (SELECT SUM(CASE WHEN c2.Revenue &gt;= c.Revenue THEN c2.Revenue ELSE 0 END)
           FROM t c2
          WHERE c.""User"" = c2.""User"")  / 
          (SELECT SUM(c2.Revenue)
           FROM t c2
           WHERE c.""User"" = c2.""User"") AS running_percentage
FROM t c
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p><kbd><strong><a href=""http://sqlfiddle.com/#!5/94a35/1/0"" rel=""nofollow noreferrer""><code>SQLFiddleDemo-SQLite</code></a></strong></kbd>  <kbd><strong><a href=""http://sqlfiddle.com/#!9/c85e83/5/0"" rel=""nofollow noreferrer""><code>SQLFiddleDemo-MySQL</code></a></strong></kbd>  </p>

<hr>

<p>I've read <a href=""http://sqlmag.com/blog/aggregates-outer-reference"" rel=""nofollow noreferrer"">Aggregates with an Outer Reference</a>:</p>

<blockquote>
  <p>The source for the restriction is in the <code>SQL-92</code> standard, and <code>SQL Server</code> inherited it from the <code>Sybase</code> codebase. The problem is that SQL Server needs to figure out which query will compute the aggregate.</p>
</blockquote>

<p>I do not search for answers that <strong>only</strong> show how to circumvent it.</p>

<p>The questions are:</p>

<ol>
<li>Which part of standard disallow or interfere with it?</li>
<li>Why other RDBMSes do not have problem with this kind of outer dependency?</li>
<li>Do they extend <code>SQL Standard</code> and <code>SQL Server</code> behaves as it should or <code>SQL Server</code> does not implement it fully(correctly?)?.</li>
</ol>

<p>I would be very grateful for references to:</p>

<ul>
<li><code>ISO standard</code> (92 or newer)</li>
<li><a href=""https://msdn.microsoft.com/en-us/library/hh501587%28v=sql.105%29.aspx"" rel=""nofollow noreferrer"">SQL Server Standards Support</a></li>
<li>official documenation from any RDBMS that explains it (<code>SQL Server/Postgresql/Oracle/...</code>).</li>
</ul>

<p><strong>EDIT:</strong></p>

<p>I know that <code>SQL-92</code> does not have concept of <code>LATERAL</code>. But version with subqueries (like in <code>SQLite/MySQL</code>) does not work too.</p>

<p><kbd><strong><a href=""http://rextester.com/ZAOJ48675"" rel=""nofollow noreferrer""><code>LiveDemo</code></a></strong></kbd></p>

<p><strong>EDIT 2:</strong></p>

<p>To simplify it a bit, let's check only correlated subquery only:</p>

<pre><code>SELECT c.Customer, c.""User"", c.Revenue,
       1.0*(SELECT SUM(CASE WHEN c2.Revenue &gt;= c.Revenue THEN c2.Revenue ELSE 0 END)
              FROM t c2
              WHERE c.""User"" = c2.""User"") 
       / (SELECT SUM(c2.Revenue)
          FROM t c2
          WHERE c.""User"" = c2.""User"") AS running_percentage
FROM t c
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p>The version above works fine in <code>MySQL/SQLite/Postgresql</code>.</p>

<p>In <code>SQL Server</code> we get error. After wraping it with subquery to ""flatten"" it to one level it works:</p>

<pre><code>SELECT c.Customer, c.""User"", c.Revenue,
      1.0 * (
              SELECT SUM(CASE WHEN r1 &gt;= r2 THEN r1 ELSE 0 END)
              FROM (SELECT c2.Revenue AS r1, c.Revenue r2
                    FROM t c2
                    WHERE c.""User"" = c2.""User"") AS S)  / 
             (SELECT SUM(c2.Revenue)
              FROM t c2
              WHERE c.""User"" = c2.""User"") AS running_percentage
FROM t c
ORDER BY ""User"", Revenue DESC;
</code></pre>

<p>The point of this question is how does <code>SQL standard</code> regulate it. </p>

<p><kbd><strong><a href=""http://rextester.com/WFYP90759"" rel=""nofollow noreferrer""><code>LiveDemo</code></a></strong></kbd></p>
","<p>It's hard to know exactly what the designers of the SQL language were thinking when they wrote the standard, <em>but here's my opinion</em>.</p>

<p>SQL, as a general rule, requires you to explicitly state your expectations and your intent. The language does not try to <em>""guess what you meant""</em>, and automatically fill in the blanks. <strong>This is a good thing</strong>.</p>

<p><strong>When you write a query the most important consideration is that it <em>yields correct results</em>.</strong> If you made a mistake, it's probably better that the SQL parser informs you, rather than making a guess about your intent and returning results that may not be correct. The declarative nature of SQL (where you state what you want to retrieve rather than the steps how to retrieve it) already makes it easy to inadvertently make mistakes. <strong>Introducing fuzziniess into the language syntax would not make this better</strong>.</p>

<p>In fact, every case I can think of where the language allows for <strong>shortcuts</strong> has caused problems. Take, for instance, natural joins - where you can omit the names of the columns you want to join on and allow the database to infer them based on column names. Once the column names change (as they naturally do over time) <em>- the semantics of existing queries changes with them</em>. <strong>This is bad ... very bad</strong> - you really don't want this kind of <em>magic</em> happening behind the scenes in your database code.</p>

<p><strong>One consequence of this design choice, however, is that SQL is a verbose language in which you must explicitly express your intent.</strong> This can result in having to write more code than you may like, and gripe about why certain constructs are so verbose ... but at the end of the day - it is what it is.</p>
"
"<p>Hi All i am getting below error message while running phoenix count query on a large table. </p>

<pre><code>0: jdbc:phoenix:hadoopm1:2181&gt; select Count(*) from PJM_DATASET;
+------------+
|  COUNT(1)  |
+------------+

java.lang.RuntimeException: org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:
Fri Jan 09 02:18:10 CST 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=62365: row '' on table 'PJM_DATASET' at region=PJM_DATASET,,1420633295836.4394a3aa2721f87f3e6216d20ebeec44., hostname=hadoopctrl,60020,1420790733247, seqNum=27753

    at sqlline.SqlLine$IncrementalRows.hasNext(SqlLine.java:2440)
    at sqlline.SqlLine$TableOutputFormat.print(SqlLine.java:2074)
    at sqlline.SqlLine.print(SqlLine.java:1735)
    at sqlline.SqlLine$Commands.execute(SqlLine.java:3683)
    at sqlline.SqlLine$Commands.sql(SqlLine.java:3584)
    at sqlline.SqlLine.dispatch(SqlLine.java:821)
    at sqlline.SqlLine.begin(SqlLine.java:699)
    at sqlline.SqlLine.mainWithInputRedirection(SqlLine.java:441)
    at sqlline.SqlLine.main(SqlLine.java:424)
0: jdbc:phoenix:hadoopm1:2181&gt;
</code></pre>

<p>please help.</p>
","<p>After several hours of frustration and resignation I found the solution:
You have to use the IP-Adress of the machine, not the name.</p>

<p>I edited my hbase-site.xml as follows:</p>

<pre><code>&lt;property&gt;
  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
  &lt;value&gt;192.168.59.81&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<p>My regionservers - file looks like:</p>

<pre><code>  localhost
</code></pre>

<p>And my zoo.cfg contains:</p>

<pre><code>  # Define Quorum-Servers
  server.1=192.168.59.81:2888:3888
</code></pre>

<p>Hope this will help some others who had the same problem of connecting to a pseudo-distributed hbase cluster remotely.</p>
"
"<p>I'm looking for a way to parse / tokenize SQL statement within a Node.js application, in order to:</p>

<ul>
<li>Tokenize all the ""basics"" SQL keywords defined in the <a href=""https://github.com/marijnh/CodeMirror/tree/master/mode/sql"" rel=""nofollow noreferrer"">ISO/IEC 9075</a> standard or <a href=""http://codemirror.net/"" rel=""nofollow noreferrer"">here</a>.</li>
<li>Validate the SQL syntax.</li>
<li>Find out what the query is gonna do (e.g. read or write?).</li>
</ul>

<p>Do you have any solution or advises peeps?</p>

<p>Linked: <a href=""https://stackoverflow.com/questions/13169783/any-javascript-jquery-library-to-validate-sql-statment"">Any Javascript/Jquery Library To validate SQL statment?</a>
<br><br></p>

<hr>

<p>I've done research and I found out some ways to do it:</p>

<p><strong>Using existing node.js libraries</strong></p>

<p>I did a <a href=""http://fr.wikipedia.org/wiki/Microsoft_SQL_Server"" rel=""nofollow noreferrer"">Google search</a> and I didn't found a consensual and popular library to use. I found those ones: </p>

<ul>
<li><a href=""http://www.mysql.fr/"" rel=""nofollow noreferrer"">simple-sql-parser</a> (22 stars on <a href=""http://fr.wikipedia.org/wiki/PL/SQL"" rel=""nofollow noreferrer"">github</a>, 16 daily download on <a href=""http://cassandra.apache.org/"" rel=""nofollow noreferrer"">npm</a>)

<ul>
<li>Supports only SELECT, INSERT, UPDATE and DELETE</li>
<li>There is a <a href=""https://mariadb.org/"" rel=""nofollow noreferrer"">v2 branch</a> on the road</li>
</ul></li>
<li><a href=""https://github.com/forward/sql-parser"" rel=""nofollow noreferrer"">sql-parser</a> (90 stars on <a href=""https://github.com/forward/sql-parser"" rel=""nofollow noreferrer"">github</a>, 6 daily downloads on <a href=""https://www.npmjs.org/package/sql-parser"" rel=""nofollow noreferrer"">npm</a>)

<ul>
<li>Only supports basic SELECT statements</li>
<li>Based on <a href=""http://jison.org"" rel=""nofollow noreferrer"">jison</a></li>
</ul></li>
<li><a href=""https://github.com/langpavel/node-sqljs"" rel=""nofollow noreferrer"">sqljs</a> (17 stars on <a href=""https://github.com/langpavel/node-sqljs"" rel=""nofollow noreferrer"">github</a>, 5 daily downloads on <a href=""https://www.npmjs.org/package/sqljs"" rel=""nofollow noreferrer"">npm</a>)

<ul>
<li>v0.0.0-3, under development... No documentation at all :)</li>
</ul></li>
</ul>

<p>Unfortunately, none of those libraries seams to be complete and trustful.</p>

<p><strong>Doing it myself based on a node.js low level tokenizer library</strong></p>

<p>I can do it my self with a low level tokenizer library like:</p>

<ul>
<li><a href=""http://jison.org"" rel=""nofollow noreferrer"">jison</a> (1,457 stars on <a href=""https://github.com/zaach/jison"" rel=""nofollow noreferrer"">github</a>, 240 daily downloads on <a href=""https://www.npmjs.org/package/jison"" rel=""nofollow noreferrer"">npm</a>)</li>
<li><a href=""https://github.com/floby/node-tokenizer"" rel=""nofollow noreferrer"">tokenizer</a> (44 stars on <a href=""https://github.com/Floby/node-tokenizer"" rel=""nofollow noreferrer"">github</a>, 10 daily downloads on <a href=""https://www.npmjs.org/package/tokenizer"" rel=""nofollow noreferrer"">npm</a>)</li>
</ul>

<p><strong>Doing it myself based on existing Javascript code beautifier</strong></p>

<p><a href=""http://codemirror.net/"" rel=""nofollow noreferrer"">CodeMirror</a> is a pretty cool Javascript library (browser side) that can recognize SQL keywords, strings, etc. Check ou the <a href=""http://codemirror.net/1/contrib/sql/"" rel=""nofollow noreferrer"">demo</a>.</p>

<p>I can build a node.js library tokenizer based on CodeMirror.
The <a href=""https://github.com/marijnh/CodeMirror/tree/master/mode/sql"" rel=""nofollow noreferrer"">SQL mode is here on github</a>, I can maybe adapt it to get tokens within a node application.</p>

<p>PS: <a href=""http://codemirror.net/"" rel=""nofollow noreferrer"">CodeMirror</a> have 5,046 stars on <a href=""https://github.com/marijnh/codemirror"" rel=""nofollow noreferrer"">github</a> and is well maintained.</p>

<hr>

<p>I figured out that there are 2 distinct problems: Tokenization and Syntax validation (which is related to tokenization).</p>

<p>I made myself a <strong>SQL tokenizer</strong> for Node.js based on the <a href=""https://github.com/marijnh/CodeMirror/tree/master/mode/sql"" rel=""nofollow noreferrer"">SQL mode</a> of the excellent <a href=""http://codemirror.net/"" rel=""nofollow noreferrer"">CodeMirror</a> (5,046 stars on github, well maintained). CodeMirror's SQL mode take in charge ""generic"" SQL and some SQL particularities like <a href=""http://fr.wikipedia.org/wiki/Microsoft_SQL_Server"" rel=""nofollow noreferrer"">MSSQL</a>, <a href=""http://www.mysql.fr/"" rel=""nofollow noreferrer"">MySQL</a>, <a href=""http://fr.wikipedia.org/wiki/PL/SQL"" rel=""nofollow noreferrer"">PL/SQL</a>, <a href=""http://cassandra.apache.org/"" rel=""nofollow noreferrer"">Cassandra</a>, Hive and <a href=""https://mariadb.org/"" rel=""nofollow noreferrer"">MariaDB</a>.</p>

<p>When my project will be mature enough, I will (probably) put it public on GitHub and let you know.</p>

<p>About the <strong>SQL syntax</strong> validation, I found no JavaScript tool (or open source project to adapt in JS) yet... </p>
","<p>You might like to take a look at <a href=""http://code.google.com/p/python-sqlparse/"">sqlparse</a></p>

<p>Blatantly stolen from their homepage:</p>

<pre><code>&gt;&gt;&gt; # Parsing
&gt;&gt;&gt; res = sqlparse.parse('select * from ""someschema"".""mytable"" where id = 1')
&gt;&gt;&gt; res
&lt;&lt;&lt; (&lt;Statement 'select...' at 0x9ad08ec&gt;,)
&gt;&gt;&gt; stmt = res[0]
&gt;&gt;&gt; stmt.to_unicode()  # converting it back to unicode
&lt;&lt;&lt; u'select * from ""someschema"".""mytable"" where id = 1'
&gt;&gt;&gt; # This is how the internal representation looks like:
&gt;&gt;&gt; stmt.tokens
&lt;&lt;&lt;
(&lt;DML 'select' at 0x9b63c34&gt;,
 &lt;Whitespace ' ' at 0x9b63e8c&gt;,
 &lt;Operator '*' at 0x9b63e64&gt;,
 &lt;Whitespace ' ' at 0x9b63c5c&gt;,
 &lt;Keyword 'from' at 0x9b63c84&gt;,
 &lt;Whitespace ' ' at 0x9b63cd4&gt;,
 &lt;Identifier '""somes...' at 0x9b5c62c&gt;,
 &lt;Whitespace ' ' at 0x9b63f04&gt;,
 &lt;Where 'where ...' at 0x9b5caac&gt;)
&gt;&gt;&gt;
</code></pre>
"
"<p>Is there a way to <em>not</em> fetch a specific column using linqtosql without having to use an anonymous type and specify each returned filed individually?</p>

<p>We use SQLMetal to generate the dbml file which contains all the types into which to put the queried data results.  However, when select columns are included in the linq query, the results go into an anonymous type instead of the type declared in the dbml file.  I would like to select <em>all but one</em> of the columns from a particular table, but still have the results returned in the related dbml type.</p>

<p>Any ideas appreciated.</p>
","<p>LINQ to SQL supports <a href=""http://en.wikipedia.org/wiki/Lazy_loading"" rel=""nofollow noreferrer"">lazy loading</a> individual properties. In the DBML designer, you can set <code>Delay Loaded</code> to <code>true</code> in a column's properties. Columns that are delay loaded will not be included in the initial <code>SELECT</code>. If you try to access the object's property and it has not been loaded yet, then another <code>SELECT</code> statement will be executed to bring this value from the DB.</p>

<p>If you are editing the DBML file by hand, set <code>&lt;Column IsDelayLoaded=""true""&gt;</code>. If you are writing your LINQ to SQL classes by hand, it is as simple as declaring the property's backing field as a <code>Link&lt;T&gt;</code> instead of <code>T</code>. For example:</p>

<pre><code>[Table]
public class Person
{
    private Link&lt;string&gt; _name;

    [Column(Storage = ""_name"")]
    public string Name
    {
        get { return _name.Value; }
        set { _name.Value = value; }
    }
}
</code></pre>

<p>See also the ""Delay/Lazy Loading"" section in <a href=""http://weblogs.asp.net/scottgu/archive/2007/05/29/linq-to-sql-part-2-defining-our-data-model-classes.aspx"" rel=""nofollow noreferrer"">this post by Scott Guthrie</a>.</p>

<hr>

<p>Update: The above answer applies if you want the column to still be available when you need it. It will not be included in <code>SELECT</code>s unless you specifically ask for it (see <code>LoadOptions</code>) or try to access it.</p>

<p>If you just don't want to use or access the column at all, and you don't want it available as a class property, then go with <a href=""https://stackoverflow.com/questions/852679/linq-to-sql-dont-fetch-a-particular-column/852934#852934"">Serapth's answer</a> of removing the column from the DBML file. Be sure you understand the implications, such as the loss of concurrency checking on that column.</p>
"
"<p>We are trying to setup a sql project, in a new machine with Windows 7, VS 2010 with SP1 &amp; SSDT 2010 (installed SSDT 2010 from iso image). But getting the below message when I open the sqlproj.</p>

<p><strong>'Verifying your model is synchronized with your source files. Your database will be ready in 12734 operations are completed.'
And the number keeps on increasing</strong>. And it keeps on running in the background.</p>

<p>Tried re-installing SSDT, VS 2010 but no help. </p>

<p>Created a new database project for Northwind db, and had the same issue. Ran  procmon and saw it's just going over and over the same files.</p>

<p>It works fine in another system with similar configuration.</p>

<p><strong>EDIT</strong></p>

<p>The issue seems to be related to TFS, if we unbind from TFS it works fine. But not sure about the exact cause.</p>

<p>Any suggestion would be really helpful.</p>
","<blockquote>
  <p>Open MS Ticket: <a href=""https://developercommunity.visualstudio.com/content/problem/67789/visual-studio-hangs-when-closing-sql-files.html"" rel=""noreferrer"">https://developercommunity.visualstudio.com/content/problem/67789/visual-studio-hangs-when-closing-sql-files.html</a></p>
</blockquote>

<p>The real solution will come with an update from MS.  For now, however, turning off my participation in the Visual Studio Experience Improvement Program seemed to solve it.</p>

<p>You can check whether or not you're signed up for this program by clicking on Help -> Send Feedback -> Settings (in 2017, not sure about 2015).</p>
"
"<p>I have this question about the MySqlParameter from the .NET connector.</p>

<p>I have this query:</p>

<pre><code>SELECT * FROM table WHERE id IN (@parameter)
</code></pre>

<p>And the MySqlParameter is:</p>

<pre><code>intArray = new List&lt;int&gt;(){1,2,3,4};

...connection.Command.Parameters.AddWithValue(""parameter"", intArray);
</code></pre>

<p>This is possible?
Is possible to pass an array of int to a single MySqlParameter?
The other solution will be convert the array of int to a string such like ""1,2,3,4"", but this, when i pass it to the MySqlParameter and this is recognized as a string, it puts in the sql query like ""1\,2\,3\,4"" and this do not return the expected values.</p>

<p>@ UPDATE: Seems like the mysql connector team should work a little bit harder.</p>
","<blockquote>
  <p>when i pass it to the MySqlParameter and this is recognized as a string, it puts in the sql query like ""1\,2\,3\,4"" and this do not return the expected values.</p>
</blockquote>

<p>I ran into this last night. I found that FIND_IN_SET works here:</p>

<pre><code>SELECT * FROM table WHERE FIND_IN_SET(id, @parameter) != 0
...
intArray = new List&lt;int&gt;(){1,2,3,4};
conn.Command.Parameters.AddWithValue(""parameter"", string.Join("","", intArray));
</code></pre>

<p>Apparently this has some length limitations (I found your post looking for an alternate solution), but this may work for you.</p>
"
"<p>I would like to create a MySQL table with Pandas' to_sql function which has a primary key (it is usually kind of good to have a primary key in a mysql table) as so:</p>

<pre><code>group_export.to_sql(con = db, name = config.table_group_export, if_exists = 'replace', flavor = 'mysql', index = False)
</code></pre>

<p>but this creates a table without any primary key, (or even without any index).</p>

<p>The documentation mentions the parameter 'index_label' which combined with the 'index' parameter could be used to create an index but doesn't mention any option for primary keys.</p>

<p><a href=""http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_sql.html"" rel=""noreferrer"">Documentation</a></p>
","<p>Simply add the primary key after uploading the table with pandas.</p>

<pre><code>group_export.to_sql(con=engine, name=example_table, if_exists='replace', 
                    flavor='mysql', index=False)

with engine.connect() as con:
    con.execute('ALTER TABLE `example_table` ADD PRIMARY KEY (`ID_column`);')
</code></pre>
"
"<p>In my Rails application I want to use the <code>will_paginate</code> gem to paginate on my SQL query. Is that possible? I tried doing something like this but it didn't work:</p>

<pre><code>@users = User.find_by_sql(""
    SELECT u.id, u.first_name, u.last_name, 
     CASE 
      WHEN r.user_accepted =1 AND (r.friend_accepted =0 || r.friend_accepted IS NULL)
       ........."").paginate(
                  :page =&gt; @page, :per_page =&gt; @per_page, 
                  :conditions =&gt; conditions_hash,
                  :order =&gt; 'first_name ASC')
</code></pre>

<p>If not, can you recommend a way around this? I don't want to have to write my own pagination.</p>
","<p>Use <code>paginate_by_sql</code>, i.e.</p>

<pre><code>sql = "" SELECT * 
        FROM   users
        WHERE  created_at &gt;= '#{2.weeks.ago}'
        ORDER BY created_at DESC ""
@users =  User.paginate_by_sql(sql, :page =&gt; @page, :per_page =&gt; @per_page)
</code></pre>

<p>As a general rule any finder can be paginated by replacing the <code>find*</code> with <code>paginate*</code>.</p>
"
"<p>I am just new to reading in an XML file into a table through SQL Server Management Studio. There are probably better ways but I would like to use this approach.</p>

<p>Currently I am reading in a standard XML file of records on people. A <code>&lt;record&gt;</code> tag is the highest level of each row of data. I want to read all the records into separate rows into my SQL table.</p>

<p>I have gotten along fine so far using the following approach as follows:</p>

<pre><code>SELECT
        -- Record
        category, editor, entered, subcategory, uid, updated,
        -- Person
        first_name, last_name, ssn, ei, title, POSITION,
FROM OPENXML(@hDoc, 'records/record/person/names')
WITH 
(
    -- Record
    category [varchar](100) '../../@category',
    editor [varchar](100) '../../@editor',
    entered Datetime '../../@entered',
    subcategory [varchar](100) '../../@subcategory',
    uid BIGINT '../../@uid',
    updated [varchar](100) '../../@updated',
    -- Person
    first_name [varchar](100) 'first_name',
    last_name [varchar](100) 'last_name',
    ssn [varchar](100) '../@ssn',
    ei [varchar](100) '../@e-i',
    title [varchar](100) '../title',
    Position [varchar](100) '../position',  
)
</code></pre>

<p>However this approach has worked fine as the tag names have all been unique to each record/person. The issue I have is within the <code>&lt;Person&gt;</code> tag I now have an <code>&lt;Aliases&gt;</code> tag that contains a list of more than 1 <code>&lt;Alias&gt; test name &lt;/Alias&gt;</code> tags. If I use the above approach &amp; reference <strong>'../aliases'</strong> I get all the Alias elements as one long String row mixed together. If I just try <strong>'../aliases/alias'</strong> ONLY the first element is returned per record row. If there was 10 Alias elements within the Aliases tag set I would like 10 rows returned for example. </p>

<p>Is there a way to specify that when there are multiple tags of the same name within a higher level tag, return them all &amp; not just one row?</p>

<p>The following is the example block within the XML I am referring to:</p>

<pre><code>- &lt;aliases&gt;
  &lt;alias&gt;test 1&lt;/alias&gt; 
  &lt;alias&gt;test 2&lt;/alias&gt; 
  &lt;alias&gt;test 3&lt;/alias&gt; 
  &lt;/aliases&gt;
</code></pre>

<p>I would like the following in the SQL table:</p>

<pre><code>Record               Aliases

Record 1             test 1
Record 1             test 2
Record 1             test 3
Record 2             test 4
Record 2             test 5
</code></pre>

<p>but all I get is:</p>

<pre><code>Record 1             test 1 
Record 2             test 4
</code></pre>

<p>Apologies if I have not explained this correctly - any help would be greatly appreciated.</p>
","<pre><code>SELECT *
FROM OPENXML (@index, 'rootnode/group/anothernode')
WITH 
(
  id int '../id',
  anothernode varchar(30) '.'
)
</code></pre>

<p>Or you can use the XML datatype instead like this:</p>

<pre><code>SELECT G.N.value('(id/text())[1]', 'int') AS id,
       A.N.value('text()[1]', 'varchar(30)') AS anothernode
FROM @XMLDoc.nodes('rootnode/group') AS G(N)
  CROSS APPLY G.N.nodes('anothernode') AS A(N)
</code></pre>
"
"<p>I need to use SQLCipher for android...i've already made my app using SQLite and want to just convert it to SQLCipher.</p>

<p>The problem is, I know nothing about SQLCipher.</p>

<p>I have read about it in this link: <a href=""http://sqlcipher.net/sqlcipher-for-android/"" rel=""noreferrer"">http://sqlcipher.net/sqlcipher-for-android/</a></p>

<p>But i'm not too clear, still.
I was wondering if you could provide some basic sqlcipher for android tutorials, where everything is taught in an easy way from the absolute basics.</p>

<p>Thanks!</p>
","<p><h1>Download and Build sqlcipher</h1> <h3>--Skip this if sqlcipher is already installed</h3>
Pull the code from <a href=""https://github.com/sqlcipher/sqlcipher"" rel=""noreferrer"">https://github.com/sqlcipher/sqlcipher</a> in a directory (say ~/sqlcipher)</p>

<pre><code>mkdir ~/bld;        #  Build will occur in a sibling directory
cd ~/bld;           #  Change to the build directory
../sqlcipher/configure --enable-tempstore=yes CFLAGS=""-DSQLITE_HAS_CODEC"" LDFLAGS=""-lcrypto""; 
                    #configure sqlcipher 

make install;       #  Install the build products
</code></pre>

<p><H1>Decrypt the database to a plaintext database</H1></p>

<pre><code>$ cd ~/;
$ ./sqlcipher encrypted.db 
sqlite&gt; PRAGMA key = 'testkey'; 
sqlite&gt; ATTACH DATABASE 'plaintext.db' AS plaintext KEY '';  -- empty key will disable encryption
sqlite&gt; SELECT sqlcipher_export('plaintext'); 
sqlite&gt; DETACH DATABASE plaintext; 
</code></pre>

<p>Find the decrypted database at ~/plaintext.db which you can use with any sqlite browser like <a href=""http://sqlitebrowser.org/"" rel=""noreferrer"">this</a>.</p>

<p><H1><strong>Update</strong> : September 2015</H1></p>

<p><a href=""http://sqlitebrowser.org"" rel=""noreferrer"">http://sqlitebrowser.org</a> now supports sqlcipher databases. That's neat.</p>
"
"<p>I found a lot of comparisions here, but not this one;
So, what is best in each one?</p>
","<p>There's a full comparison at <a href=""http://www.sqlite.org/cvstrac/wiki?p=SqliteVersusDerby"" rel=""nofollow noreferrer"">SQLite's site</a>.</p>

<p>SQLite is much more restricted, as it only supports a small subset of SQL92, whereas Derby (now JavaDB) has full support of SQL92 and SQL99.</p>
"
"<p>I had some questions around the FILESTREAM capability of SQL Server 2008.</p>

<ol>
<li><p>What would the difference in performance be of returning a file streamed from SQL Server 2008 using the FILESTREAM capability vs. directly accessing the file from a shared directory? </p></li>
<li><p>If 100 users requested 100 100Mb files (stored via FILESTREAM) within a 10 second window, would SQL Server 2008 performance slow to a crawl?</p></li>
</ol>
","<p><em>If 100 users requested 100 100Mb files (stored via FILESTREAM) within a 10 second window, would SQL Server 2008 performance slow to a crawl?</em></p>

<p>On what kind of a server?? What kind of hardware to serve those files? What kind of disks, network etc.?? So many questions.......</p>

<p>There's a really good blog post by Paul Randal on <a href=""http://www.sqlskills.com/BLOGS/PAUL/post/SQL-Server-2008-FILESTREAM-performance.aspx"" rel=""nofollow noreferrer"">SQL Server 2008: FILESTREAM Performance</a> - check it out. There's also a <a href=""http://msdn.microsoft.com/en-us/library/cc949109.aspx"" rel=""nofollow noreferrer"">25-page whitepaper on FILESTREAM</a> available - also covering some performance tuning tips.</p>

<hr>

<p>But also check out the Microsoft Research TechReport <a href=""http://research.microsoft.com/apps/pubs/default.aspx?id=64525"" rel=""nofollow noreferrer"">To BLOB or Not To BLOB</a>.</p>

<p>It's a very profound and very well based article that put all those questions through their paces. </p>

<p>Their conclusion:</p>

<blockquote>
  <p>The study indicates that if objects
  are <strong>larger than one megabyte on
  average</strong>, NTFS has a clear advantage
  over SQL Server. If the objects are
  <strong>under 256 kilobytes, the database has
  a clear advantage</strong>. Inside this range,
  it depends on how write intensive the
  workload is, and the storage age of a
  typical replica in the system.</p>
</blockquote>

<p>So judging from that - if your blobs are typically less than 1 MB, just store them as a VARBINARY(MAX) in the database. If they're typically larger, then just the FILESTREAM feature.</p>

<p>I wouldn't worry so much about performance rather than other benefits of FILESTREAM over ""unmanaged"" storage in a NTFS file folder: storing files outside the database without FILESTREAM, you have no control over them:</p>

<ul>
<li>no access control provided by the database</li>
<li>the files aren't part of your SQL Server backup</li>
<li>the files aren't handled transactionally, e.g. you could end up with ""zombie"" files which aren't referenced from the database anymore, or ""skeleton"" entries in the database without the corresponding file on disk</li>
</ul>

<p>Those features alone make it absolutely worthwhile to use FILESTREAM.</p>
"
"<p>I have difficulty querieing for users, which is defined as:</p>

<pre><code>type User struct {
    ID           int       `db:""id"" json:""id""`            
    UserName     string    `db:""username"" json:""username""` 
    Email        string    `db:""email"" json:""email""`
    CreatedAt    time.Time `db:""created_at"" json:""created_at""`
    StatusID     uint8     `db:""status_id"" json:""status_id""`
    Deleted      uint8     `db:""deleted"" json:""deleted""`
... 
}
</code></pre>

<p>And the table in MariaDB is defined as:</p>

<pre><code>+--------------+------------------+------+-----+-------------------+----------------+
| Field        | Type             | Null | Key | Default           | Extra          |
+--------------+------------------+------+-----+-------------------+----------------+
| id           | int(10) unsigned | NO   | PRI | NULL              | auto_increment |
| username     | varchar(50)      | NO   |     | NA                |                |
| email        | varchar(255)     | NO   |     | NULL              |                |
| created_at   | datetime         | NO   |     | CURRENT_TIMESTAMP |                |
| status_id    | tinyint(1)       | NO   |     | 0                 |                |
| deleted      | tinyint(1)       | NO   |     | 0                 |                |
...              |
</code></pre>

<p>However when I query it like:</p>

<pre><code>func GetUsers(c *gin.Context) {
    var users []model.User
    err := shared.Dbmap.Select(&amp;users, ""SELECT * FROM user"")

    if err == nil {
        c.JSON(200, users)
    } else {
        fmt.Println(""%v \n"", err)
        c.JSON(http.StatusInternalServerError, gin.H{""error"": ""no user(s) in the table or problem in the query""})
    }

    // curl -i http://127.0.0.1:8080/api/v1/users
}
</code></pre>

<p>I get this error:</p>

<pre><code>sql: Scan error on column index 3: unsupported Scan, storing driver.Value type []uint8 into type *time.Time
</code></pre>

<p>while there are some rows in the table. </p>

<p>I have also tried <code>created_at</code> as  <code>timestamp</code> but still get the same error.</p>

<p>So I'm left clueless as what wrong here? How can I fix it? </p>

<p><strong>P.S.</strong> Though my question turned out to have the same answer as <a href=""https://stackoverflow.com/questions/29341590/go-parse-time-from-database"">this</a> but here the context is different (<code>sqlx</code> instead of <code>go-sql-driver/mysql</code>). Also since here the error is the subject it probably more searchable for people who google the same error. So perhaps this worth keeping as a separate question. </p>
","<p>Alright, I found the solution, thanks <a href=""https://stackoverflow.com/a/29343013/5774375"">this</a> answer.
The problem goes by adding <code>?parseTime=true</code> to the db mapper. Like this:</p>

<pre><code>db, err := sqlx.Connect(""mysql"", ""myuser:mypass@tcp(127.0.0.1:3306)/mydb?parseTime=true"")
</code></pre>
"
"<p>In MySQL I tried to define a trigger like this:</p>

<pre class=""lang-sql prettyprint-override""><code>DELIMITER $$  
CREATE TRIGGER vipInvite  
AFTER INSERT ON meetings  
FOR EACH ROW   
BEGIN     
IF(NOT EXISTS (SELECT * FROM participants 
   WHERE meetid = NEW.meetid AND pid ='vip'))
    THEN  
    IF(EXISTS(SELECT * FROM meetings WHERE meetid = NEW.meetid AND slot &gt; 16))  
    THEN  
    INSERT INTO participants(meetid, pid) 
    VALUES (NEW.meetid,(SELECT userid 
    FROM   people WHERE people.group = 'tap' GROUP BY invite));  
END IF;  
END IF;  
END $$  
DELIMITER ;  
</code></pre>

<p>Produces this error:</p>

<blockquote>
  <p>This version of MySQL doesn't yet support 'multiple triggers with the same action time and event for one table.</p>
</blockquote>

<p>Is there a way to work around this so I can define multiple triggers?</p>
","<p>This is how you need to proceed. See the example that I've worked out.</p>

<pre><code>mysql&gt; select * from test;
+------+-------+
| id   | name  |
+------+-------+
|    1 | name1 |
|    2 | name2 |
|    3 | name3 |
|    4 | name4 |
+------+-------+
4 rows in set (0.00 sec)

mysql&gt; select * from test1;
+------+------+--------+
| id   | tid  | name2  |
+------+------+--------+
|    1 |    2 | name11 |
|    2 |    3 | name12 |
|    3 |    4 | name13 |
+------+------+--------+
3 rows in set (0.00 sec)

mysql&gt; select
    -&gt;  t1.name
    -&gt; from
    -&gt;  test t1
    -&gt; join
    -&gt;  test1 t2 on t2.tid = t1.id
    -&gt; join
    -&gt;  (select id from test where id &lt;4 limit 3) as tempt on tempt.id = t1.id;
+-------+
| name  |
+-------+
| name2 |
| name3 |
+-------+
2 rows in set (0.00 sec)
</code></pre>

<p>Hope this helps.</p>
"
"<p>I am planning on moving our main project to Postgres 10 at some point.  I like to keep the local dev's database version close to what we are running on prod.</p>

<p>Currently our prod database is on Google Cloud SQL PostgreSQL 9.6.  I have not heard anything from Google on when this managed cloud sql product will offer Postgres 10.x in addition to 9.6.</p>

<p>Does anyone know when Postgres 10 will be a supported option on GCP's managed SQL product?  I would like to start planning for it.</p>
","<p>After I got information from @clemens and make some research I found that, in my dump file on section <code>CREATE SEQUENCE table_id_seq</code> has a statement <code>AS integer</code> that why when I restored into new database it did not create the <code>nextval()</code> for the sequence. If I remove the statement <code>AS integer</code> from the <code>CREATE SEQUENCE</code> section it works find.</p>

<p>In my dump file:</p>

<pre><code>CREATE SEQUENCE table_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;
</code></pre>

<p>Remove <code>AS integer</code> from dump file</p>

<pre><code>CREATE SEQUENCE table_id_seq
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;
</code></pre>
"
"<p>I am trying to use PROC SQL to query a DB2 table with hundreds of millions of records. During the development stage, I want to run my query on an arbitrarily small subset of those records (say, 1000). I've tried using INOBS to limit the observations, but I believe that this parameter is simply limiting the number of records which SAS is processing. I want SAS to only fetch an arbitrary number of records from the database (and then process all of them).</p>

<p>If I were writing a SQL query myself, I would simply use <code>SELECT * FROM x FETCH FIRST 1000 ROWS ONLY ...</code> (the equivalent of <code>SELECT TOP 1000 * FROM x</code> in SQL Server). But PROC SQL doesn't seem to have any option like this. It's taking an extremely long time to fetch the records.</p>

<p>The question: How can I instruct SAS to arbitrarily limit the number of records to <strong>return from the database</strong>.</p>

<p>I've read that PROC SQL uses ANSI SQL, which doesn't have any specification for a row limiting keyword. Perhaps SAS didn't feel like making the effort to translate its SQL syntax to vendor-specific keywords? Is there no work around?</p>
","<p>Have you tried using the <code>outobs</code> option in your <code>proc sql</code>?</p>

<p>For example,</p>

<pre><code>proc sql outobs=10; create table test
    as
    select * from schema.HUGE_TABLE
    order by n;
quit;
</code></pre>

<p>Alternatively, you can use SQL passthrough to write a query using DB2 syntax (<code>FETCH FIRST 10 ROWS ONLY</code>), although this requires you to store all your data in the database, at least temporarily.</p>

<p>Passthrough looks something like this:</p>

<pre><code>proc sql;
    connect to db2 (user=&amp;userid. password=&amp;userpw.  database=MY_DB);

    create table test as
    select * from connection to db2 (
        select * from schema.HUGE_TABLE
        order by n
        FETCH FIRST 10 ROWS ONLY
    );
quit;
</code></pre>

<p>It requires more syntax and can't access your sas datasets, so if <code>outobs</code> works for you, I would recommend that.</p>
"
"<p>How can I update table's column in a trigger after update on the same table?
<br />Here's the trigger:</p>

<pre>
<code>
CREATE TRIGGER upd_total_votes AFTER UPDATE ON products_score
FOR EACH ROW
    UPDATE
        products_score 
    SET
        products_score.votes_total =
            (SELECT
                 (votes_1 + votes_2 + votes_3 + votes_4 + votes_5)
             FROM
                 products_score
             WHERE
                 id = new.id)
</code>
</pre>

<p>Now when I update the table like</p>

<pre>
<code>
UPDATE products_score SET votes_1 = 5 WHERE id = 0
</code>
</pre>

<p>this doesn't work, as I get the following:</p>

<pre>#1442 - Can't update table 'products_score' in stored function/trigger because it is already used by statement which invoked this stored function/trigger.</pre>

<p>So how on earth I can get this to work?</p>
","<p>MySQL triggers can't manipulate the table they are assigned to. All other major DBMS support this feature so hopefully MySQL will add this support soon.</p>

<p><a href=""http://forums.mysql.com/read.php?99,122354,240978#msg-240978"" rel=""noreferrer"">http://forums.mysql.com/read.php?99,122354,240978#msg-240978</a></p>
"
"<p>I'm running R on a Windows machine which is directly linked to a PostgreSQL database. I'm not using RODBC. My database is encoded in UTF-8 as confirmed by the following R command:</p>

<pre><code>dbGetQuery(con, ""SHOW CLIENT_ENCODING"")
#   client_encoding
# 1            UTF8
</code></pre>

<p>However, when some text is read into R, it displays as strange text in R.  </p>

<p>For example, the following text is shown in my PostgreSQL database:
""Stphane""</p>

<p>After exporting to R it's shown as:
""Stphane""
(the <strong></strong> is encoded as <strong></strong>)  </p>

<p>When importing to R I use the <code>dbConnect</code> command to establish a connection and the <code>dbGetQuery</code> command to query data using SQL.  I do not specify any text encoding anywhere when connecting to the database or when running a query.</p>

<p>I've searched online and can't find a direct resolution to my issue.  I found <a href=""https://stackoverflow.com/questions/7167298/rodbc-character-encoding-error-with-postgresql"">this link</a>, but their issue is with RODBC, which I'm not using.</p>

<p><a href=""http://www.i18nqa.com/debug/utf8-debug.html"" rel=""nofollow noreferrer"">This link</a> is helpful in identifying the symbols, but I don't just want to do a find &amp; replace in R... way too much data.</p>

<p>I did try running the following commands below and I arrived at a warning.</p>

<pre><code>Sys.setlocale(""LC_ALL"", ""en_US.UTF-8"")
# [1] """"
# Warning message:
# In Sys.setlocale(""LC_ALL"", ""en_US.UTF-8"") :
#   OS reports request to set locale to ""en_US.UTF-8"" cannot be honored
Sys.setenv(LANG=""en_US.UTF-8"")
Sys.setenv(LC_CTYPE=""UTF-8"")
</code></pre>

<p>The warning occurs on the <code>Sys.setlocale(""LC_ALL"", ""en_US.UTF-8"")</code> command.  My intuition is that this is a Windows specific issue and doesn't occur with Mac/Linux/Unix.</p>
","<p>Followup info because I just had the same problem.<br>
Installing <code>yum install postgresql-devel</code> resolves the error too.</p>
"
"<p>when i run my shark queries, the memory gets hoarded in the main memory
This is my top command result.</p>

<hr>

<p>Mem:  74237344k total, 70080492k used,  4156852k free,   399544k buffers
Swap:  4194288k total,      480k used,  4193808k free, 65965904k cached</p>

<hr>

<p>this doesn't change even if i kill/stop shark,spark, hadoop processes.
Right now, the only way to clear the cache is to reboot the machine.</p>

<p>has anyone faced this issue before? is it some configuration problem  or a known issue in spark/shark?</p>
","<p>I will share a few thoughts based on my experience. But, if possible for you, please let us know about your use-case. It'll help us in answering your queries in a better manner.</p>

<p>1- If you are going to have more writes than reads, Cassandra is obviously a good choice. Having said that, if you are coming from SQL background and planning to use Cassandra then you'll definitely find CQL very helpful. But if you need to perform operations like JOIN and GROUP BY, even though CQL solves primitive GROUP BY use cases through write time and compact time sorts and implements one-to-many relationships, CQL is not the answer.</p>

<p>2- Spark SQL (Formerly Shark) is very fast for the two reasons, in-memory processing and planning data pipelines. In-memory processing makes it ~100x faster than Hive. Like Hive, Spark SQL handles larger than memory data types very well and up to 10x faster thanks to planned pipelines. Situation shifts to Spark SQL benefit when multiple data pipelines like filter and groupBy are present. Go for it when you need ad-hoc real time querying. Not suitable when you need long running jobs over gigantic amounts of data.</p>

<p>3- Hive is basically a warehouse that runs on top of your existing Hadoop cluster and provides you SQL like interface to handle your data. But Hive is not suitable for real-time needs. It is best suited for offline batch processing. Doesn't need any additional infra as it uses underlying HDFS for data storage. Go for it when you have to perform operations like JOIN, GROUP BY etc on large dataset and for OLAP.</p>

<p><code>Note :</code> Spark SQL emulates Apache Hive behavior on top of Spark, so it supports virtually all Hive features but potentially faster. It supports the existing Hive Query language, Hive data formats (SerDes), user-defined functions (UDFs), and queries that call external scripts.</p>

<p>But I think you will be able to evaluate the pros and cons of all these tools properly only after getting your hands dirty. I could just suggest based on your questions.</p>

<p>Hope this answers some of your queries.</p>

<p>P.S. : The above answer is based on solely my experience. Comments/corrections are welcome.</p>
"
"<p>I've never really understood the difference between these two indexes, can someone please explain what the difference is (performance-wise, how the index structure will look like in db, storage-wise etc)?</p>

<p>I understand this question is broad, please bear with me on this. I don't really know how to scope it down. Perhaps if you guys start explaining your know-hows I'll get pointers in the right direction enabling me to make the question more narrow?</p>

<p><strong>Included index</strong></p>

<pre><code>CREATE NONCLUSTERED INDEX IX_Address_PostalCode  
ON Person.Address (PostalCode) 
INCLUDE (AddressLine1, AddressLine2, City, StateProvinceID); 
</code></pre>

<p><strong>'Normal' index</strong></p>

<pre><code>CREATE NONCLUSTERED INDEX IX_Address_PostalCode  
ON Person.Address (PostalCode, AddressLine1, AddressLine2, City, StateProvinceID);
</code></pre>
","<p>You can see some reports in SSMS:</p>

<p>Right-click the instance name / reports / standard / top sessions</p>

<p>You can see top CPU consuming sessions.  This may shed some light on what SQL processes are using resources.  There are a few other CPU related reports if you look around.  I was going to point to some more DMVs but if you've looked into that already I'll skip it.</p>

<p>You can use <a href=""http://www.brentozar.com/archive/2014/05/introducing-sp_blitzcache/"" rel=""nofollow noreferrer"">sp_BlitzCache</a> to find the top CPU consuming queries.  You can also sort by IO and other things as well.  This is using DMV info which accumulates between restarts.</p>

<p><a href=""http://www.mssqltips.com/sqlservertip/2454/how-to-find-out-how-much-cpu-a-sql-server-process-is-really-using/"" rel=""nofollow noreferrer"">This article</a> looks promising.</p>

<p>Some <a href=""https://stackoverflow.com/questions/28952/cpu-utilization-by-database"">stackoverflow</a> goodness from Mr. Ozar.</p>
"
"<p>I've already looked into <a href=""https://stackoverflow.com/questions/11506224/connection-for-controluser-as-defined-in-your-configuration-failed-phpmyadmin-xa"">this</a> but since I am running on Ubuntu 10.04 instead of XAMPP and have already created the phpmyadmin database and I can log in through terminal with both the root and phpmyadmin users.</p>

<p>How I installed:
sudo apt-get install lamp-server^
sudo apt-get install phpmyadmin</p>

<p>I can login locally on terminal using both the root and phpmyadmin users.
I can view the phpmyadmin login page.
A phpinfo.php page I posted to server works.</p>

<ul>
<li>Ubuntu Version 10.04.4 Lucid Lynx </li>
<li>Apache   2.2.14</li>
<li>MySQL    5.1.73</li>
<li>PHP  5.3.6</li>
<li>Phpmyadmin   3.3.2</li>
</ul>
","<p>As stated in <a href=""http://dev.mysql.com/doc/refman/5.1/en/condition-handling.html"" rel=""noreferrer"">the manual</a> for MySQL version 5.1:</p>

<blockquote>
  <p>Other statements related to conditions are <code>SIGNAL</code>, <code>RESIGNAL</code>, and <code>GET DIAGNOSTICS</code>. The <code>SIGNAL</code> and <code>RESIGNAL</code> statements are not supported until MySQL 5.5. The <code>GET DIAGNOSTICS</code> statement is not supported until MySQL 5.6.</p>
</blockquote>

<p>To raise an error in older versions of MySQL, just deliberately issue an erroneous command.  I often <code>CALL</code> a non-existent procedure, for example:</p>

<pre><code>CALL raise_error;
</code></pre>
"
"<h1>Final solution:</h1>

<p>The connection was added to the connection pool. So I closed it, but it still remained physically open. With the ConnectionString Parameter ""Pooling=false"" or the static methods MySqlConnection.ClearPool(connection) and MySqlConnection.ClearAllPools the problem can be avoided. Note that the problem was, that the connection was still alive when I closed the application. Even though I closed it. So either I don't use connection pooling at all or I clear the specific pool before closing the connection and the problem is solved. I'll take my time figuring out what's the best solution in my case.</p>

<p>Thanks to all who answered! It helped my understand the concepts of C# better and I learned alot from the useful input. :)</p>

<p>===</p>

<h1>Original Problem:</h1>

<p>I've searched for a while now and haven't found the solution for my problem:
I'm new to C# and try to write a class to make MySql Connections easier. My problem is, after I open a connection and close it. It is still open in the Database and gets aborted.</p>

<p>I'm using the 'using' statement' of course, but the connection is still open and gets aborted after I exit the program.</p>

<p>Here's what my code looks like:</p>

<pre><code>using (DatabaseManager db = new DatabaseManager())
{
using (MySqlDataReader result = db.DataReader(""SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC""))
{
    foreach (MySqlDataReader result in db.DataReader(""SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC""))
    {
        //Do stuff here
    }
}
}
</code></pre>

<p>The class Database manager opens the connection and closes it when disposed:</p>

<pre><code>public DatabaseManager()
{
    this.connectionString = new MySqlConnectionStringBuilder(""Server=localhost;Database=businessplan;Uid=root;"");
    connect();
}
private bool connect()
{
    bool returnValue = true;
    connection = new MySqlConnection(connectionString.GetConnectionString(false));
    connection.Open();
}

public void Dispose()
{
    Dispose(true);
}

public void Dispose(bool disposing)
{
    if (disposing)
    {
        if (connection.State == System.Data.ConnectionState.Open)
        {
            connection.Close();
            connection.Dispose();
        }
    }
    //GC.SuppressFinalize(this);//Updated
}
//Updated
//~DatabaseManager()
//{
//  Dispose(false);
//}
</code></pre>

<p>So, I checked it in the debugger and the Dispose()-method is called and executes correctly.
What am I missing? Is there something I did wrong or misunderstood?</p>

<p>Any help is appreciated!</p>

<p>Greetings,
Simon</p>

<p>P.S.:
Just in case, the DataReader()-method (Updated version):</p>

<pre><code>public IEnumerable&lt;IDataReader&gt; DataReader(String query)
    {
        using (MySqlCommand com = new MySqlCommand())
        {
            com.Connection = connection;
            com.CommandText = query;
            using (MySqlDataReader result = com.ExecuteReader(System.Data.CommandBehavior.CloseConnection))
            {
                while (result.Read())
                {
                    yield return (IDataReader)result;
                }
            }
        }
    }
</code></pre>

<hr>

<p>Ok, I tried to use the yield return:</p>

<pre><code>foreach (MySqlDataReader result in db.DataReader(""SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC""))
{
    //...
}
</code></pre>

<p>And I changed the DataReader-method:</p>

<pre><code>public IEnumerable&lt;IDataReader&gt; DataReader(String query)
    {
        using (MySqlCommand com = new MySqlCommand())
        {
            com.Connection = connection;
            com.CommandText = query;
            using (MySqlDataReader result = com.ExecuteReader())
            {
                while (result.Read())
                {
                    yield return (IDataReader)result;
                }
            }
        }
    }
</code></pre>

<p>It works in the way that I can retrieve the data, yet I still have the same problem: The connection isn't closed properly. :(</p>
","<p>Im unsure about mysqlconnection but the sql server counter part uses Connection pooling and does not close when you call close instead it puts it in the connection pool!</p>

<p>Edit: Make sure you dispose the Reader, Command, and Connection object!</p>

<p>Edit: Solved with the ConnectionString Parameter ""Pooling=false"" or the static methods MySqlConnection.ClearPool(connection) and MySqlConnection.ClearAllPools()</p>
"
"<p>I need an SQL guru to help me speed up my query.</p>

<p>I have 2 tables, quantities and prices. quantities records a quantity value between 2 timestamps, 15 minutes apart. prices records a price for a given timestamp, for a given price type and there is a price 5 record for every 5 minutes.</p>

<p>I need 2 work out the total price for each period, e.g. hour or day, between two timestamps. This is calculated by the sum of the (quantity multiplied by the average of the 3 prices in the 15 minute quantity window) in each period.</p>

<p>For example, let's say I want to see the total price each hour for 1 day. The total price value in each row in the result set is the sum of the total prices for each of the four 15 minute periods in that hour. And the total price for each 15 minute period is calculated by multiplying the quantity value in that period by the average of the 3 prices (one for each 5 minutes) in that quantity's period.</p>

<p>Here's the query I'm using, and the results:</p>

<pre><code>SELECT
MIN( `quantities`.`start_timestamp` ) AS `start`,
MAX( `quantities`.`end_timestamp` ) AS `end`,
SUM( `quantities`.`quantity` * (
  SELECT AVG( `prices`.`price` )
  FROM `prices`
  WHERE `prices`.`timestamp` &gt;= `quantities`.`start_timestamp`
  AND `prices`.`timestamp` &lt; `quantities`.`end_timestamp`
  AND `prices`.`type_id` = 1
) ) AS total
FROM `quantities`
WHERE `quantities`.`start_timestamp` &gt;= '2010-07-01 00:00:00'
AND `quantities`.`start_timestamp` &lt; '2010-07-02 00:00:00'
GROUP BY HOUR(  `quantities`.`start_timestamp` );

+---------------------+---------------------+----------+
| start               | end                 | total    |
+---------------------+---------------------+----------+
| 2010-07-01 00:00:00 | 2010-07-01 01:00:00 | 0.677733 |
| 2010-07-01 01:00:00 | 2010-07-01 02:00:00 | 0.749133 |
| 2010-07-01 02:00:00 | 2010-07-01 03:00:00 | 0.835467 |
| 2010-07-01 03:00:00 | 2010-07-01 04:00:00 | 0.692233 |
| 2010-07-01 04:00:00 | 2010-07-01 05:00:00 | 0.389533 |
| 2010-07-01 05:00:00 | 2010-07-01 06:00:00 | 0.335300 |
| 2010-07-01 06:00:00 | 2010-07-01 07:00:00 | 1.231467 |
| 2010-07-01 07:00:00 | 2010-07-01 08:00:00 | 0.352800 |
| 2010-07-01 08:00:00 | 2010-07-01 09:00:00 | 1.447200 |
| 2010-07-01 09:00:00 | 2010-07-01 10:00:00 | 0.756733 |
| 2010-07-01 10:00:00 | 2010-07-01 11:00:00 | 0.599467 |
| 2010-07-01 11:00:00 | 2010-07-01 12:00:00 | 1.056467 |
| 2010-07-01 12:00:00 | 2010-07-01 13:00:00 | 1.252600 |
| 2010-07-01 13:00:00 | 2010-07-01 14:00:00 | 1.285567 |
| 2010-07-01 14:00:00 | 2010-07-01 15:00:00 | 0.442933 |
| 2010-07-01 15:00:00 | 2010-07-01 16:00:00 | 0.692567 |
| 2010-07-01 16:00:00 | 2010-07-01 17:00:00 | 1.281067 |
| 2010-07-01 17:00:00 | 2010-07-01 18:00:00 | 0.652033 |
| 2010-07-01 18:00:00 | 2010-07-01 19:00:00 | 1.721900 |
| 2010-07-01 19:00:00 | 2010-07-01 20:00:00 | 1.362400 |
| 2010-07-01 20:00:00 | 2010-07-01 21:00:00 | 1.099300 |
| 2010-07-01 21:00:00 | 2010-07-01 22:00:00 | 0.646267 |
| 2010-07-01 22:00:00 | 2010-07-01 23:00:00 | 0.873100 |
| 2010-07-01 23:00:00 | 2010-07-02 00:00:00 | 0.546533 |
+---------------------+---------------------+----------+
24 rows in set (5.16 sec)
</code></pre>

<p>I need the query to run a lot faster than this, and would have though it would be possible. Here's the results from EXPLAIN EXTENDED ...</p>

<pre><code>+----+--------------------+------------+-------+-------------------+-----------------+---------+-------+-------+----------------------------------------------+
| id | select_type        | table      | type  | possible_keys     | key             | key_len | ref   | rows  | Extra                                        |
+----+--------------------+------------+-------+-------------------+-----------------+---------+-------+-------+----------------------------------------------+
|  1 | PRIMARY            | quantities | range | start_timestamp   | start_timestamp | 8       | NULL  |    89 | Using where; Using temporary; Using filesort |
|  2 | DEPENDENT SUBQUERY | prices     | ref   | timestamp,type_id | type_id         | 4       | const | 22930 | Using where                                  |
+----+--------------------+------------+-------+-------------------+-----------------+---------+-------+-------+----------------------------------------------+
2 rows in set, 3 warnings (0.00 sec)
</code></pre>

<p>I noticed the dependent sub query is not using the timestamp field in the key and the query is scanning loads of rows.</p>

<p>Can anyone help me get this running a hell of a lot faster?</p>

<p>Here are the SQL statements required to create the schema and fill it with a lot of data (2 months worth)</p>

<pre><code># Create prices table

CREATE TABLE `prices` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `timestamp` datetime NOT NULL,
  `type_id` int(11) NOT NULL,
  `price` float(8,2) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `timestamp` (`timestamp`),
  KEY `type_id` (`type_id`)
) ENGINE=MyISAM;

# Create quantities table

CREATE TABLE `quantities` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `start_timestamp` datetime NOT NULL,
  `end_timestamp` datetime NOT NULL,
  `quantity` float(7,2) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `start_timestamp` (`start_timestamp`),
  KEY `end_timestamp` (`end_timestamp`)
) ENGINE=MyISAM;

# Insert first 2 rows into prices, one for each of 2 types, starting 64 days ago

INSERT INTO `prices` (`id`, `timestamp`, `type_id`, `price`) VALUES
(NULL, DATE_SUB(CURDATE(), INTERVAL 64 DAY), '1', RAND()),
(NULL, DATE_SUB(CURDATE(), INTERVAL 64 DAY), '2', RAND());

# Fill the prices table with a record for each type, for every 5 minutes, for the next 64 days

INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 32 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 16 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 8 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 4 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 2 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 1 DAY), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 12 HOUR), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 6 HOUR), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 3 HOUR), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 90 MINUTE), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 45 MINUTE), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 20 MINUTE), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 10 MINUTE), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_ADD(`timestamp`, INTERVAL 5 MINUTE), `type_id`, RAND() FROM prices;
INSERT INTO prices (`timestamp`, `type_id`, `price`) SELECT DATE_SUB(`timestamp`, INTERVAL 5 MINUTE), `type_id`, RAND() FROM prices WHERE MOD( (TIME_TO_SEC( `timestamp`) - TIME_TO_SEC(CONCAT(DATE_SUB(CURDATE(), INTERVAL 64 DAY), ' 00:00:00')) ), 45 *60 ) = 0 AND `timestamp` &gt; CONCAT(DATE_SUB(CURDATE(), INTERVAL 64 DAY), ' 00:00:00');

# Insert first row into quantities, start timestamp is 64 days ago, end timestamp is start timestamp plus 15 minutes

INSERT INTO `quantities` (`id`, `start_timestamp`, `end_timestamp`, `quantity`) VALUES (NULL, DATE_SUB(CURDATE(), INTERVAL 64 DAY), DATE_SUB(CURDATE(), INTERVAL '63 23:45' DAY_MINUTE), RAND());

# Fill the quantities table with a record for each 15 minute period for the next 64 days

INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 32 DAY), DATE_ADD(`end_timestamp`, INTERVAL 32 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 16 DAY), DATE_ADD(`end_timestamp`, INTERVAL 16 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 8 DAY), DATE_ADD(`end_timestamp`, INTERVAL 8 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 4 DAY), DATE_ADD(`end_timestamp`, INTERVAL 4 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 2 DAY), DATE_ADD(`end_timestamp`, INTERVAL 2 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 1 DAY), DATE_ADD(`end_timestamp`, INTERVAL 1 DAY), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 12 HOUR), DATE_ADD(`end_timestamp`, INTERVAL 12 HOUR), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 6 HOUR), DATE_ADD(`end_timestamp`, INTERVAL 6 HOUR), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 3 HOUR), DATE_ADD(`end_timestamp`, INTERVAL 3 HOUR), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 90 MINUTE), DATE_ADD(`end_timestamp`, INTERVAL 90 MINUTE), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 45 MINUTE), DATE_ADD(`end_timestamp`, INTERVAL 45 MINUTE), RAND() FROM quantities;
INSERT INTO `quantities` (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_ADD(`start_timestamp`, INTERVAL 15 MINUTE), DATE_ADD(`end_timestamp`, INTERVAL 15 MINUTE), RAND() FROM quantities;
INSERT INTO quantities (`start_timestamp`, `end_timestamp`, `quantity`) SELECT DATE_SUB(`start_timestamp`, INTERVAL 15 MINUTE), DATE_SUB(`end_timestamp`, INTERVAL 15 MINUTE), RAND() FROM quantities WHERE MOD( (TIME_TO_SEC( `start_timestamp`) - TIME_TO_SEC(CONCAT(DATE_SUB(CURDATE(), INTERVAL 64 DAY), ' 00:00:00')) ), 45 * 60 ) = 0 AND `start_timestamp` &gt; CONCAT(DATE_SUB(CURDATE(), INTERVAL 64 DAY), ' 00:00:00');
</code></pre>
","<p>Here is my first attempt. 
This one is dirty and uses the following properties on data:</p>

<ul>
<li>there are three 5 minute prices for each quarter in quantities (if this is violated in data the query will not work) </li>
<li>notice for each and cardinality of three, this is not guaranteed by data integrity checks so therefore I call it dirty</li>
<li>it is also not flexible to changes in periods</li>
</ul>

<p>Query 1:</p>

<pre><code>SELECT sql_no_cache
    min(q.start_timestamp) as start,  
    max(q.end_timestamp) as end, 
    sum((p1.price + p2.price + p3.price)/3*q.quantity) as total 
FROM 
    quantities q join 
    prices p1 on q.start_timestamp = p1.timestamp and p1.type_id = 1 join 
    prices p2 on p2.timestamp = adddate(q.start_timestamp, interval 5 minute) and p2.type_id = 1 join 
    prices p3 on p3.timestamp = adddate(q.start_timestamp, interval 10 minute) and p3.type_id = 1 
WHERE 
    q.start_timestamp between '2010-07-01 00:00:00' and '2010-07-01 23:59:59' 
GROUP BY hour(q.start_timestamp);
</code></pre>

<p>This one returns results in 0.01 sec on my slow testing machine, where original query runs in ~6 sec, and gnarf's query in ~0.85 sec (all queries always tested with <code>SQL_NO_CACHE</code> keyword which does not reuse the results, but on a warm database).</p>

<p>EDIT:
Here is a version that is not sensitive to missing rows on the price side
Query 1a</p>

<pre><code>SELECT sql_no_cache
    min(q.start_timestamp) as start,  
    max(q.end_timestamp) as end, 
    sum( ( COALESCE(p1.price,0) + COALESCE(p2.price,0) + COALESCE(p3.price,0) ) / ( 
         3 -
         COALESCE(p1.price-p1.price,1) - 
         COALESCE(p2.price-p2.price,1) - 
         COALESCE(p3.price-p3.price,1)
        )
       *q.quantity) as total 
FROM 
    quantities q LEFT JOIN 
    prices p1 on q.start_timestamp = p1.timestamp and p1.type_id = 1 LEFT JOIN
    prices p2 on p2.timestamp = adddate(q.start_timestamp, interval 5 minute) and p2.type_id = 1 LEFT JOIN
    prices p3 on p3.timestamp = adddate(q.start_timestamp, interval 10 minute) and p3.type_id = 1 
WHERE 
    q.start_timestamp between '2010-07-01 00:00:00' and '2010-07-01 23:59:59' 
GROUP BY hour(q.start_timestamp);
</code></pre>

<p>EDIT2:
Query 2:
Here is a direct improvement, and different approach, to your query with minimal changes that brings the execuction time to ~0.22 sec on my machine</p>

<pre><code>SELECT sql_no_cache
MIN( `quantities`.`start_timestamp` ) AS `start`,
MAX( `quantities`.`end_timestamp` ) AS `end`,
SUM( `quantities`.`quantity` * (
  SELECT AVG( `prices`.`price` )
  FROM `prices`
  WHERE 
    `prices`.`timestamp` &gt;= '2010-07-01 00:00:00' 
    AND `prices`.`timestamp` &lt; '2010-07-02 00:00:00' 
    AND `prices`.`timestamp` &gt;= `quantities`.`start_timestamp`
    AND `prices`.`timestamp` &lt; `quantities`.`end_timestamp`
    AND `prices`.`type_id` = 1
) ) AS total
FROM `quantities`
WHERE `quantities`.`start_timestamp` &gt;= '2010-07-01 00:00:00'
AND `quantities`.`start_timestamp` &lt; '2010-07-02 00:00:00'
GROUP BY HOUR(  `quantities`.`start_timestamp` );
</code></pre>

<p>That is mysql 5.1, I think I have read that in 5.5 this kind of thing (merging indexes) will be available to the query planner. Also, if you could make your start_timestamp and timestamp be related through foreign key that should allow these kind of correlated queries to make use of indexes (but for this you would need to modify design and establish some sort of timeline table which could then be referenced by quantities and prices both).</p>

<p>Query 3:
Finally, the last version which does it in ~0.03 sec, but should be as robust and flexible as Query 2</p>

<pre><code>SELECT sql_no_cache
MIN(start),
MAX(end),
SUM(subtotal)
FROM 
(
SELECT sql_no_cache
q.`start_timestamp` AS `start`,
q.`end_timestamp` AS `end`,
AVG(p.`price` * q.`quantity`) AS `subtotal`
FROM `quantities` q
LEFT JOIN `prices` p ON p.timestamp &gt;= q.start_timestamp AND 
                        p.timestamp &lt; q.end_timestamp AND
                        p.timestamp &gt;= '2010-07-01 00:00:00' AND 
                        p.`timestamp` &lt; '2010-07-02 00:00:00' 
WHERE q.`start_timestamp` &gt;= '2010-07-01 00:00:00' 
AND q.`start_timestamp` &lt; '2010-07-02 00:00:00'
AND p.type_id = 1
GROUP BY q.`start_timestamp`
) forced_tmp
GROUP BY hour( start );
</code></pre>

<p><strong>NOTE:</strong> Do not forget to remove <strong>sql_no_cache</strong> keywords in production.</p>

<p>There are many counter intuitive tricks applied in the above queries (sometimes conditions repeated in the join condition speed up queries, sometimes they slow them down). Mysql is great little RDBMS and really fast when it comes to relatively simple queries, but when the complexity increases it is easy to run into the above scenarios. </p>

<p>So in general, I apply the following principle to set my expectations regarding the performance of a query: </p>

<ul>
<li>if the base result set has &lt; 1,000 rows then query should do its business in ~0.01 sec (base result set is the number of rows that functionally determine resulting set)</li>
</ul>

<p>In this particular case you start with less then 1000 rows (all the prices and quantities in one day, with 15 minutes precision) and from that you should be able to compute the final results.</p>
"
"<p>I've got a list of objects and I've got a db table full of records.  My list of objects has a title attribute and I want to remove any objects with duplicate titles from the list (leaving the original).  </p>

<p>Then I want to check if my list of objects has any duplicates of any records in the database and if so, remove those items from list before adding them to the database.</p>

<p>I have seen solutions for removing duplicates from a list like this: <code>myList = list(set(myList))</code>, but i'm not sure how to do that with a list of objects?  </p>

<p>I need to maintain the order of my list of objects too.  I was also thinking maybe I could use <code>difflib</code> to check for differences in the titles.</p>
","<p>The <code>set(list_of_objects)</code> will only remove the duplicates if you know what a duplicate is, that is, you'll need to define a uniqueness of an object.</p>

<p>In order to do that, you'll need to make the object hashable. You need to define both <code>__hash__</code> and <code>__eq__</code> method, here is how:</p>

<p><a href=""http://docs.python.org/glossary.html#term-hashable"" rel=""noreferrer"">http://docs.python.org/glossary.html#term-hashable</a></p>

<p>Though, you'll probably only need to define <code>__eq__</code> method.</p>

<p><strong>EDIT</strong>: How to implement the <code>__eq__</code> method:</p>

<p>You'll need to know, as I mentioned, the uniqueness definition of your object. Supposed we have a Book with attributes author_name and title that their combination is unique, (so, we can have many books Stephen King authored, and many books named The Shining, but only one book named The Shining by Stephen King), then the implementation is as follows:</p>

<pre><code>def __eq__(self, other):
    return self.author_name==other.author_name\
           and self.title==other.title
</code></pre>

<p>Similarly, this is how I sometimes implement the <code>__hash__</code> method:</p>

<pre><code>def __hash__(self):
    return hash(('title', self.title,
                 'author_name', self.author_name))
</code></pre>

<p>You can check that if you create a list of 2 books with same author and title, the book objects will <s>be the same (with <code>is</code> operator) and</s> equal (with <code>==</code> operator). Also, when <code>set()</code> is used, it will remove one book.</p>

<p><strong>EDIT</strong>: This is one old anwser of mine, but I only now notice that it has the error which is corrected with strikethrough in the last paragraph: objects with the same <code>hash()</code> won't give <code>True</code> when compared with <code>is</code>. Hashability of object is used, however, if you intend to use them as elements of set, or as keys in dictionary.</p>
"
"<p>I just downloaded the developer edition of SQL Anywhere. How can I get a list of tables in the database I'm connected to?. Also for a particular table, how do I get the meta-data for that table (column names, types, etc)?</p>
","<p>You should use</p>

<pre><code>ini_set(""memory_limit"",-1);
</code></pre>

<p>and not ""memory_set"". Also, look out for this: <a href=""https://stackoverflow.com/a/5263981/2729140"">https://stackoverflow.com/a/5263981/2729140</a> (Suhosin extension has it's own memory limit setting).</p>

<p>If your script needs a lot of memory, then you should try and revise it. One thing to be aware of are unlimited SQL queries. </p>

<p>Your tables can have A LOT of records, so it's wise to always limit your queries. If you need to fetch all the records from the table, then you should do it by pages, using <code>LIMIT ... OFFSET</code> SQL constructs.</p>
"
"<p>I have two tables Employee and Department following are the entity classes for both of them</p>

<pre><code>Department.java
@Entity
@Table(name = ""DEPARTMENT"")
public class Department {
    @Id
    @Column(name = ""DEPARTMENT_ID"")
    @GeneratedValue(strategy = GenerationType.AUTO)
    private Integer departmentId;
    @Column(name = ""DEPARTMENT_NAME"")
    private String departmentName;
    @Column(name = ""LOCATION"")
    private String location;

    @OneToMany(cascade = CascadeType.ALL, mappedBy = ""department"", orphanRemoval = true)
    @Fetch(FetchMode.SUBSELECT)
    //@Fetch(FetchMode.JOIN)
    private List&lt;Employee&gt; employees = new ArrayList&lt;&gt;();
}


Employee.java
@Entity
@Table(name = ""EMPLOYEE"")
public class Employee {
    @Id
    @SequenceGenerator(name = ""emp_seq"", sequenceName = ""seq_employee"")
    @GeneratedValue(generator = ""emp_seq"")
    @Column(name = ""EMPLOYEE_ID"")
    private Integer employeeId;
    @Column(name = ""EMPLOYEE_NAME"")
    private String employeeName;

    @ManyToOne
    @JoinColumn(name = ""DEPARTMENT_ID"")
    private Department department;
}
</code></pre>

<p>Below are the queries fired when I did <code>em.find(Department.class, 1);</code></p>

<p><strong>-- fetch mode = fetchmode.join</strong></p>

<pre><code>    SELECT department0_.DEPARTMENT_ID AS DEPARTMENT_ID1_0_0_,
      department0_.DEPARTMENT_NAME    AS DEPARTMENT_NAME2_0_0_,
      department0_.LOCATION           AS LOCATION3_0_0_,
      employees1_.DEPARTMENT_ID       AS DEPARTMENT_ID3_1_1_,
      employees1_.EMPLOYEE_ID         AS EMPLOYEE_ID1_1_1_,
      employees1_.EMPLOYEE_ID         AS EMPLOYEE_ID1_1_2_,
      employees1_.DEPARTMENT_ID       AS DEPARTMENT_ID3_1_2_,
      employees1_.EMPLOYEE_NAME       AS EMPLOYEE_NAME2_1_2_
    FROM DEPARTMENT department0_
    LEFT OUTER JOIN EMPLOYEE employees1_
    ON department0_.DEPARTMENT_ID   =employees1_.DEPARTMENT_ID
    WHERE department0_.DEPARTMENT_ID=?
</code></pre>

<p><strong>-- fetch mode = fetchmode.subselect</strong></p>

<pre><code>    SELECT department0_.DEPARTMENT_ID AS DEPARTMENT_ID1_0_0_,
      department0_.DEPARTMENT_NAME    AS DEPARTMENT_NAME2_0_0_,
      department0_.LOCATION           AS LOCATION3_0_0_
    FROM DEPARTMENT department0_
    WHERE department0_.DEPARTMENT_ID=?

    SELECT employees0_.DEPARTMENT_ID AS DEPARTMENT_ID3_1_0_,
      employees0_.EMPLOYEE_ID        AS EMPLOYEE_ID1_1_0_,
      employees0_.EMPLOYEE_ID        AS EMPLOYEE_ID1_1_1_,
      employees0_.DEPARTMENT_ID      AS DEPARTMENT_ID3_1_1_,
      employees0_.EMPLOYEE_NAME      AS EMPLOYEE_NAME2_1_1_
    FROM EMPLOYEE employees0_
    WHERE employees0_.DEPARTMENT_ID=?
</code></pre>

<p>I just wanted to know which one should we prefer <code>FetchMode.JOIN</code> or <code>FetchMode.SUBSELECT</code>? which one should we opt in which scenario?</p>
","<p>The SUBQUERY strategy that Marmite refers to is related to FetchMode.SELECT, not SUBSELECT.</p>

<p>The console output that you've posted about <strong>fetchmode.subselect</strong> is curious because this is not the way that is supposed to work.</p>

<p>The <a href=""https://docs.jboss.org/hibernate/annotations/3.5/api/org/hibernate/annotations/FetchMode.html#SUBSELECT"" rel=""noreferrer"">FetchMode.SUBSELECT</a></p>

<blockquote>
  <p>use a subselect query to load the additional collections</p>
</blockquote>

<p>Hibernate <a href=""http://docs.jboss.org/hibernate/orm/4.3/manual/en-US/html_single/#performance-fetching-subselect"" rel=""noreferrer"">docs</a>:</p>

<blockquote>
  <p>If one lazy collection or single-valued proxy has to be fetched, Hibernate will load all of them, re-running the original query in a subselect. This works in the same way as batch-fetching but without the piecemeal loading.</p>
</blockquote>

<p>FetchMode.SUBSELECT should look something like this: </p>

<pre><code>SELECT &lt;employees columns&gt;
FROM EMPLOYEE employees0_
WHERE employees0_.DEPARTMENT_ID IN
(SELECT department0_.DEPARTMENT_ID FROM DEPARTMENT department0_)
</code></pre>

<p>You can see that this second query will bring to memory <strong>all</strong> the employees that belongs to some departament (i.e. employee.department_id is not null), it doesn't matter if it is not the department that you retrieve in your first query.
So this is potentially a major issue if the table of employees is large because it may be <a href=""http://www.christophbrill.de/de_DE/hibernate-fetch-subselect-performance/"" rel=""noreferrer"">accidentially loading a whole database into memory</a>.</p>

<p>However, FetchMode.SUBSELECT reduces significatly the number of queries because takes only two queries in comparisson to the N+1 queries of the FecthMode.SELECT.</p>

<p>You may be thinking that FetchMode.JOIN makes even less queries, just 1, so why use SUBSELECT at all? Well, it's true but at the cost of duplicated data and a heavier response.</p>

<p>If a single-valued proxy has to be fetched with JOIN, the query may retrieve:</p>

<pre><code>+---------------+---------+-----------+
| DEPARTMENT_ID | BOSS_ID | BOSS_NAME |
+---------------+---------+-----------+
|             1 |       1 | GABRIEL   |
|             2 |       1 | GABRIEL   |
|             3 |       2 | ALEJANDRO |
+---------------+---------+-----------+
</code></pre>

<p>The employee data of the boss is duplicated if he directs more than one department and it has a cost in bandwith.</p>

<p>If a lazy collection has to be fetched with JOIN, the query may retrieve:</p>

<pre><code>+---------------+---------------+-------------+
| DEPARTMENT_ID | DEPARTMENT_ID | EMPLOYEE_ID |
+---------------+---------------+-------------+
|             1 | Sales         | GABRIEL     |
|             1 | Sales         | ALEJANDRO   |
|             2 | RRHH          | DANILO      |
+---------------+---------------+-------------+
</code></pre>

<p>The department data is duplicated if it contains more than one employee (the natural case).
We don't only suffer a cost in bandwidth but also we get duplicate <a href=""https://howtoprogramwithjava.com/how-to-fix-duplicate-data-from-hibernate-queries/"" rel=""noreferrer"">duplicated Department objects</a> and we must use a SET or <a href=""https://docs.jboss.org/hibernate/orm/current/javadocs/org/hibernate/criterion/CriteriaSpecification.html#DISTINCT_ROOT_ENTITY"" rel=""noreferrer"">DISTINCT_ROOT_ENTITY</a> to de-duplicate.</p>

<p>However, duplicate data in pos of a lower latency is a good trade off in many cases, like Markus Winand <a href=""http://use-the-index-luke.com/sql/join/nested-loops-join-n1-problem"" rel=""noreferrer"">says</a>.</p>

<blockquote>
  <p>An SQL join is still more efficient than the nested selects approacheven though it performs the same index lookupsbecause it <strong>avoids a lot of network communication</strong>. It <strong>is even faster if the total amount of transferred data is bigger because of the duplication</strong> of employee attributes for each sale. That is because of the two dimensions of performance: response time and throughput; in computer networks we call them latency and bandwidth. Bandwidth has only a minor impact on the response time but <strong>latencies have a huge impact</strong>. That means that the number of database round trips is more important for the response time than the amount of data transferred.</p>
</blockquote>

<p>So, the main issue about using SUBSELECT is that is <a href=""https://stackoverflow.com/a/7239455/3517383"">hard to control</a> and may be loading a whole graph of entities into memory.
With Batch fetching you fetch the associated entity in a separate query as SUBSELECT (so you don't suffer duplicates), gradually and most important you query only related entities (so you don't suffer from potentially load a huge graph) because the IN subquery is filtered by the IDs retrieved by the outter query).</p>

<pre><code>Hibernate: 
    select ...
    from mkyong.stock stock0_

Hibernate: 
    select ...
    from mkyong.stock_daily_record stockdaily0_ 
    where
        stockdaily0_.STOCK_ID in (
            ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
        )
</code></pre>

<p>(It may be interesting test if Batch fetching with a very high batch size would act like a SUBSELECT but without the issue of load the whole table)</p>

<p>A couple of posts showing the different fetching strategies and the SQL logs (very important):</p>

<ul>
<li><a href=""http://www.mkyong.com/hibernate/hibernate-fetching-strategies-examples/"" rel=""noreferrer"">Hibernate  fetching strategies examples</a></li>
<li><a href=""http://www.solidsyntax.be/2013/10/17/fetching-collections-hibernate/"" rel=""noreferrer"">Hibernate FetchMode explained by example</a> </li>
<li><a href=""https://technicalmumbojumbo.wordpress.com/2011/08/24/investigating-hibernate-fetch-strategy-tutorial/"" rel=""noreferrer"">Investigating Hibernate fetch strategies  A tutorial</a></li>
</ul>

<p>Summary:</p>

<ul>
<li>JOIN: avoids the major issue of N+1 queries but it may retrieve data duplicated.</li>
<li>SUBSELECT: avoids N+1 too and doesn't duplicate data but it loads all the entities of the associated type into memory.</li>
</ul>

<p>The tables were built using <a href=""https://ozh.github.io/ascii-tables/"" rel=""noreferrer"">ascii-tables</a>.</p>
"
"<p>I created a web app that uses a MySQL database, but I have to migrate the database to <strong>Microsoft SQL Server 2008 R2</strong> and I'm using the <strong>SQL Server Migration Assistant</strong> (SSMA).</p>

<p>I'm getting errors in my report for some tables that use foreign keys.</p>

<h2>1. Self-referencing foreign keys</h2>

<p>I have one table that has a parent-child relationship between rows; <strong>map</strong> table:</p>

<pre><code>| map_id | map_title           | latitude  | longitude  | map_zoom | map_parent |
|:------:|:-------------------:|:---------:|:----------:|:--------:|:----------:|
| 1      | My Parent Map       | 50.364829 | -52.635623 | 17       | NULL       |
| 2      | Some Child Map      | 50.366916 | -52.634718 |          | 1          |
| 3      | Another Child Map   | 50.364898 | -52.634543 |          | 1          |
| 4      | My Last Example Map | 50.361986 | -52.638891 |          | 3          |
</code></pre>

<p>The report generated by SQL Server Migration Assistant (SSMA) shows the SQL that would be used to create a table in SQL Server.</p>

<p><strong>MySQL</strong> (source):</p>

<pre><code>1  CREATE
2      TABLE `map`
3          (
4              `map_id` int(11) UNSIGNED NOT NULL  AUTO_INCREMENT, 
5              `map_title` varchar(50) DEFAULT NULL, 
6              `latitude` varchar(12) DEFAULT NULL, 
7              `longitude` varchar(12) DEFAULT NULL, 
8              `map_zoom` varchar(5) NOT NULL, 
9              `map_parent` int(11) UNSIGNED DEFAULT NULL, 
10               PRIMARY KEY  (`map_id`) , 
11               KEY `map_parent`  (`map_parent`) , 
12               CONSTRAINT `map_ibfk_2` FOREIGN KEY  (`map_parent`)  REFERENCES `map`  (`map_id`)   ON DELETE CASCADE   ON UPDATE CASCADE 
13          )  ENGINE = InnoDB AUTO_INCREMENT = 12 DEFAULT  CHARSET = utf8;
</code></pre>

<p><strong>SQL Server</strong> (target, SQL generated by SSMA):</p>

<pre><code>1  CREATE TABLE dbo.map
2  (
3      map_id bigint NOT NULL IDENTITY(12, 1), 
4      map_title nvarchar(50) NULL DEFAULT NULL, 
5      latitude nvarchar(12) NULL DEFAULT NULL, 
6      longitude nvarchar(12) NULL DEFAULT NULL, 
7      map_zoom nvarchar(5) NOT NULL, 
8      map_parent bigint NULL DEFAULT NULL, 
9      CONSTRAINT PK_map_map_id PRIMARY KEY (map_id), 
10      /* 
11      *   SSMA error messages:
12      *   M2SS0040: ON DELETE  CASCADE|SET NULL|SET DEFAULT action  was changed to NO ACTION to avoid circular references of cascaded foreign keys.
13  
14      CONSTRAINT map$map_ibfk_2 FOREIGN KEY (map_parent) REFERENCES dbo.map (map_id) 
15           ON DELETE NO ACTION 
16          /* 
17          *   SSMA error messages:
18          *   M2SS0036: ON UPDATE CASCADE|SET NULL|SET DEFAULT action  was changed to NO ACTION to avoid circular references of cascaded foreign keys.
19  
20           ON UPDATE NO ACTION
21          */
22  
23  
24      */
25  
26  
27  )
28  GO
29  CREATE NONCLUSTERED INDEX map_parent
30      ON dbo.map (map_parent ASC)
31  GO
</code></pre>

<p>As you can see it gives an error indicating it changed my <code>ON UPDATE CASCADE</code> and <code>ON DELETE CASCADE</code> to <code>NO ACTION</code>  in order to ""to avoid circular references of cascaded foreign keys.""</p>

<h2>2.  Many-to-many tables</h2>

<p>I have two tables that got an error for ""multiple paths"" and similarly were changed to <code>NO ACTION</code>.</p>

<p><strong>asset_property</strong> table:</p>

<pre><code>| asset_id | property_id | property_value  |
|:--------:|:-----------:|:---------------:|
| 933      | 1           | Joseph          |
| 933      | 2           | Green           |
| 936      | 1           | Jacob           |
| 936      | 2           | Yellow          |
| 942      | 1           | Susan           |
| 942      | 2           | Blue            |
</code></pre>

<p><strong>MySQL</strong> (source):</p>

<pre><code>1  CREATE
2      TABLE `asset_property`
3          (
4              `asset_id` int(11) NOT NULL, 
5              `property_id` int(11) NOT NULL, 
6              `property_value` varchar(100) DEFAULT NULL, 
7               PRIMARY KEY  (`asset_id`, `property_id`) , 
8               KEY `asset_id`  (`asset_id`) , 
9               KEY `property_id`  (`property_id`) , 
10               CONSTRAINT `asset_property_ibfk_1` FOREIGN KEY  (`asset_id`)  REFERENCES `asset`  (`asset_id`)   ON DELETE CASCADE   ON UPDATE CASCADE , 
11               CONSTRAINT `asset_property_ibfk_2` FOREIGN KEY  (`property_id`)  REFERENCES `property`  (`property_id`)   ON DELETE CASCADE   ON UPDATE CASCADE 
12          )  ENGINE = InnoDB DEFAULT  CHARSET = utf8;
</code></pre>

<p><strong>SQL Server</strong> (target, SQL generated by SSMA):</p>

<pre><code>1  CREATE TABLE dbo.asset_property
2  (
3      asset_id int NOT NULL, 
4      property_id int NOT NULL, 
5      property_value nvarchar(100) NULL DEFAULT NULL, 
6      CONSTRAINT PK_asset_property_asset_id PRIMARY KEY (asset_id, property_id), 
7      /* 
8      *   SSMA error messages:
9      *   M2SS0041: ON DELETE CASCADE|SET NULL|SET DEFAULT action was changed to NO ACTION to avoid multiple paths in cascaded foreign keys.
10  
11      CONSTRAINT asset_property$asset_property_ibfk_1 FOREIGN KEY (asset_id) REFERENCES dbo.asset (asset_id) 
12           ON DELETE NO ACTION 
13          /* 
14          *   SSMA error messages:
15          *   M2SS0037: ON UPDATE CASCADE|SET NULL|SET DEFAULT action was changed to NO ACTION to avoid multiple paths in cascaded foreign keys.
16  
17           ON UPDATE NO ACTION
18          */
19  
20  
21      */
22  
23  , 
24      CONSTRAINT asset_property$asset_property_ibfk_2 FOREIGN KEY (property_id) REFERENCES dbo.property (property_id) 
25           ON DELETE CASCADE 
26           ON UPDATE CASCADE
27  )
28  GO
29  CREATE NONCLUSTERED INDEX asset_id
30      ON dbo.asset_property (asset_id ASC) 31  GO 32  CREATE NONCLUSTERED INDEX property_id
33      ON dbo.asset_property (property_id ASC) 34  GO
</code></pre>

<p>I've only found one <a href=""http://blogs.msdn.com/b/ssma/archive/2011/03/19/mysql-to-sql-server-migration-method-for-correcting-schema-issues.aspx"" rel=""noreferrer"">article</a> that talks about these errors. The article's solution for the self-referencing table error doesn't seem to apply, and the many-to-many error solution is to just remove the constraint ""because the application or user shouldnt be modifying these values.""</p>

<p>Thanks for any help!!</p>

<hr>

<p><img src=""https://i.stack.imgur.com/hbQUC.png"" alt=""db diagram""></p>
","<p>After having spent literally hours trying to find out why it didn't work I found the solution when I started to give up and use a linked table in Access to the then a pass-through query into MSSQL.</p>

<p>I was using the wrong ODBC driver. It turns out there are 2 MySQL ODBC drivers installed an <code>ANSI</code> and a <code>Unicode</code> driver. I was using <code>ANSI</code>. When I swapped it to <code>Unicode</code> all was well!</p>

<p><img src=""https://i.stack.imgur.com/CU5rP.png"" alt=""Unicode ODBC driver""></p>
"
"<p>I have the following sample code. The objective is to run SQL statement with multiple input parameters. </p>

<pre><code>[&lt;Literal&gt;]
let connectionString = @""Data Source=Localhost;Initial Catalog=Instrument;Integrated Security=True""
[&lt;Literal&gt;]
let query = ""SELECT MacroName, MacroCode FROM Instrument WHERE MacroCode IN (@codeName)""

type MacroQuery = SqlCommandProvider&lt;query, connectionString&gt;
let cmd = new MacroQuery()
let res = cmd.AsyncExecute(codeName= [|""CPI"";""GDP""|]) |&gt; Async.RunSynchronously
</code></pre>

<p>However, codeName is inferred to be string type instead of an array or list and give me an error.</p>

<p>Alternatively, I could run the query without where statement and filter based on the result. However, in lots of other cases that returns millions of rows, I would prefer filter data at the SQL server level to be more efficient.</p>

<p>I didn't find any relevant samples on the documentation of fsharp.data.sqlclient. Please help!</p>
","<p>""See Table-valued parameters (TVPs)"" section in the documentation: 
<a href=""http://fsprojects.github.io/FSharp.Data.SqlClient/configuration%20and%20input.html"" rel=""noreferrer"">http://fsprojects.github.io/FSharp.Data.SqlClient/configuration%20and%20input.html</a></p>
"
"<p>I wanted to work with custom DB provider in Visual Studio. I need it to use Entity Framework.</p>

<p>For example, I downloaded NpgSQL,
registered them in GAC:</p>

<pre><code>gacutil  -i  c:\temp\npgsql.dll
gacutil  -i  c:\temp\mono.security.dll
</code></pre>

<p>and added to machine.config file:</p>

<pre><code>&lt;add name=""Npgsql Data Provider""
invariant=""Npgsql""  support=""FF""
description="".Net Framework Data Provider for Postgresql Server""
type=""Npgsql.NpgsqlFactory, Npgsql, Version=2.0.6.0, Culture=neutral, PublicKeyToken=5d8b90d52f46fda7"" /&gt;
</code></pre>

<p>But Npgsql did not appear in Datasource list in Visual Studio:</p>

<p><img src=""https://i.stack.imgur.com/KUT60.png"" alt=""Data source in VS""></p>

<p>How to add custom DB provider to this list?</p>

<p>UPD: If I use command string edmgen.exe I got error:</p>

<blockquote>
  <p>error 7001: Failed to find or load the registered .Net Framework Data Provider.</p>
</blockquote>
","<p>Pass it as an array:</p>

<pre><code>string[] numbers = new string[] { ""123"", ""234"" };

NpgsqlCommands cmd = new NpgsqlCommands(""select * from products where number = ANY(:numbers)"");
NpgsqlParameter p = new NpgsqlParameter(""numbers"", NpgsqlDbType.Array | NpgsqlDbType.Text);
p.value = numbers;
command.Parameters.Add(p);
</code></pre>
"
"<p>I run jboss in standalone mode and have set my datasource in the <code>standalone.xml</code> to the following:</p>

<pre><code>&lt;datasource jndi-name=""MyDenaliDS"" pool-name=""MyDenaliDs_Pool"" enabled=""true"" jta=""true"" 
                                                   use-java-context=""true"" use-ccm=""true""&gt;
    &lt;connection-url&gt;
        jdbc:sqlserver://myip:1433;databaseName=mydb;integratedSecurity=true
    &lt;/connection-url&gt;
    &lt;driver&gt;
        sqljdbc
    &lt;/driver&gt;
    &lt;security&gt;
        &lt;user-name&gt;
            username
        &lt;/user-name&gt;
        &lt;password&gt;
            password
        &lt;/password&gt;
    &lt;/security&gt;
&lt;/datasource&gt;
&lt;drivers&gt;
    &lt;driver name=""sqljdbc"" module=""com.microsoft.sqlserver.jdbc""&gt;
        &lt;driver-class&gt;
            com.microsoft.sqlserver.jdbc.SQLServerDataSource
        &lt;/driver-class&gt;
    &lt;/driver&gt;                    
&lt;/drivers&gt;
</code></pre>

<p>in the folder <code>%jbosshome%\modules\com\microsoft\sqlserver\jdbc\</code> I have the <code>sqljdb4.jar</code> and the following <code>module.xml</code>:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;module name=""com.microsoft.sqlserver.jdbc"" xmlns=""urn:jboss:module:1.0""&gt;
    &lt;resources&gt;
        &lt;resource-root path=""sqljdbc4.jar""/&gt; 
     &lt;/resources&gt;
     &lt;dependencies&gt;
        &lt;module name=""javax.api""/&gt;
        &lt;module name=""javax.transaction.api""/&gt; 
     &lt;/dependencies&gt;
&lt;/module&gt;
</code></pre>

<p>When I start the jboss it gives me the following error:</p>

<pre><code>&gt; New missing/unsatisfied dependencies:    service
&gt; jboss.jdbc-driver.sqljdbc (missing)
</code></pre>

<p>Anyone know what I've done incorrect or what I'm missing?</p>
","<p>Comment the line with <code>setEncrypt(true)</code>:</p>

<pre><code>...
dSource.setDatabaseName(REDACTED);
//dSource.setEncrypt(true);
dSource.setTrustServerCertificate(true);
...
</code></pre>

<p>You might have trouble with the encryption setting. From the <a href=""http://msdn.microsoft.com/en-us/library/bb879920.aspx"" rel=""noreferrer"">setEncrypt(...)</a> documentation:</p>

<blockquote>
  <p>If the encrypt property is set to <strong>true</strong>, the Microsoft SQL Server JDBC Driver uses the JVM's default JSSE security provider to negotiate SSL encryption with SQL Server. The default security provider may not support all of the features required to negotiate SSL encryption successfully. For example, the default security provider may not support the size of the RSA public key used in the SQL Server SSL certificate. In this case, the default security provider might raise an error that will cause the JDBC driver to terminate the connection. In order to resolve this issue, do one of the following:</p>
  
  <ul>
  <li><p>Configure the SQL Server with a server certificate that has a smaller RSA public key</p></li>
  <li><p>Configure the JVM to use a different JSSE security provider in the ""/lib/security/java.security"" security properties file</p></li>
  <li><p>Use a different JVM</p></li>
  </ul>
</blockquote>

<p><strong>Update</strong></p>

<p>With Java versions 1.6.0_29 and 7.0.0_1 Oracle introduced a security fix for the SSL/TLS BEAST attack that very likely will cause the very same problem. The above security fix is known to make trouble for database connections to MSSQL Server with both the jTDS driver and the Microsoft driver. You can either</p>

<ul>
<li>decide not to use encryption by not using <code>setEncrypt(true)</code> (as specified above) </li>
<li>or, if it is enforced by MSSQL Server, you could turn off the Java fix in your JVM by setting the <code>-Djsse.enableCBCProtection=false</code> system property. Be warned, it will affect all SSL connections within the same VM.</li>
</ul>
"
"<p>I currently have a database table setup as follows (EAV - business reasons are valid):</p>

<ul>
<li>Id - int (PK)</li>
<li>Key - unique, varchar(15)</li>
<li>Value - varchar(1000)</li>
</ul>

<p>This allows me to add in mixed values into my databse as key/value pairs. For example:</p>

<pre><code>1   | 'Some Text'      | 'Hello World'
2   | 'Some Number'    | '123456'
etc.
</code></pre>

<p>In my C# code I use ADO.Net using <code>reader.GetString(2);</code> to retrieve the value as a string, then have my code elsewhere convert it as needed, for example... <code>Int32.ParseInt(myObj.Value);</code>. I'm looking at enhancing my table by possibly changing the value column to a <code>sql_variant</code> datatype, but I don't know what the benefit of this would be? Basically, is there any advantage to having my <i>value</i> column be of <code>sql_variant</code> vs <code>varchar(1000)</code>?</p>

<hr>

<p>To be more clear, I read somewhere that sql_variant gets returned as nvarchar(4000) back to the client making the call (ouch)! But, couldn't I cast it to it's type before returning it? Obviously my code would have to be adjusted to store the value as an object instead of a string value. I guess, what are the advantages/disadvantages of using <code>sql_variant</code> versus some other type in my current situation? Oh, and it is worth mentioning that all I plan to store are datetimes, strings, and numerical types (int, decimal, etc) in the value column; I don't plan on storing and blob or images or etc.</p>
","<p>One word: <strong>DON'T</strong> </p>

<p>This is very bad practice - columns have <strong>ONE SINGLE DATATYPE</strong> for a reason. Do not abuse this and make everything into variants.......</p>

<p>Marc</p>
"
"<p>SQLite has now an experimental JSON1 extention to work with JSON fields. The functions to choose from look promising, but I don't get how to use them in the context of a query.</p>

<p>Suppose I created the following table:</p>

<pre><code>sqlite&gt; create table user(name,phone);
sqlite&gt; insert into user values('oz', json_array(['+491765','+498973']));
</code></pre>

<p>The documentation shows how to use <code>json_each</code> in a query, but all other functions lack some in context documentation.</p>

<p>Can someone with SQLite experience provide a few examples of how to use:</p>

<ul>
<li><code>json_extract</code></li>
<li><code>json_set</code></li>
</ul>
","<p>So, here is a first example of how to use <code>json_extract</code>. First, the data is a inserted in a bit different way:</p>

<pre><code>insert into user (name, phone) values(""oz"", json('{""cell"":""+491765"", ""home"":""+498973""}'));
</code></pre>

<p>Now, we can select all the users phone numbers as in normal sql:</p>

<pre><code>sqlite&gt; select user.phone from user where user.name=='oz';
{""cell"":""+491765"",""home"":""+498973""}
sqlite&gt; 
</code></pre>

<p>But, what if we don't care about land lines and we want only cell phones?<br>
Enter <code>json_extract</code>:</p>

<pre><code>sqlite&gt; select json_extract(user.phone, '$.cell') from user;
+491765
</code></pre>

<p>And this is how to use <code>json_extract</code>. </p>

<p>Using <code>json_set</code> is similar. Given that the we want to update the cell phone:</p>

<pre><code>sqlite&gt; select json_set(json(user.phone), '$.cell', 123) from \
        user;
{""cell"":123,""home"":""+498973""}
</code></pre>

<p>You can combine those function calls in other SQL queries. Thus, you can
use SQLite with structured data and with unstructured data in the form of
JSON.  </p>

<p>Here is how to update the user cell phone only:</p>

<pre><code>sqlite&gt; update user 
   ...&gt; set phone =(select json_set(json(user.phone), '$.cell', 721) from user)
   ...&gt; where name == 'oz';
sqlite&gt; select * from user;
oz|{""cell"":721,""home"":""+498973""}
</code></pre>
"
"<p><a href=""https://i.stack.imgur.com/RLwBc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RLwBc.png"" alt=""Here is error""></a></p>

<blockquote>
  <p>TITLE: Microsoft SQL Server Management Studio</p>
  
  <p>Attach database failed for Server '(localdb)\mssqllocaldb'.  (Microsoft.SqlServer.Smo)</p>
  
  <p>ADDITIONAL INFORMATION:</p>
  
  <p>At least one file is needed for Database Attach. (Microsoft.SqlServer.Smo)</p>
</blockquote>

<p>I am trying to attach this <code>.mdf</code> database file to my LocalDb instance. It's fine if I can to it to SQL Server too. I have <code>.ldf</code> file in the same directory </p>
","<p>For completion's sake - <a href=""https://stackoverflow.com/questions/40020271/trying-to-attach-mdf-file-to-localdb-throws-error-at-least-one-file-is-required#comment68313278_40020271"">Jim's comment</a> solves (half) the problem and gets you going.</p>

<p>The other ""half"" of the problem is - what if you <em>ultimately</em> want to rename the physical database file? The answer is available in this <a href=""https://www.codeproject.com/Tips/345942/Rename-a-Database-and-its-MDF-and-LDF-files-in-SQL"" rel=""noreferrer"">CodeProject post</a>.</p>

<hr>

<p>Steps:</p>

<ol>
<li><p><strong><code>ALTER DATABASE</code> to set the new <em>physical</em> filenames (data file and log file)</strong><br>
<sup><em>Won't take effect until SQL Server is restarted or the database taken offline and brought back online</em></sup></p>

<ul>
<li><code>ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName', FILENAME = '&lt;Full-Path-Required&gt;\NewDbName.mdf');</code></li>
<li><code>ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName_log', FILENAME = '&lt;Full-Path-Required&gt;\NewDbName_log.ldf');</code></li>
</ul></li>
<li><p><strong><code>ALTER DATABASE</code> again to set new <em>logical</em> file names (again, data and log files)</strong><br>
<sup><em>Takes effect immediately</em></sup></p>

<ul>
<li><code>ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName', NEWNAME = 'NewDbName');</code></li>
<li><code>ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName_log', NEWNAME = 'NewDbName_log');</code></li>
</ul></li>
<li><p><strong>Take offline and bring back online or restart SQL Server</strong></p>

<ul>
<li><strong>Using SQL Server Management Studio:</strong>

<ol>
<li>Right-click on the renamed database and click <code>Take Offline</code> under <code>Tasks</code>.</li>
<li>Right-click on the (offline) database and click <code>Bring Online</code> under <code>Tasks</code>.</li>
</ol></li>
<li><strong>Using T-SQL:</strong>

<ol>
<li><code>ALTER DATABASE [CurrentName] SET OFFLINE WITH ROLLBACK IMMEDIATE;</code> (sets it to offline and disconnects any clients)</li>
<li><code>ALTER DATABASE [CurrentName] SET ONLINE;</code></li>
</ol></li>
</ul></li>
</ol>

<hr>

<p>Full code:</p>

<pre><code>-- Find ""CurrentName"" (without quotes) and replace with the current database name
-- Find ""NewDbName"" (without quotes) and replace with the new database name


USE [CurrentName];

-- Change physical file names:
ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName', FILENAME = '&lt;Full-Path-Required&gt;\NewDbName.mdf');
ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName_log', FILENAME = '&lt;Full-Path-Required&gt;\NewDbName_log.ldf');

-- Change logical names:
ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName', NEWNAME = 'NewDbName');
ALTER DATABASE [CurrentName] MODIFY FILE (NAME = 'CurrentName_log', NEWNAME = 'NewDbName_log');

-- Take offline and back online
USE [master]
GO
ALTER DATABASE [CurrentName] SET OFFLINE WITH ROLLBACK IMMEDIATE;
-- Then navigate to &lt;Full-Path-Required&gt; and rename the files
ALTER DATABASE [CurrentName] SET ONLINE;
</code></pre>
"
"<p>I have VS2015 with <a href=""https://docs.microsoft.com/en-us/sql/ssdt/download-sql-server-data-tools-ssdt"" rel=""noreferrer"">SSDT</a> installed, along with  <a href=""https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms"" rel=""noreferrer"">SSMS</a> and  the <a href=""https://www.powershellgallery.com/packages/SqlServer/21.0.17152"" rel=""noreferrer"">SqlServer</a> PowerShell module (which includes the <code>invoke-sqlcmd</code> comand), and yet If I try to execute a query against an Azure SQL Data Warehouse like so:</p>

<pre><code>invoke-sqlcmd  -Query ""Select top 5 * from customer""  -ConnectionString ""Server=tcp:my.database.windows.net,1433;Database=Customer;  Authentication=Active Directory Integrated; Encrypt=True; ""
</code></pre>

<p>I get the following error:</p>

<pre><code>invoke-sqlcmd : Unable to load adalsql.dll (Authentication=ActiveDirectoryIntegrated). Error code: 0x2. For more information, see
http://go.microsoft.com/fwlink/?LinkID=513072
At line:1 char:1
+ invoke-sqlcmd  -Query ""Select top 5 * from vwOffer""  -ConnectionStrin ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : InvalidOperation: (:) [Invoke-Sqlcmd], SqlException
    + FullyQualifiedErrorId : SqlExectionError,Microsoft.SqlServer.Management.PowerShell.GetScriptCommand

invoke-sqlcmd :
At line:1 char:1
+ invoke-sqlcmd  -Query ""Select top 5 * from vwOffer""  -ConnectionStrin ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : ParserError: (:) [Invoke-Sqlcmd], ParserException
    + FullyQualifiedErrorId : ExecutionFailureException,Microsoft.SqlServer.Management.PowerShell.GetScriptComman
</code></pre>

<p>If I try and install <a href=""http://go.microsoft.com/fwlink/?LinkID=513072"" rel=""noreferrer"">adalsql.dll</a> directly, I get a message stating that <code>A higher version already exists</code> and I can see the both versions of the dll can be found here:</p>

<pre><code> C:\Windows\SysWOW64\adalsql.dll
 C:\Windows\System32\adalsql.dll
</code></pre>

<p>and yet, invoke-sqlcmd cant find it.   Any idea how to either (A) register the existing dll so invoke-sqlcmd can find it or (B) uninstall it so that it can be re-installed?</p>

<p>Incidentally, I <em>am</em> able to use Active Directory Authenticatoin with the 32-bit SQLCMD.exe, so I know the 32 bit dll is working fine.  It's just the 64 bit dll that isn't loading properly...</p>
","<p>So, this problem vexed me as well. I'm unclear as to how it happened, but maybe it's just coincidence that it happened when I installed the latest version of SSMS. My fix was to:</p>

<p>1) Navigate to Add or Remove Programs</p>

<p>2) In the little search window type sql, or just go find: ""Active Directory Authentication Library for SQL Server"".</p>

<p>3) Uninstall that little guy</p>

<p>4) Navigate here and download the latest ADAL library (pick x64): </p>

<p><a href=""https://www.microsoft.com/en-us/download/confirmation.aspx?id=48742"" rel=""noreferrer"">https://www.microsoft.com/en-us/download/confirmation.aspx?id=48742</a></p>

<p>5) Just for kicks, reboot</p>

<p>6) Your stuff should now properly load adalsql.dll!</p>
"
"<p>I'm starting a simple java test project using sqlite4java and building using java.</p>

<p>I can get the core sqlite4java library downloaded easily, but I'm not sure what the best (any!) way to get gradle to download the native libraries and put them in the right place.</p>

<p>This is my build.gradle file:</p>

<pre><code>apply plugin: 'java'

/* We use Java 1.7 */
sourceCompatibility = 1.7
targetCompatibility = 1.7
version = '1.0'

repositories {
    mavenCentral()
}

sourceSets {
    main {
        java.srcDir 'src'
        output.classesDir = 'build/main'
    }
    test {
        java.srcDir 'test'
        output.classesDir = 'build/test'
    }
}


dependencies {
    testCompile group: 'junit', name: 'junit', version: '4.11'
    compile ""com.almworks.sqlite4java:sqlite4java:1.0.392""
    compile ""com.almworks.sqlite4java:libsqlite4java-osx:1.0.392""
}
</code></pre>

<p>But when I run a simple test I get:</p>

<pre><code>TestTest &gt; testSqlite4Basic FAILED
    com.almworks.sqlite4java.SQLiteException at TestTest.java:15
        Caused by: java.lang.UnsatisfiedLinkError at TestTest.java:15
</code></pre>

<p>(I'm building from within IntelliJ, but am using the gradle build options - so I don't think it's ItelliJ stuffing up the class path when running the tests...)</p>

<p>I'm pretty sure the first time I tried to build I got some message about being unable to unpack the libsqlite4java-osx ZIP file, (not surprisingly as maven central says its a dylib file).</p>

<p>What do I need to do to make gradle do the right thing?</p>

<p><strong>ASIDE</strong>: I'm able to get the code to run by manually copying the downloaded <code>.dylib</code> from the maven cache into somewhere listed in my <code>java.library.path</code> (Think I used <code>$HOME/Library/Java/Extensions</code> on my mac). But this seems contrary to the whole point of packaging all setup and dependencies in the <code>.gradle</code> file, and wont let me distribute anything easily later.</p>
","<p>I suppose, you need to use additional gradle plugin to handle native libraries or make your own specific tasks, to upload and put native libs in right place, in order to they could be found and linked.</p>

<p>At the moment I know only about one such a plugin, hope it can solve your problem <a href=""https://github.com/cjstehno/gradle-natives"" rel=""nofollow"">https://github.com/cjstehno/gradle-natives</a></p>

<p><strong>Edit:</strong>
The problem with plugin in your case is the fact, that your dependecny ""com.almworks.sqlite4java:libsqlite4java-osx:1.0.392"" is native lib by itself, not a jar with included native lib as I supposed. So, in that case, you can simply add this dependency in dependencies par of build script, as it'a already done, and then create a custom copy task, to put it in any place you need. Tried to do it with gradle 2.6 on Win7, look like:</p>

<pre><code>task copyNtiveDeps(type: Copy) {
  from (configurations.compile+configurations.testCompile) {
    include ""libsqlite4java-osx-1.0.392.dylib""
  }
  into ""c:\\tmp""
}
</code></pre>

<p>In your case, you just need to set ""into"" property to some path from java.library.path. And the second, you can make this task runs automaticaly with gradle task properties dependsOn and mustRunAfter.</p>
"
"<p>I have a piece of code that needs to run every day at a specified time. The code right now is sitting as a part of my web application. There are 2 stored procedures to get/save data that the code uses. </p>

<p>How can I setup Microsoft SQL Server Management Studio 2008 R2 to execute my code as well as the stored procs in a SQL Agent Job. I have never done this before and cannot seem to find the documentation.</p>
","<p>This solution would work:</p>

<pre><code>SELECT DATEDIFF(SECOND,aj.start_execution_date,GetDate()) AS Seconds
FROM msdb..sysjobactivity aj
JOIN msdb..sysjobs sj on sj.job_id = aj.job_id
WHERE aj.stop_execution_date IS NULL -- job hasn't stopped running
AND aj.start_execution_date IS NOT NULL -- job is currently running
AND sj.name = 'JobX'
and not exists( -- make sure this is the most recent run
    select 1
    from msdb..sysjobactivity new
    where new.job_id = aj.job_id
    and new.start_execution_date &gt; aj.start_execution_date
)
</code></pre>

<p>This a more general check dependent on system tables.  If you'd prefer a custom route, you could have the job insert into a job log table you created instead.</p>
"
"<p>Recently I'm learning to use node and <a href=""https://github.com/mapbox/node-sqlite3"" rel=""noreferrer"">node-sqlite3</a> to manipulate sqlite3, here is a sample.</p>

<pre><code>var sqlite3 = require('sqlite3');
var db = new sqlite3.Database(':memory:');
db.serialize(function() {
    db.run(""CREATE TABLE test(info TEXT)"");
    db.run(""INSERT INTO test (info) VALUES ('info1')"");
})
db.close();
</code></pre>

<p>The documentation said that <code>db.serialized</code> was used to ensure SQL lines were executed in order, but I was confused, why wouldn't they get executed in order without <code>db.serialize</code>, after all they would be pulled from the event queue and executed in order? How does it work here?</p>

<p>And if there is only one sql to be executed, is it safe to run it without <code>db.serialize</code> as follows?</p>

<pre><code>var sqlite3 = require('sqlite3');
var db = new sqlite3.Database(':memory:');
db.run(""CREATE TABLE test(info TEXT)"");
db.close();
</code></pre>
","<p>Each command inside the <code>serialize()</code> function is guaranteed to <strong>finish</strong> executing before the next one starts.</p>

<p>In your example, the <code>CREATE TABLE</code> will finish before the <code>INSERT</code> gets run.  If you didn't use <code>serialize()</code> then the <code>CREATE TABLE</code> and <code>INSERT</code> statements would be run in parallel.  They would start so quickly one after the other that the <code>INSERT</code> may actually finish before the table has been created, giving you an error about trying to insert data into a table that doesn't exist.</p>

<p>This is called a <em>race condition</em>, because every time you run your program you might get a different winner.  If <code>CREATE TABLE</code> wins the race then the program will work fine.  But if <code>INSERT</code> wins the race, the program will break with an error.  Since you can't control who wins the race, <code>serialize()</code> will stop <code>INSERT</code> from even starting until <code>CREATE TABLE</code> has reached the end, ensuring you get the same outcome every time.</p>

<p>In your second example with only one statement then <code>serialize()</code> is still required.  This is because <code>run()</code> starts the SQL query but <strong>returns immediately</strong>, leaving the query to run in the background.  Since your very next command is one to <code>close()</code> the database, you'll cut it off while the query is still running.</p>

<p>Since <code>serialize()</code> doesn't return until the last of its internal queries has completed, using it will hold off the <code>close()</code> until the query has completed.</p>

<p>If you were using a different type of query (say in response to a user clicking a button on a web page, where the database is left open between calls) then you probably wouldn't need <code>serialize()</code>.  It just depends whether the code that follows each query requires that the queries before it have completed or not.</p>

<p>When deciding whether to use <code>serialize()</code> or not, it can be helpful to think of any non-serialized queries as if they are commented out, and then see if the code would still work.  In your first example above, removing the <code>CREATE TABLE</code> command would break the following <code>INSERT</code> statement (because then there'd be no table to insert into), therefore these need to be serialised.  But if you had two <code>CREATE TABLE</code> commands then removing one would not affect the other, so those two commands would not have to be serialized.</p>

<p><em>(This tip doesn't apply to <code>close()</code> however - the rule of thumb there is to only call <code>close()</code> once everything has finished running.)</em></p>
"
"<p>I am a newbie (just 1 week) to this Python world. I tried installing django-mssql, but when I tried to import the library (using <code>import sqlserver_ado.dbapi</code>), I got this error message:</p>

<pre><code>ImportError: No module named pythoncom
</code></pre>

<p>I tried to look for that library without success. </p>

<p>Can you guys point me in the right direction?</p>
","<p>You are missing the <code>pythoncom</code> package. It comes with <a href=""http://www.activestate.com/activepython"" rel=""nofollow noreferrer"">ActivePython</a> but you can get it separately on <a href=""https://github.com/mhammond/pywin32"" rel=""nofollow noreferrer"">GitHub</a> (previously on <a href=""https://sourceforge.net/projects/pywin32/files/"" rel=""nofollow noreferrer"">SourceForge</a>) as part of pywin32.</p>
"
"<p>I'm working on OS X 10.6.4. I've been using clbuild to install supporting libraries for SBCL (including clsql), and I do all my work through Aquamacs. I installed MySQL using the <a href=""http://hivelogic.com/articles/installing-mysql-on-mac-os-x"" rel=""nofollow noreferrer"">excellent instructions over at Hive Logic</a>. But when I call <code>(require 'clsql)</code> -- which seems to work fine -- and then try to execute <code>(clsql:connect '(nil ""lisp"" ""root"" """") :database-type :mysql)</code> to connect to my local running database, I get this error message:</p>

<pre><code>erred while invoking #&lt;COMPILE-OP (:VERBOSE NIL) {12096109}&gt; on
#&lt;CLSQL-MYSQL-SOURCE-FILE ""clsql_mysql"" {1208E071}&gt;
   [Condition of type ASDF:OPERATION-ERROR]
</code></pre>

<p>From my research of this problem, I think it comes from me not having a compiled version of the libmysqlclient.dylib file, of which I have a copy in /usr/local/mysql/lib/, but I'm not clear on how to go about compiling it. <a href=""http://osdir.com/ml/lisp.clsql.general/2006-11/msg00022.html"" rel=""nofollow noreferrer"">This forum post</a> seems to say that's exactly what I need to do, but there's no make file in that directory.</p>
","<p>I would find it much less surprising if you defined the traits as classes themselves and used normal inheritance:</p>

<pre><code>(def-view-class trait-mapsto-company ()
  ((company-id
    :type integer
    :initarg :company-id)
   (company
    :accessor company
    :db-kind :join
    :db-info (:join-class company
              :home-key company-id
              :foreign-key company-id
              :set nil))))

(def-view-class trait-mapsto-manager ()
  ((manager-id
    :type integer
    :initarg :manager-id)
   (manager
    :accessor manager
    :db-kind :join
    :db-info (:join-class manager
              :home-key managerid
              :foreign-key emplid
              :set nil)))

(def-view-class employee (trait-mapsto-company trait-mapsto-manager)
  ((employee-id
    :db-kind :key
    :db-constraints (:not-null)
    :type integer)
   (first-name
    :accessor employee-first-name
    :type (string 30)
    :initarg :first-name)
   (last-name
    :accessor employee-last-name
    :type (string 30)
    :initarg :last-name)
   (email
    :accessor employee-email
    :type (string 100)
    :initarg :email)))
</code></pre>

<p>This certainly does not make the accessor name dependent on the name of the inheriting class, but do you really want that?  My view is that this way to write it shows that that would actually break a decoupling principle.</p>
"
"<p>I trying to restore a database by using the Restore-SqlDatabase cmdlet. I need to relocate the files but I'm getting the following errror</p>

<pre><code>Restore-SqlDatabase : Cannot bind parameter 'RelocateFile'. Cannot convert the 
""Microsoft.SqlServer.Management.Smo.RelocateFile"" value of type 
""Microsoft.SqlServer.Management.Smo.RelocateFile"" to type 
""Microsoft.SqlServer.Management.Smo.RelocateFile"".
At line:25 char:108
+ ... e -RelocateFil $RelocateData
+                    ~~~~~~~~~~~~~
+ CategoryInfo          : InvalidArgument: (:) [Restore-SqlDatabase], ParameterBindingException
+ FullyQualifiedErrorId CannotConvertArgumentNoMessage,Microsoft.SqlServer.Management.PowerShell.RestoreSqlDatabaseCommand
</code></pre>

<p>My powershell code look like this</p>

<pre><code>$RelocateData = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile(""MyDB_Data"", ""c:\data\MySQLServerMyDB.mdf"") 
$RelocateLog = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile(""MyDB_Log"", ""c:\data\MySQLServerMyDB.ldf"") 
$file = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile($RelocateData,$RelocateLog) 
$myarr=@($RelocateData,$RelocateLog)
Restore-SqlDatabase -ServerInstance DEV\DEMO -Database ""test"" -BackupFile $backupfile -RelocateFile $myarr
</code></pre>
","<p>For solution #1, you need to specify assembly qualified name when you instanciate relocate file to use correct assembly.</p>

<pre>
$RelocateData = New-Object 'Microsoft.SqlServer.Management.Smo.RelocateFile, Microsoft.SqlServer.SmoExtended, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91' -ArgumentList ""MyDB_Data"", ""c:\data\MySQLServerMyDB.mdf""
$RelocateLog = New-Object 'Microsoft.SqlServer.Management.Smo.RelocateFile, Microsoft.SqlServer.SmoExtended, Version=11.0.0.0, Culture=neutral, PublicKeyToken=89845dcd8080cc91' -ArgumentList ""MyDB_Log"", ""c:\data\MySQLServerMyDB.ldf""
$file = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile($RelocateData,$RelocateLog) 
$myarr=@($RelocateData,$RelocateLog)
Restore-SqlDatabase -ServerInstance DEV\DEMO -Database ""test"" -BackupFile $backupfile -RelocateFile $myarr</pre>

<p>Hope it helps !</p>
"
"<p>I am executing an SQL query via jcc to run a report. When I opened the error log file for the program and examined the SQL query, everything seems to be fine (There are no extra or missing brackets, commas, etc and the syntax is good) however when I execute I am getting this error:</p>

<p>[Report.execute()] DB2 SQL Error: SQLCODE=-104, SQLSTATE=42601, SQLERRMC=,;ATE IN (1,2,3,10,1)
;, DRIVER=4.12.55</p>

<p>When I researched about the SQLCODE I found out that it means there is an illegal symbol in the query. What can I look for to find this illegal symbol?</p>

<p>This is the query</p>

<p><img src=""https://i.stack.imgur.com/Z5ixS.png"" alt=""enter image description here""></p>

<p>Sorry for the tiny font but if you zoom 200% or so you can see the query better. </p>

<p>Thanks a lot :)</p>
","<p>You have a comma (where you shouldn't) at the end of this line:</p>

<pre><code>AND Tick.STATE IN (1,2,3,10,1),
</code></pre>

<p>The following line also has the same problem.</p>
"
"<p>Here is my setup</p>

<ul>
<li>Windows Server 2008 R2 64 bit</li>
<li>Apache 2.4.4 64 bit</li>
<li>PHP 5.4.15 32 bit (64 bit is still experimental), thread safe, VC9 compiler</li>
<li>Microsoft SQL Server 2012 Native Client 64-bit</li>
<li>Microsoft Visual C++ 2010 x86 and x64</li>
</ul>

<p>I need to load Microsoft's SQLSRV library.  </p>

<p>I have added 'extension=php_sqlsrv_54_ts.dll' to php.ini and copied 'php_sqlsrv_54_ts.dll' to the ext folder where PHP is installed.</p>

<p>When I restart apache, I get the following error in my php error log, and SQLSRV is not listed in phpinfo.</p>

<pre><code>PHP Warning:  PHP Startup: Unable to load dynamic library 'C:\php5\ext\php_sqlsrv_54_ts.dll' - %1 is not a valid Win32 application.
</code></pre>

<p>Where am I going wrong?</p>

<p>EDIT
For testing purposes I've just installed PHP 5.5.10 64 bit and VC 2012 but the error remains the same :(</p>
","<p>It is because <code>sqlsrv_query()</code> uses <strong><code>SQLSRV_CURSOR_FORWARD</code></strong> cursor type by default. However, in order to get a result from <code>sqlsrv_num_rows()</code>, you should choose one of these cursor types below:</p>

<ul>
<li>SQLSRV_CURSOR_STATIC</li>
<li>SQLSRV_CURSOR_KEYSET</li>
<li>SQLSRV_CURSOR_CLIENT_BUFFERED</li>
</ul>

<p>For more information, check: <a href=""http://technet.microsoft.com/en-us/library/hh487160%28v=sql.105%29.aspx"">Cursor Types (SQLSRV Driver)</a></p>

<p>In conclusion, if you use your query like:</p>

<pre><code>$query = sqlsrv_query($conn, $result, array(), array( ""Scrollable"" =&gt; 'static' ));
</code></pre>

<p>you will get result in:</p>

<pre><code>$row_count = sqlsrv_num_rows($query);
</code></pre>
"
"<p>I am taking the backup of SQLite DB using cp commmand after running wal_checkpoint(FULL). The DB is being used in WAL mode so there are other files like -shm and -wal in my folder.
When I run wal_checkpoint(FULL), the changes in WAL file get committed to the database. I waqs wondering whether -wal and -shm files get deelted after running a checkpoint. If not, then what do they contain ?</p>

<p>I know my backup process is not good since I am not using SQLite backup APIs. This is a bug in my code.<br><br>
Can anyone please suggest what content do -shm and -wal files contain after running checkpoint.</p>

<p>Any link provided would be helpful.</p>

<p>Thanks</p>
","<p>After searching through numerous sources, I believe the following to be true:</p>

<ol>
<li>The <code>-shm</code> file contains an index to the <code>-wal</code> file. The <code>-shm</code> file improves performance when reading the <code>-wal</code> file.</li>
<li>If the <code>-shm</code> file gets deleted, it get created again during next database access.</li>
<li>If <code>checkpoint</code> is run, the <code>-wal</code> file can be deleted.</li>
</ol>

<p>To perform safe backups:</p>

<ol>
<li>It is recommended that you use SQLite backup functions for making backups. SQLite library can even make backups of an online database.</li>
<li>If you don't want to use (1), then the best way is to close the database handles. This ensures a clean and consistent state of the database file, and deletes the <code>-shm</code> and <code>-wal</code> files. A backup can then be made using <code>cp</code>, <code>scp</code> etc.</li>
<li>If the SQLite database file is intended to be transmitted over a network, then the <strong>vacuum</strong> command should be run after <code>checkpoint</code>. This removes the fragmentation in the database file thereby reducing its size, so you transfer less data through network.</li>
</ol>
"
"<p>I've been trying to ""read between the lines"" about the original (and/or current) motivation for the NetSqlAzMan project.</p>

<p>Was this written for?</p>

<ol>
<li><p>An adapter for Windows Authorization Manager (AzMan).  Where the methods in the NetSqlAzMan just passes calls to (Windows Authorization Manager (AzMan)), but perhaps with nicer/cleaner methods?</p></li>
<li><p>A replacement for (Windows Authorization Manager (AzMan)).  Where (most or all of) the features available in (Windows Authorization Manager (AzMan)) are recreated in NetSqlAzMan, but the code was developed independently.
(Perhaps to provide DotNet 4.0 support???) (Perhaps to remove any COM dependencies)</p></li>
<li><p>To provide more features than (Windows Authorization Manager (AzMan)) provided.  Aka, a ""smarter""/""better"" version of (Windows Authorization Manager (AzMan)).</p></li>
<li><p>To rewrite but also keep a semi-dead project alive through open-source.  (As in, perhaps (Windows Authorization Manager (AzMan))) is a dead or abandoned project by Microsoft).  </p></li>
<li><p>Other?</p></li>
</ol>

<p>................</p>

<p>I like the object model of NetSqlAzMan.  But I need to defend any decision to use it to my project manager(s) and other developers.
The object model seems ""just right"" (think <a href=""http://www.pmcgee.net/prodesign/goldilocks_large.jpg"" rel=""noreferrer"">goldilocks</a> and the middle bed) as far as what I desire for security.
I do NOT want to do role based security.  I want right(or task or permission) based security.</p>

<p>(See:
<a href=""http://lostechies.com/derickbailey/2011/05/24/dont-do-role-based-authorization-checks-do-activity-based-checks/"" rel=""noreferrer"">http://lostechies.com/derickbailey/2011/05/24/dont-do-role-based-authorization-checks-do-activity-based-checks/</a>
and 
<a href=""http://granadacoder.wordpress.com/2010/12/01/rant-hard-coded-security-roles/"" rel=""noreferrer"">http://granadacoder.wordpress.com/2010/12/01/rant-hard-coded-security-roles/</a>
)</p>

<p>And basically the question that came up is:  ""What is the advantage of using NetSqlAzMan instead of (Windows Authorization Manager (AzMan))?""</p>

<p>And the sub question is ""Is Windows Authorization Manager (AzMan) dead?"".  (And something along the lines of Long Live NetSqlAzMan!).</p>

<p>..................</p>

<p>My in-general requirements are:</p>

<p>Non Active-Directory users.  (Down the road Active Directory and/or LDAP support would be nice, but not a requirement).
Passwords not stored as plain text.
Be able to handle RIGHTS for security checks.<br>
Group the rights under any role.
Assign roles to users.  (But again, the code will check for the right, not the role when performing an action.)
Allow (on occasion) rights to be assigned to users.  With a Deny override.  (Aka, a single user who does on stupid thing (like ""Delete Employee"") can have that right revoked.)
Roles and Rights can be maintained for multiple applications.</p>

<p>So other ideas are welcome.  But <a href=""http://blogs.msdn.com/b/card/archive/2009/10/20/how-geneva-helps-with-access-control-and-what-is-its-relationship-to-azman.aspx"" rel=""noreferrer"">Windows Identity Foundation</a> seems like a little overkill.</p>

<p>Thanks.</p>
","<p>I finally found a ""compare"" article last night.</p>

<p><a href=""http://www.c-sharpcorner.com/uploadfile/a.ferendeles/netsqlazman12122006123316pm/netsqlazman.aspx"" rel=""noreferrer"">http://www.c-sharpcorner.com/uploadfile/a.ferendeles/netsqlazman12122006123316pm/netsqlazman.aspx</a></p>

<p>I am going to paste the relevant portion here (below).  (Just in case that website ceases to exist in the future.  Small chance, I know, but I hate ""The answer is here"" links, and when you hit the link, it is a dead one.)</p>

<p>From what I can tell.</p>

<p>NetSqlAzMan provides a (table) user-defined-function that you can overload to provide a list of users (to be assigned to roles/tasks).
NetSqlAzMan provides not only ""Yeah you can"" mappings (Grant), but also Deny and Grant-With-Delegate as well.
NetSqlAzMan and Azman allows users(groups) to role mappings.  Only NetSqlAzMan allows users to Task mappings.</p>

<p>After looking at a few samples ... the object model of NetSqlAzMan is very clean.</p>

<p>=======================================================</p>

<blockquote>
  <p>Ms Authorization Manager (AzMan) vs .NET Sql Authorization Manager
  (NetSqlAzMan)</p>
  
  <p>As pointed out before, an analogous Microsoft product already exists
  and is called Authorization Manager (AzMan); AzMan is present, by
  default, in Windows Server 2003 and, through the Admin Pack setup, in
  Windows XP.</p>
  
  <p>The important difference between AzMan and NetSqlAzMan is that the
  first is Role-based, that is, based on the belonging - Role concept
  and the operations container in each role, while the second is
  Item-based (or if you prefer Operation-based), that is users or users
  group or group of groups that can or cannot belong to Roles or execute
  such Task and/or Operations (Items).</p>
  
  <p>Here the most important features and differences between the two
  products: </p>
  
  <p>Ms AzMan:</p>

<pre><code>* It's COM.
* It's equipped by a MMC 2.0 (COM) console.
* Its storage can be an XML file or ADAM (Active Directory Application Mode - e un LDAP).
* It's role-based.
* It supports static/dynamic applicative groups, members/not-members.
* Structure based on Roles -&gt; Tasks -&gt; Operations. (Hierarchical Roles and Tasks , none Operations).
* Authorizations can be added only to Roles.
* It doesn't implement the ""delegate"" concept.
* It doesn't manage authorizations ""in the time"".
* It doesn't trigger events.
* The only type of authorization is ""Allow"".
  (to ""deny"" it needs to remove the user/group from his Role).
* It supports Scripting / Biz rules.
* It supports Active Directory users/groups and ADAM users.
</code></pre>
  
  <p>NetSqlAzMan:</p>

<pre><code>* It's .NET 2.0.
* It's equipped by a MMC 3.0 (.NET) console.
* Its storage is a Sql Server database(2000/MSDE/2005/Express).
* It's based on Tdo - Typed Data Object technology.
* It's Item-based.
* Structure based on Roles -&gt; Tasks -&gt; Operations. (all hierarchical ones).
* Authorizations can be added to Roles, Task and Operations.
* It supports static/dynamic applicative groups, members/not-members.
* LDAP query testing directly from console.
* It's time-dependant.
* It's delegate-compliant.
* It triggers events (ENS).
* It supports 4 authorization types:
      o Allow with delegation (authorized and authorized to delegate).
      o Allow (authorized).
      o Deny (not authorized).
      o Neutral (neutral permission, it depends on higher level Item permission).
* Hierarchical authorizations.
* It supports Scripting / Biz rules (compiled in .NET - C# - VB - and not interpreted)
* It supports Active Directory users/groups and custom users defined in SQL Server Database.
</code></pre>
</blockquote>

<p>Here's another gotcha.</p>

<p>Azman sample code:
<a href=""http://channel9.msdn.com/forums/sandbox/252978-AzMan-in-the-Enterprise-Sample-Code"" rel=""noreferrer"">http://channel9.msdn.com/forums/sandbox/252978-AzMan-in-the-Enterprise-Sample-Code</a>
<a href=""http://channel9.msdn.com/forums/sandbox/252973-Programming-AzMan-Sample-Code"" rel=""noreferrer"">http://channel9.msdn.com/forums/sandbox/252973-Programming-AzMan-Sample-Code</a></p>

<pre><code>using System;
using System.Security.Principal;
using System.Runtime.InteropServices;
using AZROLESLib;

namespace TreyResearch {
    public class AzManHelper : IDisposable {

        AzAuthorizationStore store;
        IAzApplication app;
        string appName;

        public AzManHelper(string connectionString, string appName) {

            this.appName = appName;

            try {
                // load and initialize the AzMan runtime
                store = new AzAuthorizationStore();
                store.Initialize(0, connectionString, null);

                // drill down to our application
                app = store.OpenApplication(appName, null);
            }
            catch (COMException x) {
                throw new AzManException(""Failed to initizlize AzManHelper"", x);
            }
            catch (System.IO.FileNotFoundException x) {
                throw new AzManException(string.Format(""Failed to load AzMan policy from {0} - make sure your connection string is correct."", connectionString), x);
            }
        }

        public void Dispose() {
            if (null == app) return;

            Marshal.ReleaseComObject(app);
            Marshal.ReleaseComObject(store);

            app = null;
            store = null;
        }

        public bool AccessCheck(string audit, Operations op,
                                WindowsIdentity clientIdentity) {

            try {
                // first step is to create an AzMan context for the client
                // this looks at the security identifiers (SIDs) in the user's
                // access token and maps them onto AzMan roles, tasks, and operations
                IAzClientContext ctx = app.InitializeClientContextFromToken(
                    (ulong)clientIdentity.Token.ToInt64(), null);

                // next step is to see if this user is authorized for
                // the requested operation. Note that AccessCheck allows
                // you to check multiple operations at once if you desire
                object[] scopes = { """" };
                object[] operations = { (int)op };
                object[] results = (object[])ctx.AccessCheck(audit, scopes, operations,
                                                             null, null, null, null, null);
                int result = (int)results[0];
                return 0 == result;
            }
            catch (COMException x) {
                throw new AzManException(""AccessCheck failed"", x);
            }
        }

        public bool AccessCheckWithArg(string audit, Operations op,
                                       WindowsIdentity clientIdentity,
                                       string argName, object argValue) {

            try {
                // first step is to create an AzMan context for the client
                // this looks at the security identifiers (SIDs) in the user's
                // access token and maps them onto AzMan roles, tasks, and operations
                IAzClientContext ctx = app.InitializeClientContextFromToken(
                    (ulong)clientIdentity.Token.ToInt64(), null);

                // next step is to see if this user is authorized for
                // the requested operation. Note that AccessCheck allows
                // you to check multiple operations at once if you desire
                object[] scopes = { """" };
                object[] operations = { (int)op };
                object[] argNames = { argName };
                object[] argValues = { argValue };
                object[] results = (object[])ctx.AccessCheck(audit, scopes, operations,
                                                             argNames, argValues,
                                                             null, null, null);
                int result = (int)results[0];
                return 0 == result;
            }
            catch (COMException x) {
                throw new AzManException(""AccessCheckWithArg failed"", x);
            }
        }

        // use this to update a running app
        // after you change the AzMan policy
        public void UpdateCache() {
            try {
                store.UpdateCache(null);
                Marshal.ReleaseComObject(app);
                app = store.OpenApplication(appName, null);
            }
            catch (COMException x) {
                throw new AzManException(""UpdateCache failed"", x);
            }
        }
    }

    public class AzManException : Exception {
        public AzManException(string message, Exception innerException)
          : base(message, innerException)
        {}
    }
}
</code></pre>

<p>That is Azman helper code.  That is ugly COM/Interopish stuff.  :&lt;</p>

<p>Now check the NetSqlAzMan code samples:</p>

<p><a href=""http://netsqlazman.codeplex.com/wikipage?title=Samples"" rel=""noreferrer"">http://netsqlazman.codeplex.com/wikipage?title=Samples</a></p>

<pre><code>/// &lt;summary&gt;
/// Create a Full Storage through .NET code
/// &lt;/summary&gt;
private void CreateFullStorage()
{
    // USER MUST BE A MEMBER OF SQL DATABASE ROLE: NetSqlAzMan_Administrators

    //Sql Storage connection string
    string sqlConnectionString = ""data source=(local);initial catalog=NetSqlAzManStorage;user id=netsqlazmanuser;password=password"";
    //Create an instance of SqlAzManStorage class
    IAzManStorage storage = new SqlAzManStorage(sqlConnectionString);
    //Open Storage Connection
    storage.OpenConnection();
    //Begin a new Transaction
    storage.BeginTransaction(AzManIsolationLevel.ReadUncommitted);
    //Create a new Store
    IAzManStore newStore = storage.CreateStore(""My Store"", ""Store description"");
    //Create a new Basic StoreGroup
    IAzManStoreGroup newStoreGroup = newStore.CreateStoreGroup(SqlAzManSID.NewSqlAzManSid(), ""My Store Group"", ""Store Group Description"", String.Empty, GroupType.Basic);
    //Retrieve current user SID
    IAzManSid mySid = new SqlAzManSID(WindowsIdentity.GetCurrent().User);
    //Add myself as sid of ""My Store Group""
    IAzManStoreGroupMember storeGroupMember = newStoreGroup.CreateStoreGroupMember(mySid, WhereDefined.Local, true);
    //Create a new Application
    IAzManApplication newApp = newStore.CreateApplication(""New Application"", ""Application description"");
    //Create a new Role
    IAzManItem newRole = newApp.CreateItem(""New Role"", ""Role description"", ItemType.Role);
    //Create a new Task
    IAzManItem newTask = newApp.CreateItem(""New Task"", ""Task description"", ItemType.Task);
    //Create a new Operation
    IAzManItem newOp = newApp.CreateItem(""New Operation"", ""Operation description"", ItemType.Operation);
    //Add ""New Operation"" as a sid of ""New Task""
    newTask.AddMember(newOp);
    //Add ""New Task"" as a sid of ""New Role""
    newRole.AddMember(newTask);
    //Create an authorization for myself on ""New Role""
    IAzManAuthorization auth = newRole.CreateAuthorization(mySid, WhereDefined.Local, mySid, WhereDefined.Local, AuthorizationType.AllowWithDelegation, null, null);
    //Create a custom attribute
    IAzManAttribute&lt;IAzManAuthorization&gt; attr = auth.CreateAttribute(""New Key"", ""New Value"");
    //Create an authorization for DB User ""Andrea"" on ""New Role""
    IAzManAuthorization auth2 = newRole.CreateAuthorization(mySid, WhereDefined.Local, storage.GetDBUser(""Andrea"").CustomSid, WhereDefined.Local, AuthorizationType.AllowWithDelegation, null, null);
    //Commit transaction
    storage.CommitTransaction();
    //Close connection
    storage.CloseConnection();
}
</code></pre>

<p>That tells a story in and of itself.</p>
"
"<p>I've searched google thoroughly for a definitive solution or set of steps to resolve this issue, but there don't seem to be many high quality results, and I haven't found the question on stack overflow. We're trying to set up MySQL replication using one slave. The slave appears to be replicating fine, and then the following error occurs:</p>

<blockquote>
  <p>Could not parse relay log event entry. The possible reasons are: the master's binary log is corrupted (you can check this by running 'mysqlbinlog' on the binary log), the slave's relay log is corrupted (you can check this by running 'mysqlbinlog' on the relay log), a network problem, or a bug in the master's or slave's MySQL code. If you want to check the master's binary log or slave's relay log, you will be able to know their names by issuing 'SHOW SLAVE STATUS' on this slave.</p>
</blockquote>

<p>In order to benefit the large number of people who will inevitably stumble upon this question from a search, it would be helpful if someone who responds provided an overview of what could be going wrong and what steps to take to resolve this issue, but I will also provide more details below related to my particular situation in hopes that someone can help me solve it.</p>

<hr>

<p>The dump that we imported into the slave to get it started was created using the following command on the master:</p>

<pre><code>mysqldump --opt --allow-keywords -q -uroot -ppassword dbname &gt; E:\Backups\dbname.sql
</code></pre>

<p>The script that performs this backup also logs the master's current binary log position. We then took the following steps to start replication on the slave:</p>

<pre><code>1. STOP SLAVE;
2. DROP DATABASE dbname;
3. SOURCE dbname.sql;
    (... waited a few hours for the 10gb dump to import)
4. RESET SLAVE;
5. CHANGE MASTER TO MASTER_HOST='[masterhostname]', MASTER_USER='[slaveusername]', MASTER_PASSWORD='[slaveuserpassword]', MASTER_PORT=[port], MASTER_LOG_FILE='[masterlogfile]', MASTER_LOG_POS=[masterlogposition];
6. START SLAVE;
</code></pre>

<p>After about a day of replication working fine, it failed again at 3:43 AM. The first thing that appeared in MySQL's error log was the error above. Then another generic error appeared after with the same timestamp:</p>

<pre><code>Error running query, slave SQL thread aborted. Fix the problem, and restart the slave SQL thread with ""SLAVE START"". We stopped at log '[masterlogfile]' position [masterlogpos]
</code></pre>

<p>For more logging information, I had set up a batch script to run ""SHOW SLAVE STATUS"" and ""SHOW FULL PROCESSLIST"" every hour. Here are the results before and after the failure:</p>

<pre><code>--Monitoring: 3:00:00.15 

Slave Status: 
*************************** 1. row ***************************
             Slave_IO_State: Waiting for master to send event
                Master_Host: 192.168.xxx.xxx
                Master_User: slave_user
                Master_Port: xxxx
              Connect_Retry: 60
            Master_Log_File: mysql-bin.000xxx
        Read_Master_Log_Pos: 316611912
             Relay_Log_File: dbname-relay-bin.00000x
              Relay_Log_Pos: 404287513
      Relay_Master_Log_File: mysql-bin.000xxx
           Slave_IO_Running: Yes
          Slave_SQL_Running: Yes
            Replicate_Do_DB: dbname
        Replicate_Ignore_DB: 
         Replicate_Do_Table: 
     Replicate_Ignore_Table: 
    Replicate_Wild_Do_Table: 
Replicate_Wild_Ignore_Table: 
                 Last_Errno: 0
                 Last_Error: 
               Skip_Counter: 0
        Exec_Master_Log_Pos: 316611912
            Relay_Log_Space: 404287513
            Until_Condition: None
             Until_Log_File: 
              Until_Log_Pos: 0
         Master_SSL_Allowed: No
         Master_SSL_CA_File: 
         Master_SSL_CA_Path: 
            Master_SSL_Cert: 
          Master_SSL_Cipher: 
             Master_SSL_Key: 
      Seconds_Behind_Master: 0

*************************** 1. row ***************************
     Id: 98
   User: system user
   Host: 
     db: NULL
Command: Connect
   Time: 60547
  State: Waiting for master to send event
   Info: NULL
*************************** 2. row ***************************
     Id: 99
   User: system user
   Host: 
     db: NULL
Command: Connect
   Time: 5
  State: Has read all relay log; waiting for the slave I/O thread to update it
   Info: NULL
*************************** 3. row ***************************
     Id: 119
   User: root
   Host: localhost:xxxx
     db: NULL
Command: Query
   Time: 0
  State: NULL
   Info: SHOW FULL PROCESSLIST

--Monitoring: 4:00:02.71 

Slave Status: 
*************************** 1. row ***************************
             Slave_IO_State: Waiting for master to send event
                Master_Host: 192.168.xxx.xxx
                Master_User: slave_user
                Master_Port: xxxx
              Connect_Retry: 60
            Master_Log_File: mysql-bin.000xxx
        Read_Master_Log_Pos: 324365637
             Relay_Log_File: dbname-relay-bin.00000x
              Relay_Log_Pos: 410327741
      Relay_Master_Log_File: mysql-bin.000xxx
           Slave_IO_Running: Yes
          Slave_SQL_Running: No
            Replicate_Do_DB: dbname
        Replicate_Ignore_DB: 
         Replicate_Do_Table: 
     Replicate_Ignore_Table: 
    Replicate_Wild_Do_Table: 
Replicate_Wild_Ignore_Table: 
                 Last_Errno: 0
                 Last_Error: Could not parse relay log event entry. The possible reasons are: the master's binary log is corrupted (you can check this by running 'mysqlbinlog' on the binary log), the slave's relay log is corrupted (you can check this by running 'mysqlbinlog' on the relay log), a network problem, or a bug in the master's or slave's MySQL code. If you want to check the master's binary log or slave's relay log, you will be able to know their names by issuing 'SHOW SLAVE STATUS' on this slave.
               Skip_Counter: 0
        Exec_Master_Log_Pos: 322652140
            Relay_Log_Space: 412041238
            Until_Condition: None
             Until_Log_File: 
              Until_Log_Pos: 0
         Master_SSL_Allowed: No
         Master_SSL_CA_File: 
         Master_SSL_CA_Path: 
            Master_SSL_Cert: 
          Master_SSL_Cipher: 
             Master_SSL_Key: 
      Seconds_Behind_Master: NULL

*************************** 1. row ***************************
     Id: 98
   User: system user
   Host: 
     db: NULL
Command: Connect
   Time: 64149
  State: Waiting for master to send event
   Info: NULL
*************************** 2. row ***************************
     Id: 122
   User: root
   Host: localhost:3029
     db: NULL
Command: Query
   Time: 0
  State: NULL
   Info: SHOW FULL PROCESSLIST
</code></pre>

<p>I tried following the instructions from the error and ran mysqlbinlog on the slave's relay log with a start_position thousands of statements before, and stop_position thousands of statements after the point of failure, and redirected the output to a text file. I did not see any corruption errors on the command line or in the log file. This is what the log file said around the point of failure:</p>

<pre><code>...
# at 410327570
#120816 3:43:26 server id 1 log_pos 322651969    Intvar
SET INSERT_ID=3842697;
# at 410327598
#120816 3:43:26 server id 1 log_pos 322651997    Query    thread_id=762340    exec_time=0   error_code=0
SET TIMESTAMP=1345113806
insert into LOGTABLENAME (UpdateDate, Description) values (now(), ""Invalid floating point operation"");
# at 410327741
#120816 3:44:26 server id 1 log_pos 322754486    Intvar
SET INSERT_ID=3842701;
# at 410327769
#120816 3:43:26 server id 1 log_pos 322754514    Query    thread_id=762340    exec_time=0   error_code=0
SET TIMESTAMP=1345113866;
insert into LOGTABLENAME (UpdateDate, Description) values (now(), ""Invalid floating point operation"");
# at 410327912
...
</code></pre>

<p>Interesting that it's logging an Invalid floating point operation at that point, but I'm not sure how that could cause replication to break at that position. I ran mysqlbinlog on the master's binary log found in SHOW SLAVE STATUS from above, and did not see any errors on the command line (but did not get a chance to open the 100mb log file that was generated since I didn't want to bog down the production server).</p>

<p>So right now I'm at a loss for what else to try. I'm basically just looking for any insights as to what might be going wrong or any suggestions for what steps to take next. Thanks!</p>
","<p>I'm not sure what the root cause may be.  But to recover from this situation, you'd want to instruct MySQL to clear out all the relay-bin-logs beyond the following point</p>

<ul>
<li>Relay_Master_Log_File: mysql-bin.000xxx</li>
<li>Exec_Master_Log_Pos: 322652140</li>
</ul>

<p>by doing the following:</p>

<p><code>STOP SLAVE; CHANGE MASTER TO MASTER_LOG_FILE = 'mysql-bin.000xxx', MASTER_LOG_POS = 322652140; START SLAVE;</code></p>

<p><b>NOTE:</b> To readers out there, do not be confused by Relay_Master_Log_File, it is NOT the same as Read_Master_Log_Pos.  And do not confuse Exec_Master_Log_Pos with Read_Master_Log_Pos.  The Read_* is a read-ahead strategy that MySQL does to download the replication bin logs from the master ahead of the actual implementation of the replication being executed locally.</p>
"
"<p>I have a large  XML note with many  nodes.</p>

<p>is there a way that I can select only a single  node and all of its contents from the larger XML?</p>

<p>i am using sql 2005</p>
","<p>You should use the <a href=""http://msdn.microsoft.com/en-us/library/ms191474.aspx"">query() Method</a> if you want to get a part of your XML.</p>

<pre><code>declare @XML xml

set @XML = 
'
&lt;root&gt;
  &lt;row1&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/row1&gt;
  &lt;row2&gt;
    &lt;value&gt;2&lt;/value&gt;
  &lt;/row2&gt;
&lt;/root&gt;
'

select @XML.query('/root/row2')
</code></pre>

<p>Result:</p>

<pre><code>&lt;row2&gt;
  &lt;value&gt;2&lt;/value&gt;
&lt;/row2&gt;
</code></pre>

<p>If you want the value from a specific node you should use <a href=""http://msdn.microsoft.com/en-us/library/ms178030.aspx"">value() Method</a>.</p>

<pre><code>select @XML.value('(/root/row2/value)[1]', 'int')
</code></pre>

<p>Result:</p>

<pre><code>2
</code></pre>

<p><strong>Update:</strong></p>

<p>If you want to shred your XML to multiple rows you use <a href=""http://msdn.microsoft.com/en-us/library/ms188282.aspx"">nodes() Method</a>.</p>

<p>To get values:</p>

<pre><code>declare @XML xml

set @XML = 
'
&lt;root&gt;
  &lt;row&gt;
    &lt;value&gt;1&lt;/value&gt;
  &lt;/row&gt;
  &lt;row&gt;
    &lt;value&gt;2&lt;/value&gt;
  &lt;/row&gt;
&lt;/root&gt;
'

select T.N.value('value[1]', 'int')
from @XML.nodes('/root/row') as T(N)
</code></pre>

<p>Result:</p>

<pre><code>(No column name)
1
2
</code></pre>

<p>To get the entire XML:</p>

<pre><code>select T.N.query('.')
from @XML.nodes('/root/row') as T(N)
</code></pre>

<p>Result:</p>

<pre><code>(No column name)
&lt;row&gt;&lt;value&gt;1&lt;/value&gt;&lt;/row&gt;
&lt;row&gt;&lt;value&gt;2&lt;/value&gt;&lt;/row&gt;
</code></pre>
"
"<p>When I compare the 2 databases with the Red Gate's SQL compare then getting below error. ""ExecuteReader: CommandText property has not been initialized""</p>

<p>I am using the SQL server 2012 express and Red Gate's SQL Compare 8</p>

<p>Is there any solution?</p>

<p>Thanks,</p>
","<p>SQL Compare 8 (current is 10.4) does not work with SQL Server 2012</p>

<p><a href=""http://www.red-gate.com/messageboard/viewtopic.php?t=15296&amp;highlight=commandtext"" rel=""noreferrer"">http://www.red-gate.com/messageboard/viewtopic.php?t=15296&amp;highlight=commandtext</a></p>
"
"<p>I get a connection failure when I try to connect to my postgres server in Azure from my app/client, which does not have SSL enabled. </p>

<p><em>Unable to connect to server:
FATAL: SSL connection is required. Please specify SSL options and retry.</em></p>

<p>Is this a strong requirement? Is there a way I can circumvent this requirement?</p>
","<p>As noted in the error text, you are required to follow the <code>&lt;username@hostname&gt;</code> format when trying to connect to postgresql server, whether you are doing it from psql client or using pgadmin. Using <code>&lt;username@hostname&gt;</code> format instead of just <code>&lt;username&gt;</code> should get rid of the error.</p>

<p>Read the quick-start documents for <a href=""https://docs.microsoft.com/en-us/azure/postgresql/quickstart-create-server-database-portal"" rel=""noreferrer"">Azure portal</a> and <a href=""https://docs.microsoft.com/en-us/azure/postgresql/quickstart-create-server-database-azure-cli"" rel=""noreferrer"">CLI</a> to understand more about how to create and configure your postgres server.</p>
"
"<p>I was wondering what's the best practice moving a documentDB to the Azure Data Lake Storage.
Should I create a file for each document in a collection or move the entire documentDB? 
Also I didn't find much information on how I can access the documentDB using U-SQL? </p>

<p>Input would be appreciated.</p>
","<p>You can think of U-SQL as Microsoft's version of Spark SQL, where you can write SQL Server styled SQL and extend with User-Defined Functions in C#.  While with Spark you write in a Semi MySQL styled SQL and extend it with either Scala or Python.</p>

<p>If you are familiar with Scala or Python then choosing HDInsight might be the best choice.  Spark comes with GraphX and MLLib which at the moment have no analogues in Data Lake Analytics.  Also if you need something that works outside of Azure then SparkSQL is your only option. </p>

<p>Another important dimension to think about is the pricing.  Data Lake Analytics only costs money while your query is executing, but HDInsight costs money for as long as the cluster is running.  Depending on the size of the data and the complexity of your queries Data Lake Analytics can be cheaper because you aren't charged while it's provisioning.</p>
"
"<p>I was looking at using <a href=""http://sailsjs.com"" rel=""nofollow"">Sails</a> for an app that we are developing.</p>

<p>I'm using the sails-postgresql adapter which uses the waterline orm.</p>

<p>I have an existing database that I want to connect to.</p>

<p>If I create a model using <code>generate something</code></p>

<p>and then in my model I have</p>

<pre><code>attributes:{
    title:{type:'String'}
}
</code></pre>

<p>If I browse to localhost/something the orm deletes all the columns in the <code>something</code> table except title.</p>

<p>Is there a way to stop it from doing this? This app should not delete columns on this database.</p>

<p>Thanks!</p>
","<p>I am the author of Sails-Postgresql. Sails has an ORM called Waterline that it uses for managing data. The default setting assumes that you would want to <code>auto-migrate</code> your database to match your model attributes. Because Postgresql is a SQL database the Sails-Postgresql adapter has a setting called syncable that defaults to true. This would be false in a NoSQL database like redis.</p>

<p>This is easy to turn off if you want to manage your database columns yourself. You can add <code>migrate: safe</code> to your model and it won't try and update your database schema when you start Sails.</p>

<pre><code>module.exports = {
  adapter: 'postgresql',
  migrate: 'safe',
  attributes: {
    title: { type: 'string' }
  }
};
</code></pre>

<p>Sails doesn't have anything like migrations in Rails. It uses auto-migrations to attempt to remove this from your development process and then leaves updating your production schema to you.</p>
"
"<p>There's another question out there similar to this, but it didn't seem to answer <em>my</em> question.</p>

<p>My question is this: why am I getting back this error <code>ERROR 1222 (21000): The used SELECT statements have a different number of columns</code> from the following SQL </p>

<pre><code>SELECT * FROM friends
LEFT JOIN users AS u1 ON users.uid = friends.fid1
LEFT JOIN users AS u2 ON users.uid = friends.fid2
WHERE (friends.fid1 = 1) AND (friends.fid2 &gt; 1)
UNION SELECT fid2 FROM friends
WHERE (friends.fid2  = 1) AND (friends.fid1 &lt; 1)
ORDER BY RAND()
LIMIT 6;
</code></pre>

<p>Here's <code>users</code>:</p>

<pre><code>+------------+---------------+------+-----+---------+----------------+
| Field      | Type          | Null | Key | Default | Extra          |
+------------+---------------+------+-----+---------+----------------+
| uid        | int(11)       | NO   | PRI | NULL    | auto_increment |
| first_name | varchar(50)   | NO   |     | NULL    |                |
| last_name  | varchar(50)   | NO   |     | NULL    |                |
| email      | varchar(128)  | NO   | UNI | NULL    |                |
| mid        | varchar(40)   | NO   |     | NULL    |                |
| active     | enum('N','Y') | NO   |     | NULL    |                |
| password   | varchar(64)   | NO   |     | NULL    |                |
| sex        | enum('M','F') | YES  |     | NULL    |                |
| created    | datetime      | YES  |     | NULL    |                |
| last_login | datetime      | YES  |     | NULL    |                |
| pro        | enum('N','Y') | NO   |     | NULL    |                |
+------------+---------------+------+-----+---------+----------------+
</code></pre>

<p>Here's <code>friends</code>:</p>

<pre><code>+---------------+--------------------------------------+------+-----+---------+----------------+
| Field         | Type                                 | Null | Key | Default | Extra          |
+---------------+--------------------------------------+------+-----+---------+----------------+
| friendship_id | int(11)                              | NO   | MUL | NULL    | auto_increment |
| fid1          | int(11)                              | NO   | PRI | NULL    |                |
| fid2          | int(11)                              | NO   | PRI | NULL    |                |
| status        | enum('pending','accepted','ignored') | NO   |     | NULL    |                |
+---------------+--------------------------------------+------+-----+---------+----------------+
</code></pre>

<p>If you want to give any feedback on anything crazy you see going on here, as well, please feel free to do so. I'll take my lumps.</p>
","<p>UNIONs (<code>UNION</code> and <code>UNION ALL</code>) require that all the queries being UNION'd have:</p>

<ol>
<li>The same number of columns in the SELECT clause</li>
<li>The column data type has to match at each position</li>
</ol>

<p>Your query has:</p>

<pre><code>SELECT f.*, u1.*, u2.* ...
UNION 
SELECT fid2 FROM friends
</code></pre>

<p>The easiest re-write I have is:</p>

<pre><code>   SELECT f.*, u.*
     FROM FRIENDS AS f
     JOIN USERS AS u ON u.uid = f.fid2
    WHERE f.fid1 = 1 
      AND f.fid2 &gt; 1
UNION 
   SELECT f.*, u.*
     FROM FRIENDS AS f
     JOIN USERS AS u ON u.uid = f.fid1
    WHERE f.fid2  = 1 
      AND f.fid1 &lt; 1
ORDER BY RAND()
LIMIT 6;
</code></pre>

<p>You've LEFT JOIN'd to the <code>USERS</code> table twice, but don't appear to be using the information.</p>
"
"<p>I am upgrading my mysql-5.5 docker container database to mysql-5.6 docker container. I was able to fix all other problems. Finally my server is running with 5.6. But when i run mysql_upgrade i am getting the following error.</p>

<p>ERROR:</p>

<pre><code>root@17aa74cbc5e2# mysql_upgrade -uroot -password           
Warning: Using a password on the command line interface can be insecure.
Looking for 'mysql' as: mysql
Looking for 'mysqlcheck' as: mysqlcheck
Running 'mysqlcheck' with connection arguments: '--port=3306' '--socket=/var/run/mysqld/mysqld.sock' 
Warning: Using a password on the command line interface can be insecure.
Running 'mysqlcheck' with connection arguments: '--port=3306' '--socket=/var/run/mysqld/mysqld.sock' 
Warning: Using a password on the command line interface can be insecure.
mysql.columns_priv                                 OK
mysql.db                                           OK
mysql.event                                        OK
mysql.func                                         OK
mysql.general_log                                  OK
mysql.help_category                                OK
mysql.help_keyword                                 OK
mysql.help_relation                                OK
mysql.help_topic                                   OK
mysql.innodb_index_stats
Error    : Table 'mysql.innodb_index_stats' doesn't exist
status   : Operation failed
mysql.innodb_table_stats
Error    : Table 'mysql.innodb_table_stats' doesn't exist
status   : Operation failed
mysql.ndb_binlog_index                             OK
mysql.plugin                                       OK
mysql.proc                                         OK
mysql.procs_priv                                   OK
mysql.proxies_priv                                 OK
mysql.servers                                      OK
mysql.slave_master_info
Error    : Table 'mysql.slave_master_info' doesn't exist
status   : Operation failed
mysql.slave_relay_log_info
Error    : Table 'mysql.slave_relay_log_info' doesn't exist
status   : Operation failed
mysql.slave_worker_info
Error    : Table 'mysql.slave_worker_info' doesn't exist
status   : Operation failed
mysql.slow_log                                     OK
mysql.tables_priv                                  OK
mysql.time_zone                                    OK
mysql.time_zone_leap_second                        OK
mysql.time_zone_name                               OK
mysql.time_zone_transition                         OK
mysql.time_zone_transition_type                    OK
mysql.user                                         OK

Repairing tables
mysql.innodb_index_stats
Error    : Table 'mysql.innodb_index_stats' doesn't exist
status   : Operation failed
mysql.innodb_table_stats
Error    : Table 'mysql.innodb_table_stats' doesn't exist
status   : Operation failed
mysql.slave_master_info
Error    : Table 'mysql.slave_master_info' doesn't exist
status   : Operation failed
mysql.slave_relay_log_info
Error    : Table 'mysql.slave_relay_log_info' doesn't exist
status   : Operation failed
mysql.slave_worker_info
Error    : Table 'mysql.slave_worker_info' doesn't exist
status   : Operation failed
Running 'mysql_fix_privilege_tables'...
Warning: Using a password on the command line interface can be insecure.
ERROR 1146 (42S02) at line 62: Table 'mysql.innodb_table_stats' doesn't exist
ERROR 1243 (HY000) at line 63: Unknown prepared statement handler (stmt) given to EXECUTE
ERROR 1243 (HY000) at line 64: Unknown prepared statement handler (stmt) given to DEALLOCATE PREPARE
ERROR 1146 (42S02) at line 66: Table 'mysql.innodb_index_stats' doesn't exist
ERROR 1243 (HY000) at line 67: Unknown prepared statement handler (stmt) given to EXECUTE
ERROR 1243 (HY000) at line 68: Unknown prepared statement handler (stmt) given to DEALLOCATE PREPARE
ERROR 1146 (42S02) at line 81: Table 'mysql.slave_relay_log_info' doesn't exist
ERROR 1243 (HY000) at line 82: Unknown prepared statement handler (stmt) given to EXECUTE
ERROR 1243 (HY000) at line 83: Unknown prepared statement handler (stmt) given to DEALLOCATE PREPARE
ERROR 1146 (42S02) at line 110: Table 'mysql.slave_master_info' doesn't exist
ERROR 1243 (HY000) at line 111: Unknown prepared statement handler (stmt) given to EXECUTE
ERROR 1243 (HY000) at line 112: Unknown prepared statement handler (stmt) given to DEALLOCATE PREPARE
ERROR 1146 (42S02) at line 128: Table 'mysql.slave_worker_info' doesn't exist
ERROR 1243 (HY000) at line 129: Unknown prepared statement handler (stmt) given to EXECUTE
ERROR 1243 (HY000) at line 130: Unknown prepared statement handler (stmt) given to DEALLOCATE PREPARE
ERROR 1146 (42S02) at line 1896: Table 'mysql.slave_master_info' doesn't exist
ERROR 1146 (42S02) at line 1897: Table 'mysql.slave_master_info' doesn't exist
ERROR 1146 (42S02) at line 1898: Table 'mysql.slave_master_info' doesn't exist
ERROR 1146 (42S02) at line 1899: Table 'mysql.slave_worker_info' doesn't exist
ERROR 1146 (42S02) at line 1900: Table 'mysql.slave_relay_log_info' doesn't exist
ERROR 1146 (42S02) at line 1904: Table 'mysql.innodb_table_stats' doesn't exist
ERROR 1146 (42S02) at line 1908: Table 'mysql.innodb_index_stats' doesn't exist
FATAL ERROR: Upgrade failed
</code></pre>
","<p>This is a known bug in MySQL 5.6, <a href=""https://bugs.mysql.com/bug.php?id=67179"" rel=""noreferrer"">it is documented here</a>.</p>

<p>According to the replies to the bug report, you can manually create the missing tables. The structure of the missing tables is provided as attachment <a href=""https://bugs.mysql.com/file.php?id=19725&amp;bug_id=67179"" rel=""noreferrer"">here</a>.</p>

<p>Steps to follow:</p>

<p>1) Drop these tables from Mysql:</p>

<pre><code> innodb_index_stats
 innodb_table_stats
 slave_master_info
 slave_relay_log_info
 slave_worker_info
</code></pre>

<p>2) Delete <code>*.frm</code> and <code>*.ibd</code> files for the 5 tables above.</p>

<p>3) Create the tables by running the following queries:</p>

<pre><code>CREATE TABLE `innodb_index_stats` (
  `database_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `table_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `index_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `stat_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `stat_value` bigint(20) unsigned NOT NULL,
  `sample_size` bigint(20) unsigned DEFAULT NULL,
  `stat_description` varchar(1024) COLLATE utf8_bin NOT NULL,
  PRIMARY KEY (`database_name`,`table_name`,`index_name`,`stat_name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0;

CREATE TABLE `innodb_table_stats` (
  `database_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `table_name` varchar(64) COLLATE utf8_bin NOT NULL,
  `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  `n_rows` bigint(20) unsigned NOT NULL,
  `clustered_index_size` bigint(20) unsigned NOT NULL,
  `sum_of_other_index_sizes` bigint(20) unsigned NOT NULL,
  PRIMARY KEY (`database_name`,`table_name`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0;

CREATE TABLE `slave_master_info` (
  `Number_of_lines` int(10) unsigned NOT NULL COMMENT 'Number of lines in the file.',
  `Master_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log currently being read from the master.',
  `Master_log_pos` bigint(20) unsigned NOT NULL COMMENT 'The master log position of the last read event.',
  `Host` char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' COMMENT 'The host name of the master.',
  `User_name` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used to connect to the master.',
  `User_password` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used to connect to the master.',
  `Port` int(10) unsigned NOT NULL COMMENT 'The network port used to connect to the master.',
  `Connect_retry` int(10) unsigned NOT NULL COMMENT 'The period (in seconds) that the slave will wait before trying to reconnect to the master.',
  `Enabled_ssl` tinyint(1) NOT NULL COMMENT 'Indicates whether the server supports SSL connections.',
  `Ssl_ca` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Authority (CA) certificate.',
  `Ssl_capath` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path to the Certificate Authority (CA) certificates.',
  `Ssl_cert` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL certificate file.',
  `Ssl_cipher` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the cipher in use for the SSL connection.',
  `Ssl_key` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL key file.',
  `Ssl_verify_server_cert` tinyint(1) NOT NULL COMMENT 'Whether to verify the server certificate.',
  `Heartbeat` float NOT NULL,
  `Bind` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays which interface is employed when connecting to the MySQL server',
  `Ignored_server_ids` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number of server IDs to be ignored, followed by the actual server IDs',
  `Uuid` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid.',
  `Retry_count` bigint(20) unsigned NOT NULL COMMENT 'Number of reconnect attempts, to the master, before giving up.',
  `Ssl_crl` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Revocation List (CRL)',
  `Ssl_crlpath` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used for Certificate Revocation List (CRL) files',
  `Enabled_auto_position` tinyint(1) NOT NULL COMMENT 'Indicates whether GTIDs will be used to retrieve events from the master.',
  PRIMARY KEY (`Host`,`Port`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Master Information';

CREATE TABLE `slave_relay_log_info` (
  `Number_of_lines` int(10) unsigned NOT NULL COMMENT 'Number of lines in the file or rows in the table. Used to version table definitions.',
  `Relay_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the current relay log file.',
  `Relay_log_pos` bigint(20) unsigned NOT NULL COMMENT 'The relay log position of the last executed event.',
  `Master_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log file from which the events in the relay log file were read.',
  `Master_log_pos` bigint(20) unsigned NOT NULL COMMENT 'The master log position of the last executed event.',
  `Sql_delay` int(11) NOT NULL COMMENT 'The number of seconds that the slave must lag behind the master.',
  `Number_of_workers` int(10) unsigned NOT NULL,
  `Id` int(10) unsigned NOT NULL COMMENT 'Internal Id that uniquely identifies this record.',
  PRIMARY KEY (`Id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Relay Log Information';

CREATE TABLE `slave_worker_info` (
  `Id` int(10) unsigned NOT NULL,
  `Relay_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  `Relay_log_pos` bigint(20) unsigned NOT NULL,
  `Master_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  `Master_log_pos` bigint(20) unsigned NOT NULL,
  `Checkpoint_relay_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  `Checkpoint_relay_log_pos` bigint(20) unsigned NOT NULL,
  `Checkpoint_master_log_name` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
  `Checkpoint_master_log_pos` bigint(20) unsigned NOT NULL,
  `Checkpoint_seqno` int(10) unsigned NOT NULL,
  `Checkpoint_group_size` int(10) unsigned NOT NULL,
  `Checkpoint_group_bitmap` blob NOT NULL,
  PRIMARY KEY (`Id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Worker Information';
</code></pre>

<p>4) Restart your MySQL server.</p>
"
"<p>I have a relational model in Oracle SQL Developer Data Modeler with tables and relationships. Is it possible to export that relational model to a image file?</p>
","<p>It is possible, but looks like is not very intuitive.</p>

<ol>
<li>Zoom your relational model to the expected resolution (at least to be readable).</li>
<li>Go to <strong>File</strong>, then <strong>Data Modeler</strong>, then <strong>Print Diagram</strong> and selected the desired format.</li>
</ol>

<p>This will generate an image file with your relational model with the current zoom level as the resolution.</p>
"
"<p>I need to <strong>make an audit module</strong> to my Java Web App. I use EclipseLink, not Hibernate (can't use Envers). I searched a lot some way to get the SQL or JPQL that JPA is executing, so I could log something like this:</p>

<pre><code>System.out.println(""User "" + user + "" from "" + ip_address + "" executed "" + jpa_statement + "" at "" + new Date());
</code></pre>

<p>Actually, I'll save this info into a history database's table. So I can easily retrieve this info any time I want. That's why ""<strong>SHOW SQL</strong>"" parameters are not enough to me. I really need the SQL string, so I can manipulate it at my source code.</p>

<p>I found at JPA spec the <strong>EntityListener</strong> feature and thought it was the perfect place to put my logging code. For example, the postUpdate method could log the time the object was updated. But my problem is that I can't have the SQL the was executed.</p>

<p>Here is an example of what I mean:</p>

<pre><code>public class AuditListener {    
  @PostUpdate
  public void postUpdate(Object object) {
    User user = (User)FacesContext.getCurrentInstance().getExternalContext().getSessionMap().get(""user"");
    String ip_address =  (User)FacesContext.getCurrentInstance().getExternalContext().getSessionMap().get(""ip_address"");
    String jpa_statement = object.getSQL();
    System.out.println(""User "" + user + "" from "" + ip_address + "" executed "" + jpa_statement + "" at "" + new Date());
  }

}
</code></pre>

<p><strong>But ""object.getSQL()"" doesn't exists. So how can I get the SQL statement?</strong></p>

<p>If anyone could point me into the right direction, I would appreciate!</p>
","<p>EclipseLink has full support for history tracking.</p>

<p>See,
<a href=""http://wiki.eclipse.org/EclipseLink/Examples/JPA/History"">http://wiki.eclipse.org/EclipseLink/Examples/JPA/History</a></p>

<p>JPA events do not contain what was changed, only the object.</p>

<p>EclipseLink also supports DesriptorEvents (see DescriptorEventListener) which also define a postUpdate but include an ObjectChangeSet that describe the changes, you also have the UpdateObjectQuery that contains the SQL and DatabaseRecord.</p>
"
"<p>I would like to record the id of a user in the session/transaction, using <code>SET</code>, so I could be able to access it later in a trigger function, using <code>current_setting</code>. Basically, I'm trying option n2 from a <a href=""https://stackoverflow.com/questions/13172524/passing-user-id-to-postgresql-triggers"">very similar ticket posted previously</a>, with the difference that I'm using PG 10.1 .</p>

<p>I've been trying 3 approaches to setting the variable:</p>

<ul>
<li><code>SET local myvars.user_id = 4</code>, thereby setting it locally in the transaction;</li>
<li><code>SET myvars.user_id = 4</code>, thereby setting it in the session;</li>
<li><code>SELECT set_config('myvars.user_id', '4', false)</code>, which depending of the last argument, will be a shortcut for the previous 2 options.</li>
</ul>

<p>None of them is usable in the trigger, which receives <code>NULL</code> when getting the variable through <code>current_setting</code>. Here is a script I've devised to troubleshoot it (can be easily used with the postgres docker image):</p>

<pre><code>database=$POSTGRES_DB
user=$POSTGRES_USER
[ -z ""$user"" ] &amp;&amp; user=""postgres""

psql -v ON_ERROR_STOP=1 --username ""$user"" $database &lt;&lt;-EOSQL
    DROP TRIGGER IF EXISTS add_transition1 ON houses;
    CREATE TABLE IF NOT EXISTS houses (
        id SERIAL NOT NULL,
        name VARCHAR(80),
        created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT now(),
        PRIMARY KEY(id)
    );

    CREATE TABLE IF NOT EXISTS transitions1 (
        id SERIAL NOT NULL,
        house_id INTEGER,
        user_id INTEGER,
        created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT now(),
        PRIMARY KEY(id),
        FOREIGN KEY(house_id) REFERENCES houses (id) ON DELETE CASCADE

    );

    CREATE OR REPLACE FUNCTION add_transition1() RETURNS TRIGGER AS \$\$
        DECLARE
            user_id integer;
        BEGIN
            user_id := current_setting('myvars.user_id')::integer || NULL;
            INSERT INTO transitions1 (user_id, house_id) VALUES (user_id, NEW.id);
            RETURN NULL;
        END;
    \$\$ LANGUAGE plpgsql;

    CREATE TRIGGER add_transition1 AFTER INSERT OR UPDATE ON houses FOR EACH ROW EXECUTE PROCEDURE add_transition1();

    BEGIN;
    %1% SELECT current_setting('myvars.user_id');
    %2% SELECT set_config('myvars.user_id', '55', false);
    %3% SELECT current_setting('myvars.user_id');
    INSERT INTO houses (name) VALUES ('HOUSE PARTY') RETURNING houses.id;
    SELECT * from houses;
    SELECT * from transitions1;
    COMMIT;
    DROP TRIGGER IF EXISTS add_transition1 ON houses;
    DROP FUNCTION IF EXISTS add_transition1;
    DROP TABLE transitions1;
        DROP TABLE houses;
EOSQL
</code></pre>

<p>The conclusion I came to was that the function is triggered in a different transaction and a different (?) session. Is this something that one can configure, so that all happens within the same context? </p>
","<p>It is not clear why you are trying to concat <code>NULL</code> to <code>user_id</code> but it is obviously the cause of the problem. Get rid of it:</p>

<pre><code>CREATE OR REPLACE FUNCTION add_transition1() RETURNS TRIGGER AS $$
    DECLARE
        user_id integer;
    BEGIN
        user_id := current_setting('myvars.user_id')::integer;
        INSERT INTO transitions1 (user_id, house_id) VALUES (user_id, NEW.id);
        RETURN NULL;
    END;
$$ LANGUAGE plpgsql;
</code></pre>

<p>Note that</p>

<pre><code>SELECT 55 || NULL
</code></pre>

<p>always gives <code>NULL</code>.</p>
"
"<p>We've got a couple of on-premises dbs and we're seeing if we can migrate them to SQL Azure. Some of those dbs have a couple of user defined functions written in C# in an assembly (SAFE). After running a search, I've found a couple of posts which contradict each other. Some say that v12 supports CLR code. Others say it doesn't. So, here are my questions:</p>

<ul>
<li>Does V12 supports embedding clr assemblies?</li>
<li>How can we export the generation script for azure with the assembly? Whenever we set the export option to azure we end up getting an error saying that clr user defined functions are not supported in azure.</li>
</ul>

<p>Thanks guys!</p>

<p>Luis</p>
","<p>CLR Functions are not supported in Azure:</p>

<p>Check here:</p>

<p><a href=""https://azure.microsoft.com/en-gb/documentation/articles/sql-database-transact-sql-information/"" rel=""noreferrer"">Azure SQL Database Transact-SQL differences</a></p>

<p>Under unsupported features it mentions "".NET Framework CLR integration with SQL Server""</p>

<p>I believe there may be some confusion as to whether it does or doesn't support them as they used to in one version, then they removed support.</p>

<p>Here is a link detailing the fact they were supported, but got pulled, apparently due to a security issue:</p>

<p><a href=""https://www.brentozar.com/archive/2016/04/breaking-news-literally-sql-clr-support-removed-azure-sql-db/"" rel=""noreferrer"">Breaking News, Literally: SQL CLR Support Removed from Azure SQL DB</a></p>
"
"<p>We have a Old 5.1 Mysql server running on server 2003. Recently we move to a newer environment with Mysql 5.6 and server 2008. Now on the new server we keep getting errors when inserting special chars like ''.</p>

<p>Now I have checked the source encoding and it is UTF-8. But the old Mysql server was configured as latin1(Server / tables / colonms) with collation latin_swedish_ci and we did not receive any errors on the old environment.</p>

<p>Now I have done some testing since we are not live on the new environment. I have tried setting all tables to tables / colonms as well as latin1. In both cases I keep getting these errors.</p>

<p>What I noticed is that on the old server the servers default char-set is latin1 and on the new server its utf-8. Could that be the problem? I find this very strange because the source is utf-8.</p>

<p>Is there maybe some option to handle this that could be turned on on the old environment? I'm not sure if something like that exists. I did compare the settings within the mysql admin tool and apart from the default char-set it looks the same.</p>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>SHOW VARIABLES LIKE 'char%';</p>
</blockquote>

<p><strong>Old server:</strong></p>

<pre><code>+--------------------------+-----------------------------------------------+
| Variable_name            | Value                                         |
+--------------------------+-----------------------------------------------+
| character_set_client     | utf8                                          | *
| character_set_connection | utf8                                          | *
| character_set_database   | latin1                                        |
| character_set_filesystem | binary                                        |
| character_set_results    | utf8                                          | *
| character_set_server     | latin1                                        |
| character_set_system     | utf8                                          |
</code></pre>

<p><strong>New Server:</strong></p>

<pre><code>+--------------------------+-----------------------------------------------+
| Variable_name            | Value                                         |
+--------------------------+-----------------------------------------------+
| character_set_client     | utf8mb4                                       | *
| character_set_connection | utf8mb4                                       | *
| character_set_database   | utf8                                          |
| character_set_filesystem | binary                                        |
| character_set_results    | utf8mb4                                       | *
| character_set_server     | utf8                                          |
| character_set_system     | utf8                                          |
</code></pre>

<p>As far as I understand from the article over at the MySQL site utf8mb4 is a super-set of utf8 this should not create a problem for encoding I think since they are basically identical on encoding right?</p>
","<p>The key question is whether your ODBC client executable -- the thing that's going to load the driver library and use the data -- is 32-bit or 64-bit.  64-bit Windows (XP, Vista, 7, 8, Server 2003, Server 2008, and all other variants to date) supports both 32-bit and 64-bit binary executables/libraries.  32-bit executables (usually found in <code>Program Files (x86)</code>) can only use 32-bit drivers; 64-bit executables (usually found in <code>Program Files</code>) can only use 64-bit drivers.</p>

<p>Once you've figured that part out, you have to install a matching 32-bit or 64-bit driver for MySQL, and configure it with the right ODBC Administrator.  The 32-bit ODBC Administrator is counterintuitively found at <code>C:\Windows\SysWow64\odbcad32.exe</code>, and the 64-bit ODBC Administrator is likewise counterintuitively found at <code>C:\Windows\System32\odbcad32.exe</code>.  (Yes, both are named <code>odbcad32</code> and the directory names suggest the other bitness -- but what I've just said is accurate.)</p>

<p>For more on this, you can read <a href=""http://wikis.openlinksw.com/UdaWikiWeb/Win32vs64OdbcAdmin"" rel=""noreferrer"">the article</a> posted on my <a href=""http://www.openlinksw.com/"" rel=""noreferrer"">employer</a>'s website</p>

<p>Note that for added fun, Microsoft's ODBC driver manager (the MDAC) has a bug in that <a href=""http://support.microsoft.com/kb/942976"" rel=""noreferrer"">it shows 32-bit User DSNs to 64-bit client executables (including the Administrator), and it shows 64-bit User DSNs to 32-bit client executables (again, including the Administrator)</a> -- even though these mis-matches cannot work together.  For this reason, I strongly recommend using <em>only</em> System DSNs in any environment that may have a mix of 32-bit and 64-bit executables/drivers/DSNs.</p>
"
"<p>I am using a <code>QSqlTableModel</code> and <code>QTableView</code> to view an SQLite database table.</p>

<p>I would like to have the table auto refresh every second or so (it's not going to be a very large table - a couple of hundred rows). And i can do this - like so:</p>

<pre class=""lang-cpp prettyprint-override""><code>QTimer *updateInterval = new QTimer(this);
updateInterval-&gt;setInterval(1000);
updateInterval-&gt;start();
connect(updateInterval, SIGNAL(timeout()),this, SLOT(update_table()));

...

void MainWindow::update_table()
{
    model-&gt;select(); //QSqlTableModel*
    sqlTable-&gt;reset(); //QTableView*
}
</code></pre>

<p>But this removes any selection I have, so the selections only last for up to a second. This is annoying, as another pane in the GUI depends on what is selected. If nothing is selected, then it resets to an explanation splash page.</p>

<p>I then tried a somewhat hacky approach, which gets the selected row number, resets the table, and then selects that row. But this doesn't work either, as the selected row can move up or down based on additions to the table.</p>

<p>I know other classes have a <code>dataChanged()</code> signal, which would be ideal.</p>

<p>Do any of you know <strong>how I could have the table refresh to reflect changes to the database</strong> (from either command line usage, or other instances of the program) <strong>AND keep the current selection?</strong></p>

<p>I know I could get data from the current selection, and then after the reset search for the same row and then reselect it, but this seems like a counter productive and bad solution to the problem.</p>

<p>EDIT: Current attempt at solution:</p>

<pre><code>void MainWindow::update_table()
{    

    QList&lt;QModelIndex&gt; selection = sqlTable-&gt;selectionModel()-&gt;selection().indexes();
    QList&lt;int&gt; selectedIDs;
    bool somethingSelected = true;

    for(QList&lt;QModelIndex&gt;::iterator i = selection.begin(); i != selection.end(); ++i){
        int col = i-&gt;column();
        QVariant data = i-&gt;data(Qt::DisplayRole);

    if(col == 0) {
            selectedIDs.append(data.toInt());
        }
    }

    if(selectedIDs.empty()) somethingSelected = false;

    model-&gt;select();
    sqlTable-&gt;reset();

    if(somethingSelected){
        QList&lt;int&gt; selectedRows;

        int rows = model-&gt;rowCount(QModelIndex());
        for(int i = 0; i &lt; rows; ++i){
            sqlTable-&gt;selectRow(i);
            if(selectedIDs.contains(sqlTable-&gt;selectionModel()-&gt;selection().indexes().first().data(Qt::DisplayRole).toInt())) selectedRows.append(i);
    }

    for(QList&lt;int&gt;::iterator i = selectedRows.begin(); i != selectedRows.end(); ++i){
        sqlTable-&gt;selectRow(*i);
    }
}
}
</code></pre>

<p>Okay so this more or less works now...</p>
","<p>The view draws the background based on the <code>Qt::BackgroundRole</code> role of the cell which is the <code>QBrush</code> value returned by <code>QAbstractItemModel::data(index, role)</code> for that role.</p>

<p>You can subclass the <code>QSqlQueryModel</code> to redefine <code>data()</code> to return your calculated color, or if you have Qt > 4.8, you can use a <a href=""http://qt-project.org/doc/qt-4.8/qidentityproxymodel.html""><code>QIdentityProxyModel</code></a>:</p>

<pre><code>class MyModel : public QIdentityProxyModel
{
    QColor calculateColorForRow(int row) const {
        ...
    }

    QVariant data(const QModelIndex &amp;index, int role)
    {
        if (role == Qt::BackgroundRole) {
           int row = index.row();
           QColor color = calculateColorForRow(row);           
           return QBrush(color);
        }
        return QIdentityProxyModel::data(index, role);
    }
};
</code></pre>

<p>And use that model in the view, with the sql model set as source with <code>QIdentityProxyModel::setSourceModel</code>.</p>

<h1>OR</h1>

<p>You can keep the model unchanged and modify the background with a delegate set on the view with <code>QAbstractItemView::setItemDelegate</code>:</p>

<pre><code>class BackgroundColorDelegate : public QStyledItemDelegate {

public:
    BackgroundColorDelegate(QObject *parent = 0)
        : QStyledItemDelegate(parent)
    {
    }
    QColor calculateColorForRow(int row) const;

    void initStyleOption(QStyleOptionViewItem *option,
                         const QModelIndex &amp;index) const
    {
        QStyledItemDelegate::initStyleOption(option, index);

        QStyleOptionViewItemV4 *optionV4 =
                qstyleoption_cast&lt;QStyleOptionViewItemV4*&gt;(option);

        optionV4-&gt;backgroundBrush = QBrush(calculateColorForRow(index.row()));
    }
};
</code></pre>

<p>As the last method is not always obvious to translate from C++ code, here is the equivalent in python:</p>

<pre><code>def initStyleOption(self, option, index):
    super(BackgroundColorDelegate,self).initStyleOption(option, index)
    option.backgroundBrush = calculateColorForRow(index.row())
</code></pre>
"
"<p>I'm trying to store a query result in a temporary table for further processing.</p>

<pre><code>create temporary table tmpTest
(
    a FLOAT,
    b FLOAT,
    c FLOAT
)
engine = memory;

insert into tmpTest
(
    select a,b,c from someTable
    where ...
);
</code></pre>

<p>But for some reason the insert takes up to a minute, whereas the subselect alone just takes a few seconds. Why would it take so much longer to write the data to a temporary table instead of printing it to my SQL management tool's output???</p>

<p><strong>UPDATE</strong>
My Setup:
MySQL 7.3.2 Cluster with
8 Debian Linux ndb data nodes
1 SQL Node (Windows Server 2012)</p>

<p>The table I'm running the select on is a ndb table.</p>

<p>I tried to find out, if the execution plan would differ when using 'insert into..', but they look the same:
(sorry for the formatting, stackoverflow doesn't have tables)</p>

<pre>
id  select_type     table       type    possible_keys   key     key_len ref                 rows        Extra
1   PRIMARY         &lt;subquery3&gt; ALL     \N              \N      \N      \N                  \N          \N
1   PRIMARY         foo         ref     PRIMARY         PRIMARY 3       &lt;subquery3&gt;.fooId   9747434     Using where
2   SUBQUERY        someTable   range   PRIMARY         PRIMARY 3       \N                  136933000   Using where with pushed condition; Using MRR; Using temporary; Using filesort
3   MATERIALIZED    tmpBar      ALL     \N              \N      \N      \N                  1000        \N
</pre>

<p>CREATE TABLE ... SELECT is slow, too. 47 seconds vs. 5 seconds without table insert/create.</p>
","<p>Error 157 is actually 'could not connect to storage engine' and the fact that MySQL fails to report that error correctly is a bug:  <a href=""http://bugs.mysql.com/bug.php?id=44817"" rel=""noreferrer"">http://bugs.mysql.com/bug.php?id=44817</a></p>

<p>The case described in that bug mentions that you get the error when you try to query a table in NDB when the cluster is still down.</p>

<p>So I'm just guessing, but I would conclude that your cluster is not started.  Either you missed starting one of the nodes, or else something went wrong starting one of the nodes.</p>
"
"<p>I'm able to connect to SQL server 2008 R2 when I use <code>Provider=SQLOLEDB</code> in my connection string. But when I use <code>Provider=SQLNCLI</code> in connection string I'm unable to connect.</p>

<blockquote>
  <p>ADODB.Connection error '800a0e7a'</p>
  
  <p>Provider cannot be found. It may not
  be properly installed.</p>
  
  <p>/test.asp, line 7</p>
</blockquote>

<p>Code written within <code>test.asp</code> is below</p>

<pre><code>&lt;%
    Set cn = Server.CreateObject(""ADODB.Connection"")

    'Doesn't work
    cn.Open ""Provider=SQLNCLI;Server=remoteServer\SQL2008R2;Database=DB;UID=MyUser;PWD=pa55word;""  

    'Works Perfectly
    'cn.Open ""Provider=SQLOLEDB;Server=remoteServer\SQL2008R2;Database=DB;UID=MyUser;PWD=pa55word;"" 

    cn.CommandTimeout = 900
    cn.Close
    Response.write(""dfjslkfsl"")
%&gt;
</code></pre>

<p>The SQL Server I'm trying to connect (from classic ASP Page within my IIS 7 on windows 7) is located on different server in a different network to which I'm connecting using VPN. </p>

<p>I tested sql native client by creating a sql native client System DSN connection to the said Sql server 2008 R2 (which is connected through VPN) from ODBC datasource administrator. And it got connected successfully.</p>

<p>These snaps are from my windows 7 system
<img src=""https://i.stack.imgur.com/HI3Ks.png"" alt=""Appwiz.cpl snap""></p>

<p><img src=""https://i.stack.imgur.com/RgOfM.png"" alt=""IIS 7 features""></p>

<p><img src=""https://i.stack.imgur.com/r5crO.png"" alt=""enter image description here""></p>

<ul>
<li>Windows 7</li>
<li>IIS 7</li>
<li>Classic ASP page (.asp)</li>
</ul>
","<p>Try changing the provider to <code>sqlncli10</code>:</p>

<pre><code>cn.Open ""Provider=SQLNCLI10;Server=remoteServer\SQL2008R2;Database=DB;UID=MyUser;PWD=pa55word;""
</code></pre>

<p>Maybe the name is differet on your machine. :)</p>
"
"<p>Hi wondering if perhaps someone could shed some light on the below error. The sql works fine locally but i get the the  below error remotely.</p>

<p>SQL query: </p>

<pre><code>   SELECT COUNT(node.nid), 
          node.nid AS nid, 
          node_data_field_update_date.field_update_date_value AS node_data_field_update_date_field_update_date_value
     FROM node node
LEFT JOIN content_type_update node_data_field_update_date ON node.vid = node_data_field_update_date.vid
    WHERE node.type IN ('update')
ORDER BY node_data_field_update_date_field_update_date_value DESC
</code></pre>

<p>MySQL said: </p>

<blockquote>
  <p>#1140 - Mixing of GROUP columns (MIN(),MAX(),COUNT(),...) with no
  GROUP columns is illegal if there is
  no GROUP BY clause`</p>
</blockquote>
","<p>When using an aggregate function, such as <code>COUNT</code>, you need to include a <a href=""http://dev.mysql.com/doc/refman/5.0/en/group-by-functions.html""><code>GROUP BY</code></a> clause.    </p>

<pre><code>SELECT 
    type as type, 
    COUNT(*) AS total
FROM page AS p
WHERE p.parent_id = p.page_id
GROUP BY type
</code></pre>

<p>As far as why this worked locally, but not on your live server; <a href=""http://dev.mysql.com/doc/refman/5.0/en/group-by-hidden-columns.html"">MySql doesn't require complete listing of non-aggregate columns in the <code>GROUP BY</code> clause by default</a>, but your live server probably has the <a href=""http://dev.mysql.com/doc/refman/5.1/en/server-sql-mode.html#sqlmode_only_full_group_by""><code>ONLY_FULL_GROUP_BY</code></a> option turned on.</p>
"
"<p>I have a set of call detail records, and from those records, I'm supposed to determine the average concurrent active calls per system, per hour (at a precision of one minute). If I query 7pm to 8pm, I should see the average concurrent calls for the hour (averaging the concurrent calls for each minute) within that hour (for each system). </p>

<p>So, I need a way to check for a count of active calls for 7:00-7:01, 7:01-7:02, etc then average those numbers. A call is considered active if the call's time and duration fall within the current minute being checked.</p>

<p>What makes this even more difficult is that it needs to span SQL 7.0 and SQL 2000 (some functions in 2000 aren't available in 7.0, such as GetUTCTime()), if I can just get 2000 working I'll be happy.</p>

<h2>What approaches to this problem can I take?</h2>

<p>I thought about looping through minutes (60) in the hour being checked and adding the count of calls that fall between that minute and then somehow cross referencing the duration to make sure that a call that starts at 7:00 pm and has a duration of 300 seconds shows active at 7:04, but I can't imagine how to approach the problem. I tried to figure out a way to weight each call against particular minute that would tell me if the call was active during that minute or not, but couldn't come up with an effective solution. </p>

<p>The data types here are the same as I have to query against. I don't have any control over the schema (other than possibly converting the data and inserting into another table with more appropriate data types). I've provided some example data that I know has concurrent active calls.</p>

<pre><code>CREATE TABLE Records(
  seconds char(10),
  time char(4),
  date char(8),
  dur int,
  system int,
  port int,
)

--seconds is an stime value. It's the difference of seconds from UTC 1/1/1970 00:00:00 to the current UTC time, we use it as an identifier (like epoch).
--time is the time the call was made.
--date is the day the call was made.
--dur is the duration of the call in seconds.
--system is the system number.
--port is the port on the system (not particularly relevant for this question).

INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924228','1923','20090416',105,2,2)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239923455','1910','20090416',884,1,97)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924221','1923','20090416',116,2,15)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924259','1924','20090416',90,1,102)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239923458','1910','20090416',891,2,1)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924255','1924','20090416',99,2,42)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924336','1925','20090416',20,2,58)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924293','1924','20090416',64,2,41)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239923472','1911','20090416',888,2,27)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924347','1925','20090416',25,1,100)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924301','1925','20090416',77,2,55)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924332','1925','20090416',52,2,43)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924240','1924','20090416',151,1,17)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924313','1925','20090416',96,2,62)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924094','1921','20090416',315,2,16)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239923643','1914','20090416',788,2,34)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924447','1927','20090416',6,2,27)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924342','1925','20090416',119,2,15)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924397','1926','20090416',76,2,41)
INSERT INTO Records(seconds, time, date, dur, system, port) VALUES('1239924457','1927','20090416',23,2,27)
</code></pre>
","<p>Check the estimated query plan for the sort, to perform one of the joins it may be choosing a merge join for example, and to achieve this it requires the data to be sorted first prior to the merge - at that point you've sorted.</p>
"
"<p>If I had both SQL Server 2008 and SQL Server 2012 installed locally, I would simply try this for myself; however I only have the newer version installed and would like to keep it that way.</p>

<ul>
<li>SQL Server 2008 comes with an assembly <code>Microsoft.SqlServer.Types.dll</code>, major version 10.</li>
<li>SQL Server 2012 comes with an assembly <code>Microsoft.SqlServer.Types.dll</code>, major version 11.</li>
</ul>

<p>Among other things, both assemblies expose a <a href=""http://msdn.microsoft.com/en-us/library/microsoft.sqlserver.types.sqlgeometrybuilder.aspx""><code>SqlGeometryBuilder</code> type</a>. The one notable difference between the two assembly versions is that the 2012 type has an additional overloaded method <code>AddCircularArc</code>, and the 2008 type does not.</p>

<p>Since <a href=""http://kentb.blogspot.com/2008/11/visual-studio-referencing-same-assembly.html"">it's not exactly trivial (and perhaps a bad idea) to reference both assemblies in parallel</a>, I wonder whether I can just use the 2012 version &mdash; even against a SQL Server 2008 instance, as long as I don't make use of <code>AddCircularArc</code>.</p>

<p>Can anyone share their experience if they have tried this?</p>
","<p>By default SqlClient uses version 10.0 of the Microsoft.SqlServer.Types assembly (even if you reference a newer version in your project). When two different versions of that assembly are loaded at the same time you may see strange runtime exceptions like ""System.InvalidCastException: Unable to cast object of type 'Microsoft.SqlServer.Types.SqlGeometry' to type 'Microsoft.SqlServer.Types.SqlGeometry'.""...</p>

<p>The following article describes some possibilities that you have to use the newer Microsoft.SqlServer.Types assemblies with SqlClient: 
<a href=""http://technet.microsoft.com/en-us/library/ms143179.aspx"">Breaking Changes to Database Engine Features in SQL Server 2012</a></p>

<p>The options are:</p>

<ul>
<li>Calling the GetSqlBytes method, instead of the Get methods (e.g. SqlGeometry.Deserialize(reader.GetSqlBytes(0)))</li>
<li>Using assembly redirection in the application configuration</li>
<li>Specifying a value of ""SQL Server 2012"" for the ""Type System Version"" attribute to force SqlClient to load version 11.0 of the assembly</li>
</ul>

<p>I personally favor the ""Type System Version"" connection string keyword. See the MSDN article here:
<a href=""http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlconnection.connectionstring.aspx"">SqlConnection.ConnectionString Property</a> and search for 'Type System Version'.</p>
"
"<p>I have a table in SQL Server 2005 which has approx 4 billion rows in it.  I need to delete approximately 2 billion of these rows.  If I try and do it in a single transaction, the transaction log fills up and it fails.  I don't have any extra space to make the transaction log bigger.  I assume the best way forward is to batch up the delete statements (in batches of ~ 10,000?).</p>

<p>I can probably do this using a cursor, but is the a standard/easy/clever way of doing this?</p>

<p>P.S. This table does not have an identity column as a PK. The PK is made up of an integer foreign key and a date.</p>
","<p>Every database in SQL Server 2000 has a <a href=""http://msdn.microsoft.com/en-us/library/aa260592%28v=sql.80%29.aspx"" rel=""noreferrer"">sysusers system table</a></p>

<p>Probably something like</p>

<pre><code>Use &lt;MyDatabase&gt;

Select 
  [name]
From
  sysusers
Where
  issqlrole = 1
</code></pre>

<p>will do the trick</p>
"
"<p>I have MySQL proxy running and I have a LUA with a function for <code>read_auth()</code> however the passwords that are passed during authentication are hashed (as expected).</p>

<p>I require them in a format which I can work with and post onwards, so cleartext. Enabling the cleartext plugin on the MySQL client has no effect, I suspect that MySQL proxy is not demanding the client sends it in cleartext so defaults to hashing.</p>

<p>So basically: do you have any ideas on how I would be able to get the clear text authentication details within the <code>read_auth()</code> function of MySQL proxy?</p>

<p>Note: my end goal is to auth with LDAP, however the only way I can get a password (hashed or not) is by actually binding to LDAP, it can not be obtained by searching.</p>
","<p>MySQL treats logins as specific to the host they originate from. You can have a different password from your home machine than the one you use on the server itself, and you can have entirely different sets of permissions granted to the same username from different origin hosts.</p>

<p>On PHPMyadmin, the database is running on the same server as the web server, and therefore refers to itself as <code>localhost</code>, with IP <code>127.0.0.1</code>.  Your machine on which Workbench is installed must access MySQL with different credentials than your <code>username@localhost</code>.  The server requires you to grant access to your username from any host you intend to connect from.</p>

<p>In PhpMyAdmin, you will need to grant access to your database from the remote host: (See also Pekka's answer for how to allow connections from <em>any</em> host)</p>

<pre><code>GRANT ALL PRIVILEGES on dbname.* TO yourusername@your_remote_hostname IDENTIFIED BY 'yourpassword';
</code></pre>

<p>To see all the grants you currently have on <code>localhost</code> so that you can duplicate them for the remote host:</p>

<pre><code>SHOW GRANTS FOR yourusername@localhost;
</code></pre>

<p>Additionally, the MySQL server needs to be setup to accept remote connections in the first place.  This isn't always the case, especially on web hosting platforms.  In the <code>my.cnf</code> file, the <code>skip-networking</code> line has to be removed or commented out. If there is no <code>skip-networking</code> line, you must comment out the line: </p>

<pre><code>bind-address = 127.0.0.1 
</code></pre>

<p>...then restart MySQL.</p>
"
"<p>I am having an issue with an SQLite database.  I am using the SQLite ODBC from <a href=""http://www.ch-werner.de/sqliteodbc/"" rel=""nofollow noreferrer"">http://www.ch-werner.de/sqliteodbc/</a>  Installed the 64-bit version and created the ODBC with these settings:</p>

<p><img src=""https://i.stack.imgur.com/TEcXd.png"" alt=""enter image description here""></p>

<p>I open my Access database and link to the datasource.  I can open the table, add records, but cannot delete or edit any records.  Is there something I need to fix on the ODBC side to allow this?  The error I get when I try to delete a record is: </p>

<blockquote>
  <p>The Microsoft Access database engine stopped the process because you and another user are attempting to change the same data at the same time.</p>
</blockquote>

<p>When I edit a record I get:</p>

<blockquote>
  <p>The record has been changed by another user since you started editing it.  If you save the record, you will overwrite the changed the other user made.</p>
</blockquote>

<p>Save record is disabled.  Only copy to clipboard or drop changes is available.</p>
","<p>My initial attempt to recreate your issue was unsuccessful. I used the following on my 32-bit test VM:</p>

<ul>
<li>Access 2010</li>
<li>SQLite 3.8.2</li>
<li>SQLite ODBC Driver 0.996</li>
</ul>

<p>I created and populated the test table [tbl1] as documented <a href=""http://www.sqlite.org/sqlite.html"" rel=""nofollow"">here</a>. I created an Access linked table and when prompted I chose both columns ([one] and [two]) as the Primary Key. When I opened the linked table in Datasheet View I was able to add, edit, and delete records without incident.</p>

<p>The only difference I can see between my setup and yours (apart from the fact that I am on 32-bit and you are on 64-bit) is that in the ODBC DSN settings I left the <code>Sync.Mode</code> setting at its default value of <code>NORMAL</code>, whereas yours appears to be set to <code>OFF</code>.</p>

<p>Try setting your <code>Sync.Mode</code> to <code>NORMAL</code> and see if that makes a difference.</p>

<p><strong>Edit re: comments</strong></p>

<p>The solution in this case was the following:</p>

<blockquote>
  <p>One possible workaround would be to create a new SQLite table with all the same columns plus a new INTEGER PRIMARY KEY column which Access will ""see"" as AutoNumber. You can create a unique index on (what are currently) the first four columns to ensure that they remain unique, but the new new ""identity"" (ROWID) column is what Access would use to identify rows for CRUD operations.</p>
</blockquote>
"
"<p>I know it's a stupid question, but I could not find the answer anywhere.
How to set a default value for a column in <strong>sqlite.net</strong> model?
Here is my model class:</p>

<pre><code>public class ItemTaxes
{
    [PrimaryKey]
    public string Sku { get; set;}
    public bool IsTaxable { get; set;}// How to set IsTaxable's default value to true?
    public decimal PriceTaxExclusive { get; set;}
}
</code></pre>

<p>I wanna set default value of <strong>Not Null</strong> column <strong>IsTaxable</strong> to <strong>true</strong>, how should I achieve that? Btw I do not want use raw sql statement, i.e. <code>conn.execute();</code></p>

<p>Thank you.</p>
","<p>If you change the Key to a nullable type (int?) it should work. then SQLite sees null coming in and generates new id when needed.  </p>

<pre><code>public class LogEntry
    {
      [PrimaryKey, AutoIncrement]
      public int? Key { get; set;}
      public DateTime Date { get; set; }
    }
</code></pre>
"
"<p>I am using <strong>Room Persistence Library 1.1.0</strong>. I could find the database file at <code>/data/data/&lt;package_name&gt;/databases/</code> using Android Studio's  Device File Explorer. It contains multiple tables and I can access contents of that tables without any problem using <code>room-DAO</code>s. However when opening with <code>sqlite-browser</code>, is shows no table.</p>

<p>What might be the reason? Is it possible to resolve the issue without switching back to old <code>SQLiteOpenHelper</code> from <code>room</code>?</p>
","<h1>Solution</h1>

<p>To open <em>such databases<sup></em></sup>* with <code>sqlite-browser</code>, <strong>you need to copy all three files</strong>. All must be in the same directory too.</p>

<p><sup>* Databases stored in multiple files as stated in the question. </sup></p>

<p><br/></p>

<h1>Why three files?</h1>

<p>As per docs, Starting from version <code>1.1.0</code>, Room uses <code>write-ahead logging</code> as default journal mode for devices which has sufficient RAM and running on API Level 16 or higher. It was <code>Truncate</code> for all devices until this version. <code>write-ahead logging</code> has different internal structure compared to <code>Truncate</code>.</p>

<p><br/></p>

<p>Take a look at the files temporary files used by <code>SQLite</code> now and then :</p>

<p><strong>Until version 1.1.0</strong></p>

<p><a href=""https://i.stack.imgur.com/IqfWG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IqfWG.png"" alt=""enter image description here""></a></p>

<p><strong>From version 1.1.0</strong></p>

<p><a href=""https://i.stack.imgur.com/Hc6em.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hc6em.png"" alt=""enter image description here""></a></p>

<p><br/></p>

<p>If you want to change the journal mode explicitly to <code>Truncate</code>, you can do it this way. But, <strong>it is not recommended</strong> because <code>WAL</code> is much better compared to <code>Truncate</code>.</p>

<pre><code>public static void initialize(Context context) {
    sAppDatabase = Room.databaseBuilder(
            context,
            AppDatabase.class,
            DATABASE_NAME)
        .setJournalMode(JournalMode.TRUNCATE).build();
}
</code></pre>

<p><br/></p>

<h1>Is it possible to move it to single file without changing to <code>Truncate</code> ?</h1>

<p>Yes, it is. Query the following statement against the database.</p>

<pre><code>@Query(""pragma wal_checkpoint(full)"")
void checkpoint ();
</code></pre>

<p><a href=""https://stackoverflow.com/a/51560124/7594961"">Here</a> is a related post by myself.</p>
"
"<p>I am looking for a definite reference of the SQL as understood by Microsoft Access. All the links I can find talk only about bits and pieces. Ideally I am looking for a grammar specification with details of what all the different keywords do.</p>

<p>Motivation: I am trying to write my own parser for Microsoft Access SQL statements.</p>
","<p>""Note"" is a reserved word in Microsoft Access.  You need to surround it with square brackets:</p>

<pre><code>INSERT INTO Grocery_Store_Prices(Store,Item,Brand,Price,Unit,Quantity,[Note])
VALUES(""Kroger"",""Cheesy Poof"",""Cartman"",0.51,""fart"",15,""what the ____"");
</code></pre>

<p>Helpful list of reserved words here: <a href=""http://support.microsoft.com/kb/286335"">http://support.microsoft.com/kb/286335</a></p>

<p>Some consider it best practice to <em>always</em> encase field names in square brackets, just so you don't have to worry about it.</p>

<p>Good luck!</p>
"
"<p>How can I create a stored procedure to start a SQL Server job?</p>
","<p>If I have open a package in BIDS (""Business Intelligence Development Studio"", the tool you use to design the packages), and do not select any item in it, I have a ""Properties"" pane in the bottom right containing - among others, the <code>MaximumErrorCount</code> property. If you do not see it, maybe it is minimized and you have to open it (have a look at tabs in the right).</p>

<p>If you cannot find it this way, try the menu: View/Properties Window.</p>

<p>Or try the F4 key.</p>
"
"<p>I am implementing repository pattern in RxJava using SqlBrite/SqlDelight for offline data storage and retrofit for Http requests</p>

<p>Here's a sample of that:</p>

<pre><code>protected Observable&lt;List&lt;Item&gt;&gt; getItemsFromDb() {
     return database.createQuery(tableName(), selectAllStatement())
             .mapToList(cursor -&gt; selectAllMapper().map(cursor));
 }


public Observable&lt;List&lt;Item&gt;&gt;getItems(){
     Observable&lt;List&lt;Item&gt;&gt; server = getRequest()
                 .doOnNext(items -&gt; {
                     BriteDatabase.Transaction transaction = database.newTransaction();
                     for (Item item : items){
                         database.insert(tableName(), contentValues(item));
                     }
                     transaction.markSuccessful();
                     transaction.end();
                 })
                 .flatMap(items -&gt; getItemsFromDbById())
                 .delaySubscription(200, TimeUnit.MILLISECONDS);
         Observable&lt;List&lt;Item&gt;&gt; db = getItemsFromDbById(id)
                 .filter(items -&gt; items != null &amp;&amp; items.size() &gt; 0);
     return Observable.amb(db, server).doOnSubscribe(() -&gt; server.subscribe(items -&gt; {}, throwable -&gt; {}));
 }
</code></pre>

<p>The current implementation uses <code>Observable.amb</code> to get latest of 2 streams and returns <code>db</code> stream in case <code>db</code> has data or server otherwise. To prevent early failure in case of no internet, <code>server</code> has a <code>delaySubscription</code> on it with <code>200ms</code>.</p>

<p>I tried using <code>Observable.concat</code> but the SqlBrite stream never calls <code>onComplete</code> so <code>server</code> observable is never triggered.</p>

<p>I also tried <code>Observable.combineLatest</code> which didn't work because it keeps waiting for <code>server</code> observable to return data before emitting anything and <code>Observable.switchOnNext</code> didn't work either.</p>

<p>What I am looking for is a repository which:</p>

<ul>
<li>Keeps the subscription to SqlBrite (DB) open, in case of DB updates</li>
<li>Always fetches data from server and writes it to database</li>
<li>Should not emit empty result in case there was nothing in database and network request is still going on. This, because the user should see a progress bar in the case of the first load.</li>
</ul>
","<p><strong>EDIT December 27th 2016:</strong>
SQLBrite version 1.1.0 and up now exposes its own <code>getWritableDatabase()</code></p>

<p>It's worth noting that calling <code>getWritableDatabase()</code> can potentially create or migrate a database so make sure you're calling it from a background thread!</p>
"
"<p>I am implementing repository pattern in RxJava using SqlBrite/SqlDelight for offline data storage and retrofit for Http requests</p>

<p>Here's a sample of that:</p>

<pre><code>protected Observable&lt;List&lt;Item&gt;&gt; getItemsFromDb() {
     return database.createQuery(tableName(), selectAllStatement())
             .mapToList(cursor -&gt; selectAllMapper().map(cursor));
 }


public Observable&lt;List&lt;Item&gt;&gt;getItems(){
     Observable&lt;List&lt;Item&gt;&gt; server = getRequest()
                 .doOnNext(items -&gt; {
                     BriteDatabase.Transaction transaction = database.newTransaction();
                     for (Item item : items){
                         database.insert(tableName(), contentValues(item));
                     }
                     transaction.markSuccessful();
                     transaction.end();
                 })
                 .flatMap(items -&gt; getItemsFromDbById())
                 .delaySubscription(200, TimeUnit.MILLISECONDS);
         Observable&lt;List&lt;Item&gt;&gt; db = getItemsFromDbById(id)
                 .filter(items -&gt; items != null &amp;&amp; items.size() &gt; 0);
     return Observable.amb(db, server).doOnSubscribe(() -&gt; server.subscribe(items -&gt; {}, throwable -&gt; {}));
 }
</code></pre>

<p>The current implementation uses <code>Observable.amb</code> to get latest of 2 streams and returns <code>db</code> stream in case <code>db</code> has data or server otherwise. To prevent early failure in case of no internet, <code>server</code> has a <code>delaySubscription</code> on it with <code>200ms</code>.</p>

<p>I tried using <code>Observable.concat</code> but the SqlBrite stream never calls <code>onComplete</code> so <code>server</code> observable is never triggered.</p>

<p>I also tried <code>Observable.combineLatest</code> which didn't work because it keeps waiting for <code>server</code> observable to return data before emitting anything and <code>Observable.switchOnNext</code> didn't work either.</p>

<p>What I am looking for is a repository which:</p>

<ul>
<li>Keeps the subscription to SqlBrite (DB) open, in case of DB updates</li>
<li>Always fetches data from server and writes it to database</li>
<li>Should not emit empty result in case there was nothing in database and network request is still going on. This, because the user should see a progress bar in the case of the first load.</li>
</ul>
","<p>The solution to your problem is actually super easy and clean if you change the way of thinking a bit. I am using the exact same data interaction (Retrofit + Sqlbrite) and this solution works perfectly.</p>

<p>What you have to do is to use two separate observable subscriptions, that take care of completely different processes.</p>

<ol>
<li><code>Database</code> <code>-&gt;</code> <code>View</code>: This one is used to attach your <code>View</code> (<code>Activity</code>, <code>Fragment</code> or whatever displays your data) to the persisted data in db. You subscribe to it ONCE for created <code>View</code>.</li>
</ol>

<p></p>

<pre><code>dbObservable
        .subscribeOn(Schedulers.io())
        .observeOn(AndroidSchedulers.mainThread())
        .subscribe(data -&gt; {
            displayData(data);
        }, throwable -&gt; {
            handleError(throwable);
        });
</code></pre>

<ol start=""2"">
<li><code>API</code> <code>-&gt;</code> <code>Database</code>: The other one to fetch the data from api and persist it in the db. You subscribe to it every time you want to refresh your data in the database.</li>
</ol>

<p></p>

<pre><code>apiObservable
        .subscribeOn(Schedulers.io())
        .observeOn(Schedulers.io())
        .subscribe(data -&gt; {
           storeDataInDatabase(data);
        }, throwable -&gt; {
            handleError(throwable);
        });
</code></pre>

<hr>

<p>EDIT:</p>

<p>You don't want to ""transform"" both observables into one, purely for the reason you've included in your question. Both observables act completely differently.</p>

<p>The <code>observable</code> from Retrofit acts like a <a href=""http://reactivex.io/RxJava/javadoc/rx/Single.html"" rel=""nofollow noreferrer""><code>Single</code></a>. It does what it needs to do, and finishes (with <code>onCompleted</code>).</p>

<p>The <code>observable</code> from Sqlbrite is a typical <code>Observable</code>, it will emit something every time a specific table changes. Theoretically it should finish in the future.</p>

<p>Ofc you can work around that difference, but it would lead you far, far away from having a clean and easily readable code.</p>

<p>If you really, really need to expose a single <code>observable</code>, you can just <strong>hide</strong> the fact that you're actually subscribing to the observable from retrofit when subscribing to your database.</p>

<ol>
<li>Wrap the Api subscription in a method:</li>
</ol>

<p></p>

<pre><code>public void fetchRemoteData() {
    apiObservable
            .subscribeOn(Schedulers.io())
            .observeOn(Schedulers.io())
            .subscribe(data -&gt; {
                persistData(data);
            }, throwable -&gt; {
                handleError(throwable);
            });
}
</code></pre>

<ol start=""2"">
<li><code>fetchRemoteData</code> on subscription</li>
</ol>

<p></p>

<pre><code>dbObservable
        .subscribeOn(Schedulers.io())
        .observeOn(AndroidSchedulers.mainThread())
        .doOnSubscribe(() -&gt; fetchRemoteData())
        .subscribe(data -&gt; {
            displayData(data);
        }, throwable -&gt; {
            handleError(throwable);
        });
</code></pre>

<p>I suggest you really think about all that. Because the fact that you're forcing yourself into the position where you need a single observable, might be restricting you quite badly. I believe that this will be the exact thing that will force you to change your concept in the future, instead of protecting you from the change itself.</p>
"
"<p>I need for a particular business scenario to set a field on an entity (not the PK) a number from a sequence (the sequence has to be a number between min and max</p>

<p>I defined the sequence like this :</p>

<pre><code>CREATE SEQUENCE MySequence
  MINVALUE 65536 
  MAXVALUE 4294967296 
  START WITH 65536
  INCREMENT BY 1
  CYCLE
  NOCACHE
  ORDER;
</code></pre>

<p>In Java code I retrieve the number from the sequence like this :</p>

<pre><code>select mySequence.nextval from dual
</code></pre>

<p>My question is :</p>

<p>If I call this ""<code>select mySequence.nextval from dual</code>"" in a transaction and in the same time in another transaction same method is called (parallel requests) it is sure that the values returned by the sequence are different ?</p>

<p>Is not possible to have like read the uncommitted value from the first transaction ?</p>

<p>Cause let's say I would have not used sequence and a plain table where I would increment myself the sequence, then the transaction 2 would have been able to read same value if the trasactinalitY  was the default ""READ COMMITTED"".</p>
","<p>The answer is NO. </p>

<p>Oracle guarantees that numbers generated by sequence are different. Even if parallel requests are issued, RAC environement or rollback and commits are mixed.</p>

<p>Sequences has nothing to do with transactions. </p>

<p>See <a href=""http://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_6015.htm"">here the docs</a>:</p>

<blockquote>
  <p>Use the CREATE SEQUENCE statement to create a sequence, which is a
  database object from which multiple users may generate <strong>unique</strong>
  integers. You can use sequences to automatically generate primary key
  values.</p>
  
  <p>When a sequence number is generated, the sequence is incremented,
  <strong>independent</strong> of the transaction committing or rolling back. If two
  users concurrently increment the same sequence, then the sequence
  numbers each user acquires may have gaps, because sequence numbers are
  being generated by the other user. One user can never acquire the
  sequence number generated by another user. After a sequence value is
  generated by one user, that user can continue to access that value
  regardless of whether the sequence is incremented by another user.</p>
  
  <p>Sequence numbers are generated independently of tables, so the same
  sequence can be used for one or for multiple tables. It is possible
  that individual sequence numbers will appear to be skipped, because
  they were generated and used in a transaction that ultimately rolled
  back. Additionally, a single user may not realize that other users are
  drawing from the same sequence.</p>
</blockquote>
"
"<p>Having an interesting issue. I'm reading from an excel file on a server via an OpenRowset in Sql2005. I've run the query a number of times without any problems. I've just gone out for a quick meeting and suddenly I get the error ""Cannot initialize the data source object of OLE DB provider ""MSDASQL"" for linked server ""(null)""""</p>

<p>I've made sure the files are not in use on the server and even deleted them and recopied them over onto the server and still I'm getting the same error.</p>

<p>UPDATE: This only seems to happen if I join two selects from different openrowsets. If I run the queries individually they still work fine. I have done the join before without any issues. Ideas?</p>
","<p>The problem comes because the Temp folder of the User under which the SQL server service is running isn't accessible under the credentials which the query is running. Try to to set the security of this temp folder with minimal restrictions. The dsn that gets created every time you run an openrowset query then can be recreated without any credentials conflict. This worked for me without any restart requirements.</p>
"
"<p>I am new to the whole 'spatial index' thing, but it seems to be the best solution for filtering based on latitude/longitude. So I added a column to my table:</p>

<p>So I created a <code>geometry</code> field:</p>

<pre><code>  ALTER TABLE `addresses` ADD `point` POINT NOT NULL 
</code></pre>

<p>And then I tried to add an index:</p>

<pre><code>  ALTER TABLE `addresses` ADD SPATIAL INDEX ( `point` ) 
</code></pre>

<p>But I get an error:</p>

<pre><code>  #1416 - Cannot get geometry object from data you send to the GEOMETRY field
</code></pre>

<p>What am I doing wrong here? </p>
","<p>OK I found the solution: Can't create a spatial index if some of the column fields contain no data. After running </p>

<pre><code>  UPDATE `addresses` SET `point` = POINT( lng, lat )
</code></pre>

<p>Everything worked fine.</p>
"
"<p>I'm querying a big mysql database with only read privileges, and I'd like to set some slow query results to a variable, 'foo', so I can use them again in other queries.</p>

<p>Basically, I want to have a variable for a cumbersome subquery, so I can reuse it without having the cost of running it every time I want to use it.</p>

<p>when I enter: </p>

<pre><code>set @foo := (select *
            from table1 join table2 
            where bar = 0 
            group by id);
</code></pre>

<p>I get: ERROR 1241 (21000): Operand should contain 1 column(s)
and if I restrict to 1 column, ERROR 1242 (21000): Subquery returns more than 1 row</p>

<p>Is there a way to store an array or a table in a variable? I don't have privileges to create temporary tables.</p>
","<p>it should be <code>@</code> when you are doing in <code>MySQL</code>.</p>

<pre><code>set @foo := (select *
            from table1 join table2 
            where bar = 0 
            group by id);
</code></pre>
"
"<p>The connection to SQL Workbench/J gets disconnected very frequently. Where can I change the settings so that it does not lose the connections for atleast an hour.</p>

<p>Thanks</p>
","<p>You don't have need to change the connection profile, you can change the autocommit property inside your SQL script ""on-the-fly"" with <a href=""http://www.sql-workbench.net/manual/wb-commands.html#command-set-autocommit"" rel=""nofollow noreferrer"">set autocommit</a></p>

<pre><code>set autocommit on;
vacuum;
set autocommit off;
</code></pre>

<p>You can also toggle the current autocommit state through the menu ""SQL -> Autocommit""</p>
"
"<p>I'd like to amend the SQL that's being generated by EF:CF when generating the database schema (DDL), as <a href=""http://entityframework.codeplex.com/workitem/2591"" rel=""noreferrer"">suggested by the Entity Framework team</a>.</p>

<p>How can this be done?</p>

<p>I couldn't find anything appropriate via Google.</p>
","<p>You can override the <a href=""https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.sql.migrationsqlgenerator.aspx""><code>MigrationSqlGenerator</code></a> that is used by Entity Framework by calling the <a href=""https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.dbmigrationsconfiguration.setsqlgenerator.aspx"">DbMigrationsConfiguration.SetSqlGenerator()</a> method in the constructor of your <code>DbMigrationsConfiguration</code> class, passing the database provider name (e.g. <code>""System.Data.SqlClient""</code> for SQL Server), and the <code>MigrationSqlGenerator</code> instance to use for that database provider.</p>

<p>Consider the example from <a href=""http://entityframework.codeplex.com/workitem/2591"">the work item</a> that you linked to:</p>

<pre class=""lang-c# prettyprint-override""><code>public class MyEntity
{
    public int Id { get; set; }

    [Required]
    [MinLength(5)]
    public string Name { get; set; }
}
</code></pre>

<p>Suppose that the table for <code>MyEntity</code> had already been generated and the <code>Add-Migration</code> command was used to add the <code>Name</code> field.</p>

<p>By default, the scaffolded migration is:</p>

<pre class=""lang-c# prettyprint-override""><code>public partial class AddMyEntity_Name : DbMigration
{
    public override void Up()
    {
        AddColumn(""dbo.MyEntity"", ""Name"", c =&gt; c.String(nullable: false));
    }

    public override void Down()
    {
        DropColumn(""dbo.MyEntity"", ""Name"");
    }
}
</code></pre>

<p>Notice that the scaffolder did not generate anything for the <code>MinLengthAttribute</code>.</p>

<p>To have EF convey the minimum length requirement, you can <a href=""http://entityframework.codeplex.com/wikipage?title=Code%20First%20Annotations"">specify an attribute-to-column annotation convention</a>.  As mentioned on that documentation page, any <a href=""https://msdn.microsoft.com/en-us/library/system.data.entity.infrastructure.annotations.annotationvalues.aspx""><code>AnnotationValues</code></a> are ignored by the default SQL generators.</p>

<p>Within your DbContext's OnModelCreating() override, add the following:</p>

<pre class=""lang-c# prettyprint-override""><code>modelBuilder.Conventions.Add(new AttributeToColumnAnnotationConvention&lt;MinLengthAttribute, Int32&gt;(""minLength"", (property, attributes) =&gt; attributes.Single().Length));
</code></pre>

<p>After adding that, you can regenerate the scaffolded migration by running <code>Add-Migration -Force AddMyEntity_Name</code>.  Now the scaffolded migration is:</p>

<pre class=""lang-c# prettyprint-override""><code>public partial class AddMyEntity_Name : DbMigration
{
    public override void Up()
    {
        AddColumn(""dbo.MyEntity"", ""Name"", c =&gt; c.String(nullable: false,
            annotations: new Dictionary&lt;string, AnnotationValues&gt;
            {
                { 
                    ""minLength"",
                    new AnnotationValues(oldValue: null, newValue: ""5"")
                },
            }));
    }

    public override void Down()
    {
        DropColumn(""dbo.MyEntity"", ""Name"",
            removedAnnotations: new Dictionary&lt;string, object&gt;
            {
                { ""minLength"", ""5"" },
            });
    }
}
</code></pre>

<p>Suppose that, as in the linked work item, you want to generate a constraint to check that the trimmed <code>Name</code> value is greater than the minLength (5 in this case).</p>

<p>You can start by creating a custom <code>MigrationSqlGenerator</code> that extends <code>SqlServerMigrationSqlGenerator</code> and call SetSqlGenerator() to install the custom <code>MigrationSqlGenerator</code>:</p>

<pre class=""lang-c# prettyprint-override""><code>internal class CustomSqlServerMigrationSqlGenerator : SqlServerMigrationSqlGenerator
{
    protected override void Generate(AddColumnOperation addColumnOperation)
    {
        base.Generate(addColumnOperation);
    }
}

internal sealed class Configuration : DbMigrationsConfiguration&lt;DataContext&gt;
{
    public Configuration()
    {
        AutomaticMigrationsEnabled = false;

        SetSqlGenerator(""System.Data.SqlClient"", new CustomSqlServerMigrationSqlGenerator());
    }

    protected override void Seed(DataContext context)
    {
        //...
    }
}
</code></pre>

<p>Right now, this <code>CustomSqlServerMigrationSqlGenerator</code> overrides the Generate(AddColumnOperation) method, but simply calls the base implementation.</p>

<p>If you look at <a href=""https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.model.addcolumnoperation.aspx"">the documentation of <code>AddColumnOperation</code></a>, you will see two important properties, <code>Column</code> and <code>Table</code>.  <code>Column</code> is the <a href=""https://msdn.microsoft.com/en-us/library/system.data.entity.migrations.model.columnmodel.aspx""><code>ColumnModel</code></a> that was created by the lambda in Up(), <code>c =&gt; c.String(nullable: false, annotations: ...)</code>.</p>

<p>In the Generate() method, you can access the custom <code>AnnotationValues</code> via the <code>Annotations</code> property of the <code>ColumnModel</code>.</p>

<p>To generate the DDL that adds the constraint, you need to generate the SQL and call the Statement() method.  For example:</p>

<pre class=""lang-c# prettyprint-override""><code>internal class CustomSqlServerMigrationSqlGenerator : SqlServerMigrationSqlGenerator
{
    protected override void Generate(AddColumnOperation addColumnOperation)
    {
        base.Generate(addColumnOperation);

        var column = addColumnOperation.Column;
        if (column.Type == System.Data.Entity.Core.Metadata.Edm.PrimitiveTypeKind.String)
        {
            var annotations = column.Annotations;
            AnnotationValues minLengthValues;
            if (annotations.TryGetValue(""minLength"", out minLengthValues))
            {
                var minLength = Convert.ToInt32(minLengthValues.NewValue);
                if (minLength &gt; 0)
                {
                    if (Convert.ToString(column.DefaultValue).Trim().Length &lt; minLength)
                    {
                        throw new ArgumentException(String.Format(""minLength {0} specified for {1}.{2}, but the default value, '{3}', does not satisfy this requirement."", minLength, addColumnOperation.Table, column.Name, column.DefaultValue));
                    }

                    using (var writer = new StringWriter())
                    {
                        writer.Write(""ALTER TABLE "");
                        writer.Write(Name(addColumnOperation.Table));
                        writer.Write("" ADD CONSTRAINT "");
                        writer.Write(Quote(""ML_"" + addColumnOperation.Table + ""_"" + column.Name));
                        writer.Write("" CHECK (LEN(LTRIM(RTRIM({0}))) &gt; {1})"", Quote(column.Name), minLength);
                        Statement(writer.ToString());
                    }
                }
            }
        }
    }
}
</code></pre>

<p>If you run <code>Update-Database -Verbose</code>, you will see an exception generated by <code>CustomSqlServerMigrationSqlGenerator</code>:</p>

<pre>
minLength 5 specified for dbo.MyEntity.Name, but the default value, '', does not satisfy this requirement.
</pre>

<p>To fix this issue, specify a defaultValue in the Up() method that is longer than the minimum length (e.g. <code>""unknown""</code>):</p>

<pre class=""lang-c# prettyprint-override""><code>    public override void Up()
    {
        AddColumn(""dbo.MyEntity"", ""Name"", c =&gt; c.String(nullable: false, defaultValue: ""unknown"",
            annotations: new Dictionary&lt;string, AnnotationValues&gt;
            {
                { 
                    ""minLength"",
                    new AnnotationValues(oldValue: null, newValue: ""5"")
                },
            }));
    }
</code></pre>

<p>Now if you re-run <code>Update-Database -Verbose</code>, you will see the <code>ALTER TABLE</code> statement that adds the column and the <code>ALTER TABLE</code> statement that adds the constraint:</p>

<pre>
ALTER TABLE [dbo].[MyEntity] ADD [Name] [nvarchar](max) NOT NULL DEFAULT 'unknown'
ALTER TABLE [dbo].[MyEntity] ADD CONSTRAINT [ML_dbo.MyEntity_Name] CHECK (LEN(LTRIM(RTRIM([Name]))) > 5)
</pre>

<p>See also: <a href=""http://romiller.com/2013/02/27/ef6-writing-your-own-code-first-migration-operations/"">EF6: Writing Your Own Code First Migration Operations</a>, which shows how to implement a custom migration operation.</p>
"
"<p>I'm trying to use the EXCEPT keyword in Oracle 10.1.0.2.0, but kept getting error 'Unknown Command'. I've tried googling around and someone said the keyword is MINUS, so I used MINUS, instead, but I still got the same error. 
Any idea?
Thanks.</p>

<p>So here's my query. 
I'm finding the name of students who enrolls in ALL courses with course number > 500</p>

<pre><code>SELECT s.name
FROM Students s
WHERE NOT EXISTS
  (
    SELECT c.id
    FROM Courses c
    WHERE c.number &gt; 500

    MINUS

    SELECT e.course_id
    FROM Enrollment e
    WHERE e.student_id = s.id
  );
</code></pre>
","<p>Oracle <code>MINUS</code> is an operator; it's equivalent to <code>EXCEPT</code> in SQL Server.  <a href=""https://stackoverflow.com/q/5557991/1275871"">Here is a previous post</a> explaining the difference.  Here's a trivial example:</p>

<pre><code>SELECT a, b, c
FROM   table_a
MINUS
SELECT a, b, c
FROM   table_b
</code></pre>

<p>If you still have problems, add the complete query you are using to your question; it's likely a simple syntax error.</p>
"
"<p>I need to drop a user with dbowner schema from a SQL Server database. I cannot drop it as it is since I get this error message</p>

<blockquote>
  <p>Drop failed for User 'network service'.  (Microsoft.SqlServer.Smo)</p>
  
  <p>The database principal owns a schema in the database, and cannot be dropped. (Microsoft SQL Server, Error: 15138)</p>
</blockquote>

<p>When I try to uncheck the schema owned by this user to remove db owner it does nothing. My question is how I can drop this user or edit its name from 'network service' to 'NT AUTHORITY\NETWORK SERVICE'</p>
","<p>take a look at this:</p>

<p><a href=""http://www.itorian.com/2012/12/the-database-principal-owns-schema-in.html"" rel=""noreferrer"">http://www.itorian.com/2012/12/the-database-principal-owns-schema-in.html</a></p>

<p>It suggests that you need to add another owner first</p>
"
"<p>Here's my table:  </p>

<pre><code>CREATE TABLE `alums_alumphoto` (  
  `id` int(11) NOT NULL auto_increment,  
  `alum_id` int(11) NOT NULL,  
  `photo_id` int(11) default NULL,  
  `media_id` int(11) default NULL,  
  `updated` datetime NOT NULL,  
  PRIMARY KEY  (`id`),  
  KEY `alums_alumphoto_alum_id` (`alum_id`),  
  KEY `alums_alumphoto_photo_id` (`photo_id`),  
  KEY `alums_alumphoto_media_id` (`media_id`),  
  CONSTRAINT `alums_alumphoto_ibfk_1` FOREIGN KEY (`media_id`) REFERENCES `media_mediaitem` (`id`),  
  CONSTRAINT `alum_id_refs_id_706915ea` FOREIGN KEY (`alum_id`) REFERENCES `alums_alum` (`id`),  
  CONSTRAINT `photo_id_refs_id_63282119` FOREIGN KEY (`photo_id`) REFERENCES `media_mediaitem` (`id`)  
) ENGINE=InnoDB AUTO_INCREMENT=63 DEFAULT CHARSET=utf8  
</code></pre>

<p>I want to delete the column <code>photo_id</code>, which presumably will also require deleting the foreign key constraint and the index.</p>

<p>The problem is that I get errors when I try to drop the column:  </p>

<pre>ERROR 1025 (HY000): Error on rename of '.\dbname\#sql-670_c5c' to '.\dbname\alums_alumphoto' (errno: 150)</pre>  

<p>... when I try to drop the index (same as above), and when I try to drop the foreign key constraint:  </p>

<pre>ERROR 1091 (42000): Can't DROP 'photo_id_refs_id_63282119'; check that column/key exists)</pre>  

<p>What order should I be doing all of this in?  What precise commands should I be using?</p>
","<p>Precisely, try this : </p>

<p>First drop the Foreign Key or Constraint :</p>

<pre><code>ALTER TABLE `alums_alumphoto` DROP FOREIGN KEY `photo_id_refs_id_63282119`;
</code></pre>

<p>The previous command removes the Foreign Key Constraint on the column. Now you can drop the column <code>photo_id</code> (the index is removed by MySQL on dropping the column) :</p>

<pre><code>ALTER TABLE `alums_alumphoto` DROP COLUMN `photo_id`;
</code></pre>

<p>Aternatively, you could combine these 2 operations into one : </p>

<pre><code>ALTER TABLE `alums_alumphoto` 
   DROP FOREIGN KEY `photo_id_refs_id_63282119` , 
   DROP COLUMN `photo_id`;
</code></pre>
"
"<p>I ran into an odd situation during a use case on a project: ESQL is calling a java method, sending it a String input parameter which the method will unmarshal, apply some logic, and then store useful info from the unmarshalled object. So, the method must either throw a JAXBException, or use a try catch in order to handle the possible exceptions.</p>

<p>The problem with this is, that ESQL cannot invoke a java method which includes a throws in the signature. BUT, we want any errors to fall through back to the previously calling MBNode so it can be handled appropriately there, so then trycatch is out of the picture.</p>

<p>It struck me that hey, is it not possible to return a type of Exception when we encounter an issue, and null if not? So I wrote a simple method doing so, and though I didn't get any warnings or errors, it seemed just wrong to me in the sense of good programming.</p>

<p>For example:</p>

<pre><code>public Exception doStuffAndCheckForErorrs(String inString)
{
    if(inString.equals(null))
    {
       return new Exception(""Your string is null"");
    }
    else
    return null;
}
</code></pre>

<p>But I just get a terrible feeling about doing anything this way.</p>

<p>I'm open to any thoughts or different solutions to this, especially if there's a way around the ESQL signature issue.</p>

<p><strong>UPDATE</strong>:</p>

<p>Adding reference as to why the ESQL procedure cannot call a java method with a throws clause in the signature.</p>

<p>Excerpt from <a href=""https://publib.boulder.ibm.com/infocenter/wmbhelp/v6r1m0/index.jsp?topic=%2Fcom.ibm.etools.mft.doc%2Fac06009_.htm"" rel=""nofollow"">This link</a> under the CREATE PROCEDURE statement section:</p>

<p>""Any Java method that you want to invoke must have the following basic signature:
        public static   (&lt; 0 - N parameters>)
        where  must be in the list of Java IN data types in the table in ESQL to Java data type mapping (excluding the REFERENCE type, which is not permitted as a return value), or the Java void data type. The parameter data types must also be in the ESQL to Java data type mapping table. In addition, <strong>the Java method is not allowed to have an exception throws clause in its signature</strong>.""</p>
","<p>XQuery is not an API, but a standard, and the full syntax can be found online: <a href=""http://www.w3.org/TR/xquery/"" rel=""nofollow"">XQuery 1.0</a> and <a href=""http://www.w3.org/TR/xquery-3/"" rel=""nofollow"">XQuery 3.0</a> (there is no 2.0). You'll also find many manuals, tutorials etc.</p>

<p>XQuery relies on <a href=""http://www.w3.org/TR/xpath20/"" rel=""nofollow"">XPath</a>, which is even wider used than XQuery and can be found in libraries for almost every general purpose language.</p>

<p>Your expression is correct XQuery, in that it considers everything a sequence, and the comma concatenates (and flattens) two sequences.</p>

<p>XPath does not know <code>NULL</code>, but it knows <code>xsi:nil</code> and <code>()</code>, the latter being the <em>empty sequence</em>. An empty sequence is removed from the result.</p>

<p>I am not sure what XQuery processor is used underneath, but the correct expression should be <code>($var0, $var1)[1]</code><sup>2</sup>, which works the same way as your <code>COALESCE</code> operation<sup>1</sup>. In XPath and XQuery, variables are referenced with the <code>$</code> sign. The number of variables or expressions separated by the comma is unbounded. If all are the empty sequence (null), the result is the empty sequence.</p>

<p>Without <code>[1]</code>, it will return all items that are non-null and discard the rest. You can use another index, like <code>[3]</code> to get the third non-null value. If no such value exists, it will return null (empty sequence).</p>

<hr>

<p><sup>1</sup> <em>which behaves not exactly the way you described it. I believe it behaves like <code>if var0 == null then var1 else var0</code>, it selects the first non-null value (I've updated the OP).</em></p>

<p><sup>2</sup> <em>as Florent has explained in the comments, a warning with this expression is in place. If you have <code>$var1 := (1, 2)</code> and <code>$var2 := (3, 4)</code>, the expression <code>$var1, $var2)[1]</code> will return <code>1</code>, not <code>(1, 2)</code>, because sequences cannot contain subsequences, and indexing a sequence with <code>[x]</code> will return the x<sup>th</sup> value of the flattened sequence. You can safe-guard your expression with <code>(zero-or-one($var1), zero-or-one($var2))[1]</code>.</em></p>
"
"<p>Hello I'm trying to import data from excel file <code>(xls)</code> to new <code>SQL table</code> so I use <code>Import and Export data 32/bit</code> to achieve that. When I load the excel file it automatically detects data types of columns. e.g. column with phone numbers is as data type to new table <code>float</code> and in excel is as Double(15) when I try to change the <code>float</code> to <code>nvarchar</code> </p>

<p>I get this :</p>

<blockquote>
  <p>Found 2 unknown column type conversion(s)
  You have selected to skip 1 potential lost column conversion(s)
  You have selected to skip 3 safe column conversion(s)</p>
</blockquote>

<p>And I'm not allowed to continue with export.</p>

<p>Is there any way to change the data types when trying to import them?</p>

<p>Thank you for your time.</p>

<p>These data are set as <code>text</code> data type in excel
Sample data from one of the columns in excel:</p>

<pre><code>5859031783

5851130582

8811014190
</code></pre>

<p>This is what I get:</p>

<p><img src=""https://i.stack.imgur.com/SIth5.jpg"" alt=""enter image description here""></p>
","<p>Select the Column in your Excel sheet and change the data type to text </p>

<p><img src=""https://i.stack.imgur.com/ZxZsU.png"" alt=""enter image description here""></p>

<p>Then go to your sql server open import-export wizard and do all the steps of select source data and bla bla when you get to the point of <code>Mapping Column</code>, it will select <code>Float</code> data type by default, You will have to change it to <code>NVARCHAR(N)</code> in my test I changed it to NVARCHAR(400), it gave me a warning that I might lose some data as I am converting data from 1 datatyep to another.</p>

<p><img src=""https://i.stack.imgur.com/iN4Px.png"" alt=""enter image description here""></p>

<p>When you get to the <code>Data Type Mapping</code> page make sure you select Convert checkbox. and stop the process of failure of as appropriate.</p>

<p><img src=""https://i.stack.imgur.com/71JrH.png"" alt=""enter image description here""></p>

<p>Going through all these steps finally got my data in the destination table with the same <code>Warning</code> that I have converted some data n bla bla after all microsoft worries too much :)</p>

<p><img src=""https://i.stack.imgur.com/8uvi6.png"" alt=""enter image description here""></p>

<p><strong>Finally Data in Sql-Server Table</strong></p>

<pre><code>
 Name      City     Country      Phone       Float_Column 

 Shaun  London      UK       04454165161665    5859031783 
 Mark   Newyork     USA      16846814618165    8811014190 
 Mike   Manchester  UK       04468151651651    5851130582 

</code></pre>
"
"<p>I've never used mysqli_multi_query before and it's boggling my brain, any examples I find on the net aren't helping me to figure out exactly what it is I want to do.</p>

<p>Here is my code:</p>

<pre><code>&lt;?php

    $link = mysqli_connect(""server"", ""user"", ""pass"", ""db"");

    if (mysqli_connect_errno()) {
        printf(""Connect failed: %s\n"", mysqli_connect_error());
        exit();
    }

    $agentsquery = ""CREATE TEMPORARY TABLE LeaderBoard (
        `agent_name` varchar(20) NOT NULL,
        `job_number` int(5) NOT NULL,
        `job_value` decimal(3,1) NOT NULL,
        `points_value` decimal(8,2) NOT NULL
    );"";
    $agentsquery .= ""INSERT INTO LeaderBoard (`agent_name`, `job_number`, `job_value`, `points_value`) SELECT agent_name, job_number, job_value, points_value FROM jobs WHERE YEAR(booked_date) = $current_year &amp;&amp; WEEKOFYEAR(booked_date) = $weeknum;"";
    $agentsquery .= ""INSERT INTO LeaderBoard (`agent_name`) SELECT DISTINCT agent_name FROM apps WHERE YEAR(booked_date) = $current_year &amp;&amp; WEEKOFYEAR(booked_date) = $weeknum;"";
    $agentsquery .= ""SELECT agent_name, SUM(job_value), SUM(points_value) FROM leaderboard GROUP BY agent_name ORDER BY SUM(points_value) DESC"";

    $i = 0;
    $agentsresult = mysqli_multi_query($link, $agentsquery);

    while ($row = mysqli_fetch_array($agentsresult)){
        $number_of_apps = getAgentAppsWeek($row['agent_name'],$weeknum,$current_year);
        $i++;
?&gt;

            &lt;tr class=""tr&lt;?php echo ($i &amp; 1) ?&gt;""&gt;
                &lt;td style=""font-weight: bold;""&gt;&lt;?php echo $row['agent_name'] ?&gt;&lt;/td&gt;
                &lt;td&gt;&lt;?php echo $row['SUM(job_value)'] ?&gt;&lt;/td&gt;
                &lt;td&gt;&lt;?php echo $row['SUM(points_value)'] ?&gt;&lt;/td&gt;
                &lt;td&gt;&lt;?php echo $number_of_apps; ?&gt;&lt;/td&gt;
            &lt;/tr&gt;

&lt;?php

    }
?&gt;
</code></pre>

<p>All I'm trying to do is run a multiple query and then use the final results from those 4 queries and put them into my tables.</p>

<p>the code above really doesn't work at all, I just get the following error:</p>

<blockquote>
  <p>Warning: mysqli_fetch_array() expects
  parameter 1 to be mysqli_result,
  boolean given in
  C:\xampp\htdocs\hydroboard\hydro_reporting_2010.php
  on line 391</p>
</blockquote>

<p>any help?</p>
","<p>While pipodesign corrected the error within the $querystring and alleviated the problem, the actual solution was not provided regarding the Strict Standards error.</p>

<p>I disagree with SirBT's advice, changing from DO WHILE to WHILE is not necessary.</p>

<p>The Strict Standards message that you receive is quite informative.
To obey, use this:</p>

<pre><code>do{} while(mysqli_more_results($db) &amp;&amp; mysqli_next_result($db));
</code></pre>

<p>Then, there is no need for you to write a conditional exit or break inside of the loop because the while condition will break the loop on the first occurrence of an error.  *note, the if statement before the do-while will deny entry to the loop if the first query has an error.</p>

<p>In your example, you are only running INSERT queries, so you won't receive any result sets to process.  If you want to count how many rows you've added, use mysqli_affected_rows().</p>

<p>As a complete solution for your question:</p>

<pre><code>if(mysqli_multi_query($db,$querystring)){
    do{
        $cumulative_rows+=mysqli_affected_rows($db);
    } while(mysqli_more_results($db) &amp;&amp; mysqli_next_result($db));
}
if($error_mess=mysqli_error($db)){echo ""Error: $error_mess"";}
echo ""Cumulative Affected Rows: $cumulative_rows"";
</code></pre>

<p>Output:</p>

<pre><code> // if no errors
Cumulative Affected Rows: 2

// if error on second query
Error: [something]
Cumulative Affected Rows: 1

// if error on first query
Error: [something]
Cumulative Affected Rows: 0
</code></pre>

<hr>

<p><strong><em>LATE EDIT:</em></strong></p>

<p>Since people new to mysqli are stumbling across this post, I'll offer a general yet robust snippet to handle queries with/without result sets using multi_query() and add a feature to display which query in the array is being handled...</p>

<p><strong>Classic ""IF(){DO{} WHILE}"" Syntax</strong>:</p>

<pre><code>if(mysqli_multi_query($mysqli,implode(';',$queries))){
    do{
        echo ""&lt;br&gt;&lt;br&gt;"",key($queries),"": "",current($queries);  // display key:value @ pointer
        if($result=mysqli_store_result($mysqli)){   // if a result set
            while($rows=mysqli_fetch_assoc($result)){
                echo ""&lt;br&gt;Col = {$rows[""Col""]}"";
            }
            mysqli_free_result($result);
        }
        echo ""&lt;br&gt;Rows = "",mysqli_affected_rows($mysqli); // acts like num_rows on SELECTs
    } while(next($queries) &amp;&amp; mysqli_more_results($mysqli) &amp;&amp; mysqli_next_result($mysqli));
}
if($mysqli_error=mysqli_error($mysqli)){
    echo ""&lt;br&gt;&lt;br&gt;"",key($queries),"": "",current($queries),""Syntax Error:&lt;br&gt;$mysqli_error"";  // display array pointer key:value
}
//if you want to use the snippet again...
$mysqli_error=null; // clear variables
reset($queries); // reset pointer
</code></pre>

<hr>

<p><strong>Reinvented Wheel ""WHILE{}"" Syntax</strong> (...for those who don't like post-test loops):</p>

<pre><code>while((isset($multi_query) &amp;&amp; (next($queries) &amp;&amp; mysqli_more_results($mysqli) &amp;&amp; mysqli_next_result($mysqli))) || (!isset($multi_query) &amp;&amp; $multi_query=mysqli_multi_query($mysqli,implode(';',$queries)))){
    echo ""&lt;br&gt;&lt;br&gt;"",key($queries),"": "",current($queries);  // display array pointer key:value
    if($result=mysqli_store_result($mysqli)){
        while($rows=mysqli_fetch_assoc($result)){
            echo ""&lt;br&gt;Col = {$rows[""Col""]}"";
        }
        mysqli_free_result($result);
    }
    echo ""&lt;br&gt;Rows = "",mysqli_affected_rows($mysqli); // acts like num_rows on SELECTs
}
if($mysqli_error=mysqli_error($mysqli)){
    echo ""&lt;br&gt;&lt;br&gt;"",key($queries),"": "",current($queries),""Syntax Error:&lt;br&gt;$mysqli_error"";  // display array pointer key:value
}
//if you want to use the snippet again...
$multi_query=$mysqli_error=null; // clear variables
reset($queries); // reset pointer
</code></pre>

<p>So, either snippet given the following queries will offer the same output:</p>

<p><strong>Query array:</strong></p>

<pre><code>$queries[]=""SELECT * FROM `TEST`"";
$queries[]=""INSERT INTO `TEST` (Col) VALUES ('string1'),('string2')"";
$queries[]=""SELECT * FROM `TEST`"";
$queries[]=""DELETE FROM `TEST` WHERE Col LIKE 'string%'"";
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>0: SELECT * FROM `TEST`
Rows = 0

1: INSERT INTO `TEST` (Col) VALUES ('string1'),('string2')
Rows = 2

2: SELECT * FROM `TEST`
Col = string1
Col = string2
Rows = 2

3: DELETE FROM `TEST` WHERE Col LIKE 'string%'
Rows = 2
</code></pre>

<p>Modify my snippets per your needs.  Leave a comment if you discover a bug.</p>
"
"<p>I have manually (GASP!) entered a MySQL command at the command line, and I received a Warning which I cannot even begin to understand.  (And before anyone says anything, yes, I KNOW: 1. Using the command line interface is not the best approach; 2. My table is NOT named ""TABLE_NAME"" and my column is NOT named ""DateColumn"" and my RecordID value is NOT really ""1234""; 3. Maybe my column type <em>should</em> be TIMESTAMP, but for now, it's not.  Moving on....) </p>

<p>Attempting to enter a value for the date ""July 26th, 2012 at 2:27 PM (GMT)"", I keyed: </p>

<pre><code>mysql&gt; update TABLE_NAME set DateColumn=""2012-07-26 14:27:00"" where RecordID=""1234"";
</code></pre>

<p>I received: </p>

<pre><code>Query OK, 1 row affected, 1 warning (0.11 sec) 
Rows matched: 1  Changed: 1  Warnings: 1
</code></pre>

<p>So, I keyed: </p>

<pre><code>mysql&gt; show warnings;
+---------+------+-----------------------------------------------------+
| Level   | Code | Message                                             |
+---------+------+-----------------------------------------------------+
| Warning | 1264 | Out of range value for column 'DateColumn' at row 1 |
+---------+------+-----------------------------------------------------+
</code></pre>

<p>Weird, I thought.  So I checked the table first to confirm the column (field) type: </p>

<pre><code>mysql&gt; describe TABLE_NAME;

+------------+----------+------+-----+-------------------+-------+
| Field      | Type     | Null | Key | Default           | Extra |
| DateColumn | datetime | YES  |     | NULL              |       |
+------------+----------+------+-----+-------------------+-------+
</code></pre>

<p>BUT the value DOES get written properly to the database, and not truncated, AFAIK:  </p>

<pre><code>mysql&gt; select * from TABLE_NAME where RecordID=""1234"";

+-----------------------------------------------+
| RecordID | Date_Column         | BlahBlahBlah |
+----------+---------------------+--------------+
|     1234 | 2012-07-26 14:27:00 | something..  | 
+----------+---------------------+--------------+
</code></pre>

<p>I've already searched StackOverflow.com for a solution.  I've already Googled for an explanation.  I've already read up at <a href=""http://dev.mysql.com/doc/refman/5.5/en/datetime.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.5/en/datetime.html</a> where it says:</p>

<pre><code>    MySQL retrieves and displays DATETIME values in 'YYYY-MM-DD HH:MM:SS' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'. 
</code></pre>

<p>I even had a slight suspicion that it had something to do with the date or time at which I was making the entry; so I will state that the server on which the database is located is on Pacific Daylight Time (GMT-8, except right now GMT-7 for DST); I am logged in (SSH) from a client on EDT (which should not matter); and I am storing all Date_Column values as GMT.  At the time I was entering the value ""2012-07-26 14:27:00"" all three dates were well AFTER that, on 7/30/12.  Not that it should matter -- I <em>should</em> be able to enter <em>future</em> dates without getting an error -- but thought it might be helpful for you to know.  So -- </p>

<p><strong>WHY, OH WHY is ""2012-07-26 14:27:00"" an Out-of-Range Value?</strong>  </p>

<p>My MySQL client API version is 5.1.49.   </p>

<p>This is the first time I've ever posted on StackOverflow.  Thank you in advance for your suggestions. </p>
","<p><strong>Float(9,2)</strong> would allow for <em>7 numbers before the decimal, and 2 after</em>, <strong>you have 8 before and 2 after</strong>.</p>

<p>You need to increase the size of the filed if you wish to change the value to what you require.</p>

<p>You should have a look at <a href=""http://dev.mysql.com/doc/refman/5.0/en/numeric-types.html"" rel=""nofollow noreferrer"">Numeric Types</a></p>

<blockquote>
  <p>For example, a column defined as
  FLOAT(7,4) will look like -999.9999</p>
</blockquote>
"
"<p>I am using Apache Camel SQL batch insertion process.</p>

<ol>
<li><p>My application is reading tickets from the Active MQ which contains around 2000 tickets.</p></li>
<li><p>I have updated the batch as 100.</p></li>
<li><p>The query which i am firing is as follows:</p>

<p><code>sql.subs.insertCdr=
insert into subscription_logs(master_id,request_type,req_desc,msisdn,amount,status,resp_code,resp_desc,channel,transaction_id,se_mode,be_mode,sub_type,sub_timeleft,srv_name,srv_id,start_date,end_date,operator,circle,country,time_offset,retry_count,user_status,previous_state,se_reqrecvtime,se_respsenttime,be_reqsenttime,be_resprecvtime,cp_id,cp_name,sub_srvname,sub_srvid,msg_senderid,msg_text,call_back_url,call_back_resp,client_ip,se_sysIp,language,cp_callbackurlhittime,action,alert,notification_url,notification_resp) 
values(:#masterId, :#requestType,:#reqDesc,:#msisdnCdr,:#price,:#status,:#responseCode,:#reason,:#channel,:#transactionId,:#seMode,:#beMode,:#subType,:#subTimeLeft,:#serviceName,:#serviceId,:#subStartDate,:#cdrEndDate,:#operator,:#circle,:#country,:#timeOffset,:#retryCount,:#userStatus,:#previousState,:#seReqRecvTime,:#seRespSentTime,:#beReqSentTime,:#beRespRecvTime,:#cpId,:#cpName,:#subServiceName,:#subServiceId,:#shortCode,:#message,:#callBackUrl,:#callBackResp,:#clientIp,:#seSysIp,:#language,:#cpCallbackUrlHitTime,:#action,:#alert,:#notificationUrl,:#notificationResponse)</code></p></li>
<li><p>The SQL batch route is defined as follows:</p>

<pre><code>&lt;pipeline&gt;
   &lt;log message=""Going to insert in database""&gt;&lt;/log&gt;
   &lt;transform&gt;
      &lt;method ref=""insertionBean"" method=""subsBatchInsertion""&gt;&lt;/method&gt;
   &lt;/transform&gt;
   &lt;choice&gt;
       &lt;when&gt;
           &lt;simple&gt;${in.header.subsCount} == ${properties:batch.size}&lt;/simple&gt;
           &lt;to uri=""sql:{{sql.subs.insertCdr}}?batch=true""&gt;&lt;/to&gt;
           &lt;log message=""Inserted rows ${body}""&gt;&lt;/log&gt;
       &lt;/when&gt;
   &lt;/choice&gt;
&lt;/pipeline&gt;
</code></pre></li>
<li><p>Below is my java code:</p>

<pre><code>public List&lt;Map&lt;String, Object&gt;&gt; subsBatchInsertion(Exchange exchange) {
if (subsBatchCounter &gt; batchSize) {
    subsPayLoad.clear();
    subsBatchCounter = 1;
}
subsPayLoad.add(generateInsert(exchange.getIn().getBody(SubscriptionCdr.class)));
exchange.getIn().setHeader(""subsCount"", subsBatchCounter);
subsBatchCounter++;
return subsPayLoad;
}

public Map&lt;String, Object&gt; generateInsert(Cdr cdr) {
Map&lt;String, Object&gt; insert = new HashMap&lt;String, Object&gt;();
try {
    insert = BeanUtils.describe(cdr);
} catch (Exception e) {
    Logger.sysLog(LogValues.error, this.getClass().getName()+"" | ""+Thread.currentThread().getStackTrace()[1].getMethodName(), coreException.GetStack(e));
} 
for (String name : insert.keySet()) {
    Logger.sysLog(LogValues.APP_DEBUG, this.getClass().getName(), name + "":""+ insert.get(name) + ""\t"");
}
return insert;
}
</code></pre></li>
</ol>

<p>Now the problem is when there are around 120 ticket in ActiveMQ, SQL batch should have started to insert the values in to the database. But it is taking a lot more time. It starts insertion process when there are around 500 tickets in ActiveMQ.
Can anyboody help in optimizing the insertion process?
Or any other approach?</p>
","<p>The problem was with ActiceMQ consumer numbers.</p>

<blockquote>
  <p>When i changed the consumer count back to 1, the batch getting updated
  on time.</p>
</blockquote>

<p>Actually, when the consumer count was 10, the tickets were consumed in parallel. That means, for 100 tickets consumed from activemq with 10 consumers, there were approx 10 tickets with each consumer, thus adding more time. When any one of the consumer got 100 tickets, the batch was getting updated.</p>

<blockquote>
  <p>So changing the consumer count to 1 made all the tickets to be
  processed by single consumer, and thus performing the batch update
  fine.</p>
</blockquote>
"
"<p>I looked on the documentation for google big query data types, checking the differences between TimeStamp to Datetime data types.</p>

<p>As I understand the main difference is:</p>

<blockquote>
  <p>Unlike Timestamps, a DATETIME object does not refer to an absolute instance in time. Instead, it is the civil time, or the time that a user would see on a watch or calendar.</p>
</blockquote>

<p>So when should I use Timestamp/Datetime?</p>

<p>Thanks</p>
","<p>In most cases you will want to use the <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#timestamp-type"" rel=""noreferrer"">timestamp data type</a>. It refers to an actual point in time, including the timezone.</p>

<p>Very rarely would you use a <a href=""https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#datetime-type"" rel=""noreferrer"">datetime data type</a>, which is a date and a time but no time zone. The example I like to give is that you'd use a datetime to represent pi day, 2017, since it occurs at <code>2017-03-14 15:09:26.535898</code> in each time zone separately.</p>
"
"<p>I am using greendao ORM. I am trying to encrypt my database using SQLCipher. Greendao automativally supports sqlcipher. So I wrote the following code for encryption.</p>

<pre><code> DaoMaster.DevOpenHelper helper = new DaoMaster.DevOpenHelper(context, ""encrypted-db"",null);

        Database db = helper.getEncryptedWritableDb(""mySecretPassword"");
        DaoSession session = new DaoMaster(db).newSession();
        return session;
</code></pre>

<p>However whenever I perform any database operation using this session,it gives an error </p>

<pre><code> Caused by: java.lang.NoClassDefFoundError: Failed resolution of: Lorg/greenrobot/greendao/database/DatabaseOpenHelper$EncryptedHelper;
                                                                       at org.greenrobot.greendao.database.DatabaseOpenHelper.checkEncryptedHelper(DatabaseOpenHelper.java:121)
                                                                       at org.greenrobot.greendao.database.DatabaseOpenHelper.getEncryptedWritableDb(DatabaseOpenHelper.java:133)
</code></pre>

<p>My gradle dependencies are-></p>

<pre><code>compile fileTree(include: ['*.jar'], dir: 'libs')
    testCompile 'junit:junit:4.12'
    compile 'com.android.support:appcompat-v7:24.2.0'
    compile 'org.greenrobot:greendao:3.2.0'
    compile 'com.google.code.gson:gson:2.8.0'
</code></pre>

<p>My proguard rules are</p>

<pre><code>-keepclassmembers class * extends org.greenrobot.greendao.AbstractDao {
public static java.lang.String TABLENAME;
}
-keep class **$Properties
# If you do not use Rx:
-dontwarn rx.**
</code></pre>

<p>So how to encrypt my database using greendao and SQLCipher?</p>

<p>PS: <code>Database db = helper.getEncryptedWritableDb(""mySecretPassword"");</code>
this line generates the error on performing any database operation.</p>

<pre><code> Database db = helper.getEncryptedWritableDb(""mySecretPassword"");
</code></pre>
","<p>add this to your build.gradle of your app and it should work out of the box:</p>

<pre><code>dependencies {
     compile 'net.zetetic:android-database-sqlcipher:3.5.2@aar'
     ...
}
</code></pre>

<p>In your code, you should load the 'native libraries' as this 'aar' file contains a few of them.</p>

<pre><code>SQLiteDatabase.loadLibs(context);
</code></pre>

<p>NOTE that you should use the <code>net.sqlcipher.database.SQLiteDatabase</code> instead of <code>android.database.sqlite.SQLiteDatabase</code>, just like a couple of other SQLite classes.</p>
"
"<p>I'm trying to use the sql-maven-plugin to execute a PL/SQL script on an Oracle 11 database. Although the script is valid PL/SQL (as far as I can tell), the execution gives me a PLS-00103 error: </p>

<p>The SQL script: (drop_all_tables.sql)</p>

<pre><code>BEGIN
   EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE';
   EXCEPTION
   WHEN OTHERS THEN
      IF SQLCODE != -942 THEN
         RAISE;
      END IF;
END;
</code></pre>

<p>And my plugin configuration:</p>

<pre><code>&lt;plugin&gt;
            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
            &lt;artifactId&gt;sql-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;1.5&lt;/version&gt;

            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;oracle&lt;/groupId&gt;
                    &lt;artifactId&gt;jdbc&lt;/artifactId&gt;
                    &lt;version&gt;11.2.0.2.0&lt;/version&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;create-schema&lt;/id&gt;
                    &lt;phase&gt;process-test-resources&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;execute&lt;/goal&gt;
                    &lt;/goals&gt;
                    &lt;configuration&gt;
                        &lt;driver&gt;oracle.jdbc.driver.OracleDriver&lt;/driver&gt;
                        &lt;url&gt;${jdbc.url}&lt;/url&gt;
                        &lt;username&gt;${jdbc.username}&lt;/username&gt;
                        &lt;password&gt;${jdbc.password}&lt;/password&gt;
                        &lt;autocommit&gt;true&lt;/autocommit&gt;
                        &lt;srcFiles&gt;
                            &lt;srcFile&gt;src/main/resources/sql/drop_all_tables.sql&lt;/srcFile&gt;
                        &lt;/srcFiles&gt;
                    &lt;/configuration&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
</code></pre>

<p>And this is the output from the Maven execution:</p>

<pre><code>[ERROR] Failed to execute:  BEGIN
EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE';
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD ERROR
[INFO] ------------------------------------------------------------------------
[INFO] ORA-06550: line 2, column 43:
PLS-00103: Encountered the symbol ""end-of-file"" when expecting one of the following:

   ( begin case declare end exception exit for goto if loop mod
   null pragma raise return select update while with
   &lt;an identifier&gt; &lt;a double-quoted delimited-identifier&gt;
   &lt;a bind variable&gt; &lt;&lt; continue close current delete fetch lock
   insert open rollback savepoint set sql execute commit forall
   merge pipe purge
</code></pre>
","<p>I guess the plugin is splitting the sql script by semicolons and trying to execute each part independently. The first statement would be</p>

<pre><code>BEGIN
    EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE';
</code></pre>

<p>Which is incomplete as far as oracle is concerned. The plugin does have two configuration parameters to change this behaviour, <a href=""http://mojo.codehaus.org/sql-maven-plugin/execute-mojo.html#delimiter"" rel=""noreferrer"">delimiter</a> and <a href=""http://mojo.codehaus.org/sql-maven-plugin/execute-mojo.html#delimiterType"" rel=""noreferrer"">delimiterType</a>. By changing the configuration like below and separating your <code>BEGIN</code> blocks by a <code>/</code> on a line by itself you should be able to execute this script.</p>

<pre><code>&lt;configuration&gt;
    &lt;delimiter&gt;/&lt;/delimiter&gt;
    &lt;delimiterType&gt;row&lt;/delimiterType&gt;
&lt;/configuration&gt;
</code></pre>
"
"<p>I have created the following MySQL table to store latitude/longitude coordinates along with a name for each point:</p>

<pre><code>CREATE TABLE `points` (
  `id` int(10) unsigned NOT NULL AUTO_INCREMENT,
  `name` varchar(128) NOT NULL,
  `location` point NOT NULL,
  PRIMARY KEY (`id`),
  SPATIAL KEY `location` (`location`)
) ENGINE=MyISAM DEFAULT CHARSET=latin1 AUTO_INCREMENT=1;
</code></pre>

<p>I am trying to query:</p>

<ul>
<li>all points within an <i>n</i> mile radius of a given point;</li>
<li>the distance of each returned point from the given point</li>
</ul>

<p>All of the examples I have found refer to using a minimum bounding rectangle (MBR) rather than a radius. The table contains approximately 1 million points, so this need needs to be as efficient as possible.</p>
","<p>It appears that the <code>PHP</code> <code>unpack</code> function is what is needed to extract the coordinate information.</p>

<p><code>MySQL</code> stores the geometry fields in the WKB (Well Known Binary) format. The <code>unpack</code> function is able to extract that information when provided the proper format specifier. The following code is from a test script which successfully extracted the desired information.  </p>

<p>Please note that my <code>format</code> for unpack differs slightly from that of the reference.  This is because the WKB string wasn't retrieved from MySQL with the <code>AsWKB()</code> function, so it <a href=""https://bugs.mysql.com/bug.php?id=69798"" rel=""noreferrer"">contains extra padding</a>.</p>

<pre><code>&lt;?php
    $padding = 0;
    $order   = 1;
    $gtype   = 1;
    $lon     = -73.91353;
    $lat     = 42.80611;

    $bindata = pack('LcLd2', $padding, $order, $gtype, $lat, $lon);

    printf(""Packed: %s\n\n"", bin2hex($bindata));

    $result = unpack('Lpadding/corder/Lgtype/dlatitude/dlongitude', $bindata);
    var_dump($result);
?&gt;
</code></pre>

<p>References <a href=""http://dev.mysql.com/doc/refman/5.7/en/gis-data-formats.html#gis-wkb-format"" rel=""noreferrer"">MySQL</a> and <a href=""http://edndoc.esri.com/arcsde/9.0/general_topics/wkb_representation.htm"" rel=""noreferrer"">ESRI Network</a>.</p>
"
"<p>I'm aware of <a href=""https://stackoverflow.com/questions/1707531/get-max-min-in-one-line-with-linq"">this</a> question, but what I would like to do is obtain something close to this generated SQL:</p>

<pre class=""lang-sql prettyprint-override""><code>select MAX(Column), MIN(Column) from Table WHERE Id = 1
</code></pre>

<p>When I try this:</p>

<pre class=""lang-cs prettyprint-override""><code>var query = from d in db.Table
            where d.Id == 1
            select new
            {
                min = db.Table.Max(s =&gt; s.Column),
                max = db.Table.Min(s =&gt; s.Column)
            };
</code></pre>

<p>The generated sql looks like this:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT 
    [Extent1].[Id] AS [Id], 
    [GroupBy1].[A1] AS [C1], 
    [GroupBy2].[A1] AS [C2]
    FROM   [dbo].[Table] AS [Extent1]
    CROSS JOIN  (SELECT 
        MAX([Extent2].[Column]) AS [A1]
        FROM [dbo].[Table] AS [Extent2] ) AS [GroupBy1]
    CROSS JOIN  (SELECT 
        MIN([Extent3].[Column]) AS [A1]
        FROM [dbo].[Table] AS [Extent3] ) AS [GroupBy2]
    WHERE ([Extent1].[Id] = 1) AND (1 IS NOT NULL)
</code></pre>

<p>I also tried this:</p>

<pre class=""lang-cs prettyprint-override""><code>var query = from d in db.Table
           where d.Id == 1
           group d by d.Id into grp
           let min = grp.Min(s =&gt; s.Column)
           let max = grp.Max(s =&gt; s.Column)
           select new { min, max };
</code></pre>

<p>Which gives this:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT 
    [Project2].[Id] AS [Id], 
    [Project2].[C1] AS [C1], 
    [Project2].[C2] AS [C2]
    FROM ( SELECT 
        [Project1].[C1] AS [C1], 
        [Project1].[Id] AS [Id], 
        (SELECT 
            MAX([Extent2].[Column]) AS [A1]
            FROM [dbo].[Table] AS [Extent2]
            WHERE ([Extent2].[Id] = 1) AND (1 IS NOT NULL) AND ([Project1].[Id] = [Extent2].[Id])) AS [C2]
        FROM ( SELECT 
            [GroupBy1].[A1] AS [C1], 
            [GroupBy1].[K1] AS [Id]
            FROM ( SELECT 
                [Extent1].[Id] AS [K1], 
                MIN([Extent1].[Column]) AS [A1]
                FROM [dbo].[Table] AS [Extent1]
                WHERE ([Extent1].[Id] = 16) AND (16 IS NOT NULL)
                GROUP BY [Extent1].[Id]
            )  AS [GroupBy1]
        )  AS [Project1]
    )  AS [Project2]
</code></pre>

<p>They both work, and the performance hit is probably negligible, so it's mostly aesthetic:<br>
The two generated queries both hurt my eyes.</p>
","<p>You can view the generated SQL from an IQueryable using <code>.ToString()</code>, e.g. </p>

<pre><code>var query = context.People.Where(x =&gt; x.DomainId == 1);
Console.WriteLine(query.ToString());
</code></pre>
"
"<p>I am using sails js with it <strong>sails-mssqlserver</strong> adapter. The problem with it is that if my stored procedure returns multiple result sets then I only receive one result set which is the latest of all. 
The same stored procedure is working fine with Java and I get to iterate over the relevant result sets.</p>

<p>I need to know if there is some specific way to access all result sets in sails-mssqlserver?</p>
","<p>The sails-mssqlserver adapter is a wrapper of the official Microsoft SQL Server client for Node.js available <a href=""https://www.npmjs.com/package/mssql#query-command-callback"" rel=""nofollow noreferrer"">here</a> its dependecy however is not on the latest release.</p>

<p><strong>Option 1:</strong>
As per this official documentation of the MsSQL package, you can enable multiple recordsets in queries with the request.multiple = true command. </p>

<p>To enable multiple queries/recordsets in the sails-mssqlserver adapter, a hackish workaround is to open sails-mssqlserver/lib/adapter.js and edit the raw query function. Adding request.multiple = true below var request = new mssql.Request(mssqlConnect). As shown in the example below.</p>

<pre><code>// Raw Query Interface
query: function (connection, collection, query, data, cb) {
  if (_.isFunction(data)) {
    if (debugging) {
      console.log('Data is function. A cb was passed back')
    }
    cb = data
    data = null
  }

  adapter.connectConnection(connection, function __FIND__ (err, uniqId) {
    if (err) {
      console.error('Error inside query __FIND__', err)
      return cb(err)
    }

    uniqId = uniqId || false
    var mssqlConnect
    if (!uniqId) {
      mssqlConnect = connections[connection].mssqlConnection
    } else {
      mssqlConnect = connections[connection].mssqlConnection[uniqId]
    }

    var request = new mssql.Request(mssqlConnect)

    // Add it here
    request.multiple = true

    request.query(query, function (err, recordset) {
      if (err) return cb(err)
      if (connections[connection] &amp;&amp; !connections[connection].persistent) {
        mssqlConnect &amp;&amp; mssqlConnect.close()
      }
      cb(null, recordset)
    })
  })
},
</code></pre>

<p>Now the returned recordset should contain multiple results.</p>

<p><strong>Option 2:</strong>
A more sustainable option for use cases where running a stored procedure which returns multiple recordsets, is to use the latest version of the official Microsoft SQL Server client for Node.js. Information on running stored procedures is available <a href=""https://www.npmjs.com/package/mssql#execute-procedure-callback"" rel=""nofollow noreferrer"">here</a></p>

<p>First install the latest package:</p>

<pre><code>npm install mssql --save
</code></pre>

<p>In your code where you would like to run the stored procedure add a connection to the mssql database:</p>

<pre><code>// require the mssql package
const sql = require('mssql')

// make a connection, you can use the values you have already stored in your adapter
const pool = new sql.ConnectionPool({
    user: sails.config.connections.&lt;yourMsSQLConnection&gt;.user,
    password: sails.config.connections.&lt;yourMsSQLConnection&gt;.password,
    server: sails.config.connections.&lt;yourMsSQLConnection&gt;.server,
    database: sails.config.connections.&lt;yourMsSQLConnection&gt;.database
})

// connect the pool and test for error 
pool.connect(err =&gt; {
    // ...
})

// run the stored procedure using request
const request = new sql.Request()
request.execute('procedure_name', (err, result) =&gt; {
    // ... error checks 
    console.log(result.recordsets.length) // count of recordsets returned by the procedure
    console.log(result.recordsets[0].length) // count of rows contained in first recordset
    console.log(result.recordset) // first recordset from result.recordsets
    console.log(result.returnValue) // procedure return value
    console.log(result.output) // key/value collection of output values
    console.log(result.rowsAffected) // array of numbers, each number represents the number of rows affected by executed statemens 
    // ...
})

// you can close the pool using
pool.close()
</code></pre>

<p>In cases, where the sails-* database adapter doesn't include all the functionality you require. I find it best to create a sails Service that wraps the additional functionality. It is a really clean solution.</p>
"
"<p>I am getting some weird behavior of Pro-C procedure as shown below:</p>

<pre><code>#define BGHCPY_TO_ORA(dest, source) \
{ \
(void)strcpy((void*)(dest).arr, (void*)(source)); \
(dest).len = strlen((const char *)(dest).arr); \
}
#define BGHCPY_FROM_ORA(dest, source) \
{ \
(void)memcpy((void*)(dest), (void*)(source).arr, (size_t)(source).len); \
(dest)[(source).len] = '\0'; \
} 

long fnSQLMarkProcessed (char *pszRowId, char *pszMarker)
{
  BGHCPY_TO_ORA (O_rowid_stack,    pszRowId);
  BGHCPY_TO_ORA (O_cust_processed, pszMarker);

  EXEC SQL
       UPDATE  document_all
          SET  processed_by_bgh = :O_cust_processed
        WHERE  rowid = :O_rowid_stack;

  return (sqlca.sqlcode);
}
</code></pre>

<p>The input arguments values passed to above function is </p>

<pre><code>pszRowId = [AAAF1lAAIAABOoRAAB], pszMarker=X
</code></pre>

<p>The query return the error code:02115 with following message:</p>

<pre><code>SQL Error:02115 Code interpretation problem -- check COMMON_NAME usage
</code></pre>

<p>I am using Oracle as the backend database.</p>

<p><strong>Can anyone provide me information on what are the possible causes for this failed query?</strong></p>

<p>Any help is highly appreciated.</p>

<p>Flags used during PRO-C Compilation is defined below:</p>

<pre><code>------/u01/app/oracle/product/8.1.6/ORACLE_HOME/bin/proc `echo  -Dbscs5 -Dsun5 -I/export/home/bscsobw/bscs6/src/CoreDumpIssue/final_Code_Fix_004641  -DNDEBUG -DSunOS53 -D_POSIX_4SOURCES -I/usr/generic++/generic++2.5.3.64_bit/include  -DFEATURE_212298 -DBSCS_CONFIG -I/export/home/bscsobw/bscs6//src/bat/include -DFEATURE_00203808_GMD -DFEATURE_00241737  -DORACLE_DB_BRAND -I/u01/app/oracle/product/8.1.6/ORACLE_HOME/rdbms/demo -I/u01/app/oracle/product/8.1.6/ORACLE_HOME/precomp/public -I/export/home/bscsobw/bscs6/src/CoreDumpIssue/final_Code_Fix_004641/include -I../bat/include -DFEATURE61717 -DFEATURE52824 -DFEATURE56178 -DD236312_d -DSDP -g | sed -e 's/-I/INCLUDE=/g' -e 's/-D[^ ]=[^ ]*//g' -e 's/-D\([^ ]*\)/DEFINE=\1/g'` select_error=no   DEFINE=FEATURE61717 DEFINE=FEATURE52824 DEFINE=FEATURE56178 \
                lines=yes iname=bgh_esql.pc oname=bgh_esql.c lname=bgh_esql.lis
</code></pre>
","<p>Embedded SQL was one of the the most popular way to do SQL in C during the ""old days"" (C++ was not yet invented).</p>

<p>These days mostly we'll be using an ORM library.  It is not recommended to do embedded SQL any more because, as you put it well, it depends on a proprietary pre-processor and makes code difficult to debug, manage, and maintain.  It also <em>hooks</em> you to one single database vendor and your code will be extremely difficult to move to another database backend.  Generally, we don't do it in ""real life"".</p>

<p>But as it is only a class, your prof is probably interested in teaching you SQL and database concepts.  Embedded SQL is only a tool.  You're supposed to learn SQL and databases, not embedded SQL in C++.</p>

<p>However, I believe that you're missing the point by asking about PHP and Java.  Not to mention that PHP is a scripting language, and Java is another language that you can (potentially) write a processor for embedded SQL.</p>

<p>So your point about embedded SQL really has nothing with <em>language</em> choices.  It has to do with the tradeoffs and balance between (1) proprietary embedded system with preprocessor, (2) using an ORM library, or a data-access library (e.g. ODBC).</p>

<p><em>Off-Topic:</em></p>

<p>I first started using embedded SQL when I was in College (that was about 30 years ago!).  Actually got programming jobs out of College and still used it, but obviously it was on the way out.  Never seen it used ever since 1990 or so.</p>
"
"<p>Is there a way to call Backup-SqlDatabase cmdlet but have it connect to SQL Server 2012 with SQL Server credentials? </p>

<p>The command I am currently using is:</p>

<pre><code>Backup-SqlDatabase -ServerInstance $serverName -Database $sqldbname -BackupFile ""$($backupFolder)$($dbname)_db_$($addinionToName).bak""
</code></pre>

<p>But this relies on the user, under which it is being call, and by default, Windows Authentication is being used.</p>
","<p>The backup-sqldatabase cmdlet supports the Credential parameter. If you look at the help for the cmdlet there's even an example (from help backup-sqldatabase -full):</p>

<pre><code> -------------------------- EXAMPLE 4 --------------------------

 C:\PS&gt;Backup-SqlDatabase -ServerInstance Computer\Instance -Database MyDB -Credential (Get-Credential sa)


 Description
 -----------
 This command creates a complete database backup of the database 'MyDB', using the sa SQL Server credential. This co
 mmand will prompt you for a password to complete SQL Server authentication.
</code></pre>
"
"<p>I'm looking for a way to do multiple row inserts when I'm only inserting data for a single column.</p>

<p>Here is the example table:</p>

<pre><code>+-------+-------------+------+-----+---------+----------------+
| Field | Type        | Null | Key | Default | Extra          |
+-------+-------------+------+-----+---------+----------------+
| id    | tinyint(4)  | NO   | PRI | NULL    | auto_increment | 
| name  | varchar(40) | NO   | UNI | NULL    |                | 
+-------+-------------+------+-----+---------+----------------+
</code></pre>

<p>I want to be able to insert something like ('admin', 'author', 'mod', 'user', 'guest') into the name column for each row.</p>

<p>The MySQL documentation shows that multiple inserts should be in the format:</p>

<pre><code>INSERT INTO tbl_name (a,b,c) VALUES(1,2,3),(4,5,6),(7,8,9);
</code></pre>

<p>However my statement ends up looking like this:</p>

<pre><code>INSERT INTO User_Role(name) VALUES ('admin','author','mod','user','guest');
</code></pre>

<p>And I get the following:<br>
ERROR 1136 (21S01): Column count doesn't match value count at row 1</p>

<p>Meaning that it thinks I'm trying to do a single row insert.</p>

<p>I'm not sure if I'm just missing something simple here, but I don't see anything in particular in the MySQL docs for this use case.</p>
","<p>your syntax is a bit off. put parentheses around each data ""set"" (meaning a single value in this case) that you are trying to insert.</p>

<pre><code>INSERT INTO User_Roll(name) VALUES ('admin'), ('author'), ('mod'), ('user'), ('guest');
</code></pre>
"
"<p>Note: this is with SQLite, although I expect the problem is on the Qt side.</p>

<p>First, I set up a database table from the SQLite command line tool:</p>

<pre><code>sqlite&gt; create table testtable ( id INTEGER PRIMARY KEY NOT NULL, state INTEGER );
sqlite&gt; insert into testtable (state) values (0);
sqlite&gt; insert into testtable (state) values (1);
sqlite&gt; insert into testtable (state) values (9);
sqlite&gt; insert into testtable (state) values (20);
</code></pre>

<p>Then I test my query:</p>

<pre><code>sqlite&gt; SELECT id,state FROM testtable WHERE state IN (0,1,2);
1|0
3|1
</code></pre>

<p>(Those are expected results.)</p>

<p>Then I run this C++ code:</p>

<pre><code>void runQuery() {
        QSqlQuery qq;
        qq.prepare( ""SELECT id,state FROM testtable WHERE state IN (:states)"");
        QList&lt;QVariant&gt; statesList = QList&lt;QVariant&gt;();
        statesList.append(0);
        statesList.append(1);
        statesList.append(2);
        qq.bindValue("":states"", statesList);
        qq.exec();
        qDebug() &lt;&lt; ""before"";
        while( qq.next() ) {
            qDebug() &lt;&lt; qq.value(0).toInt() &lt;&lt; qq.value(1).toInt();
        }
        qDebug() &lt;&lt; ""after"";
}
</code></pre>

<p>which prints this:</p>

<blockquote>
  <p>before<br>
  after</p>
</blockquote>

<p>No rows were printed. I assume this is because I can't bind a list directly to a placeholder in an ""in"" clause. But is there a way to do it? I haven't been able to find anything about this.</p>
","<p><code>QSqlDriver</code> supports notifications which emit a signal when a specific event has occurred. To subscribe to an event just use <code>QSqlDriver::subscribeToNotification( const QString &amp; name )</code>. When an event that youre subscribing to is posted by the database the driver will emit the notification() signal and your application can take appropriate action.</p>

<pre><code>db.driver()-&gt;subscribeToNotification(""someEventId"");
</code></pre>

<p>The message can be posted automatically from a trigger or a stored procedure. The message is very lightweight: nothing more than a string containing the name of the event that occurred.</p>

<p>You can connect the <code>notification(const QString&amp;)</code>signal to your slot like:</p>

<pre><code>QObject::connect(db.driver(), SIGNAL(notification(const QString&amp;)), this, SLOT(refreshView()));
</code></pre>

<p>I should note that this feature is not supported by MySQL as it does not have an event posting mechanism.</p>
"
"<p>I am working on an WinRT app. I want to use <code>sqlite-net-extensions</code> to support <code>OneToMany</code>, <code>ManyToMany</code>. </p>

<pre><code>using SQLiteNetExtensions.Attributes;
using SQLite;

[Table(""WorkFlow"")]
public class Workflow
{
    [PrimaryKey, AutoIncrement]
    public int WorkflowId { get; set; }
    public string Name { get; set; }
    public int Revision { get; set; }
    [OneToMany]
    public List&lt;Step&gt; Steps { get; set; }
}

[Table(""Step"")]
public class Step
{
    public string Type { get; set; }
    public string Description { get; set; }
    [ManyToOne]
    public Workflow Workflow { get; set; }
}
</code></pre>

<p>When I try to generate the tables for the database, it raises the exception: </p>

<blockquote>
  <p>An exception of type 'System.NotSupportedException' occurred in
  app_name.exe but was not handled in user code Additional information:
  Don't know about System.Collections.Generic.List`1 [app_name.Model.modelName]</p>
</blockquote>

<p>This is coming from the <code>SqlType</code> in <code>SQLite.cs</code>.</p>

<p>But from the example on the <code>sqlite-net-extensions</code> <a href=""https://bitbucket.org/twincoders/sqlite-net-extensions"">homepage</a>, <code>List</code> property should work fine.</p>

<p>This is a copy of their example:</p>

<pre><code>public class Stock
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }
    [MaxLength(8)]
    public string Symbol { get; set; }

    [OneToMany]      // One to many relationship with Valuation
    public List&lt;Valuation&gt; Valuations { get; set; }
}

public class Valuation
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }

    [ForeignKey(typeof(Stock))]     // Specify the foreign key
    public int StockId { get; set; }
    public DateTime Time { get; set; }
    public decimal Price { get; set; }

    [ManyToOne]      // Many to one relationship with Stock
    public Stock Stock { get; set; }
}
</code></pre>

<p>Can anyone give me some suggestions to solve this problem? Thanks.</p>
","<p>Yes, but you have to explicitly declare the foreign keys and inverse properties in the relationship attribute, because otherwise the library may get the wrong foreign key for the relationship.</p>

<pre><code>public class ClassA
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }

    [OneToMany(""O2MClassAKey"", ""BObjectsInverse"")]
    public List&lt;ClassB&gt; BObjects { get; set; }

    [OneToOne(""O2OClassAKey"", ""BObjectInverse"")]
    public ClassB BObject { get; set; }

    // Other properties
    public string Bar { get; set; }
}

public class ClassB
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }

    [ForeignKey(typeof (ClassA))]
    public int O2MClassAKey { get; set; }

    [ForeignKey(typeof (ClassA))]
    public int O2OClassAKey { get; set; }

    // Inverse relationships, these are optional
    [ManyToOne(""O2MClassAKey"", ""BObjects"")]
    public ClassA BObjectsInverse { get; set; }
    [OneToOne(""O2OClassAKey"", ""BObject"")]
    public ClassA BObjectInverse { get; set; }

    // Other properties
    public string Foo { get; set; }
}
</code></pre>

<p>Please note that the foreign key <code>O2OClassAKey</code> for the <code>OneToOne</code> relationship can be declared in any of the classes.</p>

<p>If you don't need inverse properties, you can skip them in the relationship attribute:</p>

<pre><code>public class ClassA
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }

    [OneToMany(""O2MClassAKey"")]
    public List&lt;ClassB&gt; BObjects { get; set; }

    [OneToOne(""O2OClassAKey"")]
    public ClassB BObject { get; set; }

    // Other properties
    public string Bar { get; set; }
}

public class ClassB
{
    [PrimaryKey, AutoIncrement]
    public int Id { get; set; }

    [ForeignKey(typeof (ClassA))]
    public int O2MClassAKey { get; set; }

    [ForeignKey(typeof (ClassA))]
    public int O2OClassAKey { get; set; }

    // Other properties
    public string Foo { get; set; }
}
</code></pre>
"
"<p>I want to implement NDB Cluster for MySQL Cluster 6. I want to do it for very huge data structure with minimum 2 million records.</p>

<p>I want to know is if there are any limitations of implementing NDB cluster. For example, RAM size, number of databases, or size of database for NDB cluster. </p>
","<p>2 million databases? I asssume you meant ""rows"".</p>

<p>Anyway, concerning limitations: one of the most important things to keep in mind is that NDB/MySQL Cluster is not a general purpose database. Most notably, join operations, but also subqueries and range opertions (queries like: orders created between now and a week ago), can be considerably  slower than what you might expect. This is in part due to the fact that the data is distributed across multiple nodes. Although some improvements have been made, Join performance can still be very disappointing. </p>

<p>On the other hand, if you need to deal with many (preferably small) concurrent transactions (typically single row updates/inserts/delete lookups by primary key) and you mangage to keep all of your data in memory, then it can be a very scalable and performant solution. </p>

<p>You should ask yourself why you want cluster. If you simply want your ordinary database that you have now, except with added 99,999% availability, then you may be disappointed. Certainly MySQL cluster can provide you with great availability and uptime, but the workload of your app may not be very well suited for the thtings cluster is good for. Plus you may be able to use another high availability solution to increase the uptime of your otherwise traditional database.</p>

<p>BTW - here's a list of limitations as per the doc: <a href=""http://dev.mysql.com/doc/refman/5.1/en/mysql-cluster-limitations.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.1/en/mysql-cluster-limitations.html</a></p>

<p>But whatever you do, try out cluster, see if its good for you. MySQL cluster is not ""MySQL + 5 nines"". You'll find out when you try.</p>
"
"<p>I have a problem using mySQL.</p>

<p>This error pops up:</p>

<p><code>Invalid use of null value</code></p>

<p>I was trying to make two attributes in a table the primary key; here is my code: </p>

<pre><code>alter table contact_info
add primary key(phone_number, contactID);
</code></pre>

<p>Here are the alter statements I put into my contact_info table:</p>

<pre><code>alter table contact_info
add contactID varchar(10);

alter table contact_info
add suffixID varchar(8);

alter table contact_info
add titleID varchar(12);

alter table contact_info
add companyID varchar(12);

alter table contact_info
add phonetypeID char(2);
</code></pre>

<p>Does anybody know what's wrong? Thanks in advance.</p>
","<p>Look for a contact_info that has a null value in phone_number or contactID.  You can't add a primary key with existing null values in the table.</p>

<pre><code>select *
from contact_info
where (phone_number is null or contactID is null)
</code></pre>

<p>Run that SQL to find any records where the values are null.  Update the records, then try applying your primary key again.</p>

<p>I'm not sure what you are doing, but you may want to <strong>back up your data first!!!</strong> before running any updates.  Here is an update you might be able to use to set a contactID:</p>

<pre><code>update contact_info
set contactID = (select max(contactID) from contact_info) + 1
where contactID is null

update contact_info
set phone_number = '555-1212'
where phone_number is null
</code></pre>

<p>If you have duplicates in your data, you need to find them and update those as well.  Here's how you can find duplicates:</p>

<pre><code>-- Assuming the phone_number is duplicate (2 people living in the same house with the same phone number)
select a.phone_number, count(1)
from contact_info a
group by a.phone_number
having count(1) &gt; 1
</code></pre>
"
"<p>I'm attempting to use the cloud sql proxy to connect to 2 different cloud sql instances...</p>

<p>In the docs I found a line about <code>Use -instances parameter. For multiple instances, use a comma-separated list.</code> but not sure how to make that look. <a href=""https://cloud.google.com/sql/docs/sql-proxy"" rel=""noreferrer"">https://cloud.google.com/sql/docs/sql-proxy</a>. I'm using Google Container engine, and with a single CloudSQL instance it works great:</p>

<pre><code>- name: cloudsql-proxy
  image: b.gcr.io/cloudsql-docker/gce-proxy:1.05
  command: [""/cloud_sql_proxy"", ""--dir=/cloudsql"",
            ""-instances=starchup-147119:us-central1:first-db=tcp:3306"",
            ""-credential_file=/secrets/cloudsql/credentials.json""]
  volumeMounts:
  - name: cloudsql-oauth-credentials
    mountPath: /secrets/cloudsql
    readOnly: true
  - name: ssl-certs
    mountPath: /etc/ssl/certs
</code></pre>

<p>But for multiple I've tried the <code>-instances</code> section as such:</p>

<pre><code>-instances=starchup-147119:us-central1:first-db,starchup-147119:us-central1:second-db=tcp:3306  
and  
-instances=starchup-147119:us-central1:first-db=tcp:3306,starchup-147119:us-central1:second-db=tcp:3306
</code></pre>

<p>but they all give various errors; <code>ECONNREFUSED 127.0.0.1:3306</code>, <code>ER_DBACCESS_DENIED_ERROR</code>, and <code>ER_ACCESS_DENIED_ERROR</code></p>

<p>Any help is much appreciated!</p>
","<p>You cannot have two databases hosted on the same TCP port. Instead, specify ports for each database in the comma-separated list:</p>

<pre><code>-instances=project:region:db=tcp:3306,project:region:db-2=tcp:3307
</code></pre>

<p>I used 3306 and 3307 here, but you can use any ports you want! Make sure that the rest of your Container Engine config allows for communication between nodes on these ports (maybe that's true by default, I don't use GKE).</p>

<p>Most mysql drivers connect to port 3306 by default but have a way to specify another port. You'll have to arrange for your code to connect to the different port you choose for the second database.</p>
"
"<p>I'd like to switch PDO INSERT and UPDATE prepared statements to INSERT and ON DUPLICATE KEY UPDATE since I think it'll be a lot more efficient than what I'm currently doing, but I'm having trouble figuring out the correct syntax to use with named placeholders and bindParam.</p>

<p>I found several similar question on SO, but I'm new to PDO and couldn't successfully adapt the code for my criteria. This is what I've tried, but it doesn't work (it doesn't insert or update): </p>

<pre><code>try { 
  $stmt = $conn-&gt;prepare('INSERT INTO customer_info (user_id, fname, lname) VALUES(:user_id, :fname, :lname)'          
 'ON DUPLICATE KEY UPDATE customer_info SET fname= :fname, 
                                            lname= :lname   
                                            WHERE user_id = :user_id'); 
  $stmt-&gt;bindParam(':user_id', $user_id);  
  $stmt-&gt;bindParam(':fname', $_POST['fname'], PDO::PARAM_STR);
  $stmt-&gt;bindParam(':lname', $_POST['lname'], PDO::PARAM_STR);      
  $stmt-&gt;execute();
}
</code></pre>

<p>This is a simplified version of my code (I have several queries, and each query has between 20 - 50 fields).  I'm currently updating first and checking if the number of rows updated is greater than 0 and if not then running the Insert, and each of those queries has it's own set of bindParam statements.</p>
","<p>It's not a parameter of the query, in that you don't have to supply a value to MySQL.</p>

<pre><code>$insert = $mysqli-&gt;prepare(""INSERT INTO posts (post_name, publish_date) VALUES (?, NOW())"");
</code></pre>
"
"<p>I'm beginning to incorporate Alembic into my project which already uses SQLAlchemy table definitions. At present my DB schema is managed external to my application, and I want to bring the entire schema into my table definitions file.</p>

<p>In PostgreSQL I use a custom domain for storing email addresses. The PostgreSQL DDL is:</p>

<pre><code>CREATE DOMAIN email_address TEXT CHECK (value ~ '.+@.+')
</code></pre>

<p>How do I represent the creation of this domain, and the usage of it as a column data type, in SQLAlchemy?</p>
","<p>Domain type is based on some basic ""buildin"" type always. This type specifies a binary format. So if your domain is based on ""text"" type, then you store data as text.</p>
"
"<p>I want to save some data from a system table user_tab_cols, to a temp table so I can take a dump from it.</p>

<p>There are 100,000 rows in it , I have select from user_tab_cols about 1,000 records and d save them into a temp table with this query:</p>

<pre><code>create table temp table as 
select * from user_tab_cols where condition...
</code></pre>

<p>I had error 'illegal use of longtype' , because of the column DATA_DEFAULT that contain a type of long.</p>

<p>Is there an alterantive way where I can store a long type into anotehr table?</p>
","<blockquote>
  <p>ORA-00997: illegal use of LONG datatype</p>
</blockquote>

<p>It is a <strong>restriction</strong> on usage of <strong>LONG</strong> data type. <em>You cannot create an object type with a LONG attribute.</em></p>

<pre><code>SQL&gt; CREATE TABLE t AS SELECT data_default FROM user_tab_cols;
CREATE TABLE t AS SELECT data_default FROM user_tab_cols
                         *
ERROR at line 1:
ORA-00997: illegal use of LONG datatype


SQL&gt;
</code></pre>

<p>Alternatively, you could use <strong>TO_LOB</strong> as a workaround. Which would convert it into CLOB data type.</p>

<p>For example,</p>

<pre><code>SQL&gt; CREATE TABLE t AS SELECT TO_LOB(data_default) data_default FROM user_tab_cols;

Table created.

SQL&gt; desc t;
 Name                                      Null?    Type
 ----------------------------------------- -------- ----------------------------
 DATA_DEFAULT                                       CLOB

SQL&gt;
</code></pre>

<p>See more examples of workarounds <a href=""http://www.oracle-developer.net/display.php?id=430"">here</a>.</p>
"
"<p>I want to add a column to my table using <code>ALTER TABLE</code> and <code>UPDATE</code> statements not to recreate the full table.</p>

<p>When using a subquery in my <code>UPDATE</code> statement I don't get the output I expect.</p>

<p><strong>build reproducible data</strong></p>

<pre><code>library(dplyr)
library(dbplyr)
library(DBI)
con &lt;- DBI::dbConnect(RSQLite::SQLite(), path = "":memory:"")
copy_to(con, iris[c(1,2,51),],""iris"")

tbl(con,""iris"")
# # Source:   table&lt;iris&gt; [?? x 5]
# # Database: sqlite 3.19.3 []
#   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
#          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;chr&gt;
# 1          5.1         3.5          1.4         0.2     setosa
# 2          4.9         3.0          1.4         0.2     setosa
# 3          7.0         3.2          4.7         1.4 versicolor
</code></pre>

<p><strong>create the new column in a separate table</strong></p>

<pre><code>DBI::dbSendQuery(con, ""CREATE TABLE new_table AS SELECT t2.new_col from
                 iris t1 inner join 
                 (SELECT Species, sum(`Sepal.Width`) as new_col FROM iris GROUP BY Species) t2
                 on t1.Species = t2.Species"")

tbl(con,""new_table"")
# # Source:   table&lt;new_table&gt; [?? x 1]
# # Database: sqlite 3.19.3 []
#   new_col
#     &lt;dbl&gt;
# 1     6.5
# 2     6.5
# 3     3.2
</code></pre>

<p><strong>create the new column in the old table</strong></p>

<pre><code>DBI::dbSendQuery(con, ""ALTER TABLE iris ADD COLUMN new_col DOUBLE"")
</code></pre>

<p><strong>try to plug the new column from <code>new_table</code> there</strong></p>

<pre><code>DBI::dbSendQuery(con, ""UPDATE iris SET new_col = (SELECT new_col FROM new_table)"")

tbl(con,""iris"")
# # Source:   table&lt;iris&gt; [?? x 6]
# # Database: sqlite 3.19.3 []
#   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species new_col
#          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;      &lt;chr&gt;   &lt;dbl&gt;
# 1          5.1         3.5          1.4         0.2     setosa     6.5
# 2          4.9         3.0          1.4         0.2     setosa     6.5
# 3          7.0         3.2          4.7         1.4 versicolor     6.5
</code></pre>

<p>As you can see my <code>new_col</code> contains only the value <code>6.5</code> where I expected to have <code>3.2</code> on the last row. How can I fix this ?</p>
","<p>SQLite will only see the string passed down for the query, so what you do is something like</p>

<pre><code>  sqlcmd &lt;- paste(""SELECT * FROM annual WHERE fiscal="", fiscal.year, sep="""")
  data.annual &lt;- dbGetQuery(db, sqlcmd)
</code></pre>

<p>The nice thing is that you can use this the usual way to unwind loops.  Forgetting for a second that you have ram restrictions, conceptually you can do</p>

<pre><code>  years &lt;- seq(2000,2010)
  data &lt;- lapply(years, function(y) {
     dbGetQuery(db, paste(""SELECT * FROM annual WHERE fiscal="", y, sep="""")
  }
</code></pre>

<p>and now data is a list containing all your yearly data sets.  Or you could keep the data, run your regression and only store the summary object. </p>
"
"<p>I am trying to re-create a database (MyDB) from one SQL server (<strong>Source</strong>) to another one (<strong>Target</strong>). <strong>Source</strong> is located on my local machine and is SQL Server 2014. <strong>Target</strong> is located on a remote machine and it's SQL Server 2012. Here are the steps I've taken:</p>

<ol>
<li>On my local machine I go to SQL Server Management studio, I right click on MyDB and go to Tasks--> Generate Scripts.

<ol start=""2"">
<li>There I select ""Script entire database and all database objects"".</li>
<li>I click Next and on the next page, under Advanced, I select ""Schema and data"".</li>
<li>That generates a SQL file (scripts.sql) that contains the definition for MyDB.</li>
<li>Then I use the following osql command to re-create the database on <strong>Target</strong>:</li>
</ol></li>
</ol>

<blockquote>
  <p>osql -S target -d master -E -i scripts.sql -o output.log</p>
</blockquote>

<ol start=""6"">
<li>After execution is finished I get this error in the log file ""output.log"":</li>
</ol>

<blockquote>
  <p>1> 2> 1> 2> 3> 4> 5> 6> 7> 8> Msg 5133, Level 16, State 1, Server
  Target,    Line 2 Directory lookup for the file ""C:\Program
  Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB.mdf""
  failed with the operating system error 3(The system cannot find the
  path specified.). Msg 1802, Level 16, State 1, Server Target, Line 2
  CREATE DATABASE failed. Some file names listed could not be created.
  Check related errors.
  1> 2> Msg 5011, Level 14, State 5, Server
  Target, Line 1 User does not have permission to alter database 'MyDB',
  the database does not exist, or the database is not in a state that
  allows access checks. Msg 5069, Level 16, State 1, Server Target, Line
  1 ALTER DATABASE statement failed.</p>
</blockquote>

<p>Here are the first few lines of ""scripts.sql"":</p>

<pre><code>USE [master]
GO
/****** Object:  Database [MyDB]    Script Date: 4/12/2016 4:30:20 PM ******/
CREATE DATABASE [MyDB]
 CONTAINMENT = NONE
 ON  PRIMARY 
( NAME = N'MyDB', FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB.mdf' , SIZE = 513024KB , MAXSIZE = UNLIMITED, FILEGROWTH = 262144KB )
 LOG ON 
( NAME = N'MyDB_log', FILENAME = N'C:\Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB_log.ldf' , SIZE = 1317504KB , MAXSIZE = 2048GB , FILEGROWTH = 131072KB )
GO
ALTER DATABASE [MyDB] SET COMPATIBILITY_LEVEL = 100
GO
IF (1 = FULLTEXTSERVICEPROPERTY('IsFullTextInstalled'))
begin
EXEC [MyDB].[dbo].[sp_fulltext_database] @action = 'enable'
end
GO
ALTER DATABASE [MyDB] SET ANSI_NULL_DEFAULT ON
</code></pre>

<p>I do have the file MyDB.mdf at the location it's complaining about on <strong>Source</strong>, but not on <strong>Target</strong>. There is no directory ""MSSQL12.MSSQLSERVER"" on <strong>Target</strong>. How can I fix this?</p>
","<p>For those interested in the solution to this, the problem was that there was no ""MSSQL12.MSSQLSERVER"" directory on <strong>Target</strong> because it's on a different version of SQL Server, namely 2012. What I had to do was create the directory manually and it started working after that.</p>
"
"<p>I am trying to migrate a postgresql database to mysql following this tutorial:  <a href=""http://mysqlworkbench.org/2012/11/how-to-migrate-postgresql-databases-to-mysql-using-the-mysql-workbench-migration-wizard/"" rel=""nofollow noreferrer"">http://mysqlworkbench.org/2012/11/how-to-migrate-postgresql-databases-to-mysql-using-the-mysql-workbench-migration-wizard/</a></p>

<p>I am experiencing this error when I try to test my connection</p>

<blockquote>
  <p>Could not connect to Source DBMS [IM002][Microsoft][ODBC Driver
  Manager] Data soure name not found and no default driver specified
  (0)(SQLDriverConnect)</p>
</blockquote>

<p>ODBC connection string</p>

<pre><code>Driver=psqlodc;SERVER=127.0.0.1;PORT=5432;DATA...
</code></pre>

<p>Does anyone know how to correct this error?</p>
","<p>In my case I was using Windows 64bit and using ""PostgreSQL ANSI(x64)"" did connect to the Postgres database but it gave errors while migrating the database from Postgres to Mysql. I used the driver ""PostgreSQL Unicode(x64)"" and it worked. If your data contains unicode characters i.e. non ASCII characters use ""PostgreSQL Unicode(x64)"" drivers.
<a href=""https://i.stack.imgur.com/BMxpl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BMxpl.png"" alt=""enter image description here""></a></p>
"
"<p>I have created a procedure like :</p>

<pre><code>CREATE OR REPLACE FUNCTION insert_user_ax_register(
   user_name character varying(50), 
   password character varying(300), 
   role_id character varying(10), 
   created_dt date, 
   status boolean, 
   email character varying(50), 
   join_date character varying(30), 
   phone_no bigint, 
   client_address character varying(200), 
   full_name character varying(100), 
   financial_year character varying(10)) 
RETURNS void 
AS $BODY$ 
declare 
begin 
  INSERT INTO ax_register(user_name,password,role_id,created_dt,status,email,join_date,phone_no,client_address,full_name,financial_year) 
  VALUES (user_name,password,role_id,now(),true,email,join_date,phone_no,client_address,full_name,financial_year); 
end 
$BODY$ 
LANGUAGE plpgsql VOLATILE
</code></pre>

<p>and tried to execute it like this:</p>

<pre><code>SELECT * from insert_user_ax_register('debasrita','debasrita','client001',now(),'t','abc@gmail.com',now(),'ctc','debasrita','2014-15',9090909090);
</code></pre>

<p>but it throws the following error :</p>

<p>ERROR: function insert_user_ax_register(unknown, unknown, unknown, timestamp with time zone, unknown, unknown, timestamp with time zone, unknown, unknown, unknown, bigint) does not exist
SQL state: 42883
Hint: No function matches the given name and argument types. You might need to add explicit type casts.
Character: 16</p>

<p>Please help me out on this. I am new to pgsql and not able to find out any solution from google. I am using <code>pgsql 9.1.3</code></p>

<p>May I know what is the correct way to achieve my objective?</p>
","<p>The error message tells you what you need to be looking for:</p>

<blockquote>
  <p><i>""No function matches the given name <b>and argument types</b>""</i></p>
</blockquote>

<p>As the function <em>name</em> seems correct, it can only be the parameters you are passing. So write down which value is passed for which parameter:</p>

<pre>
'debasrita'                 --&gt; user_name character varying(50)
'debasrita'                 --&gt; password character varying(300)
'client001'                 --&gt; role_id character varying(10)
created_dt date             --&gt; now()
status boolean,             --&gt; 't'
email varchar(50)           --&gt; 'abc@gmail.com'
join_date varchar(30)       --&gt; now()       <b>&lt;&lt; first error: now() is not a character constant</b>
phone_no bigint             --&gt; 'ctc'       <b>&lt;&lt; second error: 'ctc' is not a bigint</b>
client_address varchar(200) --&gt;  'debasrita'
full_name varchar(100)      --&gt; '2014-15'
financial_year varchar(10)  --&gt; 9090909090  <b>&lt;&lt; third error: 9090909090  is not a character literal </b>
</pre>

<p>So you need to either adjust the parameter types, e.g. define <code>join_date</code> as date, not as <code>varchar</code> or adjust the values that you pass for each parameter.</p>

<p>And finally you need to call the function like this:</p>

<pre><code>SELECT insert_user_ax_register(...);
</code></pre>

<p>rather than <code>select * from ...</code></p>
"
"<p>I asked a question a few days ago (<a href=""https://stackoverflow.com/questions/2795723/access-to-sql-server-2005-from-a-non-domain-machine-using-windows-authentication"">Access to SQL Server 2005 from a non-domain machine using Windows authentication</a>) which got some interesting, but not usable suggestions. I'd like to ask the question again, but make clear what my constraints are:</p>

<p>I have a Windows domain within which a machine is running SQL Server 2005 and which is configured to support only Windows authentication. I would like to run a C# client application on a machine on the same network, but which is NOT on the domain, and access a database on the SQL Server 2005 instance.</p>

<p>I CANNOT create or modify OS or SQL Server users on either machine, and I CANNOT make any changes to permissions or impersonation, and I CANNOT make use of runas.</p>

<p>I know that I can write Perl and Java applications that can connect to the SQL Server database using only these four parameters: server name, database name, username (in the form domain\user), and password.</p>

<p>In C# I have tried various things around:</p>

<pre><code>string connectionString = ""Data Source=server;Initial Catalog=database;User Id=domain\user;Password=password"";
SqlConnection connection = new SqlConnection(connectionString);
connection.Open();
</code></pre>

<p>and tried setting integrated security to true and false, but nothing seems to work. Is what I am trying to do simply impossible in C#?</p>

<p>Thanks for any help, Martin</p>
","<p>I had a similar problem where I was writing a tool that needed to run on a machine on one domain and authenticate with a SQL server on another domain using a trusted connection. Everything I could find on the subject said it couldn't be done. Instead you must join the domain, use SQL authentication, get involved with some chap called Kerberos, or get your network guys to setup a trusted relationship, to name a few alternatives.</p>

<p>The thing is I knew I could get it working in some way using RUNAS because I'd proven it with SSMS:</p>

<pre><code>C:\WINDOWS\system32\runas.exe /netonly /savecred /user:megacorp\joe.bloggs ""C:\Program Files\Microsoft SQL Server\90\Tools\Binn\VSShell\Common7\IDE\SqlWb.exe""
</code></pre>

<p>The /netonly flag allowed me to execute the exe with the local credentials and access the network with the remote credentials,  I think, anyway I got the result set I expected from the remote server. The problem was the runas command made it very difficult to debug the application, and it didn't smell good.</p>

<p>Eventually I found this article on <a href=""http://www.codeproject.com/KB/cs/cpimpersonation1.aspx"" rel=""noreferrer"">the code project</a> which was talking about authenticating to manipulate Active Directory, Here is the main class that does the impersonation:</p>

<pre>
    using System;
    using System.Runtime.InteropServices;  // DllImport
    using System.Security.Principal; // WindowsImpersonationContext

    namespace TestApp
    {
        class Impersonator
        {
            // group type enum
            enum SECURITY_IMPERSONATION_LEVEL : int
            {
                SecurityAnonymous = 0,
                SecurityIdentification = 1,
                SecurityImpersonation = 2,
                SecurityDelegation = 3
            }

            // obtains user token
            [DllImport(""advapi32.dll"", SetLastError = true)]
            static extern bool LogonUser(string pszUsername, string pszDomain, string pszPassword,
                int dwLogonType, int dwLogonProvider, ref IntPtr phToken);

            // closes open handes returned by LogonUser
            [DllImport(""kernel32.dll"", CharSet = CharSet.Auto)]
            extern static bool CloseHandle(IntPtr handle);

            // creates duplicate token handle
            [DllImport(""advapi32.dll"", CharSet = CharSet.Auto, SetLastError = true)]
            extern static bool DuplicateToken(IntPtr ExistingTokenHandle,
                int SECURITY_IMPERSONATION_LEVEL, ref IntPtr DuplicateTokenHandle);

            WindowsImpersonationContext newUser;

            /// 
            /// Attempts to impersonate a user.  If successful, returns 
            /// a WindowsImpersonationContext of the new users identity.
            /// 
            /// Username you want to impersonate
            /// Logon domain
            /// User's password to logon with
            /// 
            public Impersonator(string sUsername, string sDomain, string sPassword)
            {
                // initialize tokens
                IntPtr pExistingTokenHandle = new IntPtr(0);
                IntPtr pDuplicateTokenHandle = new IntPtr(0);
                pExistingTokenHandle = IntPtr.Zero;
                pDuplicateTokenHandle = IntPtr.Zero;

                // if domain name was blank, assume local machine
                if (sDomain == """")
                    sDomain = System.Environment.MachineName;

                try
                {
                    const int LOGON32_PROVIDER_DEFAULT = 0;

                    // create token
                    // const int LOGON32_LOGON_INTERACTIVE = 2;
                    const int LOGON32_LOGON_NEW_CREDENTIALS = 9;
                    //const int SecurityImpersonation = 2;

                    // get handle to token
                    bool bImpersonated = LogonUser(sUsername, sDomain, sPassword,
                        LOGON32_LOGON_NEW_CREDENTIALS, LOGON32_PROVIDER_DEFAULT, ref pExistingTokenHandle);

                    // did impersonation fail?
                    if (false == bImpersonated)
                    {
                        int nErrorCode = Marshal.GetLastWin32Error();

                        // show the reason why LogonUser failed
                        throw new ApplicationException(""LogonUser() failed with error code: "" + nErrorCode);
                    }

                    bool bRetVal = DuplicateToken(pExistingTokenHandle, (int)SECURITY_IMPERSONATION_LEVEL.SecurityImpersonation, ref pDuplicateTokenHandle);

                    // did DuplicateToken fail?
                    if (false == bRetVal)
                    {
                        int nErrorCode = Marshal.GetLastWin32Error();
                        CloseHandle(pExistingTokenHandle); // close existing handle

                        // show the reason why DuplicateToken failed
                        throw new ApplicationException(""DuplicateToken() failed with error code: "" + nErrorCode);
                    }
                    else
                    {
                        // create new identity using new primary token
                        WindowsIdentity newId = new WindowsIdentity(pDuplicateTokenHandle);
                        WindowsImpersonationContext impersonatedUser = newId.Impersonate();

                        newUser = impersonatedUser;
                    }
                }
                finally
                {
                    // close handle(s)
                    if (pExistingTokenHandle != IntPtr.Zero)
                        CloseHandle(pExistingTokenHandle);
                    if (pDuplicateTokenHandle != IntPtr.Zero)
                        CloseHandle(pDuplicateTokenHandle);
                }
            }

            public void Undo()
            {
                newUser.Undo();
            }
        }
    }
</pre>

<p>To use it just:</p>

<pre><code>Impersonator impersonator = new Impersonator(""username"", ""domain"", ""password"");

//Connect to and use SQL server

impersonator.Undo();
</code></pre>

<p>I added in the Undo method otherwise the impersonator object tended to get garbage collected. I also altered the code to use LOGON32_LOGON_NEW_CREDENTIALS but this was a poke and run to make it work; I still need to understand fully what it does, I have a feeling its the same as the /netonly flag on runas. I'm also going to break down the constructor a bit.</p>
"
"<p><a href=""http://sqlinq.codeplex.com/"" rel=""nofollow noreferrer"">Sqlinq</a> is an opensource project to create sql queries from linq.  I'm using it with <a href=""https://github.com/SamSaffron/dapper-dot-net/"" rel=""nofollow noreferrer"">Dapper</a>, but unfortunately it doesn't support JOINs. </p>

<p>Is there are any other libraries that do the same thing and supports JOIN too?</p>

<p>If not, what could be a solution to avoid hard coding sql select queries?</p>
","<p>How about the build-in linq that comes with .NET? </p>

<p><a href=""http://msdn.microsoft.com/en-us/library/bb397927.aspx"" rel=""nofollow"">http://msdn.microsoft.com/en-us/library/bb397927.aspx</a></p>
"
"<p>This seems to be a simple question, but I wonder the disadvantages of not calling the  ""close()"" function. </p>
","<p>In your current usage, it will close for you:</p>

<blockquote>
  <p>If the IDbConnection
  is closed before Fill is called, it is opened to retrieve data and
  then closed. If the connection is open before Fill is called, it
  remains open.</p>
</blockquote>

<p><a href=""http://msdn.microsoft.com/en-us/library/zxkb3c3d.aspx"" rel=""noreferrer"">http://msdn.microsoft.com/en-us/library/zxkb3c3d.aspx</a></p>

<p>I think it's always better to explicitly cater for it yourself with a <code>using</code> statement:</p>

<pre><code>using (SqlConnection conn = new SqlConnection(""""))
{
    conn.Open();

    // Do Stuff.

} // Closes here on dispose.
</code></pre>

<p>This is often more readable and doesn't rely on people understanding the inner workings of <code>SqlDataAdapter.Fill</code>, just the <code>using</code> statement and connections.</p>

<p>However, if you <em>know</em> the connection is closed before the adapter uses it (as in, you've just created the connection) and it's not used for anything else, your code is perfectly safe and valid.</p>

<p>Personally, I'd write something like this:</p>

<pre><code>    string cnStr = ""Data Source=TEST;Initial Catalog=Suite;Persist Security Info=True;User ID=app;Password=Immmmmm"";
    DataSet ds = new DataSet();

    using (SqlConnection cn = new SqlConnection(cnStr))
    using (SqlCommand cmd = new SqlCommand(""SELECT TOP 10 * FROM Date"", cn))
    using (SqlDataAdapter adapter = new SqlDataAdapter(cmd))
    { 
        conn.Open();
        adapter.Fill(ds);       
    }
</code></pre>
"
"<p>When trying to connect to my MySQL server in Azure from mysql client, I get the following error, even though I am using the correct username and server name. How can I fix this?</p>

<p><em>The connection string may not be right. Please visit portal for references.</em></p>
","<p>When connecting to your server instance of Azure Database for MySQL, you are required to follow the <code>&lt;username@hostname&gt;</code> format , whether you are doing it from mysqlexe client or MySQL workbench. We recommend you get the complete connection string for your client from the Azure portal and use it when connecting to your MySQL server.</p>

<p>Read <a href=""https://docs.microsoft.com/en-us/azure/mysql/quickstart-create-mysql-server-database-using-azure-portal#get-connection-information"" rel=""noreferrer"">How to get Connection Information</a> to understand more about how to connect to your MySQL server from various clients.</p>
"
"<p>SQLCL is exactly what I need, but I've a big difficulty on one little thing :</p>

<p>I want to make a script (batch file) with Auto connection and just after an EXPORT CSV (on a remote desktop : not on the server).</p>

<p>So I'm using the pipe method with SQLCL in a Batch File: </p>

<pre><code>echo SET SQLFORMAT CSV &lt;
echo SPOOL export.csv &lt;
echo SELECT COUNT(*) FROM ARTICLE; &lt;
echo SPOOL OFF | C:\Work\Soft\sqlcl\bin\sql.exe login/passwd@xxx.xxx.xxx.xxx:1521/DB.SCH
</code></pre>

<p>It's working (no errors in console) but, impossible to find the file <code>export.csv</code> : when I change the destination <code>c:\...</code> it's working too but impossible to find the created file. It's working fine with SQL Developer and the file is created on my dekstop, so I don't understand why it's not the same case for SQLCL.</p>
","<p>It looks like your SID and service name are not the same. In SQL Developer you seem to be using the SID - at least in the custom JDBC URL you showed - as denoted by the colon in <code>:vdbsl4</code>.</p>

<p>Your SQLcl URL is using the service name, as denoted by the slash in <code>/vdbsl14</code>. Using the SID instead (i.e. changing the / to :) in that URL should work since it's using JDBC:</p>

<pre><code>sqlcl username/pass@delphix-vdb-n-1.va2.b2c.nike.com:1521:vdbsl14
</code></pre>

<p>Alternatively (and preferably, in my opinion) find out what your service name actually is. If you have sufficient privileges on the database you can do <code>show parameters service_names</code> from SQL Devleoper, or if you have access to the server as DBA you can do <code>lsnrctl services</code>, or even look at the <code>tnsnames.ora</code> in case there is a TNS alias defined that shows the service name. (<code>listener.ora</code> isn't likely to help, but could give hints or if you're lucky show a default service name).</p>

<p>You can use that service name in a JDBC URL, as <code>/service_name</code>.</p>

<p>You can also use a TNS alias from SQLcl (or SQL*Plus). You may already have a <code>tnsnames.ora</code> available; if not you might be able to copy it from your server, or create your own. That can refer to the SID or the service name.</p>

<p>You can even pass a full TNS description to SQL*Plus (not sure about SQLcl) but that's a bit unpleasant. If you don't have/want a <code>tnsnames.ora</code> you can use 'easy connect' syntax, which is the same as you're using for SQLcl - but that <em>has</em> to be the service name, it doesn't allow SIDs. </p>
"
"<p>Do most people use .NET's SqlMembershipProvider, SqlRoleProvider, and SqlProfileProvider when developing a site with membership capabilities?</p>

<p>Or do many people make their own providers, or even their own membership systems entirely?</p>

<p>What are the limitations of the SQL providers that would make you roll your own?</p>

<p>Is it easy to extend the SQL providers to provide additional functionality?</p>

<p><strong>For Reference</strong><br>
<a href=""http://weblogs.asp.net/scottgu/archive/2006/04/13/442772.aspx"" rel=""nofollow noreferrer"">Per Scott Gu's Blog</a>, <a href=""http://download.microsoft.com/download/a/b/3/ab3c284b-dc9a-473d-b7e3-33bacfcc8e98/ProviderToolkitSamples.msi"" rel=""nofollow noreferrer"">Microsoft provides the source code for the SqlMembershipProvider</a> so that you can customize it, instead of starting from scratch.  Just an FYI.</p>
","<p>We use everything except the Profile Provider. The Profile Provider is completly text based and does full text seearches - this becomes exceedingly slow as you user base gets larger. We have found it a much better solution to ""role our own"" profile section of the membership api database that is keyed to the userid in membership.</p>
"
"<p>I am attempting to create a very simple http server that does one thing. Upon receiving an HttpRequest, it runs a query on the local database server, and returns a string based on that query.</p>

<p>I am learning Dart, and I am having trouble grasping Futures. I thought I understood them, but this example leads me to believe I really have no idea how they work. So, not only am I looking for a solution to this problem, but any pointers I will gladly accept as well.</p>

<p>Note: This code is a very primitive example of what I have been trying to accomplish, and for the sake of reaching out to the Stackoverflow community, I have shortened / simplified it as much as possible, while keeping the problem intact. </p>

<p>Here is my server.dart code</p>



<pre class=""lang-dart prettyprint-override""><code>import 'dart:io';
import 'package:sqljocky/sqljocky.dart';


final connection = new ConnectionPool(host: 'localhost', port: 3306, user: 'root', password: null, db: 'server1');

main() {

  HttpServer.bind(InternetAddress.ANY_IP_V4, 9090)..then((server) {
    print(""serving generic database query on localhost:9090"");
    server.listen((request) {
      if (request.method == ""GET"") {
        request.response.write(getResults());
        request.response.close();
      }
      else {
        request.response.statusCode = HttpStatus.BAD_REQUEST;
      }
    });
  });
}


String getResults() {

  StringBuffer sb = new StringBuffer();
  sb.write(""START--"");
  connection.query(""select name, email, zkey from users"")
      ..then((results) {
    results.forEach((row) {
      sb.write(row.toString());
      print(row.toString());
    });
  });

  sb.write(""--END"");
  print(sb.toString());

  return sb.toString();
}
</code></pre>

<p>So if I send a request to this server it returns ""START----END"".
The server prints out the expected query result, and then prints out ""START----END"".
This leads me to believe that my request response is closing and returning before the query result is done processing.</p>

<p>So whether I curl localhost:9090/asdf or actually build a client http request sender, I do not get the response I am expecting... which is a database query result.</p>

<p>Thanks in advance</p>
","<p>That <code>""START----END""</code> is printed 'out of order' is a behavior of futures that is confusing to most developers in the beginning.<br>
A call like <code>connection.query()</code> which returns a future is not executed immediately but enlisted in a queue for later execution. The current thread of execution is continued until finished and then the queue is processed one after another.</p>

<p>What doesn't work in your code is that you do an async call <code>connection.query()</code> and continue as if it was a sync call. That does never work in Dart. When you start an async execution you cant go back to sync. (as far as I know the planned async/await should solve this).</p>



<p>More details at dartlang.org <a href=""https://www.dartlang.org/articles/event-loop/"" rel=""nofollow noreferrer"">The Event Loop and Dart</a></p>

<h2>EDIT tested code</h2>

<pre class=""lang-dart prettyprint-override""><code>import 'dart:io';
import 'package:sqljocky/sqljocky.dart';

final connection = new ConnectionPool(host: 'localhost', port: 3306, user: 'root', password: null, db: 'server1');

main() {    
  HttpServer.bind(InternetAddress.ANY_IP_V4, 9090)..then((server) {
    print(""serving generic database query on localhost:9090"");
    server.listen((request) {
      if (request.method == ""GET"") {
        getResults()
        .then((result) { 
          print('Result: $result'); 
          request.response.write(result);
          request.response.close();
        });
      }
      else {
        request.response.statusCode = HttpStatus.BAD_REQUEST;
      }
    });
  });
}

Future&lt;String&gt; getResults() { 

  StringBuffer sb = new StringBuffer();
  sb.write(""START--"");
  return connection.query(""select name, email, zkey from users"") 
  .then((Result results) =&gt; results.toList())
  .then((list) {
    list.forEach((row) {
      sb.write(row.toString());
    });
    sb.write(""--END"");
  })
  .then((_) =&gt; sb.toString());
}
</code></pre>

<p>See also Gregs answer on this question how to read SqlJocky results <a href=""https://stackoverflow.com/questions/20411171"">sqljocky querying database synchronously</a></p>
"
"<p>I am using JSQLPARSER for the first time. I have some SQL files which come dynamically, i need to read table and column names from that SQL. After lot of googling  I tried with JSQLPARSER. I am trying to read column names from the file but I am unable to read column names due to expression, please can any one correct my code where I went wrong. I am getting CLASSCASTEXCEPTION 
code:</p>

<pre><code>public static void main(String[] args) throws JSQLParserException
    {
        // TODO Auto-generated method stub
         String statement=""SELECT LOCATION_D.REGION_NAME, LOCATION_D.AREA_NAME, COUNT(DISTINCT INCIDENT_FACT.TICKET_ID) FROM LOCATION_D, INCIDENT_FACT WHERE ( LOCATION_D.LOCATION_SK=INCIDENT_FACT.LOCATION_SK ) GROUP BY LOCATION_D.REGION_NAME, LOCATION_D.AREA_NAME""; 
         CCJSqlParserManager parserManager = new CCJSqlParserManager();
         Select select=(Select) parserManager.parse(new StringReader(statement));

         PlainSelect plain=(PlainSelect)select.getSelectBody();     
         List selectitems=plain.getSelectItems();
         System.out.println(selectitems.size());
         for(int i=0;i&lt;selectitems.size();i++)
         {
            Expression expression=((SelectExpressionItem) selectitems.get(i)).getExpression();  
            System.out.println(""Expression:-""+expression);
            Column col=(Column)expression;
            System.out.println(col.getTable()+"",""+col.getColumnName());      
         }
    }
</code></pre>
","<p>You do not have sample code, so the code below is based on my guessing that you want to show that the count(*) is actually a function call only.</p>

<pre><code>import java.io.StringReader;

import net.sf.jsqlparser.JSQLParserException;
import net.sf.jsqlparser.expression.Function;
import net.sf.jsqlparser.parser.CCJSqlParserManager;
import net.sf.jsqlparser.statement.select.PlainSelect;
import net.sf.jsqlparser.statement.select.Select;
import net.sf.jsqlparser.statement.select.SelectExpressionItem;

public class MySQLParser
{
    CCJSqlParserManager parserManager = new CCJSqlParserManager();

    public MySQLParser() throws JSQLParserException
    {
        String statement = ""SELECT COUNT(*) FROM db.table1"";
        PlainSelect plainSelect = (PlainSelect) ((Select) parserManager.parse(new StringReader(statement))).getSelectBody();        
        System.out.format(""%s is function call? %s"",
                plainSelect.getSelectItems().get(0),
                ((Function)((SelectExpressionItem) plainSelect.getSelectItems().get(0)).getExpression()).isAllColumns());
    }
    public static void main(String[] args) throws JSQLParserException
    {

        new MySQLParser();

    }

}
</code></pre>
"
"<p>The background:</p>

<p>We have an application where the main entity is a customer. All information in this applications starts from the customer. We thought it would be really nice if we can use this for some kind of partitioning. We designed the service with Azure SQL Database as a backend.</p>

<p>Our tables look like this (only the relevant part is left for brevity):</p>

<pre><code>TABLE dbo.Orders
(
     CustomerId INT NOT NULL DEFAULT( FEDERATION_FILTERING_VALUE( 'FEDERATION_BY_CUSTOMER' ) ),
     OrderId INT NOT NULL,
     ....,
     CONSTRAINT PK_Orders PRIMARY KEY CLUSTERED ( CustomerId, OrderId )
) FEDERATED ON ( FEDERATION_BY_CUSTOMER = CustomerId );
</code></pre>

<p>Now this allowed us to do some crazy things. Our entry points to all SQL related stuff always contains the following command first:</p>

<pre><code>USE FEDERATION GroupFederation( FEDERATION_BY_CUSTOMER = 1 ) WITH RESET, FILTERING = ON
</code></pre>

<p>In this case this statement:</p>

<pre><code>SELECT * FROM Orders
</code></pre>

<p>or</p>

<pre><code>INSERT INTO Orders ( OrderId ) VALUES ( 10 );
</code></pre>

<p>will work without a problem, working only on the data of the given customer. The CustomerId COLUMN will always be inferred from the system function FEDERATION_FILTERING_VALUE;</p>

<p>Now we could have all our customer in a single database without a problem, and they would be isolated from each other. If sometime in the future, one of them got too big, we could SPLIT the federation at that particular customer ID and we don't have to change anything in our code to support it.</p>

<p>Heck, we could have each customer in separated federation database and the service using it wouldn't know a single thing about it.</p>

<p>We were very happy with our solution and I thought I was very clever coming up with it. Not until recently when Microsoft announced that they are deprecating the azure federations feature with the new azure database editions that are coming up. Read more about it <a href=""http://msdn.microsoft.com/library/azure/dn741330.aspx"">here</a> and <a href=""http://blogs.msdn.com/b/windowsazure/archive/2014/04/24/azure-sql-database-introduces-new-service-tiers.aspx?CommentPosted=true#commentmessage"">here</a>.</p>

<p>I hope you see my problem. What do you think my alternatives are? Do you use Azure Federations and how are you going to transition?</p>

<p>Thank you.</p>
","<p>Jon,
See a session here - the referenced sample library should be available soon:</p>

<p>SQL Database Sharding Patterns: <a href=""http://channel9.msdn.com/Shows/Data-Exposed/SqlDbShardingIntro"" rel=""nofollow"">http://channel9.msdn.com/Shows/Data-Exposed/SqlDbShardingIntro</a></p>

<p>-Simon.</p>
"
"<p>The schema is as follows:</p>

<pre><code>CREATE TABLE [Structure](
    [StructureId] [uniqueidentifier] NOT NULL,
    [SequenceNumber] [int] NOT NULL, -- order for siblings, unique per parent
    [ParentStructureId] [uniqueidentifier] NULL,
 CONSTRAINT [Structure_PK] PRIMARY KEY CLUSTERED 
(
    [StructureId] ASC
)
) ON [PRIMARY]

ALTER TABLE [Structure]  WITH CHECK ADD  CONSTRAINT [Structure_FK1] 
FOREIGN KEY([ParentStructureId])
REFERENCES [Structure] ([StructureId])
</code></pre>

<p>Currently, I can get all the logical data out with the follow CTE, but I would like to print it directly in a depth first fashion.</p>

<pre><code>WITH SCTE (StructureId, Level, Seq, ParentId)
AS
(
  SELECT StructureId,  0, SequenceNumber, [ParentStructureId]
    FROM Structure
    WHERE [ParentStructureId] IS NULL 
          AND StructureId = 'F6C5F016-1270-47C1-972F-349C32DFC92A'

  UNION ALL

  SELECT Structure.StructureId, Level + 1, SequenceNumber, ParentStructureId
  FROM Structure
  INNER JOIN SCTE ON SCTE.StructureId = Structure.ParentStructureId
)

SELECT * FROM SCTE
ORDER BY Level, ParentId, Seq
</code></pre>

<p>The output is as follows (truncated here):</p>

<pre><code>StructureId                     Level   Seq ParentId
F6C5F016-1270-47C1-972F-349C32DFC92A    0   0   NULL
D2E34429-401A-4A49-9E18-E81CCA0FB417    1   0   F6C5F016-1270-47C1-972F-349C32DFC92A
0CC5E16C-9194-40CA-9F72-1CED2972D7CA    1   1   F6C5F016-1270-47C1-972F-349C32DFC92A
1ECD1D30-EB85-42B0-969F-75794343E3B4    1   2   F6C5F016-1270-47C1-972F-349C32DFC92A
EEC3A981-B790-4600-8CD1-F15972CD9230    2   0   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
4406F639-2F58-4918-A9EF-A4B0F379BEA0    2   1   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
FCAF7870-C606-4AA6-85EE-57B90B1B0CC3    2   2   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
855DF5FB-1593-4E5B-8EF9-3770B45F89D6    2   3   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
3D16DF32-C04F-49B4-B0D9-5BDC9104F810    2   4   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
A1084D00-0198-47D9-87E0-BB8234233F14    2   5   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
CE443C0D-376F-46EC-9914-32C6B7200DB1    2   6   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
0DEA587D-4FCF-414C-AD71-FB00829F8082    2   7   0CC5E16C-9194-40CA-9F72-1CED2972D7CA
CC9FC8D3-254A-486B-8DC4-07E57627476C    2   0   1ECD1D30-EB85-42B0-969F-75794343E3B4
215565CC-501F-4850-B8AE-5466DA5E6854    2   1   1ECD1D30-EB85-42B0-969F-75794343E3B4
D4E6C8E5-5ADD-4AD1-B59B-1A672F66888A    2   2   1ECD1D30-EB85-42B0-969F-75794343E3B4
796C65BF-4714-4DBF-A97A-2150DBE3098C    2   3   1ECD1D30-EB85-42B0-969F-75794343E3B4
B39DEB9C-BE42-43B4-9C38-968399D7D1E2    2   4   1ECD1D30-EB85-42B0-969F-75794343E3B4
6C2F70C6-1DA0-4E1A-BBC1-D7FCAFE6AFEE    2   0   D2E34429-401A-4A49-9E18-E81CCA0FB417
75D7B43B-C971-46B4-BC42-58C3605ADD79    2   1   D2E34429-401A-4A49-9E18-E81CCA0FB417
0B5AAAA0-A69F-431E-86BA-148444D7B1E6    2   2   D2E34429-401A-4A49-9E18-E81CCA0FB417
CB3CF66B-D83A-45E2-953A-6F0CEE094F5B    2   3   D2E34429-401A-4A49-9E18-E81CCA0FB417
1D5F69C3-F036-4667-BD75-A0DC1506DB6D    2   4   D2E34429-401A-4A49-9E18-E81CCA0FB417
71B894F7-B9FC-44DE-AEDB-E6FA026A6082    2   5   D2E34429-401A-4A49-9E18-E81CCA0FB417
F1DFA1E1-013B-449C-9D9D-14C64E75D418    2   6   D2E34429-401A-4A49-9E18-E81CCA0FB417
</code></pre>

<p>As you can see, the result is 'breadth first' which makes printing a tree kinda impossible as it is now.</p>

<p>Is there any way (there probably is a trivial way, but my SQL skills are extremely poor) to get the resultant list in 'tree printing friendly' format?</p>

<p>I know I could just dump the results into a program and code the output, but as an exercise I would prefer doing this in SQL itself.</p>

<p>Thanks</p>
","<p>Edited after comment.  You could add the path to a node, and order on that:</p>

<pre><code>declare @t table (id int, parent int)
insert @t (id, parent) values (1, null), (2,1), (3,2), (4,3), (5,null), (6,5)

; with cte as (
    select  id, parent
    ,       cast(RIGHT(REPLICATE('0',12) + 
                 CONVERT(varchar(12),id),12) as varchar(max)) Path
    from    @t
    where   parent is null
    union all
    select  child.id, child.parent
    ,       parent.Path + RIGHT(REPLICATE('0',12) + 
                                CONVERT(varchar(12),child.id),12) as Path
    from    @t child
    join    cte parent
    on      parent.id = child.parent
)
select  *
from    cte
order by
        Path
</code></pre>

<p>This prints the root first, followed by leaves in order.  If your id can be larger than 12 digits, increase the number in the <code>char(x)</code> casts.</p>
"
"<p>We're using the Sql Server 2012 SSDT which removed the deploy option in Visual Studio for the database projects (now sql projects). We'd like to automate the Publish step as we had for deploy, but it's not clear how to do this. so thA couple of questions:</p>

<ol>
<li><p>I've added the .publish.xml to the project (after the first manual publish, checking add to project). Even after that, and setting it to the default, when I double click it, it builds, but always pops up settings window, where I need to click the ""Publish"" button to continue. Is there a setting that would skip this prompt and use the current values?</p></li>
<li><p>It seems that each publish generates a version of the sql output. How can I suppress this- i.e. overwrite the base file each time?</p></li>
<li><p>And lastly, any pointers for updating the build to use the new project type and publish command for the automated builds would be appreciated.</p></li>
</ol>
","<p><strong>How to restore the Deploy option:</strong> <em>(Visual Studio 2010/2012 only -- this is no longer supported in Visual Studio 2013)</em></p>

<p>The Deploy option is still present but for some reason it's not available in the menus. (Cursed Visual Studio team!) I've worked around this by adding the Deploy option to one of the toolbars as follows:</p>

<ol>
<li>Click the arrow on the right-hand side of a toolbar.</li>
<li>Click ""Add or Remove Buttons"", then Customize.</li>
<li>In the Customize dialog click Add Command.</li>
<li>Select the ""Build"" category, then select the ""Deploy Selection"" command.</li>
<li>After saving your selection the ""Deploy [project name]"" option will appear on the toolbar. <em>You'll need to select your project in Solution Explorer for the button to become enabled.</em></li>
</ol>

<p>Note that the deployment settings are different than the publish settings. The deployment settings are configured in the project's properties on the Debug tab.</p>

<hr>

<p>To answer your questions about the Publish option:</p>

<p><strong>1) How to use a specific publish file by default and avoid the annoying prompt</strong></p>

<p>I don't think there's a way around this.</p>

<p><strong>2) How to publish the entire database, not just the changes</strong></p>

<p>Open your .publish.xml file in a text editor and add <code>&lt;AlwaysCreateNewDatabase&gt;true&lt;/AlwaysCreateNewDatabase&gt;</code>.</p>

<p>For example:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;Project ToolsVersion=""4.0"" xmlns=""http://schemas.microsoft.com/developer/msbuild/2003""&gt;
  &lt;PropertyGroup&gt;
    &lt;TargetDatabaseName&gt;MyDatabase&lt;/TargetDatabaseName&gt;
    &lt;DeployScriptFileName&gt;MyDatabaseProject.sql&lt;/DeployScriptFileName&gt;
    &lt;TargetConnectionString&gt;Data Source=localhost\SQL2012;Integrated Security=True;Pooling=False&lt;/TargetConnectionString&gt;
    &lt;PublishDependentProjects&gt;False&lt;/PublishDependentProjects&gt;
    &lt;ProfileVersionNumber&gt;1&lt;/ProfileVersionNumber&gt;
    &lt;AlwaysCreateNewDatabase&gt;true&lt;/AlwaysCreateNewDatabase&gt;
  &lt;/PropertyGroup&gt;
&lt;/Project&gt;
</code></pre>

<p><strong>3) Command-line syntax for automated builds</strong></p>

<p>First build your project with msbuild as you normally would so that the .dacpac file is created in the bin.</p>

<p>Then use <code>sqlpackage.exe</code> to publish using your .publish.xml file:</p>

<p><code>C:\Program Files\Microsoft Visual Studio 10.0\Microsoft SQL Server Data Tools\sqlpackage.exe /Action:Publish /SourceFile:C:\[path to my project]\bin\Debug\MyDatabaseProject.dacpac /Profile:C:\[path to my project]\MyDatabaseProject.publish.xml</code></p>

<p>Note that the path to sqlpackage.exe may be different.</p>
"
"<p>So I'm trying to run a lambda on amazon and narrowed down the error finally by testing the lambda in amazons testing console.</p>

<p>The error I got is this.</p>

<pre><code>{
  ""errorMessage"": ""Please install mysql2 package manually"",
  ""errorType"": ""Error"",
  ""stackTrace"": [
    ""new MysqlDialect (/var/task/node_modules/sequelize/lib/dialects/mysql/index.js:14:30)"",
    ""new Sequelize (/var/task/node_modules/sequelize/lib/sequelize.js:234:20)"",
    ""Object.exports.getSequelizeConnection (/var/task/src/twilio/twilio.js:858:20)"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:679:25)"",
    ""__webpack_require__ (/var/task/src/twilio/twilio.js:20:30)"",
    ""/var/task/src/twilio/twilio.js:63:18"",
    ""Object.&lt;anonymous&gt; (/var/task/src/twilio/twilio.js:66:10)"",
    ""Module._compile (module.js:570:32)"",
    ""Object.Module._extensions..js (module.js:579:10)"",
    ""Module.load (module.js:487:32)"",
    ""tryModuleLoad (module.js:446:12)"",
    ""Function.Module._load (module.js:438:3)"",
    ""Module.require (module.js:497:17)"",
    ""require (internal/module.js:20:19)""
  ]
}
</code></pre>

<p>Easy enough, so I have to install mysql2.  So I added it to my package.json file.</p>

<pre><code>{
  ""name"": ""test-api"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""handler.js"",
  ""scripts"": {
    ""test"": ""echo \""Error: no test specified\"" &amp;&amp; exit 0""
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""devDependencies"": {
    ""aws-sdk"": ""^2.153.0"",
    ""babel-core"": ""^6.26.0"",
    ""babel-loader"": ""^7.1.2"",
    ""babel-plugin-transform-runtime"": ""^6.23.0"",
    ""babel-preset-es2015"": ""^6.24.1"",
    ""babel-preset-stage-3"": ""^6.24.1"",
    ""serverless-domain-manager"": ""^1.1.20"",
    ""serverless-dynamodb-autoscaling"": ""^0.6.2"",
    ""serverless-webpack"": ""^4.0.0"",
    ""webpack"": ""^3.8.1"",
    ""webpack-node-externals"": ""^1.6.0""
  },
  ""dependencies"": {
    ""babel-runtime"": ""^6.26.0"",
    ""mailgun-js"": ""^0.13.1"",
    ""minimist"": ""^1.2.0"",
    ""mysql"": ""^2.15.0"",
    ""mysql2"": ""^1.5.1"",
    ""qs"": ""^6.5.1"",
    ""sequelize"": ""^4.31.2"",
    ""serverless"": ""^1.26.0"",
    ""serverless-plugin-scripts"": ""^1.0.2"",
    ""twilio"": ""^3.10.0"",
    ""uuid"": ""^3.1.0""
  }
}
</code></pre>

<p>I noticed when I do sls deploy however, it seems to only be packaging some of the modules?</p>

<pre><code>Serverless: Package lock found - Using locked versions
Serverless: Packing external modules: babel-runtime@^6.26.0, twilio@^3.10.0, qs@^6.5.1, mailgun-js@^0.13.1, sequelize@^4.31.2, minimi
st@^1.2.0, uuid@^3.1.0
Serverless: Packaging service...
Serverless: Uploading CloudFormation file to S3...
Serverless: Uploading artifacts...
Serverless: Validating template...
Serverless: Updating Stack...

Serverless: Checking Stack update progress...
................................
Serverless: Stack update finished...
</code></pre>

<p>I think this is why it's not working.  <strong>In short, how do I get mysql2 library to be packaged correctly with serverless so my lambda function will work with the sequelize library?</strong></p>

<p>Please note that <strong>when I test locally my code works fine.</strong></p>

<p>My serverless file is below</p>

<pre><code>service: testapi

# Use serverless-webpack plugin to transpile ES6/ES7
plugins:
  - serverless-webpack
  - serverless-plugin-scripts
  # - serverless-domain-manager

custom:
  #Define the Stage or default to Staging.
  stage: ${opt:stage, self:provider.stage}
  webpackIncludeModules: true
  #Define Databases Here
  databaseName: ""${self:service}-${self:custom.stage}""
  #Define Bucket Names Here
  uploadBucket: ""${self:service}-uploads-${self:custom.stage}""
  #Custom Script setup
  scripts:
    hooks:
      #Script below will run schema changes to the database as neccesary and update according to stage.
      'deploy:finalize':  node database-schema-update.js --stage ${self:custom.stage}
  #Domain Setup
  # customDomain:
  #    basePath: ""/""
  #    domainName: ""api-${self:custom.stage}.test.com""
  #    stage: ""${self:custom.stage}""
  #    certificateName: ""*.test.com""
  #    createRoute53Record: true

provider:
  name: aws
  runtime: nodejs6.10
  stage: staging
  region: us-east-1
  environment:
    DOMAIN_NAME: ""api-${self:custom.stage}.test.com""
    DATABASE_NAME: ${self:custom.databaseName}
    DATABASE_USERNAME: ${env:RDS_USERNAME}
    DATABASE_PASSWORD: ${env:RDS_PASSWORD}
    UPLOAD_BUCKET: ${self:custom.uploadBucket}
    TWILIO_ACCOUNT_SID: """"
    TWILIO_AUTH_TOKEN: """"
    USER_POOL_ID: """"
    APP_CLIENT_ID: """"
    REGION: ""us-east-1""
    IDENTITY_POOL_ID: """"
    RACKSPACE_API_KEY: """"
  #Below controls permissions for lambda functions.
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:DescribeTable
        - dynamodb:UpdateTable
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
      Resource: ""arn:aws:dynamodb:us-east-1:*:*""

functions:
  create_visit:
    handler: src/visits/create.main
    events:
      - http:
          path: visits
          method: post
          cors: true
          authorizer: aws_iam
  get_visit:
    handler: src/visits/get.main
    events:
      - http:
          path: visits/{id}
          method: get
          cors: true
          authorizer: aws_iam
  list_visit:
    handler: src/visits/list.main
    events:
      - http:
          path: visits
          method: get
          cors: true
          authorizer: aws_iam
  update_visit:
    handler: src/visits/update.main
    events:
      - http:
          path: visits/{id}
          method: put
          cors: true
          authorizer: aws_iam
  delete_visit:
    handler: src/visits/delete.main
    events:
      - http:
          path: visits/{id}
          method: delete
          cors: true
          authorizer: aws_iam
  twilio_send_text_message:
    handler: src/twilio/twilio.send_text_message
    events:
      - http:
          path: twilio/sendtextmessage
          method: post
          cors: true
          authorizer: aws_iam
  #This function handles incoming calls and where to route it to.
  twilio_incoming_call:
    handler: src/twilio/twilio.incoming_calls
    events:
      - http:
          path: twilio/calls
          method: post
  twilio_failure:
    handler: src/twilio/twilio.twilio_failure
    events:
      - http:
          path: twilio/failure
          method: post
  twilio_statuschange:
    handler: src/twilio/twilio.statuschange
    events:
      - http:
          path: twilio/statuschange
          method: post
  twilio_incoming_message:
    handler: src/twilio/twilio.incoming_message
    events:
      - http:
          path: twilio/messages
          method: post
  twilio_whisper:
    handler: src/twilio/twilio.whisper
    events:
      - http:
          path: twilio/whisper
          method: post
      - http:
          path: twilio/whisper
          method: get
  twilio_start_call:
    handler: src/twilio/twilio.start_call
    events:
      - http:
          path: twilio/startcall
          method: post
      - http:
          path: twilio/startcall
          method: get

resources:
  Resources:
    uploadBucket:
       Type: AWS::S3::Bucket
       Properties:
         BucketName: ${self:custom.uploadBucket}
    RDSDatabase:
      Type: AWS::RDS::DBInstance
      Properties:
        Engine : mysql
        MasterUsername: ${env:RDS_USERNAME}
        MasterUserPassword: ${env:RDS_PASSWORD}
        DBInstanceClass : db.t2.micro
        AllocatedStorage: '5'
        PubliclyAccessible: true
        #TODO: The Value of Stage is also available as a TAG automatically which I may use to replace this manually being put here..
        Tags:
          -
            Key: ""Name""
            Value: ${self:custom.databaseName}
      DeletionPolicy: Snapshot
    DNSRecordSet:
      Type: AWS::Route53::RecordSet
      Properties:
        HostedZoneName: test.com.
        Name: database-${self:custom.stage}.test.com
        Type: CNAME
        TTL: '300'
        ResourceRecords:
        - {""Fn::GetAtt"": [""RDSDatabase"",""Endpoint.Address""]}
      DependsOn: RDSDatabase
</code></pre>

<p>UPDATE:: So I confirmed that running sls package --stage dev seems to create this in the zip folder that would eventually upload to AWS.  <strong>This confirms that serverless is not creating the package correctly with the mysql2 reference</strong> for some reason? <strong>Why is this?</strong></p>

<p><a href=""https://i.stack.imgur.com/mVNYs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mVNYs.png"" alt=""enter image description here""></a></p>

<p><strong>webpack config file as requested</strong></p>

<pre><code>const slsw = require(""serverless-webpack"");
const nodeExternals = require(""webpack-node-externals"");

module.exports = {
  entry: slsw.lib.entries,
  target: ""node"",
  // Since 'aws-sdk' is not compatible with webpack,
  // we exclude all node dependencies
  externals: [nodeExternals()],
  // Run babel on all .js files and skip those in node_modules
  module: {
    rules: [
      {
        test: /\.js$/,
        loader: ""babel-loader"",
        include: __dirname,
        exclude: /node_modules/
      }
    ]
  }
};
</code></pre>
","<p>Turns out disabling minification in Webpack fixed the issue.</p>

<p>This can be done by updating the <code>webpack.config.js</code> to include </p>

<pre><code>optimization: {
    // We no not want to minimize our code.
    minimize: false
},
</code></pre>
"
"<p>Do most people use .NET's SqlMembershipProvider, SqlRoleProvider, and SqlProfileProvider when developing a site with membership capabilities?</p>

<p>Or do many people make their own providers, or even their own membership systems entirely?</p>

<p>What are the limitations of the SQL providers that would make you roll your own?</p>

<p>Is it easy to extend the SQL providers to provide additional functionality?</p>

<p><strong>For Reference</strong><br>
<a href=""http://weblogs.asp.net/scottgu/archive/2006/04/13/442772.aspx"" rel=""nofollow noreferrer"">Per Scott Gu's Blog</a>, <a href=""http://download.microsoft.com/download/a/b/3/ab3c284b-dc9a-473d-b7e3-33bacfcc8e98/ProviderToolkitSamples.msi"" rel=""nofollow noreferrer"">Microsoft provides the source code for the SqlMembershipProvider</a> so that you can customize it, instead of starting from scratch.  Just an FYI.</p>
","<p><em><a href=""http://www.urbandictionary.com/define.php?term=blergh"" rel=""nofollow noreferrer"">blergh</a></em></p>

<p>Googling with the tags Stack Overflow provided I came across this site:
<a href=""http://www.lhotka.net/weblog/CallingRolesGetRolesForUserInAWCFService.aspx"" rel=""nofollow noreferrer"">http://www.lhotka.net/weblog/CallingRolesGetRolesForUserInAWCFService.aspx</a></p>

<p>In short: 
apparently something broke between .net 3.5 and .net 4. </p>

<p>To solve this issue call:</p>

<pre><code>string[] roles = Roles.Provider.GetRolesForUser(ServiceSecurityContext.Current.PrimaryIdentity.Name);
</code></pre>

<p>instead of </p>

<pre><code>string[] roles = Roles.GetRolesForUser(ServiceSecurityContext.Current.PrimaryIdentity.Name);
</code></pre>

<p>The difference is in the <code>.Provider</code> which is added in the middle. After adding this it worked fine.</p>
"
"<p>I'm trying to get multiple inserts to work in a single script using the SQL Compact 4.0 Toolbox and not having any luck.  I keep getting a parsing error.</p>

<p>I've even tried adding GO; between each statement like so..</p>

<pre><code>INSERT INTO ... ;

GO;

INSERT INTO ... ;
</code></pre>

<p>with no luck.. so am I out of luck? Do I have to just execute each statement one at a time?</p>
","<p>Update to the latest release version (2.3). There was a bug with multiple statement execution, that has been fixed in the latest release version.
Seperate each statement with GO on a separate line, no semicolon after...
Like the INSERT statements created by the tool:</p>

<pre><code>INSERT INTO [Shippers] ([Shipper ID],[Company Name]) VALUES (1,N'Speedy Express');
GO
INSERT INTO [Shippers] ([Shipper ID],[Company Name]) VALUES (2,N'United Package');
GO
INSERT INTO [Shippers] ([Shipper ID],[Company Name]) VALUES (3,N'Federal Shipping');
GO
</code></pre>
"
"<p>I am using SqlCommandProvider and I need to get some data for each id in an id collection</p>

<pre><code>let ids=[""B058A99-C4B2-4CC3-BA9F-034B1F1ECCBD"";""A09C01C-D51B-41C1-B44C-0995DD285088""]
[&lt;Literal&gt;]
let qLogo =""""""SELECT Id,LogoUrl FROM Hotels WHERE Id IN (@IDS)""""""
let queryLogo = new SqlCommandProvider&lt;qLogo,constring&gt;()
queryLogo .Execute(ids)//i need to be able to pass a collection here
</code></pre>

<p>`</p>
","<p>This is the kind of exception that you may get when you don't have correct <code>bindingRedirect</code> for <code>FSharp.Core.dll</code>. Check out <a href=""http://blog.ploeh.dk/2014/01/30/how-to-use-fsharpcore-430-when-all-you-have-is-431/"" rel=""nofollow"">this article by Mark Seemann</a>. In principle, I think that adding <code>app.config</code> with <code>bindingRedirect</code> to your application should solve the problem.</p>

<p>This typically happens when using a library that is compiled against older version of FSharp.Core in an application that uses newer version of FSharp.Core. The .NET runtime loads the new version of FSharp.Core but it does not know that types from the older version (like <code>FSharpFunc</code>) should be mapped to corresponding types in the new version - and so you get MethodMissing, because .NET thinks that <code>FSharpFunc</code> is a different type than the loaded one. (Though things get a bit more complicated with type providers.)</p>
"
"<p>I have a new project that needs SQL Server unit test, and CI/CD with VSTS.</p>

<p>Below is the features that are required</p>

<ul>
<li><p>SQL server unit test against stored procedure, initial target tables setup for each test and clean up</p></li>
<li><p>Unit test in sql</p></li>
<li><p>CI/CD with VSTS and Git</p></li>
<li><p>Easy setup and easy to use</p></li>
</ul>

<p>I looked into SSDT 2017, which seems good. But it seems it lacks a feature where common setup script can be shared easily between each test in Pre-Test step. It might lack other features that should be available for daily usage. But I might be wrong.</p>

<p>Which tool fits better for general sql server unit testing in 2017?</p>

<p><a href=""https://www.visualstudio.com/vs/ssdt/"" rel=""noreferrer"">SQL Server Data Tools for Visual Studio</a></p>

<p><a href=""http://tsqlt.org/"" rel=""noreferrer"">TSQLT</a></p>
","<p>One of the reasons why there aren't more unit testing solutions out there for SQL development is because proper unit testing is inherently harder with databases so people don't do it.  This is because databases maintain state and also referential integrity.  Imagine writing a unit test for a stored procedure (<code>order_detail_update_status</code>) that updates a status flag on an <code>order_detail</code> table.  The order_detail table has a dependency on the <code>order_header</code> and <code>product</code> tables, order_header in turn has foreign keys to <code>customer</code> and <code>employee</code> whilst the product table may depend on <code>product_category</code>, <code>product_type</code> and <code>supplier</code>.  That is at least seven tables (probably more) that need to be populated with valid data just to write one test and all but one of those tables have nothing to do with the code under test.</p>

<p>So what you should be looking for in a unit testing solution is exactly that - the ability to test discrete units of code - with the minimum of set-up.  So ideally, you would be able to just set up the required test data in <code>order_detail</code> and ignore the rest of the tables - I am aware of only one testing framework that allows you to do that.</p>

<p>Additionally, unit tests should have minimal reasons to fail, in the above example, <code>order_detail_update_status</code> just updates a single row on the order_detail table.  If a new not null column is added to the customer table, which is not handled by the test set-up then you have a scenario where our test could fail for a totally unrelated reason.  This makes for very brittle tests and, under the pressure of tight delivery deadlines, developers will quickly give up writing and maintaining tests.</p>

<p>A suite of unit tests should be runnable in any order, with no interdependencies and a good test framework should support this along with set-up, tear down and support for mocking objects (which may or may not be part of the same framework).  In the above scenario, the ability to mock the <code>order_detail</code> table to test a module that only touches the order_detail table is one of the most important features if you don't want to spend huge amounts of time fixing tests that are failing for no ""good"" reason.</p>

<p>So in terms of your requirements, and the above points, there is only one framework that I am aware of that does all of this - <a href=""http://tsqlt.org"" rel=""noreferrer"">tSQLt</a>.  This is based on real-world experience - we had over 6,000 tSQLt unit tests on my last project. It includes the following feautures:</p>

<ul>
<li>Unit test stored procedures, functions and views</li>
<li>Mock tables, views and functions</li>
<li>Mock (or spy) stored procedures - either for isolation,
replay or pre-defined outcomes</li>
<li>Suite set-up</li>
<li>Automatic tear-down (as every test runs in it's own translation)</li>
<li>Unit tests are completely isolated and can be executed in any order</li>
</ul>

<p>It works very well with VSTS in a CI/CD and, as all the unit tests are written in T-SQL, it is very easy to use.</p>

<p>The best way to use tSQLt in Visual Studio is to make use of composite projects - where application database objects and modules are maintained in one project whilst the tSQLt framework and all unit tests are part of a second project.  There is a good aticle to get you started on this <a href=""https://kzhendev.wordpress.com/2014/01/08/setting-up-ssdt-database-projects-and-tsqlt/"" rel=""noreferrer"">here</a>.</p>

<p>I wrote a more detailed article on the benefits of tSQLt for <a href=""https://www.red-gate.com/simple-talk/sql/t-sql-programming/test-driven-database-development-why-tsqlt/"" rel=""noreferrer"">Simple-Talk</a> a few year back which might also be helpful</p>
"
"<p>I have a result set I pull from a large database:</p>

<pre><code>$result = mysql_query($sql);
</code></pre>

<p>I loop through this recordset once to pull specific bits of data and get averages using <code>while($row = mysql_fetch_array($result))</code>. Later in the page, I want to loop through this same recordset again and output everything - but because I used the recordset earlier, my second loop returns nothing. </p>

<p>I finally hacked around this by looping through a second identical recordset (<code>$result2 = mysql_query($sql);</code>), but I hate to make the same SQL call twice. Any way I can loop through the same dataset multiple times?</p>
","<p>Use:</p>

<pre><code>mysql_data_seek($result, 0);
</code></pre>

<p>You get this ""free"", since it's already buffered.</p>

<p>As a separate note, you can explicitly do an unbuffered query with <a href=""http://www.php.net/manual/en/function.mysql-unbuffered-query.php"" rel=""noreferrer""><code>mysql_unbuffered_query</code></a>.</p>
"
"<p>I have several jobs and several packages. In SQL Server 2005 we used to use DTS Packages, but they are now defunct (I know that I can re-enable them, but that's not what I'm after). I receive the following error by running one of my packages:</p>

<blockquote>
  <p>Message: SSIS Warning Code
  DTS_W_MAXIMUMERRORCOUNTREACHED.  The
  Execution method succeeded, but the
  number of errors raised (1) reached
  the maximum allowed (1); resulting in
  failure. This occurs when the number
  of errors reaches the number specified
  in MaximumErrorCount. Change the
  MaximumErrorCount or fix the errors.</p>
</blockquote>

<p>Obviously, this is a result of an earlier error. I cannot fix that error, so I want to increase the <code>MaximumErrorCount</code>. But even though there are numerous posts on the internet explaining that you should select Properties on the package (or the job?), the package doesn't have <em>Properties</em> anymore (I found them under <em>Integration Services</em> where they're listed under <em>DTS Packages</em> which is odd, considering a wizard created the package and DTS is not supported by 2008?), I've no clue how to look into the package, really. And the job does have <em>Properties</em>, but nowhere is there a setting <em>Maximum error count</em>.</p>

<p>Can someone be the eyes for me and see what I cannot see? Where do I increase the MaximumErrorCount as requested by the error message?</p>
","<p>If I have open a package in BIDS (""Business Intelligence Development Studio"", the tool you use to design the packages), and do not select any item in it, I have a ""Properties"" pane in the bottom right containing - among others, the <code>MaximumErrorCount</code> property. If you do not see it, maybe it is minimized and you have to open it (have a look at tabs in the right).</p>

<p>If you cannot find it this way, try the menu: View/Properties Window.</p>

<p>Or try the F4 key.</p>
"
"<p>Is there a way in ABAP's OpenSQL to simplify the select columns in a <code>JOIN</code> when I want to grab all the fields of one table but only selected fields from the other table(s)?</p>

<p>For instance, in mysql <a href=""https://stackoverflow.com/a/14000583/1103571"">we can simply do</a>:</p>

<pre><code>SELECT  tb1.*, tb2.b, tb2.d
FROM       tableA tb1
INNER JOIN tableB tb2 ON tb1.x = tb2.a
</code></pre>

<p>However, OpenSQL does not seem to allow selecting <code>tb1~*, tb2~b, tb2~d</code> so I have to resort to this:</p>

<pre><code>SELECT  tb1.x, tb1.y, tb1.z, tb2.b, tb2.d
FROM       tableA tb1
INNER JOIN tableB tb2 ON tb1.x = tb2.a
</code></pre>

<p>For very large tables, especially standard tables, this becomes unwieldy, difficult to read and more annoying to maintain. </p>

<p><strong>Is there a better way to select all fields of tb1 and some fields from tb2?</strong></p>
","<p>The system variable SY-DBCNT should give you the number of rows selected, but only after the select ends.</p>

<p>The alternative to SELECT-ENDSELECT is to select all the rows at once with SELECT INTO TABLE into an internal table (provided you are not selecting too much at once!).</p>

<p>For example: </p>

<pre><code>data: lt_t000 type table of t000.

select * from t000 into table lt_t000.
</code></pre>

<p>This will select everything from that table in one go into the internal table. So what you could do is to declare an internal table with all the fields currently in your INTO clause and then specify INTO TABLE for your internal table.</p>

<p>After the SELECT executes, SY-DBCNT will contain the number of selected rows.</p>

<p>Here is a complete example, built around the SELECT statement in your question, which I have not checked for sanity, so I hope it works!</p>

<pre><code>tables: spfli.

select-options: s_carrid for spfli-carrid.

* Definition of the line/structure
data: begin of ls_dat,
        carrid type s_carr_id,
        carrname type s_carrname,
        planetype type s_planetye,
        fldate type s_date,
        price type s_price,
        cityfrom type s_from_cit,
        cityto type s_to_city,
      end of ls_dat.
* Definition of the table:
data: lt_dat like table of ls_dat.

* Select data
select spfli~carrid scarr~carrname sflight~planetype sflight~fldate sflight~price spfli~cityfrom spfli~cityto
  into table lt_dat
  from spfli
  inner join sflight
  on spfli~carrid = sflight~carrid and spfli~connid = sflight~connid
  inner join scarr
  on scarr~carrid = spfli~carrid
  where spfli~carrid = s_carrid-low.

* Output data
write: 'Total records selected', sy-dbcnt.
loop at lt_dat into ls_dat.
  write: / ls_dat-carrid, ls_dat-carrname, ls_dat-planetype, ls_dat-fldate, ls_dat-price, ls_dat-cityfrom, ls_dat-cityto.
endloop.
</code></pre>

<p>Note: Report (type 1) programs still support the notion of declaring internal tables with header lines for backward compatibility, but this is not encouraged! Hope it works!</p>
"
"<p><strong>Sql:</strong></p>

<pre><code>SELECT date,total_usage_T1 as TotalUsageValue,'T1' as UsageType FROM TblSayacOkumalari
UNION ALL
SELECT date,total_usage_T2 as TotalUsageValue,'T2' as UsageType FROM TblSayacOkumalari
</code></pre>

<p>And I try to do to convert it to linq</p>

<pre><code>IEnumerable&lt;TblSayacOkumalari&gt; sayac_okumalari = entity.TblSayacOkumalari
.Select(x =&gt; new 
    { x.date, x.total_usage_T1 })
.Union(entity.TblSayacOkumalari.Select(x =&gt; new 
    { x.date, x.total_usage_T2 }));
</code></pre>

<p>But I dont know how to convert <code>'T1' as UsageType</code> to linq. Also my union using is incorrect too.</p>

<p>My table fields like this:</p>

<pre><code>| date | total_usage_T1 | total_usage_T2 |

| 2010 |             30 |             40 |
| 2011 |             40 |             45 |
| 2012 |             35 |             50 |
</code></pre>

<p>I want like this</p>

<pre><code>| date | TotalUsageValue | UsageType     |

| 2010 |             30 |             T1 |
| 2011 |             40 |             T1 |
| 2012 |             35 |             T1 |
| 2010 |             40 |             T2 |
| 2011 |             45 |             T2 |
| 2012 |             50 |             T2 |
</code></pre>

<p>I tried very hard, but could not. Please help.</p>
","<p><strong>EDIT</strong></p>

<pre><code>Def. from MSDN
Enumerable.Concat  - Concatenates two sequences.
Enumerable.Union    - Produces the set union of two sequences by using the default equality comparer.
</code></pre>

<p>My post : <a href=""http://pranayamr.blogspot.ca/2012/06/concat-vs-union.html"" rel=""noreferrer""><strong>Concat() vs Union()</strong></a> </p>

<pre><code>    IEnumerable&lt;TblSayacOkumalari&gt; sayac_okumalari = 
   entity.TblSayacOkumalari
     .Select(x =&gt; new
          {     
                date= x.date, 
                TotalUsageValue = x.total_usage_T1,
                UsageType     = ""T1"" 
           })
     .Concat(entity.TblSayacOkumalari
      .Select(x =&gt; new
          { 
                date= x.date,
                TotalUsageValue =  x.total_usage_T2, 
                UsageType     = ""T2"" }
   )); 
</code></pre>

<p>for usage type you juse need to add <code>UsageType = ""T2""</code> in your new anonymous type as i did above this will do the task for you</p>

<hr>

<p>Than you should go for Concat method rather than Union method ..</p>

<p>Example </p>

<pre><code> int[] ints1 = { 1, 2, 3 }; int[] ints2 = { 3, 4, 5 };
 IEnumerable&lt;INT&gt; union = ints1.Union(ints2);
 Console.WriteLine(""Union"");
 foreach (int num in union)
 {
    Console.Write(""{0} "", num);
 }
 Console.WriteLine();
 IEnumerable&lt;INT&gt; concat = ints1.Concat(ints2);
 Console.WriteLine(""Concat"");
 foreach (int num in concat)
 {
    Console.Write(""{0} "", num);
 } 
</code></pre>

<p>output </p>

<p><img src=""https://i.stack.imgur.com/XXGDk.jpg"" alt=""enter image description here""></p>

<p><strong>Fact about Union and Concat</strong></p>

<p>The output shows that Concat() method just combine two enumerable collection to single one but doesn't perform any operation/ process any element just return single enumerable collection with all element of two enumerable collections.</p>

<p>Union() method return the enumerable collection by eliminating the duplicate i.e just return single element if the same element exists in both enumerable collection on which union is performed.</p>

<p><strong><em>Important point to Note</em></strong></p>

<ul>
<li><p>By this fact we can say that Concat() is faster than Union() because it doesn't do any processing.  </p></li>
<li><p>But if after combining two collection using Concat() having single collection with too many number of duplicate element and if you want to perform further operation on that created collection takes longer time than collection created using Union() method, because Union() eliminate duplicate and create collection with less elements. </p></li>
</ul>
"
"<p>Hi I'm logging slow queries because we're having some performance 
issues and I have read about mysqldumpslow and thought that would be a good 
way to sort through the queries.</p>

<p>At the command prompt I type <code>mysqldumpslow</code> and I get this:</p>

<blockquote>
  <p>'mysqldumpslow' is not recognized as
  an internal or external command, 
  operable program or batch file.</p>
</blockquote>

<p>I'm using MySQL version 5.0.79 on Windows Vista</p>

<p>Note: 
c:\Program Files\MySQL\MySQL Server 5.0\bin is my path and
I have searched the drive for 'mysqldumpslow' and can not find it.</p>

<p>What am I doing wrong ?</p>

<p>Note:
MySql 5.0.x does support the mysqldumpslow command follow this <a href=""http://dev.mysql.com/doc/refman/5.0/en/mysqldumpslow.html"" rel=""noreferrer"">link</a> to manual</p>
","<p><em>edit : oops, I read the manual wrong, and gave wrong information :-( sorry :-( let's give it another try...</em></p>

<p>I've just installed MySQL on windows, to try using <code>mysqldumpslow</code>, and I don't have <code>mysqldumpslow</code> installed either :-( So, you are not alone, and it doesn't seem to be a problem with your install <em>(I've tried 5.1.x, but as you highlighted, it should be the same for 5.0.x)</em></p>

<p>Looking at the ""<code>mysqldumpslow</code>"" I have under Linux, it appears it is a Perl script ; and Perl is not often installed on a Windows machine. Maybe that would be a hint to a solution...</p>

<p>Well, after a bit more testing, when installing MySQL, it seems <strong>you have to select ""Developpers Components > Scripts, examples""</strong>, which is not installed by default (at least on windows) -- no need to reinstall everything : you can ""modify"" the installation, to add this option.</p>

<p>Then, you will have a ""script"" directory next to the ""bin"" one.
For instance, on your install, it should be something like ""c:\Program Files\MySQL\MySQL Server 5.0\scripts"".</p>

<p>In this directory, there are some scripts ; one of them is <strong>mysqldumpslow.pl</strong> ; which is what you are looking for ;-)</p>

<p>Now, you ""just"" have to get Perl installed and running on your machine (sorry, I've never installed Perl on windows ; but you can find some informations <a href=""http://www.perl.com/download.csp#win32"" rel=""noreferrer"">here</a>)</p>

<p>Hope this helps better than what I posted before !</p>
"
"<p>I have a 350MB table that's fairly wide with two varchar(2000) columns. Via an SSIS data flow it takes 60 minutes to load via OLEDB ""fast load"" destination to Azure SQL DW. I changed the destination on that data flow to be the Azure Blob Destination (from the <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=47366"">SSIS Azure feature pack</a>) and that same data flow completed in 1.5 minutes (and Polybase from that new flat file takes about 2 minutes).</p>

<p>For another source I have an existing 1GB flat file. SSIS data flow into an OLEDB destination in Azure SQL DW takes 90 minutes. Copy the file to blob storage and Polybase load takes 5 minutes.</p>

<p>SSIS is SSIS 2014 and it's running on an Azure VM in the same region as Azure SQL DW. I know that bulk load is much slower than Polybase since bulk load funnels through the control node but Polybase is parallelized on all compute nodes. But those bulk load numbers are extremely slow.</p>

<p>What are the optimal settings for the SSIS data flow and destination in order to load to an Azure SQL DW stage table as fast as possible via bulk load? Particularly I'm interested in the optimal value for the following settings in addition to any other settings I'm not considering:</p>

<ul>
<li>Stage table geometry = HEAP (is the fastest I believe)</li>
<li>Data flow settings:

<ul>
<li>DefaultBufferMaxRows = ?</li>
<li>DefaultBufferSize = ?</li>
</ul></li>
<li>OLEDB destination settings

<ul>
<li>Data access mode = Table or view - fast load</li>
<li>Keep Identity = unchecked</li>
<li>Keep Nulls = ?</li>
<li>Table Lock = ?</li>
<li>Check constraints = ?</li>
<li>Rows per batch = ?</li>
<li>Maximum insert commit size = ? </li>
</ul></li>
</ul>
","<p>I've experienced this. Your connection isn't actually recognised as a SQL DW connection. I bet your query window is a .sql file, not a .dsql as it needs to be.</p>

<p>Go back into the Azure portal and use the link to connect using SSDT from there. You should get a connection in the SQL Server Explorer pane which looks different, and when you start a New Query based on it, you should get a .dsql window, not a .sql one.</p>
"
"<p>I'm very excited by several of the more recently-added Postgres features, such as foreign data wrappers.  I'm not aware of any other RDBMS having this feature, but before I try to make the case to my main client that they should begin preferring Postgres over their current cocktail of RDBMSs, and include in my case that no other database can do this, I'd like to verify that.</p>

<p>I've been unable to find evidence of any other database supporting SQL/MED, and things like this short note stating that <a href=""http://docs.oracle.com/cd/E11882_01/server.112/e26088/ap_standard_sql007.htm#SQLRF55527"" rel=""noreferrer"">Oracle does not support SQL/MED</a>.</p>

<p>The main thing that gives me doubt is a statement on <a href=""http://wiki.postgresql.org/wiki/SQL/MED"" rel=""noreferrer"">http://wiki.postgresql.org/wiki/SQL/MED</a>:</p>

<blockquote>
  <p>SQL/MED is Management of External Data, a part of the SQL standard that deals with how a database management system can integrate data stored outside the database. </p>
</blockquote>

<p>If FDWs are based on SQL/MED, and SQL/MED is an open standard, then it seems likely that other RDBMSs have implemented it too.  </p>

<h1>TL;DR:</h1>

<p>Does any database besides Postgres support SQL/MED?</p>
","<ul>
<li><a href=""http://www.ibm.com/developerworks/data/library/techarticle/0203haas/0203haas.html#iso"" rel=""noreferrer"">IBM DB2</a> claims compliance with SQL/MED (including full FDW API);</li>
<li><a href=""https://dev.mysql.com/doc/refman/5.1/en/federated-storage-engine.html"" rel=""noreferrer"">MySQL</a>'s FEDERATED storage engine <a href=""https://dev.mysql.com/doc/refman/5.0/en/federated-limitations.html"" rel=""noreferrer"">can connect to another MySQL database, but <strong><em>NOT</em></strong> to other RDBMSs</a>;</li>
<li><a href=""https://mariadb.com/kb/en/introduction-to-the-connect-engine/"" rel=""noreferrer"">MariaDB</a>'s CONNECT engine allows access to various file formats (CSV, XML, Excel, etc), gives access to ""any"" ODBC data sources (Oracle, DB2, SQLServer, etc) and can access data on the storage engines MyIsam and InnoDB.</li>
<li><a href=""http://farrago.sourceforge.net/design/sqlmed.html"" rel=""noreferrer"">Farrago</a> has some of it too;</li>
<li><a href=""http://www.postgresql.org/docs/9.3/static/fdwhandler.html"" rel=""noreferrer"">PostgreSQL</a> implements parts of it (notably it does not implement routine mappings, and has a simplified FDW API). It is usable as readeable since PG 9.1 and writeable since 9.3, and prior to that there was the <a href=""http://pgfoundry.org/projects/dbi-link/"" rel=""noreferrer"">DBI-Link</a>.</li>
</ul>

<p>PostgreSQL communities have a plenty of nice <a href=""http://wiki.postgresql.org/wiki/Foreign_data_wrappers"" rel=""noreferrer"">FDW</a> like noSQL FDW (couchdb_fdw, mongo_fdw, redis_fdw), Multicorn (for using Python output instead of C for the wrapper per se), or the nuts <a href=""http://wiki.postgresql.org/wiki/PGStrom"" rel=""noreferrer"">PGStrom</a> (which uses GPU for some operations!)</p>
"
"<p>I have the following tables:</p>

<pre><code>Employees
-------------
ClockNo     int
CostCentre  varchar
Department  int
</code></pre>

<p>and</p>

<pre><code>Departments
-------------
DepartmentCode  int
CostCentreCode  varchar
Parent          int
</code></pre>

<p>Departments can have other departments as parents meaning there is infinite hierarchy. All departments belong to a cost centre and so will always have a <code>CostCentreCode</code>. If <code>parent = 0</code> it is a top level department</p>

<p>Employees <em>must</em> have a <code>CostCentre</code> value but may have a <code>Department</code> of 0 meaning they are not in a department</p>

<p><strong>What I want to try and generate is a query that will give the up to four levels of hierarchy. Like this:</strong></p>

<pre><code>EmployeesLevels
-----------------
ClockNo
CostCentre
DeptLevel1
DeptLevel2
DeptLevel3
DeptLevel4
</code></pre>

<p>I've managed to get something to display the department structure on it's own, but I can't work out how to link this to the employees without creating duplicate employee rows:</p>

<pre><code>SELECT d1.Description AS lev1, d2.Description as lev2, d3.Description as lev3, d4.Description as lev4
FROM departments AS d1
LEFT JOIN departments AS d2 ON d2.parent = d1.departmentcode
LEFT JOIN departments AS d3 ON d3.parent = d2.departmentcode
LEFT JOIN departments AS d4 ON d4.parent = d3.departmentcode
WHERE d1.parent=0;
</code></pre>

<hr>

<p>SQL To create Structure and some sample data:</p>

<pre><code>CREATE TABLE Employees(
ClockNo integer NOT NULL PRIMARY KEY,
CostCentre varchar(20) NOT NULL,
Department integer NOT NULL);

CREATE TABLE Departments(
DepartmentCode integer NOT NULL PRIMARY KEY,
CostCentreCode varchar(20) NOT NULL,
Parent integer NOT NULL
);

CREATE INDEX idx0 ON Employees (ClockNo);
CREATE INDEX idx1 ON Employees (CostCentre, ClockNo);
CREATE INDEX idx2 ON Employees (CostCentre);

CREATE INDEX idx0 ON Departments (DepartmentCode);
CREATE INDEX idx1 ON Departments (CostCentreCode, DepartmentCode);

INSERT INTO Employees VALUES (1, 'AAA', 0);
INSERT INTO Employees VALUES (2, 'AAA', 3);
INSERT INTO Employees VALUES (3, 'BBB', 0);
INSERT INTO Employees VALUES (4, 'BBB', 4);
INSERT INTO Employees VALUES (5, 'CCC', 0); 
INSERT INTO Employees VALUES (6, 'AAA', 1);
INSERT INTO Employees VALUES (7, 'AAA', 5);
INSERT INTO Employees VALUES (8, 'AAA', 15);

INSERT INTO Departments VALUES (1, 'AAA', 0);
INSERT INTO Departments VALUES (2, 'AAA', 1);
INSERT INTO Departments VALUES (3, 'AAA', 1);
INSERT INTO Departments VALUES (4, 'BBB', 0);
INSERT INTO Departments VALUES (5, 'AAA', 3);
INSERT INTO Departments VALUES (12, 'AAA', 5);
INSERT INTO Departments VALUES (15, 'AAA', 12);
</code></pre>

<p>This gives the following structure (employee clock numbers in square brackets):</p>

<pre><code>Root
  |
  |---AAA                   [1]
  |    \---1                [6]
  |       |---2     
  |       \---3             [2]
  |          \---5          [7]
  |             \---12
  |                \---15   [8]
  |
  |---BBB                   [3]
  |    \---4                [4]
  |
  \---CCC                   [5]
</code></pre>

<p><strong>The query should return the following:</strong></p>

<pre><code>ClockNo CostCentre Level1 Level2 Level3 Level4
1       AAA        
2       AAA        1      3
3       BBB
4       BBB        4
5       CCC
6       AAA        1
7       AAA        1      3       5
8       AAA        1      3       5      12  *
</code></pre>

<p><code>*</code> In the case of Employee 8, they are in level5. Ideally I would like to show all their levels down to level4, but I am happy just to show the CostCentre in this case</p>
","<p>Just use the ""SUBSTRING"" function :</p>

<pre><code>SELECT * FROM ""BOM_SUB_LEVEL"" where SUBSTRING(TOP_CODE, 11, 1) = ""_""
</code></pre>

<p>Marc</p>
"
"<p>In a SQL Server database, one can use table variables like this:</p>

<pre><code>declare @table as table (a int)
</code></pre>

<p>In an Azure Data Warehouse, that throws an error.  </p>

<blockquote>
  <p>Parse error at line: 1, column: 19: Incorrect syntax near 'table'</p>
</blockquote>

<p>In an Azure Data Warehouse, you can use temporary tables:</p>

<pre><code>create table #table (a int)
</code></pre>

<p>but not inside functions.  </p>

<blockquote>
  <p>Msg 2772, Level 16, State 1, Line 6 Cannot access temporary tables
  from within a function.</p>
</blockquote>

<p><a href=""https://docs.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/faster-temp-table-and-table-variable-by-using-memory-optimization"" rel=""nofollow noreferrer"">This document</a> from Microsoft says, </p>

<blockquote>
  <p>Must be declared in two steps (rather than inline): CREATE TYPE
  my_type AS TABLE ...; , then  DECLARE @mytablevariable my_type;.</p>
</blockquote>

<p>But when I try this:</p>

<pre><code>create type t as table (a int);
drop type t;
</code></pre>

<p>I get this : </p>

<blockquote>
  <p>Msg 103010, Level 16, State 1, Line 1 Parse error at line: 1, column:
  8: Incorrect syntax near 'type'.</p>
</blockquote>

<p>My objective is to have a function in an Azure Data Warehouse which uses a temporary table.  Is it achievable?</p>

<p><strong>Edit Start Here</strong></p>

<p>Note that I am not looking for other ways to create one specific function.  I have actually done that and moved on.  I'm a veteran programmer but an Azure Data Warehouse rookie.  I want to know if it's possible to incorporate some concept of temporary tables in an Azure Data Warehouse function.  </p>
","<p>Drop the External Table and the External File Format. Then recreate the External File Format with <code>FIRST_ROW=2</code> which will skip one row as mentioned in the <a href=""https://msdn.microsoft.com/library/dn935026.aspx"" rel=""noreferrer"">documentation</a>:</p>

<pre><code>CREATE EXTERNAL FILE FORMAT TextFileFormat
WITH
(   FORMAT_TYPE = DELIMITEDTEXT
,    FORMAT_OPTIONS    (   FIELD_TERMINATOR = '|'
                    ,    STRING_DELIMITER = ''
                    ,    DATE_FORMAT         = 'yyyy-MM-dd HH:mm:ss.fff'
                    ,    USE_TYPE_DEFAULT = FALSE
                    ,    FIRST_ROW = 2
                    )
);
</code></pre>
"
"<p>I have a table which has 3 fields, I want to rank column based on user_id and game_id.</p>

<p>Here is SQL Fiddle :
<a href=""http://sqlfiddle.com/#!9/883e9d/1"" rel=""nofollow noreferrer"">http://sqlfiddle.com/#!9/883e9d/1</a></p>

<p>the table already I have :</p>

<pre><code> user_id | game_id |   game_detial_sum  |
 --------|---------|--------------------|
 6       | 10      |  1000              |   
 6       | 11      |  260               |
 7       | 10      |  1200              |
 7       | 11      |  500               |
 7       | 12      |  360               |
 7       | 13      |  50                | 
</code></pre>

<p>expected output :</p>

<pre><code>user_id  | game_id |   game_detial_sum  |  user_game_rank  |
 --------|---------|--------------------|------------------|
 6       | 10      |  1000              |   1              |
 6       | 11      |  260               |   2              |
 7       | 10      |  1200              |   1              |
 7       | 11      |  500               |   2              |
 7       | 12      |  360               |   3              |
 7       | 13      |  50                |   4              |
</code></pre>

<p>My efforts so far : </p>

<pre><code>SET @s := 0; 
SELECT user_id,game_id,game_detail, 
       CASE WHEN user_id = user_id THEN (@s:=@s+1) 
            ELSE @s = 0 
       END As user_game_rank 
FROM game_logs
</code></pre>

<p><strong>Edit:</strong> (From OP <a href=""https://stackoverflow.com/questions/53465111/set-rank-based-on-multiple-columns/53465139#comment93801173_53465111"">Comments</a>): Ordering is based on the descending order of <code>game_detail</code></p>

<blockquote>
  <p>order of game_detail</p>
</blockquote>
","<p>In a <a href=""https://dev.mysql.com/doc/refman/8.0/en/derived-tables.html"" rel=""nofollow noreferrer"">Derived Table</a> (subquery inside the <code>FROM</code> clause), we order our data such that all the rows having same <code>user_id</code> values come together, with further sorting between them based on <code>game_detail</code> in Descending order.</p>

<p>Now, we use this result-set and use conditional <code>CASE..WHEN</code> expressions to evaluate the row numbering. It will be like a Looping technique (which we use in application code, eg: PHP). We would store the previous row values in the User-defined variables, and then check the current row's value(s) against the previous row. Eventually, we will assign row number accordingly.</p>

<p><strong>Edit:</strong> Based on MySQL <a href=""https://dev.mysql.com/doc/refman/8.0/en/user-variables.html"" rel=""nofollow noreferrer"">docs</a> and @Gordon Linoff's observation:</p>

<blockquote>
  <p>The order of evaluation for expressions involving user variables is
  undefined. For example, there is no guarantee that SELECT @a, @a:=@a+1
  evaluates @a first and then performs the assignment.</p>
</blockquote>

<p>We will need to evaluate row number and assign the <code>user_id</code> value to <code>@u</code> variable within the same expression.</p>

<pre><code>SET @r := 0, @u := 0; 
SELECT
  @r := CASE WHEN @u = dt.user_id 
                  THEN @r + 1
             WHEN @u := dt.user_id /* Notice := instead of = */
                  THEN 1 
        END AS user_game_rank, 
  dt.user_id, 
  dt.game_detail, 
  dt.game_id 

FROM 
( SELECT user_id, game_id, game_detail
  FROM game_logs 
  ORDER BY user_id, game_detail DESC 
) AS dt 
</code></pre>

<p><strong>Result</strong></p>

<pre><code>| user_game_rank | user_id | game_detail | game_id |
| -------------- | ------- | ----------- | ------- |
| 1              | 6       | 260         | 11      |
| 2              | 6       | 100         | 10      |
| 1              | 7       | 1200        | 10      |
| 2              | 7       | 500         | 11      |
| 3              | 7       | 260         | 12      |
| 4              | 7       | 50          | 13      |
</code></pre>

<p><a href=""https://www.db-fiddle.com/f/hoWE58V5RS4nRSdHsXoPrH/3"" rel=""nofollow noreferrer""><strong>View on DB Fiddle</strong></a></p>

<hr>

<p>An interesting note from MySQL <a href=""https://dev.mysql.com/doc/refman/8.0/en/user-variables.html"" rel=""nofollow noreferrer"">Docs</a>, which I discovered recently:</p>

<blockquote>
  <p>Previous releases of MySQL made it possible to assign a value to a
  user variable in statements other than SET. This functionality is
  supported in MySQL 8.0 for backward compatibility but is subject to
  removal in a future release of MySQL.</p>
</blockquote>

<p>Also, thanks to a fellow SO member, came across this blog by MySQL Team: <a href=""https://mysqlserverteam.com/row-numbering-ranking-how-to-use-less-user-variables-in-mysql-queries/"" rel=""nofollow noreferrer"">https://mysqlserverteam.com/row-numbering-ranking-how-to-use-less-user-variables-in-mysql-queries/</a></p>

<p>General observation is that using <code>ORDER BY</code> with evaluation of the user variables in the same query block, does not ensure that the values will be correct always. As, MySQL optimizer <em>may</em> come into place and change our <em>presumed</em> order of evaluation.</p>

<p>Best approach to this problem would be to upgrade to MySQL 8+ and utilize the <a href=""https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number"" rel=""nofollow noreferrer""><code>Row_Number()</code></a> functionality:</p>

<p><strong>Schema (MySQL v8.0)</strong></p>

<pre><code>SELECT user_id, 
       game_id, 
       game_detail, 
       ROW_NUMBER() OVER (PARTITION BY user_id 
                          ORDER BY game_detail DESC) AS user_game_rank 
FROM game_logs 
ORDER BY user_id, user_game_rank;
</code></pre>

<p><strong>Result</strong></p>

<pre><code>| user_id | game_id | game_detail | user_game_rank |
| ------- | ------- | ----------- | -------------- |
| 6       | 11      | 260         | 1              |
| 6       | 10      | 100         | 2              |
| 7       | 10      | 1200        | 1              |
| 7       | 11      | 500         | 2              |
| 7       | 12      | 260         | 3              |
| 7       | 13      | 50          | 4              |
</code></pre>

<p><a href=""https://www.db-fiddle.com/f/hoWE58V5RS4nRSdHsXoPrH/1"" rel=""nofollow noreferrer""><strong>View on DB Fiddle</strong></a></p>
"
"<p>Created stream with the following field </p>

<pre><code>CREATE STREAM pageviews_original_string(view_time string, user_id varchar, pageid varchar) WITH (kafka_topic='pageviews',value_format='DELIMITED',KEY='pageid');
</code></pre>

<p>Changed the pageid into the uppercase along with following values. </p>

<pre><code>create stream up_case AS SELECT UCASE(pageid), user_id FROM PAGEVIEWS_ORIGINAL_STRING where user_id = 'User_9';
</code></pre>

<p>outcome</p>

<pre><code>PAGE_26 | User_9
PAGE_67 | User_9
PAGE_39 | User_9
PAGE_80 | User_9
PAGE_40 | User_9
PAGE_92 | User_9
</code></pre>

<p>Now what i want is the condition satisfied data has to be modified and extracted along with remaining field values
something like this </p>

<pre><code>****PAGE_26 | User_9
PAGE_67 | User_9
PAGE_39 | User_9
PAGE_80 | User_9
PAGE_40 | User_9
PAGE_92 | User_9****
Page_66 | User_7
Page_25 | User_2
Page_41 | User_3
Page_34 | User_1
Page_28 | User_2
Page_55 | User_5
Page_77 | User_5
Page_32 | User_8
Page_60 | User_4
</code></pre>

<p>can you please help me in solving this use case</p>
","<p>You cannot have both counts for the all and count for each key in the same query. You can have two queries here, one for counting each value in the given column and another for counting all values in the given column.
Let's assume you have a stream with two columns, col1 and col2.
To count each value in col1 with infinite window size you can use the following query:</p>

<pre><code>SELECT col1, count(*) FROM mystream1 GROUP BY col1;
</code></pre>

<p>To count all the rows you need to write two queries since KSQL always needs GROUP BY clause for aggregation. First you create a new column with constant value and then you can count the values in new column and since it is a constant, the count will represent the count of all rows. Here is an example:
</p>

<pre><code>CREATE STREAM mystream2 AS SELECT 1 AS col3 FROM mystream1;
SELECT col3, count(*) FROM mystream2 GROUP BY col3;
</code></pre>
"
"<pre><code>Fabric : 10.101.90.5
Master : 10.101.90.6
Slave: 10.101.90.7
</code></pre>

<p>I add the slave to the group and I get the below error:<br /></p>

<pre><code>mysqlfabric group add mygroup 10.101.90.7
Password for admin: 
Fabric UUID: 5calable-a007-feed-food-cab3fe13249e
Time-to-live: 1
ServerError: Error accessing server(10.101.90.7): 
2003: Cannot connect to MySQL server on '10.101.90.7:3306'(113 No route to host)
</code></pre>

<p>Does anyone know why this has happened? How can I solve it?</p>
","<p>Just noticed that the non-fabric driver was also in the jar, setting ENVIRONMENT.DRIVER to this and some modifications to Environment.URL fixed it. </p>

<pre><code> Configuration cfg = new Configuration()
                .setProperty(Environment.URL,""jdbc:mysql://localhost/songkong"")
                .setProperty(Environment.DRIVER,""com.mysql.jdbc.Driver"")
                .setProperty(Environment.DIALECT, ""org.hibernate.dialect.MySQL5Dialect"")
                .setProperty(""hibernate.show_sql"", ""true"")
                .setProperty(""hibernate.connection.username"",""dbuser"")
                .setProperty(""hibernate.connection.password"", ""dbpassword"");;
</code></pre>
"
"<p>I have a database including some tables, when I want to delete data from tables which includes an ""Auto Increment"" field, using this query: </p>

<pre><code>delete from test.table1 ;
</code></pre>

<p>I got this error:</p>

<pre><code> Error Code: 1030Got error -1 from storage engine
</code></pre>

<p>Why this happens? What should I do?</p>
","<p>Try to change <code>innodb_force_recovery</code> value (in your <code>/etc/my.cnf</code>). </p>

<p><strong>Error -1</strong> says NOTHING. Without your tables creation code (<code>SHOW CREATE TABLE table_name</code>) can not say where exactly problem is.</p>
"
"<p>I am trying to update the <code>Time_Stamp</code> field in my table, <code>simple_pack_data</code>, to match the values in the similarly titled field in my <code>temp_data</code> table. The tables each have fields called <code>Test_Number</code> and <code>Time_Marker</code>, which I'm using to <code>INNER JOIN</code> the tables. <code>Time_Marker</code> is like a reading count, where <code>Time_Stamp</code> is an actual time from the start of the test. </p>

<p>I want to update the <code>Time_Stamp</code> one test at a time, so the code I have been trying is: </p>

<pre><code>UPDATE simple_pack_data s
INNER JOIN (
    SELECT *
    FROM temp_data t
    WHERE t.Test = ""3""
    ) AS tmp
ON s.Test_Number = tmp.Test_Number AND s.Time_Marker = tmp.Time_Marker
SET s.Time_Stamp = tmp.Time_Stamp
WHERE s.Test_Number = ""3"";
</code></pre>

<p>When I run this it takes over 50 seconds and I get the 1205 error. If I run a similarly structured select statement:</p>

<pre><code>SELECT *
FROM simple_pack_data s
INNER JOIN (
    SELECT *
    FROM temp_data t
    WHERE t.Test = ""3""
    ) AS tmp
ON s.Test_Number = tmp.Test AND s.Time_Marker = tmp.Time_Marker
WHERE s.Test_Number = ""3"";
</code></pre>

<p>It takes much less than a second and I know join is working fine. Is the update really taking that long? If so, is there any way to change the timeout value so it can get through it?</p>
","<p>This error is entirely MySQL not doing as it should. The best solution is to get off of MySQL, but lacking that ability, this <a href=""http://mysqlperformanceblog.com/2012/03/27/innodbs-gap-locks"" rel=""nofollow"">performance blog post</a> has helped me get around this in the past.</p>

<p>MySQL has a lot of these little gotchas. It's like working in Access, half the time the program is going to do the wrong thing and not raise an error.</p>
"
"<p>We are trying to use FOR JSON Path in SQL Server 2016 for forming a Nested Array from a SQL Query.</p>

<p>SQL Query:</p>

<pre><code>SELECT A, 
B.name as [child.name],
B.date as [child.date]
 from Table 1 join Table 2 on Table 1.ID=Table 2.ID FOR JSON PATH
</code></pre>

<p>Desired Output:</p>

<pre><code>[{
A:""text"",
   ""child:""[
         {""name"":""value"", ""date"":""value""},
         {""name"":""value"", ""date"":""value""}

       ]
}]
</code></pre>

<p>However what we are getting is:</p>

<pre><code> [{
    A:""text"",
    ""child:"" {""name"":""value"", ""date"":""value""}
  },
{
   A:""text"",
  ""child"":{""name"":""value"", ""date"":""value""}
}]
</code></pre>

<p>How can we use FOR JSON PATH to form nested child array.</p>
","<p>With SQL Server 2016, it can be done using the built-in functions to manipulate JSON data. The following function will return the modified JSON data:</p>

<pre><code>JSON_MODIFY(JsonColumn, '$.Info2', 'Value2')
</code></pre>

<p>The expression can be used in normal <code>UPDATE</code> statement:</p>

<pre><code>UPDATE Table1
SET JsonColumn = JSON_MODIFY(JsonColumn, '$.Info2', 'Value2')
</code></pre>

<p>The <code>NULL</code> values in <code>JsonColumn</code> will be updated to  <code>{ ""Info2"":""Value2""}</code>.</p>

<p>If <code>JsonColumn</code> contains another value for <code>Info2</code> key, it will be overwritten.</p>
"
"<p>I feel like a dork just for asking this, but I'm not getting any help from Google, and I paged through all of SO's results on a simple search for SMO and didn't see it either.</p>

<p>The short version is that I'm starting to play around with T4. I'm expanding on <a href=""http://www.olegsych.com/2008/09/t4-tutorial-creatating-your-first-code-generator/"" rel=""noreferrer"">Oleg Sych's initial tutorial</a> to provide enumeration over all tables to create a (IMHO rather silly) delete proc. This is just an experiment, so its utter uselessness doesn't bother me. :)</p>

<p>My expansion to Oleg's tutorial looks like this:</p>

<pre><code>&lt;#@ template language=""C#"" hostspecific=""true"" #&gt;
&lt;#@ output extension=""SQL"" #&gt;
&lt;#@ assembly name=""Microsoft.SqlServer.ConnectionInfo"" #&gt;
&lt;#@ assembly name=""Microsoft.SqlServer.Smo"" #&gt;
&lt;#@ import namespace=""Microsoft.SqlServer.Management.Smo"" #&gt;
&lt;#@ include file=""T4Toolbox.tt"" #&gt;
&lt;#
    // Config variables
    string serverName = ""dbserver\\dbinstance"";
    string dbName = ""dbname"";
#&gt;
USE &lt;#= dbName #&gt;
&lt;#  
    // Iterate over tables and generate procs
    Server server = new Server(serverName);
    Database database = new Database(server, dbName);

    WriteLine(""/* Number of tables: "" + database.Tables.Count.ToString() + "" */"");

    foreach (Table table in database.Tables)
    {
        table.Refresh();
#&gt;
CREATE PROCEDURE &lt;#= table.Name #&gt;_Delete
&lt;#
        PushIndent(""    "");
        foreach (Column column in table.Columns)
        {
            if (column.InPrimaryKey)
                WriteLine(""@"" + column.Name + "" "" + column.DataType.Name);
        }
        PopIndent();
#&gt;
AS
    DELETE FROM 
        &lt;#= table.Name #&gt;
    WHERE
&lt;#
        PushIndent(""        "");
        foreach (Column column in table.Columns)
        {
            if (column.InPrimaryKey)
                WriteLine(column.Name + "" = @"" + column.Name);
        }
        PopIndent();
        WriteLine(""GO"");
    }
#&gt; 
</code></pre>

<p>The issue is that no tables are returned from the <code>Tables</code> collection. This is validated by the table count SQL comment I'm generating, which outputs <code>0</code>. </p>

<p>As written, the code above generates the following:</p>

<pre><code>USE dbname
/* Number of tables: 0 */
</code></pre>

<p>However, if I remove the for loop and manually supply a valid table name which exists in this database, it generates the (again silly) proc -- for that table.</p>

<p>The tables are separated into a schema, would that matter? Also, this is going against a SQL2005 instance -- would that potentially cause issues?</p>

<p>Finally, I'm also finding that I can't enumerate synonyms via the Synonyms collection. (I thought I'd be clever and go that route since the tables are in a schema, but have synonyms defined. But ... no dice.)</p>

<p>Again, to reiterate, the above code is naturally not production, nor even production worthy. I'm just trying to learn both T4 and SMO, and hit a roadblock trying to do something which I'd thought would be ridiculously simple. :) </p>
","<p>SMO does not retrieve metadata automatically if you simply create a new instance of the Database class. Retrieving metadata can take a while, especially in a cold environment. Call database.Refresh() before the loop. </p>
"
"<p>Can I ask what is the difference between <code>xp_sendmail</code> and <code>sp_send_dbmail</code> proc? They are both send e-mail message, which may include a query result set attachment, to the specified recipients.....</p>

<p>What is the difference? </p>
","<p><code>xp_sendmail</code> requires a MAPI client installed, such as Outlook, on the server. This is the only option for SQL Server 2000 and before.</p>

<p><code>sp_send_dbmail</code> is a simple SMTP solution, added for SQL Server 2005+</p>

<p><code>sp_send_dbmail</code> is by far better.</p>
"
"<p>I have a system where multiple satellites create financial transactions and they need to sync up with a core server. The satellites are remote servers that run Rails apps with a local Postgres database. The core is another Rails app with its own Postgres database. The satellites and core have pretty much the same schema (but not identical). Everything is containerized (apps and database). Very rarely, the core server does update some data that all satellites need. Currently I have one satellite, but this number will grow to a couple (I dont think more than 100 in the distant future). There is no problem of sequence or contention between the core and the satellites. The core will never update the same transaction as any of the satellites and no satellites will update the same transaction as any of the other satellites. Even better, the financial transactions have a uuid as the primary key.</p>

<p>Since this is a multi-master sync problem, I naturally came across BDR. I have the following questions:</p>

<ol>
<li>Is BDR production ready and stable? Im reading about several competing technologies (like  Bucardo and Londiste). Will it really be part of Postgres 9.6?</li>
<li>Can BDR handle a disconnected model? I dont think this will be very often, but my satellites could be disconnected for hours.</li>
<li>Can BDR do selective syncs? For example, Id only want certain tables be sync-ed.</li>
<li>Could BDR handle 100 satellites?</li>
</ol>
","<blockquote>
  <p>Is BDR production ready and stable?</p>
</blockquote>

<p>Yes, BDR 1.0 for BDR-Postgres 9.4 is production-ready and stable. But then I would say that <a href=""http://2ndquadrant.com/bdr"" rel=""noreferrer"">since I work for 2ndQuadrant, who develop BDR</a>.</p>

<p>It is <em>not</em> a drop-in replacement for standalone PostgreSQL that you can use without application changes though. See the overview section of the manual.</p>

<blockquote>
  <p>Im reading about several competing technologies (like Bucardo and Londiste).</p>
</blockquote>

<p>They're all different. Different trade-offs. There's some discussion of them in the BDR manual, but of course, take that with a grain of salt since we can hardly claim to be unbiased.</p>

<blockquote>
  <p>Will it really be part of Postgres 9.6?</p>
</blockquote>

<p>No, definitely not. Where have you seen that claim?</p>

<p>There will in future (but is not yet) be an <em>extension</em> released to <em>add</em> BDR to PostgreSQL 9.6 when it's ready. But it won't be <em>part of</em> PostgreSQL 9.6, it'll be something you install on top.</p>

<blockquote>
  <p>Can BDR handle a disconnected model? I dont think this will be very often, but my satellites could be disconnected for hours.</p>
</blockquote>

<p>Yes, it handles temporary partitions and network outages well, with some caveats around global sequences. See the manual for details.</p>

<blockquote>
  <p>Can BDR do selective syncs?</p>
</blockquote>

<p>Yes. See the manual for replication sets.</p>

<p>Table structure is always replicated. So are initial table contents at the moment. But table changes can be replicated selectively, table-by-table.</p>

<blockquote>
  <p>For example, Id only want certain tables be sync-ed.</p>
</blockquote>

<p>Sure.</p>

<blockquote>
  <p>Could BDR handle 100 satellites?</p>
</blockquote>

<p>Not well. It's a mesh topology that would expect every satellite to talk to every other satellite. Also, you'd have 198 backends (99 walsenders + 99 apply workers) per node. Not pretty.</p>

<p>You really want a star-and-hub model where each satellite only talks to the hub. That's not supported in BDR 1.0, nor is it targeted for support in BDR 2.0.</p>

<p>I think this is a better use case for <a href=""http://2ndquadrant.com/"" rel=""noreferrer"">pglogical</a> or Londiste.</p>

<p>I can't really go into more detail here, since it overlaps with commercial consulting services I am involved in. The team I work with designs things like this for customers as a <a href=""https://www.postgresql.org/support/professional_support/"" rel=""noreferrer"">professional service</a>.</p>
"
"<p>Familiar question, but with Vertica. I'd like to return the top 5 geo_country rows based on sum(imps) for each tag_id. This is the query I started:</p>

<pre><code>SELECT tag_id,
       geo_country,
       SUM(imps) AS imps,
       RANK() OVER (PARTITION BY tag_id ORDER BY SUM(imps) DESC) AS rank
FROM table1
WHERE tag_id IN (2013150,1981153)
AND ymd &gt; CURRENT_DATE - 3
GROUP BY 1,
         2 LIMIT 10;
</code></pre>

<p>This actually returns only rows from the first tag in the WHERE clause (2013150). I know that the other tag has sum(imps) values high enough which should include it in the results.</p>

<p>Also, how do I implement the Top N part? I tried adding a LIMIT clause within the OVER function, but it doesn't look like it is an accepted parameter.</p>
","<p>In <code>vsql</code> type:</p>

<pre><code>\timing 
</code></pre>

<p>and then hit Enter. You'll like what you'll see :-)</p>

<p>Repeating that will turn it off.</p>
"
"<p>I read on MOS Doc ID 1945619.1 that starting with the 12.1.3 Oracle HTTP Server (OHS), the mod_plsql feature has been deprecated and will not be included with the 12.2 Oracle HTTP Server.</p>

<p>For the future, Oracle recommends moving to Oracle REST Data Services (formerly known as Oracle APEX Listener) as an alternative to mod_plsql.</p>

<p>Our shop have a lot of mod_plsql applications (i.e. applications written usinjg HTP/HTF packages) in production. Since I don't know anything about Oracle REST Data Services I'm asking you if we can migrate the old applications to this new product without changing the code.</p>

<p>Thank you.</p>

<p>Kind regards, Cristian</p>
","<p>If you don't have access to the Apache config, you can probably put the following code at the top of your Oracle procedure:</p>

<pre><code>if owa_util.get_cgi_env('REQUEST_METHOD') != 'POST' then
    raise_application_error(-20001,'Only POST request method is allowed.');
end if; 
</code></pre>
"
"<pre><code>@Entity
public class FruitStore {

@Id
private Long storeId;

@ElementCollection
private Set&lt;Fruit&gt; fruits;

}
</code></pre>

<p>Of course, the <code>Fruit</code> class is marked <code>@Embeddable</code>.</p>

<p>In the database (postgresql to be exact, although it shouldn't matter), there is a table created called <code>fruitstore_fruits</code>. It grows huge, and queries on it become very very slow. I have manually modified the database, such that the <code>fruitstore_fruits</code> table indexes on the <code>FruitStore</code> <code>id</code> column. Happily, this dramatically improves performance. I want this to be done automatically.</p>

<p>The question is, how can I annotate my code to get Hibernate to automatically index the fruitstore_fruits on the <code>FruitStore</code> <code>id</code> column?</p>

<p><strong>EDIT:</strong> <a href=""https://hibernate.atlassian.net/browse/HHH-4263"" rel=""nofollow"">This Hibernate bug</a> has removed lots of hope. I think what I want is simply not supported right now. Which is kinda sad, because the feature isn't that exotic (indexing a element collection with a foreign column). However, I'd <em>love</em> to be proven wrong here.</p>
","<p>There's a difference between Hibernate configuration file (hibernate.cfg.xml) and Hibernate mapping files (*.hbm). If you have heard about generated code it concerns last ones, but first one should have included those mapping files. There's two or three different approaches to development. First to create Java classes with mappings and generate database schema, Second create database schema and generate Java classes (reverse engineering), Third is create classes and mapping for existed schema. Whichever approach you use is up to you, but <code>hibernate.cfg.xml</code> you should create manually if it's not created by IDE. Some frameworks like Spring can provide it's own configuration for Hibernate, thus completely ignoring <code>hibernate.cfg.xml</code>. There's nothing magical with creating <code>hibernate.cfg.xml</code> file. You can start with   </p>

<pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;!DOCTYPE hibernate-configuration PUBLIC
  ""-//Hibernate/Hibernate Configuration DTD 3.0//EN""
  ""http://hibernate.sourceforge.net/hibernate-configuration-3.0.dtd""&gt;

&lt;hibernate-configuration&gt;

  &lt;session-factory&gt;
  &lt;!-- here you can place properties and mapping --&gt;

  &lt;/session-factory&gt;

&lt;/hibernate-configuration&gt;
</code></pre>

<p>And you are right, it should be in the <code>src</code> folder of your project. At runtime it should be where the <code>.class</code> files generated, i.e. on classpath.</p>
"
"<p>I'm trying to add an Azure SQL Server connection string into my <code>app.config</code> file, but there are red underlines all over the connection string when I try to copy and paste it from Azure. I'm using Windows Forms in Visual Studio. </p>

<p>Here is my connection string:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt;
&lt;configuration&gt;
    &lt;connectionStrings&gt;
        &lt;add name=""AddSales""
             Server=""tcp:Sales99.database.windows.net,1433;Initial"" Catalog=""Sales;Persist"" Security="""" Info=""False;User"" ID=""""{your_username};Password=""""{your_password};MultipleActiveResultSets=""""False;Encrypt=""""True;TrustServerCertificate=""False;Connection"" Timeout=""30""
             providerName=""System.Data.SqlClient"" /&gt;
    &lt;/connectionStrings&gt;
&lt;/configuration&gt;
</code></pre>

<p>Is there a way to fix this issue? Please advise.</p>
","<p>Microsoft reached out to me and provided a sample resource template to accomplish this:</p>

<pre><code>{
    ""$schema"": ""http://schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json#"",
    ""contentVersion"": ""1.0.0.0"",
    ""parameters"": {
        ""SQL Administrator Login"": {
            ""type"": ""String""
        },
        ""SQL Administrator Password"": {
            ""type"": ""SecureString""
        },
        ""AAD Admin Login"": {
            ""type"": ""String""
        },
        ""AAD Admin ObjectID"": {
            ""type"": ""String""
        },
        ""AAD TenantId"": {
            ""type"": ""String""
        },
        ""Location (Region)"": {
            ""type"": ""String""
        },
        ""Server Name"": {
            ""type"": ""String""
        }
    },
    ""variables"": {},
    ""resources"": [
        {
            ""type"": ""Microsoft.Sql/servers"",
            ""name"": ""[parameters('Server Name')]"",
            ""apiVersion"": ""2014-04-01-preview"",
            ""location"": ""[parameters('Location (Region)')]"",
            ""properties"": {
                ""administratorLogin"": ""[parameters('SQL Administrator Login')]"",
                ""administratorLoginPassword"": ""[parameters('SQL Administrator Password')]"",
                ""version"": ""12.0""
            },
            ""resources"": [
                {
                    ""type"": ""firewallrules"",
                    ""name"": ""AllowAllWindowsAzureIps"",
                    ""apiVersion"": ""2014-04-01-preview"",
                    ""location"": ""[parameters('Location (Region)')]"",
                    ""properties"": {
                        ""endIpAddress"": ""0.0.0.0"",
                       ""startIpAddress"": ""0.0.0.0""
                    },
                    ""dependsOn"": [
                        ""[concat('Microsoft.Sql/servers/', parameters('Server Name'))]""
                    ]
                },
                {
                    ""type"": ""administrators"",
                    ""name"": ""activeDirectory"",
                    ""apiVersion"": ""2014-04-01-preview"",
                    ""location"": ""[parameters('Location (Region)')]"",
                    ""properties"": {
                        ""administratorType"": ""ActiveDirectory"",
                        ""login"": ""[parameters('AAD Admin Login')]"",
                        ""sid"": ""[parameters('AAD Admin ObjectID')]"",
                        ""tenantId"": ""[parameters('AAD TenantID')]""
                    },
                    ""dependsOn"": [
                        ""[concat('Microsoft.Sql/servers/', parameters('Server Name'))]""
                    ]
                }
            ]
        }
    ]
}
</code></pre>
"
"<p>I'm using Hugsql with Clojure to access a Postgresql db. Several of my database tables have optional columns - for a simple example consider a ""users"" table with various address columns - address1, address2, city, etc. </p>

<p>When I write the Hugsql query specification for an ""update"" I don't know which values will be present in the map I pass in.  So if I write a query:</p>

<pre><code>-- :name update-user! :! :n
UPDATE users set firstname = :firstname, address1 = :address1 where id = :id
</code></pre>

<p>but pass in a user map </p>

<pre><code>(update-user! {:id ""testuser"" :firstname ""Bartholamew""})
</code></pre>

<p>then an exception is thrown. I'd expect it to create something like</p>

<pre><code>UPDATE users SET firstname='Bartholamew', address1=NULL where id='testuser'
</code></pre>

<p>I've looked at the Hugsql source - it calls a (validate-parameters) function that throws the exception that I can't see a way around.  I'm sure I'm missing something obvious: this doesn't seem like an unusual requirement, and I sure don't want to write a distinct SQL query spec for every possible combination of optional columns.</p>

<p>Is there a way to handle missing parameters that I'm missing? Am I abusing the database by having optional columns? </p>
","<p>If you're using Postgres 9.5 or newer (which I assume you are, since it was released back in January 2016), there's a very useful <code>ON CONFLICT</code> cluase you can use:</p>

<pre><code>INSERT INTO mytable (id, col1, col2)
VALUES (123, 'some_value', 'some_other_value')
ON CONFLICT (id) DO NOTHING
</code></pre>
"
"<p>I'm compiling a project in XCode where MySQL++ in included and linked to.  For some reason, I keep getting the following compiler error:</p>

<p>'assert was not declared in this scope</p>

<p>originating from cpool.h, a header file that's part of MySQL++.  Does anyone know why this is being triggered?</p>

<p>EDIT: For reference, MySQL++ was installed via Macports.</p>
","<p>While stacker's answer will work, MySQL++ wraps that function as <a href=""http://tangentsoft.net/mysql++/doc/html/refman/classmysqlpp_1_1SimpleResult.html#c7493c4b03818e8a88b69abc987286b6"" rel=""noreferrer"">SimpleResult::insert_id()</a>. Example:</p>

<pre><code>Query q = conn.query();
q.insert(something);
if (SimpleResult res = q.execute()) {
    cout &lt;&lt; ""Auto-increment value: "" &lt;&lt; res.insert_id() &lt;&lt; endl;
}
</code></pre>
"
"<p>I need to use foreign keys for update and cascade, etc. </p>

<pre><code>ALTER TABLE topics
  ADD FOREIGN KEY(topic_by) REFERENCES users(user_id)
  ON DELETE RESTRICT ON UPDATE CASCADE;
</code></pre>

<p>but I am not able to make foreign keys in SQL Buddy.</p>

<p>Any way to do that?</p>
","<p>I have same issue and I solve it. You must have a folder named ""exports"" or ""export"" (read about this somewhere), I use the first one name. Create it in the folder sqlbuddy folder </p>

<pre><code>/../sqlbuddy/calvinlough-sqlbuddy-1b38cf4/exports/
</code></pre>

<p>the path maybe different in your case, but the principle is the same.</p>

<p>After that you must give that folder 777 permission.</p>

<pre><code>chmod -R 777 /../sqlbuddy/calvinlough-sqlbuddy-1b38cf4/exports/
</code></pre>

<p>Ok, done. Sqlbuddy now will export your sql base to a file.
This may be not a very secure, so I advise you to remove your .sql file from the server after every export. Hope it help you, as it help me.</p>
"
"<p>This is a follow-up to <a href=""https://stackoverflow.com/questions/49752388/editable-qtableview-of-complex-sql-query"">this question</a>. In there, we created an editable subclass of QSqlQueryModel, to use with complex queries. 
Now I need to add a functionality like QTableModel's setEditStrategy so I can cache all changes and accept or revert them using buttons. PyQt apparently doesn't allow multiple inheritance and I cannot find sufficient documentation to re-implement this method in my custom model, therefor here's the question:</p>

<p><strong>How can I re-implement QSqlTableModel.setEditStragety (or something like it) including RevertAll() and SubmitAll() in an editable QSqlQueryModel?</strong></p>

<p>Here's a CVME: (I have out-commented the parts of of my Example class I would like to get working)</p>

<pre><code>import sys

from PyQt5.QtCore import Qt
from PyQt5.QtSql import QSqlDatabase, QSqlQuery, QSqlQueryModel, QSqlTableModel
from PyQt5.QtWidgets import QApplication, QTableView, QWidget, QGridLayout
from PyQt5.Qt import QPushButton

db_file = ""test.db""


def create_connection(file_path):
    db = QSqlDatabase.addDatabase(""QSQLITE"")
    db.setDatabaseName(file_path)
    if not db.open():
        print(""Cannot establish a database connection to {}!"".format(file_path))
        return False
    return True


def fill_tables():
    q = QSqlQuery()
    q.exec_(""DROP TABLE IF EXISTS Manufacturers;"")
    q.exec_(""CREATE TABLE Manufacturers (Company TEXT, Country TEXT);"")
    q.exec_(""INSERT INTO Manufacturers VALUES ('VW', 'Germany');"")
    q.exec_(""INSERT INTO Manufacturers VALUES ('Honda' , 'Japan');"")

    q.exec_(""DROP TABLE IF EXISTS Cars;"")
    q.exec_(""CREATE TABLE Cars (Company TEXT, Model TEXT, Year INT);"")
    q.exec_(""INSERT INTO Cars VALUES ('Honda', 'Civic', 2009);"")
    q.exec_(""INSERT INTO Cars VALUES ('VW', 'Golf', 2013);"")
    q.exec_(""INSERT INTO Cars VALUES ('VW', 'Polo', 1999);"")


class SqlQueryModel_editable(QSqlQueryModel):
    """"""a subclass of QSqlQueryModel where individual columns can be defined as editable
    """"""
    def __init__(self, editables):
        """"""editables should be a dict of format: 
        {INT editable_column_nr : (STR update query to be performed when changes are made on this column
                                   INT model's column number for the filter-column (used in the where-clause),
                                   )} 
        """"""
        super().__init__()
        self.editables = editables

    def flags(self, index):
        fl = QSqlQueryModel.flags(self, index)
        if index.column() in self.editables:
            fl |= Qt.ItemIsEditable
        return fl

    def setData(self, index, value, role=Qt.EditRole):
        if role == Qt.EditRole:
            mycolumn = index.column()
            if mycolumn in self.editables:
                (query, filter_col) = self.editables[mycolumn]
                filter_value = self.index(index.row(), filter_col).data()
                q = QSqlQuery(query.format(value, filter_value))
                result = q.exec_()
                if result:
                    self.query().exec_()
                else:
                    print(self.query().lastError().text())
                return result
        return QSqlQueryModel.setData(self, index, value, role)

    def setFilter(self, myfilter):
        text = (self.query().lastQuery() + "" WHERE "" + myfilter)
        self.setQuery(text)


class Example(QWidget):
    def __init__(self):
        super().__init__()
        self.resize(400, 150)

        self.createModel()
        self.initUI()

    def createModel(self):
        editables = {1 : (""UPDATE Manufacturers SET Country = '{}' WHERE Company = '{}'"", 2)}
        self.model = SqlQueryModel_editable(editables)
        query = '''
            SELECT (comp.company || "" "" || cars.model) as Car,
                    comp.Country,
                    cars.company,
                    (CASE WHEN cars.Year &gt; 2000 THEN 'yes' ELSE 'no' END) as this_century
            from manufacturers comp left join cars
                on comp.company = cars.company
            '''
        q = QSqlQuery(query)
        self.model.setQuery(q)
        self.model.setFilter(""cars.Company = 'VW'"")
#         self.model.setEditStrategy(QSqlTableModel.OnManualSubmit)

    def initUI(self):
        self.layout = QGridLayout()
        self.setLayout(self.layout)
        self.view = QTableView()
        self.view.setModel(self.model)
        self.view.hideColumn(2)
        self.layout.addWidget(self.view,0,0,1,2)

        self.accept_btn = QPushButton(""Accept Changes"")
#         self.accept_btn.clicked.connect(self.model.submitAll)
        self.layout.addWidget(self.accept_btn, 1,0)
        self.reject_btn = QPushButton(""Reject Changes"")
#         self.reject_btn.clicked.connect(self.model.revertAll)
        self.layout.addWidget(self.reject_btn, 1,1)


if __name__ == '__main__':
    app = QApplication(sys.argv)
    if not create_connection(db_file):
        sys.exit(-1)

    fill_tables()

    ex = Example()
    ex.show()
    sys.exit(app.exec_())
</code></pre>

<p><strong>Edit to clarify:</strong></p>

<p>I need an editable QSqlQueryModel, on which I can use <code>submitAll()</code> and <code>revertAll()</code>, so that changes to the model's data are only accepted after an Accept-button is clicked, or can be reverted using a ""Reject"" button.</p>
","<p>Using information from <em>A more generic approach</em> of 
<a href=""https://wiki.qt.io/How_to_Use_a_QSqlQueryModel_in_QML"" rel=""nofollow noreferrer""><em>How to Use a QSqlQueryModel in QML</em></a> you can build a general model, to make it easy to use from QML you can create a property to pass the query.</p>

<p><strong>sqlquerymodel.h</strong></p>

<pre><code>#ifndef SQLQUERYMODEL_H
#define SQLQUERYMODEL_H

#include &lt;QSqlQuery&gt;
#include &lt;QSqlQueryModel&gt;
#include &lt;QSqlRecord&gt;

class SqlQueryModel : public QSqlQueryModel
{
    Q_OBJECT
    Q_PROPERTY(QString query READ queryStr WRITE setQueryStr NOTIFY queryStrChanged)
    Q_PROPERTY(QStringList userRoleNames READ userRoleNames CONSTANT)
public:
    using QSqlQueryModel::QSqlQueryModel;
    QHash&lt;int, QByteArray&gt; roleNames() const
    {
       QHash&lt;int, QByteArray&gt; roles;
       for (int i = 0; i &lt; record().count(); i ++) {
           roles.insert(Qt::UserRole + i + 1, record().fieldName(i).toUtf8());
       }
       return roles;
   }
    QVariant data(const QModelIndex &amp;index, int role) const
    {
        QVariant value;
        if (index.isValid()) {
            if (role &lt; Qt::UserRole) {
                value = QSqlQueryModel::data(index, role);
            } else {
                int columnIdx = role - Qt::UserRole - 1;
                QModelIndex modelIndex = this-&gt;index(index.row(), columnIdx);
                value = QSqlQueryModel::data(modelIndex, Qt::DisplayRole);
            }
        }
        return value;
    }
    QString queryStr() const{
        return query().lastQuery();
    }
    void setQueryStr(const QString &amp;query){
        if(queryStr() == query)
            return;
        setQuery(query);
        emit queryStrChanged();
    }
    QStringList userRoleNames() const {
        QStringList names;
        for (int i = 0; i &lt; record().count(); i ++) {
            names &lt;&lt; record().fieldName(i).toUtf8();
        }
        return names;
    }
signals:
    void queryStrChanged();
};
#endif // SQLQUERYMODEL_H
</code></pre>

<p><strong>main.cpp</strong></p>

<pre><code>#include ""sqlquerymodel.h""

#include &lt;QGuiApplication&gt;
#include &lt;QQmlApplicationEngine&gt;
#include &lt;QDebug&gt;
#include &lt;QSqlError&gt;

static bool createConnection()
{

    QSqlDatabase db = QSqlDatabase::addDatabase(""QSQLITE"");
    db.setDatabaseName("":memory:"");
    if (!db.open()) {
        qDebug()&lt;&lt;""Cannot open database\n""
                  ""Unable to establish a database connection.\n""
                  ""This example needs SQLite support. Please read ""
                  ""the Qt SQL driver documentation for information how ""
                  ""to build it.\n\n""
                  ""Click Cancel to exit."";
        return false;
    }

    QSqlQuery query;
    if(!query.exec(""CREATE TABLE COMPANY(""
                   ""ID INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,""
                   ""NAME           TEXT    NOT NULL,""
                   ""AGE            INT     NOT NULL,""
                   ""SALARY         REAL""
                   "")"")){
        qDebug()&lt;&lt;query.lastError().text();
    }
    for(int i=0; i &lt; 10; i++){
        query.prepare(""insert into COMPANY(NAME, AGE, SALARY) values(:name, :age, :salary)"");
        query.bindValue("":name"", QString(""name-%1"").arg(i));
        query.bindValue("":age"",  (i+1)*1000);
        query.bindValue("":salary"", (11-i)*11.5);
        if(!query.exec()){
            qDebug()&lt;&lt;query.lastError().text();
        }
    }
    return true;
}


int main(int argc, char *argv[])
{
    qmlRegisterType&lt;SqlQueryModel&gt;(""Foo"", 1, 0, ""SqlQueryModel"");
    QCoreApplication::setAttribute(Qt::AA_EnableHighDpiScaling);

    QGuiApplication app(argc, argv);
    if(!createConnection())
        return -1;

    QQmlApplicationEngine engine;
    engine.load(QUrl(QStringLiteral(""qrc:/main.qml"")));
    if (engine.rootObjects().isEmpty())
        return -1;

    return app.exec();
}
</code></pre>

<p><strong>main.qml</strong></p>

<pre><code>import QtQuick 2.9
import QtQuick.Window 2.2
import QtQuick.Controls 1.4

import Foo 1.0

Window {
    visible: true
    width: 640
    height: 480
    title: qsTr(""SqlQueryModel"")
    SqlQueryModel{
        id: sqlmodel
        query: ""select * from COMPANY""
    }
    Component{
        id: columnComponent
        TableViewColumn{width: 100 }
    }
    TableView {
        id: view
        anchors.fill: parent
        resources:{
            var roleList = sqlmodel.userRoleNames
            var temp = []
            for(var i in roleList){
                var role  = roleList[i]
                temp.push(columnComponent.createObject(view, { ""role"": role, ""title"": role}))
            }
            return temp
        }
        model: sqlmodel
    }
}
</code></pre>

<p><a href=""https://i.stack.imgur.com/t2eYk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t2eYk.png"" alt=""enter image description here""></a></p>
"
"<p>I have SQL Server installed on Linux. It was installed from Microsoft's repos as described here:
<a href=""https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu</a></p>

<p>In MySql I used to write <code>EXPLAIN</code> in front of my query to see the execution plan. In SQL Server it doesn't seem to work. But I don't have the studio program installed, only just SQL Server and the <code>sqlcmd</code> tool.</p>

<p>How do I see the execution plan of a query in SQL Server on Linux?</p>
","<p>Microsoft released a new tool called <a href=""https://github.com/Microsoft/sqlopsstudio"" rel=""nofollow noreferrer"">SQL Operations studio</a>,this is similar to SSMS,but is available on Windows,Linux,Macos.</p>

<p><strong>location for download:</strong><br>
<a href=""https://docs.microsoft.com/en-us/sql/sql-operations-studio/download"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/sql-operations-studio/download</a></p>

<p>Below is a screenshot of how it looks like</p>

<p><a href=""https://i.stack.imgur.com/THna9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/THna9.jpg"" alt=""enter image description here""></a></p>

<p><strong>To view actual execution plan using sqlopsstudio(steps same for all platforms)</strong>  </p>

<ul>
<li>Press <kbd> CTRL+SHIFT+P</kbd>  </li>
<li>Type run query with actual execution plan  as shown below and select the highlighted, you will get an actual execution plan  </li>
</ul>

<p><a href=""https://i.stack.imgur.com/NAlW1.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NAlW1.gif"" alt=""enter image description here""></a></p>

<p><strong>To view estimated execution plan :</strong>  </p>

<p>Just press the ICON shown below</p>

<p><a href=""https://i.stack.imgur.com/Q3HfU.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q3HfU.gif"" alt=""enter image description here""></a>  </p>

<p><strong>You can also use a keybinding to view actual execution plan .Below are the steps</strong>    </p>

<p>1.Press <kbd> CTRL+SHIFT+P</kbd><br>
2.Type keyboard shortcuts<br>
3.In the search plan type actual as shown below<br>
<a href=""https://i.stack.imgur.com/WKSW8.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WKSW8.gif"" alt=""enter image description here""></a>
4.Right click actual query plan shortcut and say <code>add key binding</code> with a key of your choice(for me it is <kbd>CTRL+M</kbd>  </p>

<p><a href=""https://i.stack.imgur.com/5C3aH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5C3aH.gif"" alt=""enter image description here""></a></p>

<p><strong>Below Part of the answer was  written during the time when SQLOPS studio is not available.This can ben helpfull for any one who don't have SQLopsstudio:</strong>  </p>

<p>Currently viewing execution plan is supported only if you are on Windows,using SSMS or some third party tool like SQLSentry..</p>

<p>There is a feature request being tracked here :<a href=""https://github.com/Microsoft/vscode-mssql/issues/509"" rel=""nofollow noreferrer"">Return ShowPlan data as Text or XML with Query Execution</a></p>

<p>one more option is to connect using VSCODE on linux and set <code>show plan xml</code> as shown in screenshot below..this provides xml of execution plan</p>

<pre><code>SET showplan_xml ON;
</code></pre>

<p><a href=""https://i.stack.imgur.com/bkvJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bkvJX.png"" alt=""enter image description here""></a></p>

<p>you can take that xml and upload it <a href=""https://www.brentozar.com/pastetheplan/"" rel=""nofollow noreferrer"">Paste The Plan</a> website  and can view plans </p>

<p>Below is a screenshot of above XML</p>

<p><a href=""https://i.stack.imgur.com/XVFqj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XVFqj.png"" alt=""enter image description here""></a></p>

<p>you can also view it in SQLSENTRY plan explorer as well(Windows only) for more indepth analysis</p>

<p><a href=""https://i.stack.imgur.com/IZ1Dk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IZ1Dk.png"" alt=""enter image description here""></a></p>
"
"<p>How do I tell SQLAlchemy to automatically reflect basic Foreign Key references as references to other ORM objects and not integer fields?</p>

<p>In both <a href=""http://www.sqlalchemy.org/docs/orm/tutorial.html#building-a-relationship"">SQLAlchemy</a> and it's <a href=""http://www.sqlalchemy.org/docs/orm/extensions/sqlsoup.html#relationships"">SqlSoup</a>, table columns are reflected automatically and relations can be defined manually:</p>

<pre><code>class User(Base):
    __table__ = metadata.tables['users']
    loan = relation(Loans)
</code></pre>

<p>...</p>

<pre><code>You can define relationships on SqlSoup classes:
&gt;&gt;&gt; db.users.relate('loans', db.loans)
</code></pre>
","<p>In python, you can pass a dictionary as <em>keyword arguments</em> to any function by using the <code>**</code> argument packing syntax.</p>

<p>If you have a dictionary <code>arguments</code> defined as:</p>

<pre><code>arguments = {'blah_field': 'blah_value'}
</code></pre>

<p>You can call <code>db.blah_table.insert</code> with that dictionary as keyword arguments like this:</p>

<pre><code>db.blah_table.insert(**arguments)
</code></pre>

<p>Under the hood this is equivalent to:</p>

<pre><code>db.blah_table.insert(blah_field=blah_value)
</code></pre>

<p>As an aside, while <code>**</code> can unpack named arguments from a dictionary, a single <code>*</code> can be used to unpack positional arguments from a list or tuple.</p>
"
"<p>I have SQL Server installed on Linux. It was installed from Microsoft's repos as described here:
<a href=""https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu</a></p>

<p>In MySql I used to write <code>EXPLAIN</code> in front of my query to see the execution plan. In SQL Server it doesn't seem to work. But I don't have the studio program installed, only just SQL Server and the <code>sqlcmd</code> tool.</p>

<p>How do I see the execution plan of a query in SQL Server on Linux?</p>
","<p>Microsoft released a new tool called <a href=""https://github.com/Microsoft/sqlopsstudio"" rel=""nofollow noreferrer"">SQL Operations studio</a>,this is similar to SSMS,but is available on Windows,Linux,Macos.</p>

<p><strong>location for download:</strong><br>
<a href=""https://docs.microsoft.com/en-us/sql/sql-operations-studio/download"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/sql-operations-studio/download</a></p>

<p>Below is a screenshot of how it looks like</p>

<p><a href=""https://i.stack.imgur.com/THna9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/THna9.jpg"" alt=""enter image description here""></a></p>

<p><strong>To view actual execution plan using sqlopsstudio(steps same for all platforms)</strong>  </p>

<ul>
<li>Press <kbd> CTRL+SHIFT+P</kbd>  </li>
<li>Type run query with actual execution plan  as shown below and select the highlighted, you will get an actual execution plan  </li>
</ul>

<p><a href=""https://i.stack.imgur.com/NAlW1.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NAlW1.gif"" alt=""enter image description here""></a></p>

<p><strong>To view estimated execution plan :</strong>  </p>

<p>Just press the ICON shown below</p>

<p><a href=""https://i.stack.imgur.com/Q3HfU.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q3HfU.gif"" alt=""enter image description here""></a>  </p>

<p><strong>You can also use a keybinding to view actual execution plan .Below are the steps</strong>    </p>

<p>1.Press <kbd> CTRL+SHIFT+P</kbd><br>
2.Type keyboard shortcuts<br>
3.In the search plan type actual as shown below<br>
<a href=""https://i.stack.imgur.com/WKSW8.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WKSW8.gif"" alt=""enter image description here""></a>
4.Right click actual query plan shortcut and say <code>add key binding</code> with a key of your choice(for me it is <kbd>CTRL+M</kbd>  </p>

<p><a href=""https://i.stack.imgur.com/5C3aH.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5C3aH.gif"" alt=""enter image description here""></a></p>

<p><strong>Below Part of the answer was  written during the time when SQLOPS studio is not available.This can ben helpfull for any one who don't have SQLopsstudio:</strong>  </p>

<p>Currently viewing execution plan is supported only if you are on Windows,using SSMS or some third party tool like SQLSentry..</p>

<p>There is a feature request being tracked here :<a href=""https://github.com/Microsoft/vscode-mssql/issues/509"" rel=""nofollow noreferrer"">Return ShowPlan data as Text or XML with Query Execution</a></p>

<p>one more option is to connect using VSCODE on linux and set <code>show plan xml</code> as shown in screenshot below..this provides xml of execution plan</p>

<pre><code>SET showplan_xml ON;
</code></pre>

<p><a href=""https://i.stack.imgur.com/bkvJX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bkvJX.png"" alt=""enter image description here""></a></p>

<p>you can take that xml and upload it <a href=""https://www.brentozar.com/pastetheplan/"" rel=""nofollow noreferrer"">Paste The Plan</a> website  and can view plans </p>

<p>Below is a screenshot of above XML</p>

<p><a href=""https://i.stack.imgur.com/XVFqj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XVFqj.png"" alt=""enter image description here""></a></p>

<p>you can also view it in SQLSENTRY plan explorer as well(Windows only) for more indepth analysis</p>

<p><a href=""https://i.stack.imgur.com/IZ1Dk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IZ1Dk.png"" alt=""enter image description here""></a></p>
"
"<p>Usually in Java I execute a SELECT statement and check the size of ResultSet. If it is zero I issue a INSERT and otherwise an UPDATE.</p>

<p>Since Groovy provides syntactic sugar on top of JDBC, I'm wondering if it provides a way to ease this process? Is there an easier way to save or update a record?</p>

<p>Note: 
I know that Hibernate offers this, but I'd rather stick only with Groovy API.</p>
","<p>The difference here is that the Groovy Sql class explicitly works with GStrings to ensure parameters are properly quoted (<a href=""http://groovy.codehaus.org/api/groovy/sql/Sql.html"" rel=""nofollow"">as explained in the documentation</a>).</p>

<p>So it converts the first example to</p>

<pre><code>truncate 'my_table'
</code></pre>

<p>Which is wrong (as the error explains)</p>

<p>You can also use:</p>

<pre><code>sql.execute ""truncate ${Sql.expand(tableName)}""
</code></pre>
"
"<p>My problem is probably straight forward, but I can't figure out what's happening behind the scenes. I'm looping through a series of domains in a database table, calling out to them, grabbing the SSL certificate and storing information about it back into the database. </p>

<p>For the most part, it's working - except when the loop exits any calls that haven't get completed just stop dead.</p>

<p>Database retrieval that begins check:</p>

<pre><code>function queryRows() {
complete = false;
var query = c.query(""SELECT * FROM domains LIMIT 100 OFFSET "" + offset);
query.on('result', function(res) {
res.on('data', function(row) {
  checkUrl(row)
}).on('end', function() {
    complete = true;
});
}).on('end', function() {
    console.log(complete);
    offset += 100;
    if(offset &lt;= (parseInt(rows) + 400)){
      queryRows();
    } else {
        console.log(""Done, waiting"");
        setTimeoutPromise(600000, 'foobar').then((value) =&gt; {
            console.log(""restarting"")
            offset = 0;
            getTotal();
        });
    }
  });
 }
</code></pre>

<p>And the code that checks the SSL: </p>

<pre><code>function checkSSL(id, domain){
complete = false 
var options = {
  host: domain,
  rejectUnauthorized: false
};

callback = function(response) {
    var str = '';
    try {
        if(domain == ""arstechnica.com""){
            console.log(""Found ars - savingCertificate"");
        }
        cert = response.connection.getPeerCertificate(true);
        complete = hasSSL(cert, domain, id);
        // updateDomainRecord(cert, domain, id)
    } catch (error){
        console.log(error);
        complete = true;
        noSSLRecord(domain, id);
    }
}
const req = https.request(options, callback);
req.on('error', (e) =&gt; {
  // console.error(e);
});
}
</code></pre>

<p>It's worth noting that if I put a console.log before https.request, I see it in my console. However any logs within the callback fail to trigger (because the callback itself never fires). </p>

<p>Again, some of the time the callback does. It is only near the end of the database loop where it appears to stop working. Any advice would be appreciated!</p>
","<p>Looks like your request is never being sent, so the <code>callback</code> will never get fired. Make sure your request is actually being sent, so you have to add one line in the end:</p>

<pre><code>req.end();
</code></pre>
"
"<p>(Apologies if necessary--my first Stack Overflow question. I'll be happy to modify it if anyone has suggestions. I have looked for an answer but I'm afraid my grasp of the terminology isn't good enough to make a complete search.)</p>

<p>I'm accustomed to using mysql_fetch_array to get records from a database.
When getting records that way, mysql_num_rows gives me a count of the rows.
On my current project, however, I'm using mysql_fetch_object.
mysql_num_rows doesn't seem to work with this function, and when I do a 'count' on the results of the query I get the expected answer: 1 (one object).</p>

<p>Is there a way to 'see into' the object and count the elements inside it?</p>
","<p>The function <code>mysql_num_rows</code> works on your result resource, not your object row.</p>

<h3>Example</h3>

<pre><code>$link = mysql_connect(""localhost"", ""mysql_user"", ""mysql_password"");
mysql_select_db(""database"", $link);

$sql = ""SELECT id, name FROM myTable"";

$result = mysql_query($sql, $link);

$rowCount = mysql_num_rows($result);

while($row = mysql_fetch_object){
    echo ""id: "".$row-&gt;id."" name: "".$row-&gt;name.""&lt;BR&gt;"";
}
echo ""total: "".$rowCount;
</code></pre>
"
"<p>I want to validate a System.DateTime value before I add it as a parameter to my SqlCommand instance.</p>

<p>The MSDN documentation for the SqlDbType enumeration says:</p>

<p><strong>Date and time data ranging in value from January 1, 1753 to December 31, 9999 to an accuracy of 3.33 milliseconds.</strong></p>

<p>To validate the value, I'm using</p>

<pre><code>public readonly DateTime SqlDateTimeMin = new DateTime(1753, 1, 1);
public readonly DateTime SqlDateTimeMax = new DateTime(9999, 12, 31);

if (value &lt; SqlDateTimeMin || value &gt; SqlDateTimeMax)
    // Validation check fails
else
    // Validation check succeeds
</code></pre>

<p>Is this the best way?  Is there an alternative to hard coding these min and max values?</p>
","<p>What about <a href=""http://msdn.microsoft.com/en-us/library/system.data.sqltypes.sqldatetime.minvalue.aspx"">SqlDateTime.MinValue</a> and <a href=""http://msdn.microsoft.com/en-us/library/system.data.sqltypes.sqldatetime.maxvalue.aspx"">SqlDateTime.MaxValue</a>?</p>

<p>Note: these are <em>SQL</em> type min/max not the .net types like the previous 2 answers :-)</p>
"
"<p><strong>TLDR:</strong></p>

<blockquote>
  <p>The following code is run in the different databases, Oracle: <code>select sysdate from dual</code> SQLite <code>select datetime('now')</code></p>
  
  <p>When doing <code>Session.CreateSQLQuery(cmd).UniqueResult&lt;DateTime&gt;()</code> the result is a DateTime when working against Oracle but a string when working against SQLite.</p>
  
  <p>It feels like a bug in the SQLite driver and a hack to check the returned type and do a DateTime.Parse() if it is a string. I could do that but are there any ways to have NHibernate return the correct type?</p>
</blockquote>

<p>I am trying to fetch current database time from the database. It works fine when using Oracle but when I try to do it against SQLite (in my unit tests) it breaks as the date returned is not a DateTime but a string.</p>

<p>I've seen solutions using custom IUserType but I cannot see how I should use that in this case. Any suggestions?</p>

<pre><code>using System;
using System.Collections.Generic;
using NHibernate;
using NHibernate.Criterion;
using NHibernate.Dialect.Function;

namespace My.Common.Types {

    public class MyNHibernateDialectException : Exception {
        public MyNHibernateDialectException(string message) : base(message) { }
    } 

    /// &lt;summary&gt;
    /// Define all custom functions here by name. It is important that when adding a new custom sql function, that function will work
    /// in all dialects supported.
    /// &lt;/summary&gt;
    public static class MyDatabaseDialects {

        public enum Query {
            SysDate
        }

        /// &lt;summary&gt;
        ///  Dialect implementations will use this function to verify that they all implement the same functions.
        /// &lt;/summary&gt;
        /// &lt;param name=""dialect""&gt;&lt;/param&gt;
        public static void VerifyRegistrations(this NHibernate.Dialect.Dialect dialect) {
            // Verify that the required function are there
            foreach (var func in Enum.GetValues(typeof(Function))) {
                var enumName = func.ToString();
                if (!dialect.Functions.ContainsKey(enumName)) {
                    throw new MyNHibernateDialectException(
                        string.Format(""The custom function '{0}' is not defined. Did you forget it in factory '{1}'?"", enumName, dialect));
                }
            }
        }

    }

    /// &lt;summary&gt;
    /// An interface to reveal more advanced functionality that is database specific
    /// &lt;/summary&gt;
    public interface IDialectExtensions {

        /// &lt;summary&gt;
        /// Fetch a query specfic for the current database.
        /// &lt;/summary&gt;
        ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query);

        /// &lt;summary&gt;
        /// Fetch a parameterized query specfic for the current database.
        /// &lt;/summary&gt;
        ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query, params object[] queryParams);
    }

    /// &lt;summary&gt;
    /// Class to store database specific objects except functions (which are supported by NHibernate).
    /// &lt;/summary&gt;
    class DialectExtension {

        private readonly Dictionary&lt;MyDatabaseDialects.Query, string&gt; queryDictionary = new Dictionary&lt;MyDatabaseDialects.Query, string&gt;();

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query) {
            return this.GetQuery(session, query, null);
        }

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query, params object[] queryParams) {
            var cmd = (queryParams == null) ? queryDictionary[query] : string.Format(queryDictionary[query], queryParams);
            return session.Session.CreateSQLQuery(cmd);
        }

        public void RegisterQuery(MyDatabaseDialects.Query query, string hqlString) {
            queryDictionary.Add(query, hqlString);
        }

        public void VerifyQueryRegistrations() {

            foreach (var query in Enum.GetValues(typeof(MyDatabaseDialects.Query))) {
                if (!queryDictionary.ContainsKey((MyDatabaseDialects.Query)query)) {
                    throw new MyNHibernateDialectException(string.Format(""The custom query '{0}' is not defined."", query.ToString()));
                }
            }
        }
    }

    public class MyOracle10gDialect : NHibernate.Dialect.Oracle10gDialect, IDialectExtensions {

        private readonly DialectExtension dialectExtension = new DialectExtension();

        public MyOracle10gDialect() {

            #region Dialect extensions

            dialectExtension.RegisterQuery(MyDatabaseDialects.Query.SysDate, @""select sysdate from dual"");

            dialectExtension.VerifyQueryRegistrations();

            #endregion Dialect extensions

        }

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query) {
            return dialectExtension.GetQuery(session, query);
        }

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query, params object[] queryParams) {
            return dialectExtension.GetQuery(session, query, queryParams);
        }
    }

    public class MySqliteDialect : NHibernate.Dialect.SQLiteDialect, IDialectExtensions {

        private readonly DialectExtension dialectExtension = new DialectExtension();

        public MySqliteDialect() {

            #region Dialect extensions

            dialectExtension.RegisterQuery(MyDatabaseDialects.Query.SysDate, @""select datetime('now')"");

            dialectExtension.VerifyQueryRegistrations();

            #endregion Dialect extensions
        }

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query) {
            return dialectExtension.GetQuery(session, query);
        }

        public ISQLQuery GetQuery(ISession session, MyDatabaseDialects.Query query, params object[] queryParams) {
            return dialectExtension.GetQuery(session, query, queryParams);
        }

    }

}
</code></pre>

<p>And I use the code above like this:</p>

<pre><code>/// &lt;summary&gt;
/// Fetches a DialectExtensions object allowing us to have more advanced functionality that is database specific
/// &lt;/summary&gt;
public static IDialectExtensions GetDialectExtensions(this IOperationContext operationContext) {
    return Session.GetSessionImplementation().Factory.Dialect as IDialectExtensions;
}

/// &lt;summary&gt;
/// Get the database time by executing a raw SQL statement.
/// &lt;/summary&gt;
public static DateTime? GetDatabaseTime() {
    DateTime? result = null;
    try {
        result = GetDialectExtensions()
            .GetQuery(Session, MyDatabaseDialects.Query.SysDate)
            .UniqueResult&lt;DateTime&gt;();
    } catch {
        // SQLite will throw exception here as the result is returned as a string instead of a DateTime
    }
    return result;
}
</code></pre>
","<p>You need to add <code>GROUP BY</code></p>

<pre><code>SELECT fname, SUM(salary) 
FROM employee
GROUP BY fname
</code></pre>

<p>Most RDBMSs would reject your original query as invalid.</p>

<p>Or in response to the comment to get the <code>SUM</code> from the whole table as an additional column if the <code>OVER</code> clause is supported you can use</p>

<pre><code>SELECT fname, SUM(salary) OVER ()
FROM employee
</code></pre>

<p>And if it isn't you can use a non correlated sub query.</p>

<pre><code>SELECT fname, (SELECT SUM(salary) FROM employee)
FROM employee
</code></pre>
"
"<p>I'm looking at the <a href=""https://www.gaia-gis.it/fossil/libspatialite/wiki?name=spatialite-android-tutorial"">tutorial for Spatialite-Android</a>, and I'm noticing the following code samples:</p>

<pre><code>String query = ""SELECT AsText(Transform(MakePoint("" + TEST_LON + "", "" + TEST_LAT + "", 4326), 32632));"";
sb.append(""Execute query: "").append(query).append(""\n"");
try {
    Stmt stmt = db.prepare(query);
    if (stmt.step()) {
        String pointStr = stmt.column_string(0);
        sb.append(""\t"").append(TEST_LON + ""/"" + TEST_LAT + ""/EPSG:4326"").append("" = "")//
                .append(pointStr + ""/EPSG:32632"").append(""...\n"");
    }
    stmt.close();
} catch (Exception e) {
    e.printStackTrace();
    sb.append(ERROR).append(e.getLocalizedMessage()).append(""\n"");
}
</code></pre>

<p>In particular, I noticed that poor practice is done of simply stringing together a SQL query, instead of a more proper method, such as is used by the Android SQLite library. Is there a way that I can make Spatialite use true prepared statements?</p>

<p>Just to be clear, I'm looking for something like this, using the standard SQLite database in Android:</p>

<pre><code>String query=""SELECT * FROM table WHERE _id=?"";
Cursor data=db.rawQuery(query,new String[]{id});
</code></pre>
","<p>There are a few tricks. They all use the exec() call, which has 3 arguments for this version. The statement from <a href=""https://github.com/mrenouf/android-spatialite/blob/master/src/jsqlite/Database.java"" rel=""nofollow"">the source code</a> is:</p>

<pre><code>public void exec(String sql, jsqlite.Callback cb, String args[])
</code></pre>

<p>A jsqlite.Callback is an interface, of which there can be several. But the best way seems to be using a <code>db.get_table(query,args)</code> function. <code>%q</code> is the effective replacement for <code>?</code> in the Android SQLite representation. Here's the transformation of the given code:</p>

<pre><code>String query = ""SELECT AsText(Transform(MakePoint(%q, %q, 4326), 32632));"";
TableResult result=db.get_table(query,new String[]{""""+TEST_LONG,""""+TEST_LAT});
</code></pre>

<p>From there, you just have to get the results from TableResult. There isn't a method call to get the results from here, you actually have to grab the publicly declared variable and manually parse through it. Here's an example of how that can be done.</p>

<pre><code>TableResult result=db.get_table(query,new String[]{""""+lng,""""+lat});
Vector&lt;String[]&gt; rows=result.rows;
for (String[] row:rows)
{
    for (String val:row)
    {
        Log.v(TAG,val);
    }
}
</code></pre>

<p>If you aren't doing a select, try something like this:</p>

<pre><code>TableResult result=new TableResult();
db.exec(""ATTACH DATABASE %q AS newDb"",result,new String[]{path});
</code></pre>

<p>I assume the same pattern will work for INSERTS and the like</p>
"
"<p>I am using SSMS v17.6 with a SQL Server Express v14.0.1000.169. </p>

<p>When I run the following <code>DELETE</code> statement:</p>

<pre><code>delete from foo
go
</code></pre>

<p>I get the error:</p>

<blockquote>
  <p>Incorrect syntax near 'go'.</p>
</blockquote>

<p>But if I execute a similar <code>SELECT</code> statement:</p>

<pre><code>select * from foo 
go
</code></pre>

<p>Then there is no error. </p>

<p>It seems the error happens with any statement that doesn't return results. I have checked the query execution settings in SSMS and the batch separator is set to go. It seems SSMS is sending to the server which it shouldn't do.</p>

<p><strong>I have seen this on multiple machines.</strong></p>
","<p>If you are using a newer version of SQL Server as you are, you can should look into <a href=""https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-tables?view=sql-server-2017"" rel=""nofollow noreferrer"">temporal tables</a> as this might be your best option.</p>

<p>If you need to support older versions, my preferred method is to have a history table with a new PK column, change flag (I,U,D), a date modified, user that made the change, and all of the columns from the primary table. I then index the column related to the PK of the  non-history table. Triggers don't impact performance too much if you don't put logic in them.
Example (pseudocode):</p>

<pre><code>Table: Car
Column: CarID INT IDENTITY(1,1) Primary Key
Column: Name varchar

Table: Car_hist
Column: Car_histID INT IDENTITY(1,1) Primary Key
Column: Change char(1)
Column: DateOfChange DateTime2
Column: ChangedByUser (varchar or int)
Column: CarID &lt;-add a unique non-clustered index
Column: Name varchar
</code></pre>

<p>You can write a generator in SQL that generates the script to create the history table, indexes, etc. It helps if you have a consistent table design practice.</p>

<p>Now the reason: I rarely have to query history tables, but when I do, it is almost always for a single record to see what happened and who changed it. This method allows you to select from the history on the parent table's PK value quickly and read it as a historical change log easily (who changed what and when). I don't see how you can do that in your design. If you are really slick, you can find or write a grid that diffs rows for you and you can quickly see what changed.</p>
"
"<p>In order to host my Python/Django app on Heroku, I'm trying to convert my db from MySQL to Postgres following the instructions on <a href=""https://realpython.com/blog/python/migrating-your-django-project-to-heroku/"" rel=""noreferrer"">https://realpython.com/blog/python/migrating-your-django-project-to-heroku/</a>. I'm currently running OSX 10.9, and am using the tool mysql2pgsql to make the transfer. </p>

<p>When I try to run the command ""<code>py-mysql2pgsql -v -f mysql2pgsql.yml</code>"" to actually transfer the db, it copies over the first three tables, and then hits a snag on auth_user, returning the error ""<code>raise Exception('unknown %s' % column['type']) Exception: unknown datetime(6)</code>"". This seems strange, because auth_user is generated by one of Django's default installed apps, so I wouldn't expect it to cause any errors. </p>

<p>Any idea what could be causing this error or what I should be doing differently? Thanks. </p>
","<p>in the directory 'Lib\site-packages\py_mysql2pgsql-0.1.6-py2.7.egg\mysql2pgsql\lib' edit the like 76 postgres_writer.py file 
as</p>

<p>from </p>

<pre><code> elif column['type'] == 'datetime':
</code></pre>

<p>to</p>

<pre><code>elif column['type'] == 'datetime' or column['type'].startswith('datetime('):
</code></pre>

<p>I was facing the same problem, this solution worked for me. </p>
"
"<p>This is my first day with <code>tsqlt</code> so you can expect some vague statements.</p>

<p>I am trying to test a storedProcedure which has a <code>Try Catch Block</code> but the actual statements in test are insert and update command.</p>

<p>now I want to test if in case there was an ErrorRaised does my catch block performs the expected tasks.</p>

<p>Can you please guide me on how do I Raise an Error from a stored procedure in test where we do not have anything to mock/fake inside.</p>

<p>Hope my question is understandable, happy to clarify if needed.</p>
","<p>So if I understand your question correctly, you are trying to test that your catch block works?</p>

<p>The way to do this will depend on what happens within your catch block.  Imagine this scenario:</p>

<pre><code>create table mySimpleTable
(
  Id int not null primary key
, StringVar varchar(8) null 
, IntVar tinyint null
)
go
</code></pre>

<p>We have a stored procedure that inserts data into this table thus.</p>

<p>This is based on a template that I use in many of my procedures.  It starts by validating the input(s), then does the work it needs to do.  Naming each step can be particularly useful to understand where the error occurred in more complex, multi-step procedures.  The catch block uses my Log4TSql logging framework that you can read more about <a href=""http://datacentricity.net/2011/11/t-sql-tuesday-024-exception-handling-made-simple/"" rel=""nofollow"" title=""on my blog"">on my blog</a> and download from <a href=""http://sourceforge.net/projects/log4tsql/"" rel=""nofollow"" title=""SourceForge"">SourceForge</a>.</p>

<p>The pattern I follow is to capture information about the exception and what the procedure was doing at the time of the error within the catch block but ensure that an error is still thrown at the end of the procedure.  You could also choose to call raiserror (also <code>throw</code> on SQL2012) within the catch block.  Either way, I believe that if a procedure hits an exception it should always be notified up the chain (i.e. never hidden). </p>

<pre><code>create procedure mySimpleTableInsert
(
  @Id int
, @StringVar varchar(16) = null
, @IntVar int = null
)
as
begin
    --! Standard/ExceptionHandler variables
    declare @_FunctionName nvarchar(255) = quotename(object_schema_name(@@procid))
             + '.' + quotename(object_name(@@procid));
    declare @_Error int = 0;
    declare @_ReturnValue int;
    declare @_RowCount int = 0;
    declare @_Step varchar(128);
    declare @_Message nvarchar(1000);
    declare @_ErrorContext nvarchar(512);

    begin try
        set @_Step = 'Validate Inputs'
        if @Id is null raiserror('@Id is invalid: %i', 16, 1, @Id);

        set @_Step = 'Add Row'
        insert dbo.mySimpleTable (Id, StringVar, IntVar)
        values (@Id, @StringVar, @IntVar)
    end try
    begin catch
        set @_ErrorContext = 'Failed to add row to mySimpleTable at step: '
                 + coalesce('[' + @_Step + ']', 'NULL')

        exec log4.ExceptionHandler
                  @ErrorContext   = @_ErrorContext
                , @ErrorProcedure = @_FunctionName
                , @ErrorNumber    = @_Error out
                , @ReturnMessage  = @_Message out
        ;
    end catch

    --! Finally, throw any exception that will be detected by the caller
    if @_Error &gt; 0 raiserror(@_Message, 16, 99);

    set nocount off;

    --! Return the value of @@ERROR (which will be zero on success)
    return (@_Error);
end
go
</code></pre>

<p>Let's start by creating a new schema (class) to hold our tests.</p>

<pre><code>exec tSQLt.NewTestClass 'mySimpleTableInsertTests' ;
go
</code></pre>

<p>Our first test is the simplest and just checks that even if an exception is caught by our catch block, an error is still returned by the procedure.  In this test we simply use <code>exec tSQLt.ExpectException</code> to check that an error is raised when @Id is supplied as NULL (which fails our input validation checks)</p>

<pre><code>create procedure [mySimpleTableInsertTests].[test throws error from catch block]
as
begin
    exec tSQLt.ExpectException @ExpectedErrorNumber = 50000;

    --! Act
    exec dbo.mySimpleTableInsert @Id = null
end;
go
</code></pre>

<p>Our second test is a little more complex and makes use of <code>tsqlt.SpyProcedure</code> to ""mock"" the ExceptionHandler that would otherwise record the exception.  Under the hood, when we mock a procedure in this way, tSQLt creates a table named after the procedure being spied and replaces the spied procedure with one that just writes the input parameter values to that table.  This is all rolled back at the end of the test.  This allows us to can check that ExceptionHandler was called and what values were passed into it.  In this test we check that ExceptionHander was called by mySimpleTableInsert as a result of an input validation error.</p>

<pre><code>create procedure [mySimpleTableInsertTests].[test calls ExceptionHandler on error]
as
begin
    --! Set the Error returned by ExceptionHandler to zero so the sproc under test doesn't throw the error
    exec tsqlt.SpyProcedure 'log4.ExceptionHandler', 'set @ErrorNumber = 0;';

    select
          cast('Failed to add row to mySimpleTable at step: [Validate inputs]' as varchar(max)) as [ErrorContext]
        , '[dbo].[mySimpleTableInsert]' as [ErrorProcedure]
    into
        #expected

    --! Act
    exec dbo.mySimpleTableInsert @Id = null

    --! Assert
    select
          ErrorContext
        , ErrorProcedure
    into
        #actual
    from
        log4.ExceptionHandler_SpyProcedureLog;

    --! Assert
    exec tSQLt.AssertEqualsTable '#expected', '#actual';
end;
go
</code></pre>

<p>Finally the following (somewhat contrived) examples use the same pattern to check that an error is caught and thrown if the value of @IntVar is too big for the table:</p>

<pre><code>create procedure [mySimpleTableInsertTests].[test calls ExceptionHandler on invalid IntVar input]
as
begin
    --! Set the Error returned by ExceptionHandler to zero so the sproc under test doesn't throw the error
    exec tsqlt.SpyProcedure 'log4.ExceptionHandler', 'set @ErrorNumber = 0;';

    select
          cast('Failed to add row to mySimpleTable at step: [Add Row]' as varchar(max)) as [ErrorContext]
        , '[dbo].[mySimpleTableInsert]' as [ErrorProcedure]
    into
        #expected

    --! Act
    exec dbo.mySimpleTableInsert @Id = 1, @IntVar = 500

    --! Assert
    select
          ErrorContext
        , ErrorProcedure
    into
        #actual
    from
        log4.ExceptionHandler_SpyProcedureLog;

    --! Assert
    exec tSQLt.AssertEqualsTable '#expected', '#actual';
end;
go
create procedure [mySimpleTableInsertTests].[test throws error on invalid IntVar input]
as
begin
    exec tSQLt.ExpectException @ExpectedErrorNumber = 50000;

    --! Act
    exec dbo.mySimpleTableInsert @Id = 1, @IntVar = 500
end;
go
</code></pre>

<p>If this doesn't answer your question, perhaps you could post an example of what you are trying to achieve.</p>
"
"<p>I have a database table with 6 columns. The primary key is a composite key made up of 5 of the 6 columns</p>

<p>I am trying to use the <code>SqlClient.SqlCommandBuilder.GetDeleteCommand</code> to delete the row.</p>

<p>However I am getting the following error:</p>

<blockquote>
  <p>""System.InvalidOperationException : Dynamic SQL generation for the
  DeleteCommand is not supported against a SelectCommand that does not
  return any key column information.""</p>
</blockquote>

<p>The <code>SelectCommmand</code> contains all the columns in the table:</p>

<pre><code>SELECT  TABLENAME.COL1, TABLENAME.COL2, TABLENAME.COL3, 
        TABLENAME.COL4, TABLENAME.COL5, TABLENAME.COL6
FROM TABLENAME  
</code></pre>

<p>Could the problem be the composite key?</p>
","<p>Your information supplied is useless.  But I can explain the meaning of the error.</p>

<p>Every update command written in ADO.Net is of the form: </p>

<pre><code>Update col1, col2 where col1=col1value AND col2=col2value
</code></pre>

<p>ADO.Net keeps the value of the column when it was selected from the database. When it performs the update the condition is that none of the columns have changed when you commit.</p>

<p>The reason you see the error is because the database row has changed in between you performing the select and calling <code>da2.UpdateChanges(ds2)</code>.  If you look at the logic perhaps you have selected the value for the row into two separate datasets (or in two different threads) and performed an update to it after performing the select.</p>
"
"<p>I'm supporting an IIS web application that constructs and sends SELECT statements to SQL Server. Sometimes the statements are not very efficient or are against quite large tables so they take three or four minutes to complete when run from SQL Management Studio. When the statements are sent from the application, the following time-out is reported by it:</p>

<blockquote>
  <p>ERROR [HYT00] [Microsoft][ODBC SQL Server Driver]Timeout expired SQL:
  SELECT ... large statement here ...</p>
</blockquote>

<p>It's not possible to (immediately) improve the SQL statements sent so I need to temporarily increase whatever time-outs are being hit. But I cannot seem to find a time-out that corresponds to this error message. I am hoping that someone here can tell me what time-out it refers to and where it can be viewed/changed?</p>
","<p>Note you can set the protocol used in the connection string, there is no need to configure the machine.   I would recommend NOT changing the machine configuration using cliconfg, since this impacts all applications running on the machine.</p>

<p>So there are 2 ways to set the network protocol.</p>

<ol>
<li><p>Use the protocol prefix:</p>

<p><code>Server=tcp:myserver</code><br>
<code>Server=np:myserver</code></p>

<p><code>tcp:</code> prefix means use tcp protocol.  <code>np:</code> prefix means use named pipes protocol.  Just stick this in front of the server name you are connecting to.</p></li>
<li><p>Second ways is to set the Network keyword in the connection string:</p>

<p><code>Network=dbmssocn</code><br>
<code>Network=dbnmpntw</code></p></li>
</ol>

<p>I prefer the protocol prefix because I can never remember these network type abbreviations.</p>
"
"<p>I just got started looking at Lua as an easy way to access the SQLite DLL, but I ran into an error while trying to use the DB-agnostic LuaSQL module:</p>

<pre><code>require ""luasql.sqlite""
module ""luasql.sqlite""

print(""Content-type: Text/html\n"")

print(""Hello!"")
</code></pre>

<p>Note that I'm trying to start from the most basic setup, so only have the following files in the work directory, and sqlite.dll is actually the renamed sqlite3.dll from the <a href=""http://luaforge.net/frs/?group_id=12"" rel=""noreferrer"">LuaForge</a> site:</p>

<pre>
Directory of C:\Temp
&lt;DIR&gt; luasql
lua5.1.exe
lua5.1.dll
hello.lua

Directory of C:\Temp\luasql
sqlite.dll
</pre>

<p>Am I missing some binaries that would explain the error?</p>

<p>Thank you.</p>

<hr>

<p>Edit: I renamed the DLL to its original sqlite3.dll and updated the source to reflect this (originally renamed it because that's how it was called in a sample I found).</p>

<p>At this point, here's what the code looks like...</p>

<pre><code>require ""luasql.sqlite3""

-- attempt to call field 'sqlite' (a nil value)
env = luasql.sqlite()

env:close()
</code></pre>

<p>...  and the error message I'm getting:</p>

<pre><code>C:\&gt;lua5.1.exe hello.lua
lua5.1.exe: hello.lua:4: attempt to call field 'sqlite' (a nil value)
</code></pre>

<hr>

<p>Edit: Found what it was: env = luasql.sqlite3() instead of env = luasql.sqlite().</p>

<p>For newbies like me, here's a complete example with the latest <a href=""http://www.keplerproject.org/luasql/"" rel=""noreferrer"">SQLite LuaSQL driver</a>:</p>

<pre><code>require ""luasql.sqlite3""

env = luasql.sqlite3()
conn = env:connect(""test.sqlite"")

assert(conn:execute(""create table if not exists tbl1(one varchar(10), two smallint)""))
assert(conn:execute(""insert into tbl1 values('hello!',10)""))
assert(conn:execute(""insert into tbl1 values('goodbye',20)""))

cursor = assert(conn:execute(""select * from tbl1""))
row = {}
while cursor:fetch(row) do
    print(table.concat(row, '|'))
end

cursor:close()
conn:close()

env:close()
</code></pre>

<p>Thank you.</p>
","<p>You could try</p>

<pre><code>luasql = require ""luasql.postgres""
env = assert (luasql.postgres())
</code></pre>
"
"<p>I'm using SQL Server CE as my database.</p>

<p>Can I create a view in SQL Server CE 3.5 ? I tried to create but its saying create view statement not supported.</p>

<p>In my application I have table called <code>Alarm</code> with 12 columns. But I'm always accessing only
three columns. So I want to create the view with those three columns.</p>

<p>Will it improve performance?</p>
","<p>Instead of <code>true</code> and <code>false</code> use <code>1</code> and <code>0</code>. Eg:</p>

<pre><code>insert into EMP(ROW_ID, NAME, TEST)
values('123','XYZ',1);
</code></pre>

<p>This is for <a href=""http://msdn.microsoft.com/en-us/library/ms177603%28v=sql.90%29.aspx"">SQL Server 2005 <code>bit</code></a>:</p>

<blockquote>
  <p>The string values TRUE and FALSE can be converted to bit values: TRUE is converted to 1 and FALSE is converted to 0. </p>
</blockquote>

<p>You can try it. If it applies to CE following code (<code>'TRUE'</code> as string) might work as well:</p>

<pre><code>insert into EMP(ROW_ID, NAME, TEST)
values('123','XYZ', 'TRUE');
</code></pre>
"
"<p>I am using a platform (perfectforms) that requires me to use stored procedures for most of my queries, and having never used stored procedures, I can't figure out what I'm doing wrong.  The following statement executes without error:</p>

<pre><code>DELIMITER //
DROP PROCEDURE IF EXISTS test_db.test_proc//
CREATE PROCEDURE test_db.test_proc() SELECT 'foo'; //
DELIMITER ;
</code></pre>

<p>But when I try to call it using:</p>

<pre><code>CALL test_proc();
</code></pre>

<p>I get the following error:</p>

<pre><code>#1312 - PROCEDURE test_db.test_proc can't return a result set in the given context
</code></pre>

<p>I am executing these statements from within phpmyadmin 3.2.4, PHP Version 5.2.12 and the mysql server version is 5.0.89-community.</p>

<p>When I write a stored procedure that returns a parameter, and then select it, things work fine (e.g.):</p>

<pre><code>DELIMITER //
DROP PROCEDURE IF EXISTS test_db.get_sum//
CREATE PROCEDURE test_db.get_sum(out total int)
BEGIN
SELECT SUM(field1) INTO total FROM test_db.test_table;
END //
DELIMITER ;
</code></pre>

<p>works fine, and when I call it:</p>

<pre><code>CALL get_sum(@t); SELECT @t;
</code></pre>

<p>I get the sum no problem.</p>

<p>Ultimately, what I need to do is have a fancy SELECT statement wrapped up in a stored procedure, so I can call it, and return multiple rows of multiple fields.  For now I'm just trying to get <em>any</em> select working.</p>

<p>Any help is greatly appreciated.</p>
","<p>Figured it out.  This is not a bug with PHP (though it used to be) - it's a bug in some versions of phpmyadmin.  The same bug intermittently reappears and is then fixed in various subversions (see above):</p>

<pre><code>#1312 - PROCEDURE [name] can't return a result set in the given context
</code></pre>

<p>This behavior appears limited to <strong><em>SELECT statements within stored procedures inside phpmyadmin</em></strong>.</p>

<p>Using a client like MySQL Workbench works around the problem (or you could upgrade phpmyadmin, but that's a pain if you're on a shared server like I am).</p>

<p>Anyway, thanks to everyone for your help.</p>
"
"<p>With a <code>date</code> field I can do <a href=""https://stackoverflow.com/a/12402084/96588"">this</a>:</p>

<pre><code>ORDER BY ABS(expiry - CURRENT_DATE)
</code></pre>

<p>With a <code>timestamp</code> field I get the following error:</p>

<blockquote>
  <p>function abs(interval) does not exist</p>
</blockquote>
","<p>Use <a href=""http://www.postgresql.org/docs/current/interactive/functions-datetime.html#FUNCTIONS-DATETIME-CURRENT"" rel=""noreferrer"">now() or CURRENT_TIMESTAMP</a> for the purpose.</p>

<p>The reason for the different outcome of your queries is this:</p>

<p>When you subtract two values of type <code>date</code>, the result is an <strong><code>integer</code></strong> and <code>abs()</code> is applicable.<br>
When you subtract two values of type <code>timestamp</code> (or just one is a <code>timestamp</code>), the result is an <strong><code>interval</code></strong>, and <code>abs()</code> is not applicable. You could substitute with a <code>CASE</code> expression:</p>

<pre><code>ORDER BY CASE WHEN expiry &gt; now() THEN expiry - now() ELSE now() - expiry END
</code></pre>

<p>Or you can <a href=""http://www.postgresql.org/docs/current/interactive/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT"" rel=""noreferrer""><code>extract()</code></a> the unix <code>epoch</code> from the resulting <code>interval</code> like @Craig already demonstrated. I quote: ""for interval values, the total number of seconds in the interval"". Then you can use <code>abs()</code> again:</p>

<pre><code>ORDER BY abs(extract(epoch from (expiry - now())));
</code></pre>

<p><a href=""http://www.postgresql.org/docs/9.1/interactive/functions-datetime.html#FUNCTIONS-DATETIME-TABLE"" rel=""noreferrer""><code>age()</code></a> would just add a more human readable representation to the interval by summing up days into months and years for for bigger intervals. But that's beside the point: the value is only used for sorting.</p>

<p>As your column is of type timestamp, you should use <code>CURRENT_TIMESTAMP</code> (or <code>now()</code>) instead of  <code>CURRENT_DATE</code>, or you will get inaccurate results (or even incorrect for ""today"").</p>
"
"<p>The error is</p>

<pre><code>*** Exception: Incompatible {errSQLType = ""int8"", errHaskellType = ""Int"", errMessage = ""types incompatible""}
</code></pre>

<p>It looks like any value returned by <code>count(*)</code> in the query must be converted into <code>Integer</code> rather than <code>Int</code>. If I change those specific variables to type Integer, the queries work.</p>

<p>But this error wasn't being raised on another machine with the same exact code. The first machine was 32 bit and this other one 64-bit. That's the only difference I could discern.</p>

<p>Does anyone have any insight into what is going on?</p>
","<p>The module <code>Database.PostgreSQL.Simple</code> only exports the type classes <code>ToRow</code> and <code>FromRow</code>, without any of their methods.  </p>

<p>For those methods you need to import the modules <code>Database.PostgreSQL.Simple.ToRow</code> and <code>Database.PostgreSQL.Simple.FromRow</code>, as is done in the sample you linked to.</p>
"
"<p>I am using MySQL v5.1.36 and I am trying to create a stored function using this code.</p>

<pre><code>DELIMITER //
CREATE FUNCTION `modx`.getSTID (x VARCHAR(255)) RETURNS INT DETERMINISTIC
    BEGIN
    DECLARE y INT;
    SELECT id INTO y
    FROM `modx`.coverage_state
    WHERE `coverage_state`.name = x;
    RETURN y;
    END//
</code></pre>

<p>When entered into the MySQL Console I get this response.</p>

<pre><code>mysql&gt;  DELIMITER //
mysql&gt;  CREATE FUNCTION `modx`.getSTID (x VARCHAR(255)) RETURNS INT DETERMINISTIC
    -&gt;          BEGIN
    -&gt;          DECLARE y INT;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that
corresponds to your MySQL server version for the right syntax to use near '' at
line 3
mysql&gt;          SELECT id INTO y
    -&gt;          FROM `modx`.coverage_state
    -&gt;          WHERE `coverage_state`.name = x;
ERROR 1327 (42000): Undeclared variable: y
mysql&gt;          RETURN y;
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that
corresponds to your MySQL server version for the right syntax to use near 'RETUR
N y' at line 1
mysql&gt;          END//
</code></pre>

<p>From what I can find online my syntax is correct. What am I doing wrong?</p>
","<p>When creating a function/procedure from mysql console, the first command should be <code>DELIMITER //</code>. Otherwise, it uses default delimiter (<code>;</code>),</p>
"
"<p>Trying to install oursql driver for python3x and sqlalchemy0.8 on ubuntu 12.10. It fails with the following error. </p>

<pre><code>sudo pip-3.2 install oursql
Downloading/unpacking oursql
Running setup.py egg_info for package oursql
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 16, in &lt;module&gt;
  File ""/tmp/pip-build/oursql/setup.py"", line 53
    print ""cython not found, using previously-cython'd .c file.""
                                                               ^
SyntaxError: invalid syntax
Complete output from command python setup.py egg_info:
Traceback (most recent call last):

File ""&lt;string&gt;"", line 16, in &lt;module&gt;

File ""/tmp/pip-build/oursql/setup.py"", line 53

print ""cython not found, using previously-cython'd .c file.""

                                                           ^

SyntaxError: invalid syntax
</code></pre>

<p>When I try to install <strong>cython</strong> I seem to already have it:</p>

<pre><code>sudo pip-3.2 install cython
Requirement already satisfied (use --upgrade to upgrade): cython in /usr/local/lib/python3.2/dist-packages
Cleaning up.
</code></pre>

<p>What can I do to make it run?</p>
","<p>The difference is that MySQLdb does some hackery to your query while oursql does not...</p>

<p>Taking this:</p>

<pre><code>cursor.executemany(""INSERT INTO sometable VALUES (%s, %s, %s)"",
    [[1,2,3],[4,5,6],[7,8,9]])
</code></pre>

<p>MySQLdb translates it before running into this:</p>

<pre><code>cursor.execute(""INSERT INTO sometable VALUES (1,2,3),(4,5,6),(7,8,9)"")
</code></pre>

<p>But if you do:</p>

<pre><code>cursor.executemany(""INSERT INTO sometable VALUES (?, ?, ?)"",
    [[1,2,3],[4,5,6],[7,8,9]])
</code></pre>

<p>In oursql, it gets translated into something like this pseudocode:</p>

<pre><code>stmt = prepare(""INSERT INTO sometable VALUES (?, ?, ?)"")
for params in [[1,2,3],[4,5,6],[7,8,9]]:
    stmt.execute(*params)
</code></pre>

<p>So if you want to emulate what mysqldb is doing but benefit from prepared statements and other goodness with oursql, you need to do this:</p>

<pre><code>from itertools import chain
data = [[1,2,3],[4,5,6],[7,8,9]]
one_val = ""({})"".format(','.join(""?"" for i in data[0]))
vals_clause = ','.join(one_val for i in data)
cursor.execute(""INSERT INTO sometable VALUES {}"".format(vals_clause),
    chain.from_iterable(data))
</code></pre>

<p>I bet oursql will be faster when you do this :-)</p>

<p>Also, if you think its ugly, you are right. But just remember MySQL db is doing something uglier internally - its using regular expressions to parse your INSERT statement and break off the parameterized part and THEN doing what I suggested you do for oursql.</p>
"
"<p>I have a web app in azure which let user to input some data. I want to save them in sql db which I created in azure. Is there a way to check/view the entered data . I am referring to something like ""mysql workbench for mysql""</p>
","<p>You may also view the data in your browser using <a href=""https://portal.azure.com/"" rel=""noreferrer"">Azure Portal</a></p>

<p>Select your DB and then ""Query editor"" in the menu and start querying.</p>

<p><a href=""https://i.stack.imgur.com/fWOY5.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fWOY5.png"" alt=""...""></a></p>
"
"<pre><code>D:\Python27&gt;python sqlmap\sqlmap.py -u www.mail.ru --tor

    sqlmap/1.0-dev - automatic SQL injection and database takeover tool
    http://sqlmap.org

[!] legal disclaimer: Usage of sqlmap for attacking targets without prior mutual
 consent is illegal. It is the end user's responsibility to obey all applicable
local, state and federal laws. Developers assume no liability and are not respon
sible for any misuse or damage caused by this program

[*] starting at 22:28:49

[22:28:49] [WARNING] increasing default value for option '--time-sec' to 10 beca
use switch '--tor' was provided
[22:28:49] [INFO] setting Tor HTTP proxy settings
[22:28:52] [CRITICAL] can't establish connection with the Tor proxy. Please make
 sure that you have Vidalia, Privoxy or Polipo bundle installed for you to be ab
le to successfully use switch '--tor' (e.g. https://www.torproject.org/projects/
vidalia.html.en)

[*] shutting down at 22:28:52
</code></pre>

<p>[CRITICAL] can't establish connection with the Tor proxy.
Vidalia Tor already instaled and runing.</p>

<p>How to use TOR with <a href=""http://sqlmap.org/"">sqlmap</a>?</p>
","<p>the p option can be used in the following way</p>

<p><code>-u ""http://localhost/vuln/test.php?feature=music&amp;song=1"" -p song</code></p>
"
"<p>I usually use linux for mysql and have something called mysqltuner.pl and this is great but we have a mysql server on a windows 2008 r2 server that we have and was wondering if any of you know of something simular to mysqltuner that i can use.  I have seen on the project page that they have a windows version but it only works on winXP.</p>

<p>Any pointers or if anyone has ported there own version would be great.</p>

<p>many thanks</p>
","<p>When it come to optimization, I believe in ""don't fix if it isn't broke"". that said heres an excellent post regarding temp tables on disk</p>

<p><a href=""https://dba.stackexchange.com/questions/17677/why-is-mysql-is-creating-so-many-temporary-tables-on-disk"">https://dba.stackexchange.com/questions/17677/why-is-mysql-is-creating-so-many-temporary-tables-on-disk</a></p>

<p>if you are not sure what to do with those settings then it is a huge sign saying ""Do not touch"". Maybe you should change your question a bit, tell everyone WHY would you want to optimize the DB. Is it because something ran extremly slow? Is it because the DB is taking up too much resource of the server? Optimization without a specific goal is quite dangerous, optimization means tweaking a system in a way that best suits the way you use it, which implies a  lot of the time you are trading the preformance in some other area that you don't need, which in your case we have no idea what you don't need. </p>
"
"<p>I've set up Postgres 9.6 and checked on a large table of random integers that parallel queries are working.
However, a simple XPath query on an XML column of another table is always sequential. Both XPath functions are marked as parallel safe in Postgres. I tried to alter XPath cost, so the expected cost skyrocketed, but it didn't change anything.
What am I missing?</p>

<p>Example table DDL:
<code>CREATE TABLE ""test_table"" (""xml"" XML );</code></p>

<p>Example query:
<code>SELECT xpath('/a', ""xml"") FROM ""test_table"";</code></p>

<p>Example data:
<code>&lt;a&gt;&lt;/a&gt;</code>.
Note that real data contains XMLs that are 10-1000kB in size.</p>

<pre><code>&gt; select pg_size_pretty(pg_total_relation_size('test_table'));
28 MB

&gt; explain (analyze, verbose, buffers) select xpath('/a', ""xml"") from test_table;
Seq Scan on public.test_table  (cost=0.00..64042.60 rows=2560 width=32) (actual time=1.420..4527.061 rows=2560 loops=1)
  Output: xpath('/a'::text, xml, '{}'::text[])
  Buffers: shared hit=10588
Planning time: 0.058 ms
Execution time: 4529.503 ms
</code></pre>
","<p>The relevant point here is likely the distinction between ""relation size"" and ""total relation size"":</p>

<pre><code>CREATE TABLE test_table AS
  SELECT ('&lt;a&gt;' || repeat('x', 1000000) || '&lt;/a&gt;')::xml AS ""xml""
  FROM generate_series(1, 2560);

SELECT
  pg_size_pretty(pg_relation_size('test_table')) AS relation_size,
  pg_size_pretty(pg_total_relation_size('test_table')) AS total_relation_size;

 relation_size | total_relation_size
---------------+---------------------
 136 kB        | 30 MB
</code></pre>

<p>Large column values like these are not stored within the main relation, but instead are pushed to its associated <a href=""https://www.postgresql.org/docs/current/static/storage-toast.html"" rel=""nofollow noreferrer"">TOAST table</a>. This external storage does not count towards <code>pg_relation_size()</code>, which is what the optimiser appears to be comparing against <a href=""https://www.postgresql.org/docs/current/static/runtime-config-query.html#GUC-MIN-PARALLEL-RELATION-SIZE"" rel=""nofollow noreferrer""><code>min_parallel_relation_size</code></a> when evaluating a parallel plan:</p>

<pre><code>SET parallel_setup_cost = 0;
SET parallel_tuple_cost = 0;
SET min_parallel_relation_size = '144kB';
EXPLAIN SELECT xpath('/a', ""xml"") FROM test_table;

                          QUERY PLAN
---------------------------------------------------------------
 Seq Scan on test_table  (cost=0.00..49.00 rows=2560 width=32)
</code></pre>



<pre><code>SET min_parallel_relation_size = '136kB';
EXPLAIN SELECT xpath('/a', ""xml"") FROM test_table;

     QUERY PLAN
------------------------------------------------------------------------------
 Gather  (cost=0.00..38.46 rows=2560 width=32)
   Workers Planned: 1
   -&gt;  Parallel Seq Scan on test_table  (cost=0.00..35.82 rows=1506 width=32)
</code></pre>
"
"<p>Right. So I've created a stored procedure in a MySQL DB which happens to use SUBSTRING.</p>

<p>Running the procedure via a query gives me:</p>

<blockquote>
  <p>SQL Error 1630: Function mydatabase.SUBSTRING does not exist</p>
</blockquote>

<p>Beg your pardon?</p>
","<p>Is there a space after the method call to <code>Substring</code> before the first parenthesis?</p>

<p>It appears on Line 40: </p>

<pre><code> IF i &gt; 1 AND j &gt; 1 AND (s1_char = SUBSTRING (s2, j - 1, 1))
</code></pre>

<p>i.e. Ensure </p>

<pre><code>select substring(CustomerName, 1, 4) AS CustName from MyTable;
</code></pre>

<p>instead of:</p>

<pre><code>select substring (CustomerName, 1, 4) AS CustName from MyTable;
</code></pre>
"
"<p>I have been using a SQL Server project to manager the structure of a database.</p>

<p>First I created the project, then imported a database.  </p>

<p>Then, when I need to make a schema change, say change a field name, I do it in the SQL Server Project, and then publish it to the actual database using the Schema Compare Option.</p>

<p>I would like to take this a step further to hold basic data that the database needs. Say for example a <code>OrderType</code> table with 2 records ""PHONE"", ""WEB""</p>

<p>This data will be needed on all new instances of the database. Is there a way to keep these in the SQL Server project as well so that they don't get lost?</p>

<p>It seems the only way to do this now, is to keep an actual copy of the master database with the metadata in it, and then use a data-compare. But it would be great if the data could just get published at the same time as the schema so the resulting database is complete.</p>
","<p>There are two ways to preserve static data and publish it with a database.</p>

<ol>
<li><p>Have a ""reference"" database with static data populated.  At the time of publishing a new instance, SQL Server Data tools has a ""Data Compare"" tool which allows you to compare to live databases, and creates a custom script to update one database with data from the other.</p></li>
<li><p>Create scripts that contain insert statements, and then run these scripts at publish time.  SQL Server Data tools has two tools to assist in this.   </p>

<p>a.  Open the data table (right click on SQL Server object explorer, and select ""View Data""), and then click on the ""Script"" button at the top.  It will create an insert script for all rows in the table.  <a href=""https://msdn.microsoft.com/en-us/library/dn266029(v=vs.103).aspx"" rel=""nofollow"">More on Comparing Data from MSDN</a>  </p>

<p>b.  Then take this created script, and add it to the Database Project as a ""Post Deployment"" script.  When you create a publish script for the database, any Post Deployment scripts in the project are automatically included in the master script. <a href=""https://msdn.microsoft.com/en-us/library/aa833410(v=vs.100).aspx"" rel=""nofollow"">More on post deployment scripts from MSDN</a></p></li>
</ol>
"
"<p>The following command fails</p>

<pre><code>mysqldbcompare --server1=un:pw@server1 --server2=un:pw@server2 --difftype=sql store-staging:store-beta
</code></pre>

<p>with the following error:</p>

<blockquote>
  <p>mysqldbcompare: error: Cannot parse the specified database(s): 'store-staging:store-beta'. Please verify that the database(s) are specified in a valid format (i.e., db1[:db2]) and that backtick quotes are properly used when required. The use of backticks is required if non alphanumeric characters are used for database names. Parsing the specified database results in db1 = 'store' and db2 = 'store'.</p>
</blockquote>

<p>My question is how can I 'escape' the schemas so that they can be run as part of this command? </p>

<p>I have tried all of the following:</p>

<pre><code>'store-staging:store-beta'
""store-staging:store-beta""
`store-staging:store-beta`

'store-staging':'store-beta'
""store-staging"":""store-beta""
`store-staging`:`store-beta`
</code></pre>

<p>and they all fail.</p>
","<p>It is probable that any backticks you intend for <code>mysqldbcompare</code> are actually being interpreted by the shell before <code>mysqldbcompare</code> actually sees them.</p>

<p>Try including the backticks inside quotes to ensure that they get properly passed, so your command looks something like this:</p>

<p><code>mysqldbcompare --server1=un:pw@server1 --server2=un:pw@server2 --difftype='`sql store-staging`:`store-beta`'</code></p>
"
"<p>Does anyone know why that was canned a long time ago? Seemed like it wasn't such a bad idea. My very naive guess is that ORM tools didn't need SQLJ and also plugged the gap.
Anyway, still curious.</p>
","<p>This isn't an authoritative answer, but I can see these reasons:</p>

<ul>
<li>In the early 2000's, powerful IDEs (Eclipse, Netbeans, JBuilder) made their way into the Java ecosystem. SQLJ doesn't work well with IDEs</li>
<li>In the mid 2000's, everyone thought SQL itself was dead as ORMs started to hide SQL away</li>
<li>Today, there are more typesafe alternatives like <a href=""http://www.jooq.org"" rel=""nofollow noreferrer"">jOOQ</a> (or LINQ in .NET. Disclaimer: I work for the company  behind jOOQ).</li>
</ul>

<p>Besides, there are two major flaws to SQLJ:</p>

<ul>
<li>SQLJ simplified embedding SQL, but never brought any additional compile-time safety, such as type checks and syntax checks.</li>
<li>SQLJ is good for static SQL, but dynamic SQL (with dynamic predicate or table source composition) cannot be achieved easily.</li>
</ul>
"
"<p>I am using <code>encrypted-core-data</code> to encrypts all data that is persisted , previously simple <code>CoreData</code> was using. <code>persistentStoreCoordinator</code> creation code are as follows . </p>

<pre><code>- (NSPersistentStoreCoordinator *)persistentStoreCoordinator {
if (_persistentStoreCoordinator != nil) {
    return _persistentStoreCoordinator;
}

NSURL *oldStoreURL = [[self applicationDocumentsDirectory] URLByAppendingPathComponent:@""VistaJetApp.sqlite""];
NSURL *newStoreURL = [[self applicationDocumentsDirectory] URLByAppendingPathComponent:@""VistaJet.sqlite""];
NSError *error = nil;

NSString *currentPassword = [[VJAesCryptoWrapper getInstance] getCurrentPassword];
NSDictionary *options = [self getEncryptedStoreOptionsWithPassword:currentPassword andDatabaseStore:newStoreURL];

_persistentStoreCoordinator = [[NSPersistentStoreCoordinator alloc] initWithManagedObjectModel:[self managedObjectModel]];

//if old store not exists , it means fresh installation
if([[NSFileManager defaultManager] fileExistsAtPath:oldStoreURL.path] == NO) {             
    if (![_persistentStoreCoordinator addPersistentStoreWithType:EncryptedStoreType configuration:nil URL:newStoreURL options:options error: &amp;error]) {         
    }        
} else {

    if (![_persistentStoreCoordinator addPersistentStoreWithType:NSSQLiteStoreType configuration:nil URL:oldStoreURL options:@{NSMigratePersistentStoresAutomaticallyOption:@YES, NSInferMappingModelAutomaticallyOption:@YES} error: &amp;error]) {                     
    }

    NSPersistentStore *oldUnsecureStore = [_persistentStoreCoordinator persistentStoreForURL:oldStoreURL];
    [ConsoleLogger logText:[NSString stringWithFormat:@""Migration started""]];

    //start migration
    if(![_persistentStoreCoordinator migratePersistentStore:oldUnsecureStore toURL:newStoreURL options:options withType:EncryptedStoreType error:&amp;error]) {


    } else {
        [[NSFileManager defaultManager] removeItemAtURL:oldStoreURL error:nil];
    }

}

return _persistentStoreCoordinator;

}
</code></pre>

<p>Creation of options dictionary</p>

<pre><code>- (NSDictionary*)getEncryptedStoreOptionsWithPassword:(NSString*)password andDatabaseStore:(NSURL*)storeUrl {
return @{ EncryptedStorePassphraseKey: password,
          EncryptedStoreDatabaseLocation: storeUrl,
          NSMigratePersistentStoresAutomaticallyOption:@YES,
          NSInferMappingModelAutomaticallyOption:@YES
          };
}
</code></pre>

<p>I am saving password in keychain using <code>KeychainItemWrapper</code> and my code crashing exactly on <code>getEncryptedStoreOptionsWithPassword:currentPassword</code> method. App is live and I am not able to reproduce the crash , but on crashlytics it is showing so many crashes</p>

<p><a href=""https://drive.google.com/file/d/1HOR95UtUKU2Jm-e0s1mMLLQtdXjBAVoB/view?usp=sharing"" rel=""nofollow noreferrer"">crashlytics crash logs image</a></p>

<p>Also using <code>AESCrypt</code> to encrypt  password then saving it to keychain using <code>KeychainItemWrapper</code>.</p>

<p><strong>Observation:</strong><br>
The crash that crashlytics is displaying only appear when we upload build on test flight using distribution profile. </p>

<p>crash is happening 100% on iOS 11 as reported by crashlytics</p>
","<p>You can't use a single persistent store coordinator without merging the models. However, you don't have to edit your data models-- you can merge them at run time. <code>NSManagedObjectModel</code> offers a couple of different ways to merge multiple models into a single unified model. If you load each model independently and merge them in code, you get a single <code>NSManagedObjectModel</code> representing the combined model from each model file. You could then use that combined model with a single persistent store coordinator.</p>

<p>If you're still using multiple model files, you can add each one separately. This raises a complication though-- how will Core Data know which model file to use when you create a new model object instance? You would have to use the <code>assignObject:toPersistentStore:</code> method on <code>NSManagedObjectContext</code> to tell it which one to use. Every time you create a new instance, you do this as well. This means that you need to keep references to the <code>NSPersistentStore</code> instances for each file, and know which to use in every case.</p>

<p>I should add that I have not used encrypted Core Data so I don't know if this will solve your real problem. This approach will allow multiple model files and multiple persistent stores with a single coordinator, though.</p>
"
"<p>In my application, a user has_many tickets.  Unfortunately, the tickets table does not have a user_id: it has a user_login (it is a legacy database).  I am going to change that someday, but for now this change would have too many implications.</p>

<p>So how can I build a ""user has_many :tickets"" association through the <strong>login</strong> column?</p>

<p>I tried the following finder_sql, but it does not work.</p>

<pre><code>class User  &lt; ActiveRecord::Base
  has_many :tickets,
      :finder_sql =&gt; 'select t.* from tickets t where t.user_login=#{login}'
  ...
end
</code></pre>

<p>I get a weird error:</p>

<pre><code>ArgumentError: /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:402:in `to_constant_name': Anonymous modules have no name to be referenced by
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/base.rb:2355:in `interpolate_sql'
    from /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:214:in `qualified_name_for'
    from /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:477:in `const_missing'
    from (eval):1:in `interpolate_sql'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/association_proxy.rb:95:in `send'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/association_proxy.rb:95:in `interpolate_sql'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/has_many_association.rb:143:in `construct_sql'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/has_many_association.rb:6:in `initialize'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations.rb:1032:in `new'
    from /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations.rb:1032:in `tickets'
    from (irb):1
</code></pre>

<p>I also tried this finder_sql (with double quotes around the login):</p>

<pre><code>:finder_sql =&gt; 'select t.* from tickets t where t.user_login=""#{login}""'
</code></pre>

<p>But it fails the same way (and anyway, if it worked it would be vulnerable to sql injection).</p>

<p>In a test database, I added a user_id column in the tickets table, and tried this finder_sql:</p>

<pre><code>:finder_sql =&gt; 'select t.* from tickets t where t.user_login=#{id}'
</code></pre>

<p>Now this works fine.  So apparently, my problem has to do with the fact that the users column I am trying to use is a string, not an id.</p>

<p>I searched the net for quite some time... but could not find a clue.</p>

<p>I would love to be able to pass any parameter to the finder_sql, and write things like this:</p>

<pre><code>has_many :tickets_since_subscription,
:finder_sql =&gt; ['select t.* from tickets t where t.user_login=?'+
     ' and t.created_at&gt;=?', '#{login}', '#{subscription_date}']
</code></pre>

<p>Edit: I cannot use the :foreign_key parameter of the has_many association because my users table <em>does</em> have an id primary key column, used elsewhere in the application.</p>

<p>Edit#2: apparently I did not read the documentation thoroughly enough: the has_many association can take a :primary_key parameter, to specify which column is the local primary key (default id).  Thank you Daniel for opening my eyes!  I guess it answers my original question:</p>

<pre><code>has_many tickets, :primary_key=""login"", :foreign_key=""user_login""
</code></pre>

<p>But I would still love to know how I can make the has_many :tickets_since_subscription association work.</p>
","<p>I think what you're actually looking for is this:</p>

<pre><code>has_many :posts, :finder_sql =&gt;
    proc {""SELECT p.* from posts p join topics t on p.topic_id = t.id where t.id=#{id}""}
</code></pre>

<p>As of Rails 3.1 you have to use a proc instead of a string to use fields like <code>#{id}</code>.</p>

<p>See the issue here: <a href=""https://github.com/rails/rails/issues/3920"" rel=""noreferrer"">https://github.com/rails/rails/issues/3920</a></p>
"
"<p>Our application uses SQL Server LocalDb 2014 as the database engine. The connection string we use is </p>

<pre><code>""Data Source=(localdb)\MSSQLLOCALDB;Initial Catalog=OurDatabase;MultipleActiveResultSets=True;Integrated Security=True;AttachDBFilename=|DataDirectory|OurDatabase.mdf""
</code></pre>

<p>Now, on just one of our computers, it has VS 2015SP3 and the latest version of the SQL Server objects installed, our application starts using SQL Server LocalDb 2016. This is undesirable as we exchange back-ups of the database files regularly between computers and now the back-ups that are made in the LocalDb 2016 format cannot be read on computers that do not have LocalDb 2016.</p>

<p>The problem is that the connection string does not specify which version of LocalDb should be used. It there a way to force LocalDb 2014 (or 2016, if we decide to upgrade?)</p>
","<p>The fix for this was indeed to update the version of the <code>Microsoft.SqlServer.Dac</code> assemblies that we were using - I'd discovered that I should try that shortly before seeing @kevin-cunnane 's suggestion.</p>

<p>There were a few factors that made this less than obvious, which is why it's on SO:</p>

<ol>
<li>Dac's error message ""Unable to connect to target server"" doesn't indicate a version incompatibility in any way. However from poking around the internet (eg <a href=""https://stackoverflow.com/questions/35952040/dacpac-wont-deploy-because-cant-connect-to-server"">DACPAC won&#39;t deploy because &#39;can&#39;t connect to server&#39;?</a>) it seems this error message can mean a version incompatibility in addition to incorrect connection string, firewall issue, etc.</li>
<li>There are several NuGet packages posted that contain the <code>Microsoft.SqlServer.Dac</code> and related assemblies. A few of them are not maintained by Microsoft, including the one I was using (<a href=""https://www.nuget.org/packages/Microsoft.SqlServer.Dac"" rel=""noreferrer"">Microsoft.SqlServer.Dac</a>). The official Microsoft release was not available on NuGet.org until June 2016, and it doesn't have the most obvious NuGet id (<a href=""https://www.nuget.org/packages/Microsoft.SqlServer.DacFx.x64/"" rel=""noreferrer"">Microsoft.SqlServer.DacFx.x64</a>). So running <code>update-package Microsoft.SqlServer.Dac</code> did not have the desired effect. </li>
<li>The ""official"" NuGet package isn't listed anywhere on the MSDN + DAC pages - you'd think it would be mentioned here: <a href=""https://msdn.microsoft.com/en-us/library/dn702988%28v=sql.120%29.aspx"" rel=""noreferrer"">https://msdn.microsoft.com/en-us/library/dn702988%28v=sql.120%29.aspx</a> - but it's not.</li>
<li>Visual Studio 2016 installs SQL LocalDB 2016, and it <em>does</em> include the correct Dac assemblies (<code>C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\Extensions\Microsoft\SQLDB\DAC\130\Microsoft.SqlServer.Dac.dll</code>), but they're not installed in the GAC or otherwise easily findable.</li>
</ol>

<p>The fix that worked was</p>

<pre><code># Remove the old NuGet dependencies
uninstall-package Microsoft.SqlServer.Dac

# Install the new Dac NuGet package
Install-Package Microsoft.SqlServer.DacFx.x64
</code></pre>

<p>Requests for the Dac team, if you happen to see this:</p>

<ul>
<li>Please link to the correct NuGet package from the MSDN documentation</li>
<li>Please improve the error message to indicate that newer client software is needed</li>
<li>Please request the other NuGet package maintainers to note the presence of the official NuGet package, or provide an upgrade that references the official NuGet package, b/c the presence of multiple packages is likely to cause angst.</li>
</ul>

<p>(Btw, despite the difficulties here, Dac/SSDT is <em>AWESOME</em> . I haven't seen any comparable dev tooling for any competitive relational databases.)</p>
"
"<p>Some background: I've been encountering this memory exception within SSMS 2012 since it was released coupled with Red Gate's SQLPrompt (this exception never happens in my SSMS 2008R2 on the same laptop).</p>

<p>I was originally encountering these exceptions daily (SSMS2012 and SQLPrompt) which forces me to close and reopen SSMS (along with everything I was working on). Months ago I stumbled upon a support thread that pointed possible add-on's as being the cause so I <strong>uninstalled</strong> my only add-on (SQL Prompt, but did not uninstall the rest of the developer's bundle) and went many months <em>without</em> a single exception much less the System.OutOfMemoryException exception.</p>

<p>Once SP1 was released for 2012 I applied it and reinstalled SQL Prompt again (the latest version) to see if maybe the issue had been addressed and within 7 hours of development time I was greeted again with the infamous System.OutOfMemoryException.</p>

<p>Through out all of this I've been opening tickets with Red Gate and submitting debug logs where they see the exception but since the memory exception doesn't explicitly list SQLPrompt they won't escalate the issue to the development team. Prior to this specific exception SQL Prompt however is throwing numerous exceptions (some listed below) within the SSMS 2012 IDE (Visual Studio 2010). I believe that the memory exception is symptomatic of an issue with how SQL Prompt manages cached data and eats up the available memory for SSMS which eventually throws an exception as a result.</p>

<p>I've learn how to defer this issue and how to reproduce it and it is directly related to two variables:</p>

<ol>
<li>Connecting and working on multiple instances in SSMS (Object Explorer and Query windows). I.e. Connected to 7 instances netted the exception within 2-3 hours.</li>
<li>Returning result sets from multiple instances. This includes queries used by SSMS to return information to the IDE and results returned to the individual query windows.</li>
</ol>

<p>The more instances I'm connected to the fast the exception is raised which leads me SQL Prompt's caching all of the object information per instances. Once the memory exception is raised the situation degrades until SSMS crashes completely (unless I close it first).</p>

<p>What I am after is how to collect more/better information to submit to Red Gate to correct this issue. This is where I need your help.</p>

<p>Laptop: HP Elite book 8440
RAM: 6GB</p>

<p>Current OS:
Win 7 Enterprise Ed Sp1</p>

<p>Here are some of exceptions caused by SQL Prompt:</p>

<pre><code>System.ArgumentOutOfRangeException ""Specified argument was out of the range of valid values.""   

Microsoft.VisualStudio.Text.Implementation.BinaryStringRebuilder.GetLineNumberFromPosition(N/A,N/A)
Microsoft.VisualStudio.Text.Implementation.TextSnapshot.GetLineFromPosition(Microsoft.VisualStudio.Text.Implementation.TextSnapshot,N/A)
Microsoft.VisualStudio.Editor.Implementation.VsTextBufferAdapter.GetLineIndexOfPosition(N/A,System.Int32,System.Int32&amp;,System.Int32&amp;)
RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider.PositionFromIndex(RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider,System.Int32)
RedGate.SqlPrompt.Metadata.Script.ScriptProviderBase.GetText(RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider,System.Int32,System.Int32)
RedGate.SqlPrompt.Engine.NewEngine.SqlPromptEngine.GetCandidates(RedGate.SqlPrompt.Engine.NewEngine.SqlPromptEngine,System.Int32)
RedGate.SqlPrompt.Engine.PromptEngineEmulator.get_GetSuggestions(RedGate.SqlPrompt.Engine.PromptEngineEmulator)
RedGate.SqlPrompt.Engine.AutoCompleter.m_FilterChanged(RedGate.SqlPrompt.Engine.AutoCompleter,RedGate.SqlPrompt.Engine.PromptEngineEmulator,System.EventArgs)
RedGate.SqlPrompt.Engine.PromptEngineEmulator.OnFilterChanged(RedGate.SqlPrompt.Engine.PromptEngineEmulator)
RedGate.SqlPrompt.Engine.PromptEngineEmulator.set_Index(RedGate.SqlPrompt.Engine.PromptEngineEmulator,System.Int32)
RedGate.SqlPrompt.Engine.PromptEngineEmulator.set_CaretPosition(RedGate.SqlPrompt.Engine.PromptEngineEmulator,N/A)
RedGate.SQLPrompt.CommonUI.Editor.EditorWindowBase.SetEngineCaretPosition(RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow,N/A)
RedGate.SQLPrompt.CommonUI.Editor.EditorWindowBase.UpdateUIPrompts(RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow)
RedGate.SQLPrompt.CommonVS.Editor.VSEditorWindow.OnTextViewCommandExec(RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow,RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor,RedGate.SQLPrompt.CommonVS.Editor.CommandExecEventArgs)
RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.AfterCommandExecute(RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor,RedGate.SQLPrompt.CommonVS.Editor.CommandExecEventArgs)
RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor..(RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.)
RedGate.SQLPrompt.CommonUI.Utils.ErrorDialog.Do(System.Action)
RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.(RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor,System.Guid&amp;,System.Uint32,System.Uint32,System.IntPtr,System.IntPtr)
RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor..(RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.,System.Guid&amp;,System.Uint32,System.Uint32,System.IntPtr,System.IntPtr)
Microsoft.VisualStudio.Editor.Implementation.CommandChainNode.Exec(N/A,N/A,N/A,N/A,N/A,N/A)

System.ArgumentException    00:05:14.7510000    ""The parameter is incorrect. (Exception from HRESULT: 0x80070057 (E_INVALIDARG))""   

#mMc.#JQub.#OQub(#mMc.#JQub,N/A,System.Uint32,#mMc.#k3ub&amp;)
#mMc.#JQub.#z26.#8Di(#mMc.#JQub.#z26)
RedGate.SQLSourceControl.Engine.SmartAssembly.ExceptionReporting.ErrorReporterBase.Do(RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog,System.Action,System.Predicate`1&lt;System.Exception&gt;,System.Boolean)
RedGate.SQLSourceControl.Engine.SmartAssembly.ExceptionReporting.ErrorReporterBase.Do(RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog,System.Action)
RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog.Do(System.Action)
</code></pre>

<p>And here is the Memory Exception:</p>

<p><img src=""https://i.stack.imgur.com/nbOlu.png"" alt=""Exception thrown in result set""></p>

<pre><code>System.OutOfMemoryException &lt;null&gt;  

System.Text.StringBuilder.set_Capacity(System.Text.StringBuilder,N/A)
Microsoft.SqlServer.Management.QueryExecution.QEDiskStorageView.set_MaxNumBytesToDisplay(N/A,N/A)
Microsoft.SqlServer.Management.QueryExecution.QEDiskDataStorage.GetStorageView(N/A)
Microsoft.SqlServer.Management.QueryExecution.QEResultSet.StartRetrievingData(Microsoft.SqlServer.Management.QueryExecution.QEResultSet,System.Int32,N/A)
Microsoft.SqlServer.Management.QueryExecution.ResultSetAndGridContainer.StartRetrievingData(N/A,N/A,N/A)
Microsoft.SqlServer.Management.QueryExecution.ResultsToGridBatchConsumer.OnNewResultSet(Microsoft.SqlServer.Management.QueryExecution.ResultsToGridBatchConsumer,N/A,N/A)
Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.ProcessResultSet(Microsoft.SqlServer.Management.QueryExecution.QESQLBatch,N/A)
Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.DoBatchExecution(Microsoft.SqlServer.Management.QueryExecution.QESQLBatch,System.Data.SqlClient.SqlConnection,N/A)
Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.Execute(N/A,N/A,N/A)
Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.DoBatchExecution(Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec,Microsoft.SqlServer.Management.QueryExecution.QESQLBatch)
Microsoft.SqlServer.Management.QueryExecution.QESQLExec.ExecuteBatchCommon(Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec,N/A,N/A,System.Boolean&amp;)
Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.ExecuteBatchHelper(N/A,N/A,N/A,N/A)
Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.ProcessBatch(N/A,N/A,N/A)
.BatchParser.ThunkCommandExecuter.ProcessBatch(N/A,N/A,N/A)
</code></pre>

<p>Again, to be clear, I don't know if the other exceptions are related or not but the memory exception <strong>only</strong> happens with SQL Prompt installed.</p>

<p>Thanks for your assistance!</p>
","<p>Just go to:</p>

<pre><code>Tools -&gt; Options -&gt; Text Editor -&gt; SQL Server Tools -&gt; IntelliSense
</code></pre>

<p>Uncheck ""Enable IntelliSense""</p>

<p>I hope to solve your issue.</p>
"
"<p>I beg you don't ask me why am I using SQL Server 6.5</p>

<p>There is no SELECT TOP command in SQL Server 6.5, and guess what, I need it :)</p>

<p>I need to perform something like</p>

<pre><code>Select top 1 * from persons
where name ='Mike'
order by id desc
</code></pre>

<p>I've tried something with SET ROWCOUNT 1, but in that case you cannot use order by. </p>

<p>I end up with</p>

<pre><code>Select top 1 * from persons
where id = (select max(id) from persons where name ='Mike' )
</code></pre>

<p>There must be better way!</p>

<p>Any suggestions?</p>

<p>Thanx! </p>
","<p>Try selecting into a temporary table, ordered by ID, then SET ROWCOUNT 1 and select * from temporary table. (This should work for any top N with SET ROWCOUNT N, while your existing solution will only work for top 1.)</p>
"
"<p>I would like to identify the returning customers from an Oracle(11g) table like this:</p>

<pre><code>CustID | Date
-------|----------
XC321  | 2016-04-28
AV626  | 2016-05-18
DX970  | 2016-06-23
XC321  | 2016-05-28
XC321  | 2016-06-02
</code></pre>

<p>So I can see which customers returned within various windows, for example within 10, 20, 30, 40 or 50 days. For example:</p>

<pre><code>CustID | 10_day | 20_day | 30_day | 40_day | 50_day 
-------|--------|--------|--------|--------|--------
XC321  |        |        |    1   |        |        
XC321  |        |        |        |    1   |        
</code></pre>

<p>I would even accept a result like this:</p>

<pre><code>CustID |    Date    | days_from_last_visit
-------|------------|---------------------
XC321  | 2016-05-28 |                   30        
XC321  | 2016-06-02 |                    5
</code></pre>

<p>I guess it would use a partition by windowing clause with unbounded following and preceding clauses... but I cannot find any suitable examples.
Any ideas...?
Thanks</p>
","<p>No need for window functions here, you can simply do it with conditional aggregation using <code>CASE EXPRESSION</code> :</p>

<pre><code>SELECT t.custID,
       COUNT(CASE WHEN (last_visit- t.date) &lt;= 10 THEN 1 END) as 10_day,
       COUNT(CASE WHEN (last_visit- t.date) between 11 and 20 THEN 1 END) as 20_day,
       COUNT(CASE WHEN (last_visit- t.date) between 21 and 30 THEN 1 END) as 30_day,
       .....
FROM (SELECT s.custID,
             LEAD(s.date) OVER(PARTITION BY s.custID ORDER BY s.date DESC) as last_visit
      FROM YourTable s) t
GROUP BY t.custID
</code></pre>
"
"<p>I wrote a child of mysqli with a query method that returns a child of mysqli_result. This result child will have additional methods unique to my app. </p>

<pre><code>public MySQL extends mysqli
{
    public function query($query)
    {
        if ($this-&gt;real_query($query)) {
            if ($this-&gt;field_count &gt; 0) {
                return new MySQL_Result($this);
            }
            return true;
        }
        return false;
    }

}

class MySQL_Result extends mysqli_result
{
    public function fetch_objects() {
        $rows = array();
        while($row = $this-&gt;fetch_object())
            $rows[$row-&gt;id] = $row;
        return $rows;
    }
}
</code></pre>

<p>What I can't figure out is whether <code>fetch_object()</code> uses buffered or unbuffered SQL data. </p>

<p>The constructor of <code>mysqli_result</code> isn't visible in <code>mysqli.php</code>, so I can't see if its calling <code>$mysqli-&gt;store_result()</code> or <code>$mysqli-&gt;use_result()</code>.</p>

<p>I tried adding these methods to <code>MySQL</code> but neither function is echoed. </p>

<pre><code>    public function store_result($option='a') {
        echo ""STORE RESULT&lt;br/&gt;"";
    }

    public function use_result($option='a') {
        echo ""USE RESULT&lt;br/&gt;"";
    }
</code></pre>

<p>Does this mean that the <code>mysqli_result</code> constructor doesn't call either? If so, how does it access the SQL data when <code>fetch_object</code> is called?</p>

<p>................</p>

<p>I want buffered SQL data. If I can't figure out what the child constructor is doing, I may have to replace the result child class with a decorator that calls <code>$mysqli-&gt;store_result()</code>.</p>

<p>I'm new to PHP / OOP and would very much appreciate feedback. Thank you.</p>
","<p>Missing <code>;</code> in <code>header('location:home.php')</code></p>

<p>It should be:</p>

<pre><code>if($check_user &gt; 0) {
    header('location:home.php');
    }else{
        echo ""wrong username or password"";
      }
</code></pre>

<p><strong>Upadated:</strong></p>

<p>Another Problem with <code>SELECT &gt;</code> it should be <code>SELECT *</code><br>
<strong>Your Code:</strong> </p>

<pre><code>$select_user= ""SELECT &gt; FROM example WHERE Email='$email' AND Passwords='$password'"";
</code></pre>

<p><strong>It should be:</strong></p>

<pre><code>$select_user= ""SELECT * FROM example WHERE Email='$email' AND Passwords='$password'"";
</code></pre>

<p><strong>Comments Problem:</strong></p>

<p>You have used <code>mysqli_master_query()</code> but This function is currently not documented; only its argument list is available. Please see the website <strong><a href=""http://php.net/manual/en/function.mysqli-master-query.php"" rel=""nofollow"">doc</a>.</strong></p>
"
"<p>I'm trying to do a join across a number of tables (three plus a join table in the middle).  I think korma is lazily evaluating the last join.  What I'm trying to do is to add a condition that restricts the results of the first table in the join, but I'm only interested in fields from the last table in the join.</p>

<p>So for example say I've got <code>clubs</code>, <code>people</code> and <code>hobbies</code> tables, and a <code>people-to-hobbies</code> join table for the last two.  Each club can have many people, and each person can have many hobbies.  </p>

<p>I'm trying to get the full details of all the hobbies of people who belong to a specific club, but I don't want any fields from the <code>club</code> table.  The join table means that korma will create two queries, one to get all the people that are in a specific club, and another to retrieve the hobbies for that person via the <code>people-to-hobbies</code> join table.</p>

<p>My korma query looks something like this:</p>

<pre><code>(select clubs
  (with people
    (with hobbies
      (fields :hobby-name :id)
      (where {:clubs.name ""korma coders""}))))
</code></pre>

<p>The problem is that I haven't specified which fields I want from <code>clubs</code> and <code>people</code>, and the default is to select <code>*</code>.  How can I include no fields from these tables?  Is this possible, or does the fact that the <code>hobbies</code> are lazily loaded mean korma has to return some results in the first query (which gets me a filtered list of people), so that when I come to interrogate it later for the hobbies, it has the ids it needs to run the second query?</p>
","<p>The result you get depends on the default timezone for the JVM. You can  fix that via whatever mechanism the host operating system gives you.But in my experience it's generally better to force the JVM to a known value explicitly. </p>

<p>This is achieving with a property on the command line, or in leiningen <code>project.clj</code></p>

<pre><code>:jvm-opts [""-Duser.timezone=UTC""]
</code></pre>
"
"<p>In Android Pie sqlite Write-Ahead logging (WAL) has been enabled by default. This is causing errors for my existing code only in Pie devices.
I have been unable to turn off WAL successfully using  <code>SQLiteDatabase.disableWriteAheadLogging()</code> or <code>PRAGMA journal_mode</code> due to the way I access the database. I would like to disable WAL completely with an Android setting called <em>db_compatibility_wal_supported</em> :</p>

<p><a href=""https://source.android.com/devices/tech/perf/compatibility-wal#disabling_compatibility_wal"" rel=""nofollow noreferrer"">Compatibility WAL (Write-Ahead Logging) for Apps</a></p>

<p>Does anyone know how to configure this? I don't know if this file can be altered programmatically at startup or if it is changed manually.</p>

<hr>

<h1>Further Details about the problem</h1>

<p>I have a sqlite database (20mb+ / 250k records) in my app. This db is generated using plain java on my server. It contains a food database and the user of the app can add to the database (and the server is updated). This is stored in the assets folder in android.
During first installation the database is copied from assets to the app folder so that it can be written to, using effectively this method :</p>

<p><a href=""https://stackoverflow.com/questions/16354154/copy-sqlite-database-from-assets-folder#answer-16354263"">Copy SQLite database from assets folder</a></p>

<p>Unfortunately, once I start writing to the database using SqlDroid wal is enabled and the tables which were in the original db have vanished and only any newly created tables remain. The size of the database however is still 20mb+. All the database errors are due to the missing tables.
The table copying and writing method works perfectly in versions of Android prior to Pie.</p>
","<p>@Rockvole please share error that you are facing, that help us to find appropriate solution.</p>

<p>Mean while, i understand that you want to close that <strong>WAL</strong> in android pie and you are using ""SQLDroid"" lib to create Sqlite DB. </p>

<p>This lib internally using ""<strong>SQLiteDatabase</strong>"" to store data locally, I think you need to call ""<strong>SQLiteDatabase.disableWriteAheadLogging()</strong>"" in ""<strong>SQLiteDatabase</strong>"" class where DB instance created the package name is ""<strong><em>package org.sqldroid;</em></strong>"" </p>

<p>or <strong><em>Get internal SQLiteDatabase instance</em></strong> and call disableWriteAheadLogging().</p>

<p>Second solution is create ""config.xml"" inside values folder and wirte <code>""&lt;bool name=""db_compatibility_wal_supported""&gt;false&lt;/bool&gt;""</code> and run and check its work.</p>
"
"<p>I have a data set that I want to parse for to see multi-touch attribution. The data set is made up by leads who responded to a marketing campaign and their marketing source. </p>

<p>Each lead can respond to multiple campaigns and I want to get their first marketing source and their last marketing source in the same table. </p>

<p>I was thinking I could create two tables and use a select statement from both.
The first table would attempt to create a table with the most recent marketing source from every person (using email as their unique ID). </p>

<pre><code>create table temp.multitouch1 as (
select distinct on (email) email, date, market_source as last_source 
from sf.campaignmember
where date &gt;= '1/1/2016' ORDER BY DATE DESC);
</code></pre>

<p>Then I would create a table with deduped emails but this time for the first source. </p>

<pre><code>create table temp.multitouch2 as (
select distinct on (email) email, date, market_source as first_source 
from sf.campaignmember
where date &gt;= '1/1/2016' ORDER BY DATE ASC);
</code></pre>

<p>Finally I wanted to simply select the email and join the first and last market sources to it each in their own column. </p>

<pre><code>select a.email, a.last_source, b.first_source, a.date 
from temp.multitouch1 a
left join temp.multitouch b on b.email = a.email
</code></pre>

<p>Since distinct on doesn't work on redshift's postgresql version I was hoping someone had an idea to solve this issue in another way. </p>

<p>EDIT 2/22: For more context I'm dealing with people and campaigns they've responded to. Each record is a ""campaign response"" and every person can have more than one campaign response with multiple sources. I'm trying make a select statement which would dedupe by person and then have columns for the first campaign/marketing source they've responded to and the last campaign/marketing source they've responded to respectively.</p>

<p>EDIT 2/24: Ideal output is a table with 4 columns: email, last_source, first_source, date. </p>

<p>The first and last source columns would be the same for people with only 1 campaign member record and different for everyone who has more than 1 campaign member record. </p>
","<p>As long as you have a table that has more rows than your required series has numbers, this is what has worked for me in the past:</p>

<pre class=""lang-sql prettyprint-override""><code>select
    (row_number() over (order by 1)) - 1 as hour
from
    large_table
limit 24
;
</code></pre>

<p>Which returns numbers <code>0-23</code>.</p>
"
"<p>Are there any methods to page queries in FileNet? I have a grid control with paging so i need to get count of elements total in query and possibility to get in example page 3 of results(with any page size). I found only TOP operator, but are there any SKIP or COUNT? </p>
","<p>There is no way to accomplish exactly what you want. The main reason for this is that it is very expensive for Content Engine to count all rows returned by a query (assuming there are thousands of them). There can be quite involved security restrictions that potentially have to be applied to each returned object. This will effectively exclude some objects from query results, affecting the result count. Evaluating effective security permissions on multiple objects would kill the performance, thus avoided. That is why you don't have <code>COUNT</code> or other aggregate functions in the query language.</p>

<p>If you can restrict the amount of records displayed in the grid to a reasonable number, then <code>COUNT_LIMIT</code> might work for you  see <a href=""http://www-01.ibm.com/support/knowledgecenter/SSNW2F_5.2.0/com.ibm.p8.ce.dev.ce.doc/query_sql_syntax_rel_queries.htm%23query_sql_syntax_rel_queries__Query_Options?lang=en"" rel=""nofollow"">Query Options</a> and <a href=""http://www-01.ibm.com/support/knowledgecenter/SSNW2F_5.2.0/com.ibm.p8.ce.dev.java.doc/com/filenet/api/collection/PageIterator.html?lang=en-us#getTotalCount()"" rel=""nofollow"">getTotalCount()</a>. However, this will require looping through to the required page with its inherent performance drawback.</p>
"
"<p>As far as I know a package body can be replaced and recompiled without affecting the specification. A package specification declares procedures and functions, not defines them, so they can not reference objects, that can make the package specification INVALID. </p>

<p>I know that a package specification can reference objects when it uses stand-alone subprograms and other packages to define it's variables. In this case changing referenced objects may cause the specification invalidation.</p>

<p>Is there any other way how an Oracle package specification can depend on (reference) objects and become INVALID whether when referenced objects chnge or another way?</p>
","<p>Procedures'/Functions' declarations must be equal in both <code>package</code> and its <code>body</code>. Try to avoid <em>magic numbers</em> 
like <code>p_amount NUMBER(10)</code>: what is the meaning of <code>10</code>? But use <code>p_amount receipt.Amount%Type</code> which is clealy
the type of <code>receipt.Amount</code> field.</p>

<pre><code>CREATE OR REPLACE PACKAGE form_pkg AS  
  -- interface: procedure with 3 arguments
  PROCEDURE Insert_receipts(
    p_receipt     receipt.Receipt_Number%Type, 
    p_transaction receipt.Transaction_ID%Type, 
    p_amount      receipt.Amount%Type);

  ...
END form_pkg;
/

CREATE OR REPLACE PACKAGE BODY form_pkg AS  -- package body
  -- implementation
  -- The same three arguments (no seq_value!)
  PROCEDURE Insert_receipts (
    p_receipt     receipt.Receipt_Number%Type, 
    p_transaction receipt.Transaction_ID%Type, 
    p_amount      receipt.Amount%Type) 
  IS
  BEGIN
    INSERT INTO receipt( 
      ID, 
      Receipt_Number,
      Transaction_ID,
      Amount) 
    VALUES (
      seq.NextVal, 
      p_receipt, 
      p_transaction, 
      p_amount);
  END Insert_receipts;

  ...
END form_pkg;
/
</code></pre>

<p>When implementing a routine you can see errors' details (names, lines etc.) in <code>USER_ERRORS</code> view:</p>

<pre><code>-- all (syntactic) errors in the form_pkg
select *
  from USER_ERRORS
 where Name = upper('form_pkg')
</code></pre>
"
"<p>I'm trying to setup Room database backup functionality. 
Problem is that sql database file doesn't contain latest set of data in the app once downloaded. It always misses some most recent records. 
Is there a proper way to export room database? 
P.S. I didn't face similar problems when handled my db with sqliteHelper, so I suppose it must have something to do with Room.</p>

<p>Way I'm doing it:</p>

<pre><code>@Throws(IOException::class)
private fun copyAppDbToDownloadFolder(address: String) {
    val backupDB = File(address, ""studioDb.db"") 
    val currentDB = applicationContext.getDatabasePath(StudioDatabase.DB_NAME)
    if (currentDB.exists()) {
        val src = FileInputStream(currentDB).channel
        val dst = FileOutputStream(backupDB).channel
        dst.transferFrom(src, 0, src.size())
        src.close()
        dst.close()
    }
}
</code></pre>
","<p>You need to use </p>

<blockquote>
  <p>JournalMode.TRUNCATE</p>
</blockquote>

<p>in your AppDatabase.java:</p>

<pre><code>private static AppDatabase sInstance;

public static AppDatabase getDatabase(final Context context) {
    if (sInstance == null) {
        synchronized (AppDatabase.class) {
            if (sInstance == null) {
                sInstance = Room.databaseBuilder(context, AppDatabase.class, DATABASE_NAME)
                        .setJournalMode(JournalMode.TRUNCATE)
                        .build();
            }
        }
    }
    return sInstance;
}
</code></pre>

<p>This method will not create <strong>db.bad</strong> and <strong>db.wal</strong> files that's creating hindrance in exporting room db.</p>

<p><strong><em>For exporting the DB file:</em></strong></p>

<blockquote>
  <p>Link: <a href=""https://stackoverflow.com/questions/6540906/simple-export-and-import-of-a-sqlite-database-on-android/53167018#53167018"">Exporting db with creating folder on daily basis</a></p>
</blockquote>
"
"<p>Is there a recent version of the SQLHelper class out there. I've been using one for a few years now and was wondering if there is a new version out there for .NET Framework 2.0 or 3.0. I prefer this on small projects vs Microsoft Data App Block (which I use on larger projects).</p>

<p>I came across this link </p>

<blockquote>
  <p><a href=""http://www.microsoft.com/downloads/details.aspx?familyid=f63d1f0a-9877-4a7b-88ec-0426b48df275&amp;displaylang=en"" rel=""nofollow noreferrer"">http://www.microsoft.com/downloads/details.aspx?familyid=f63d1f0a-9877-4a7b-88ec-0426b48df275&amp;displaylang=en</a> </p>
</blockquote>
","<p>You cannot use parameters for things like table and column names. For those you could have a whitelist of possible values and then use string concatenation when building the SQL query.</p>
"
"<p>I'm having a strange bug when I write a website with flask and package flask-mysql.</p>

<p>Here is the code of the bug function:</p>

<pre><code>@app.route('/calendar/editeventtitle',methods=['POST'])
def editeventtitle():
    if not session.get('logged_in'):
      abort(401)

    try:
      id = request.form.get('id',type=int)
      title = request.form['title']
      color = request.form['color']
      delete = request.form.get('delete')
    except:
      pass

    conn = mysql.connect()
    cursor = conn.cursor()
    print(id,type(id))
    # try:
    #   print(delete,type(delete))
    # except:
    #   pass

    if id and delete:
        cursor.execute('delete from events where id = %d',id)
        conn.commit()
        flash('Event canceled!')
        return redirect(url_for('calendar'))
    elif id and title and color:
        cursor.execute('update events set title = %s, color = %s where id = %d',(title,color,id))
        conn.commit()
        flash('Event updated!')
        return redirect(url_for('calendar'))
</code></pre>

<p>When I post the four variables to this page. I succesfully get them. And the result of <code>print(id,type(id))</code> is like:  </p>

<pre><code>6 &lt;class 'int'&gt;
</code></pre>

<p>We see it's really an integer, but when the code starts to update or delete data from db, here is the error message:</p>

<blockquote>
  <p>TypeError: %d format: a number is required, not str</p>
</blockquote>

<p>Really don't know the reason =-=, anyone can help me? Thank you.</p>

<p>PS: Python3.6.1, Flask 0.12.2, Flask-Mysql 1.4.0</p>
","<p>You need to keep a handle to the connection; you keep overriding it in your loop.</p>

<p>Here is a simplified example:</p>

<pre><code>con = mysql.connect()
cursor = con.cursor()

def insert(mysql, insertCmd):
     try:
        cursor.execute(insertCmd)
        con.commit()
        return True
     except Exception as e:
        print(""Problem inserting into db: "" + str(e))
        return False
</code></pre>

<p>If <code>mysql</code> is your connection, then you can just commit on that, directly:</p>

<pre><code>def insert(mysql, insertCmd):
  try:
    cursor = mysql.cursor()
    cursor.execute(insertCmd)
    mysql.commit()
    return True
  except Exception as e:
    print(""Problem inserting into db: "" + str(e))
    return False
  return False
</code></pre>
"
"<p>Executing this command:</p>

<pre><code>sqlacodegen &lt;connection-url&gt; --outfile db.py 
</code></pre>

<p>The db.py contains generated tables: </p>

<pre><code>t_table1 = Table(...)
</code></pre>

<p>and classes too:</p>

<pre><code>Table2(Base):
    __tablename__ = 'table2'
</code></pre>

<p>The problem is that a table is generated in one way only - either a table or a class.</p>

<p>I would like to make it generate models (classes) only but in the provided flags I couldn't find such an option. Any idea?</p>
","<p>It is because you did this in Python shell:</p>

<pre><code>&gt;&gt;&gt; import sqlacodegen
&gt;&gt;&gt; sqlacodegen --help
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: bad operand type for unary -: '_Helper'
</code></pre>

<p>You should have executed <code>sqlacodegen --help</code> in your <strong>Unix command shell / Windows command prompt</strong>:</p>

<pre><code>% sqlacodegen --help
usage: sqlacodegen [-h] [--version] [--schema SCHEMA] [--tables TABLES]
                   [--noviews] [--noindexes] [--noconstraints] [--nojoined]
                   [--noinflect] [--outfile OUTFILE]
                   [url]

Generates SQLAlchemy model code from an existing database.

positional arguments:
  url                SQLAlchemy url to the database

optional arguments:
  -h, --help         show this help message and exit
  --version          print the version number and exit
  --schema SCHEMA    load tables from an alternate schema
  --tables TABLES    tables to process (comma-separated, default: all)
  --noviews          ignore views
  --noindexes        ignore indexes
  --noconstraints    ignore constraints
  --nojoined         don't autodetect joined table inheritance
  --noinflect        don't try to convert tables names to singular form
  --outfile OUTFILE  file to write output to (default: stdout)
</code></pre>

<p>An example of the actual command would then be:</p>

<pre><code>% sqlacodegen --outfile models.py \
postgresql://gollyjer:swordfish@localhost:5432/mydatabase
</code></pre>

<p>Where <code>gollyjer:swordfish</code> is on the format <code>user:password</code>.</p>
"
"<p>I am attempting to enable the MySQL group replication plugin on MySQL 5.7.21, which should be available in 5.7 as per the documentation (<a href=""https://dev.mysql.com/doc/refman/5.7/en/group-replication.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/5.7/en/group-replication.html</a>)</p>

<pre><code>$ mysql --version
mysql  Ver 14.14 Distrib 5.7.21, for Linux (x86_64) using  EditLine wrapper
</code></pre>

<p>When I attempt to enable the plugin through MySQL:</p>

<pre><code>$ mysql&gt; INSTALL PLUGIN group_replication SONAME 'group_replication.so';
ERROR 1126 (HY000): Can't open shared library '/usr/lib/mysql/plugin/group_replication.so' (errno: 2 /usr/lib/mysql/plugin/group_replication.so: cannot open shared object file: No such file or directory)
</code></pre>

<p>The output of my plugins in MySQL:</p>

<pre><code>$ mysql&gt; SHOW PLUGINS;
+----------------------------+----------+--------------------+---------+---------+
| Name                       | Status   | Type               | Library | License |
+----------------------------+----------+--------------------+---------+---------+
| binlog                     | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| mysql_native_password      | ACTIVE   | AUTHENTICATION     | NULL    | GPL     |
| sha256_password            | ACTIVE   | AUTHENTICATION     | NULL    | GPL     |
| CSV                        | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| MEMORY                     | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| InnoDB                     | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| INNODB_TRX                 | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_LOCKS               | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_LOCK_WAITS          | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMP                 | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMP_RESET           | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMPMEM              | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMPMEM_RESET        | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMP_PER_INDEX       | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_CMP_PER_INDEX_RESET | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_BUFFER_PAGE         | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_BUFFER_PAGE_LRU     | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_BUFFER_POOL_STATS   | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_TEMP_TABLE_INFO     | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_METRICS             | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_DEFAULT_STOPWORD | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_DELETED          | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_BEING_DELETED    | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_CONFIG           | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_INDEX_CACHE      | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_FT_INDEX_TABLE      | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_TABLES          | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_TABLESTATS      | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_INDEXES         | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_COLUMNS         | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_FIELDS          | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_FOREIGN         | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_FOREIGN_COLS    | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_TABLESPACES     | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_DATAFILES       | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| INNODB_SYS_VIRTUAL         | ACTIVE   | INFORMATION SCHEMA | NULL    | GPL     |
| MyISAM                     | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| MRG_MYISAM                 | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| PERFORMANCE_SCHEMA         | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| ARCHIVE                    | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| BLACKHOLE                  | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| FEDERATED                  | DISABLED | STORAGE ENGINE     | NULL    | GPL     |
| partition                  | ACTIVE   | STORAGE ENGINE     | NULL    | GPL     |
| ngram                      | ACTIVE   | FTPARSER           | NULL    | GPL     |
+----------------------------+----------+--------------------+---------+---------+
44 rows in set (0.01 sec)
</code></pre>

<p>This is the contents of the plugin directory:</p>

<pre><code>$ ls -lah /usr/lib/mysql/plugin/
total 644K
drwxr-xr-x 2 root root 4.0K Sep 26 23:24 .
drwxr-xr-x 3 root root 4.0K Sep 26 23:24 ..
-rw-r--r-- 1 root root  21K Jul 19 14:10 adt_null.so
-rw-r--r-- 1 root root 6.2K Jul 19 14:10 auth_socket.so
-rw-r--r-- 1 root root  44K Jul 19 14:10 connection_control.so
-rw-r--r-- 1 root root 107K Jul 19 14:10 innodb_engine.so
-rw-r--r-- 1 root root  79K Jul 19 14:10 keyring_file.so
-rw-r--r-- 1 root root 151K Jul 19 14:10 libmemcached.so
-rw-r--r-- 1 root root 9.7K Jul 19 14:10 locking_service.so
-rw-r--r-- 1 root root  11K Jul 19 14:10 mypluglib.so
-rw-r--r-- 1 root root 6.2K Jul 19 14:10 mysql_no_login.so
-rw-r--r-- 1 root root  55K Jul 19 14:10 rewriter.so
-rw-r--r-- 1 root root  56K Jul 19 14:10 semisync_master.so
-rw-r--r-- 1 root root  15K Jul 19 14:10 semisync_slave.so
-rw-r--r-- 1 root root  27K Jul 19 14:10 validate_password.so
-rw-r--r-- 1 root root  31K Jul 19 14:10 version_token.so
</code></pre>

<p>These are the contents of my config:</p>

<blockquote>
  <p>cat /etc/mysql/my.cnf</p>
</blockquote>

<pre><code>    [mysqld_safe]
    nice                                    = 0
    socket                                  = /var/run/mysqld/mysqld.sock

    [mysqld]
    basedir                                 = /usr
    bind_address                            = 123.45.67.89
    binlog_checksum                         = NONE
    binlog_format                           = ROW
    datadir                                 = /var/lib/mysql
    enforce_gtid_consistency                = ON
    expire_logs_days                        = 10
    general_log                             = 1
    general_log_file                        = /var/log/mysql/mysql.log
    gtid_mode                               = ON
    key_buffer_size                         = 8388608
    lc_messages_dir                         = /usr/share/mysql
    log_bin                                 = binlog
    log_error                               = /var/log/mysql/mysql_error.log
    log_slave_updates                       = ON
    long_query_time                         = 60
    loose-group_replication_bootstrap_group = OFF
    loose-group_replication_enforce_update_everywhere_checks= ON
    loose-group_replication_group_name      = 34dee7cd-d20d-4f59-9500-f56ada9a1abz
    loose-group_replication_group_seeds     = 123.45.67.88:33061,123.45.67.89:33061
    loose-group_replication_ip_whitelist    = 123.45.67.88,123.45.67.89
    loose-group_replication_local_address   = 123.45.67.89:33061
    loose-group_replication_recovery_use_ssl= 1
    loose-group_replication_single_primary_mode= OFF
    loose-group_replication_ssl_mode        = REQUIRED
    loose-group_replication_start_on_boot   = OFF
    master_info_repository                  = TABLE
    max_allowed_packet                      = 16M
    max_binlog_size                         = 100M
    max_connect_errors                      = 100000000
    pid-file                                = /var/run/mysqld/mysqld.pid
    port                                    = 3306
    query_cache_limit                       = 1M
    query_cache_size                        = 16M
    relay_log                               = my-project-prod-relay-bin
    relay_log_info_repository               = TABLE
    report_host                             = 123.45.67.88
    require_secure_transport                = ON
    server_id                               = 2
    skip_external_locking
    slow_query_log                          = 1
    slow_query_log_file                     = /var/log/mysql/mysql-slow-query.log
    socket                                  = /var/run/mysqld/mysqld.sock
    thread_cache_size                       = 8
    thread_stack                            = 192K
    tmpdir                                  = /tmp
    transaction_write_set_extraction        = XXHASH64
    user                                    = mysql

    [mysqldump]
    max_allowed_packet                      = 16M
    quick
    quote_names

    [mysql]
    no-auto-rehash

    [isamchk]
    key_buffer_size                         = 16M

    [client]
    port                                    = 3306
    socket                                  = /var/run/mysqld/mysqld.sock

    !includedir /etc/mysql/conf.d/
</code></pre>

<p>I'm wondering if I'm using the wrong version of MySQL or if there is some other step needed to install the group replication plugin?</p>
","<p>FWIW I downloaded 5.7.19 Community Edihttps://dev.mysql.com/doc/refman/5.7/en/group-replication.htmltion to run it in a sandbox, and I confirm that the group replication .so file is present.</p>

<p>I got the plugin to load with the INSTALL PLUGIN statement, just like you tried.</p>

<p>First I had to add some config settings, which are documented in <a href=""https://dev.mysql.com/doc/refman/5.7/en/group-replication-configuring-instances.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/5.7/en/group-replication-configuring-instances.html</a> </p>

<pre><code>server_id=1 # or any value unique among your replica set
gtid_mode=ON
enforce_gtid_consistency=ON
master_info_repository=TABLE
relay_log_info_repository=TABLE
binlog_checksum=NONE
log_slave_updates=ON
log_bin=binlog
binlog_format=ROW
</code></pre>

<p>And of course restart mysqld after editing the config settings.</p>

<p>According to discussion <a href=""https://bugs.mysql.com/bug.php?id=85649"" rel=""nofollow noreferrer"">here</a>, it looks like the group replication plugin may have been intentionally left out of the normal MySQL server edition. </p>

<p>To know if you are running the Community Edition, run mysql:</p>

<pre><code>&gt; mysql -u root -p
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.7.20-log MySQL Community Server (GPL)
.....
</code></pre>

<p>To manually install the Community Edition 5.7 (or other archived versions here: <a href=""https://downloads.mysql.com/archives/community/"" rel=""nofollow noreferrer"">https://downloads.mysql.com/archives/community/</a>), run the following commands:</p>

<pre><code>sudo apt-get install libaio1
sudo apt-get install libmecab2
curl -OL https://downloads.mysql.com/archives/get/file/mysql-common_5.7.20-1ubuntu16.04_amd64.deb
curl -OL https://downloads.mysql.com/archives/get/file/mysql-community-client_5.7.20-1ubuntu16.04_amd64.deb
curl -OL https://downloads.mysql.com/archives/get/file/mysql-client_5.7.20-1ubuntu16.04_amd64.deb
curl -OL https://downloads.mysql.com/archives/get/file/mysql-community-server_5.7.20-1ubuntu16.04_amd64.deb
sudo dpkg -i mysql-common_5.7.20-1ubuntu16.04_amd64.deb
sudo dpkg -i mysql-community-client_5.7.20-1ubuntu16.04_amd64.deb
sudo dpkg -i mysql-client_5.7.20-1ubuntu16.04_amd64.deb
sudo dpkg -i mysql-community-server_5.7.20-1ubuntu16.04_amd64.deb   
</code></pre>
"
"<p>I am trying to use POstgreSQL database in my project.
I would like to learn code first developement..
I download dotconnect for PostgreSql (trial version) and I added Devart.Data.PostgreSql dll to my references. Then, I added those lines in my app.copnfig</p>

<pre><code> &lt;connectionStrings&gt;
        &lt;add name=""SchoolDBConnectionString"" connectionString=""Server=localhost;Port = 5432;Database=MyDataBase;user Id=postgres;password=*****"" providerName=""Devart.Data.PostgreSql"" /&gt;
    &lt;/connectionStrings&gt;
&lt;system.data&gt; 
    &lt;DbProviderFactories&gt;
        &lt;remove invariant=""Devart.Data.PostgreSql"" /&gt;
        &lt;add name=""dotConnect for PostgreSQL"" invariant=""Devart.Data.PostgreSql"" description=""Devart dotConnect for PostgreSQL"" type=""Devart.Data.PostgreSql.PgSqlProviderFactory, Devart.Data.PostgreSql, Version= 6.8.333.0, Culture=neutral, PublicKeyToken=09af7300eec23701"" /&gt;
    &lt;/DbProviderFactories&gt;
  &lt;/system.data&gt;
&lt;/configuration&gt;
</code></pre>

<p>WHen I execute my application i have this exeption :</p>

<p>An error occurred while getting provider information from the database. This can be caused by Entity Framework using an incorrect connection string. Check the inner exceptions for details and ensure that the connection string is correct.</p>
","<p>after posting the question i found out that it can't be done with TElement being DataTable, so i made my own class (entity) and i am returning that, everything works ok, but i still must ask, can it be done so it returns a datatable? (don't know, some hacks..)</p>
"
"<p>I'm writing a grammar to parse HTSQL syntax and am stuck with how to deal with the reuse of the <code>/</code> character for both the segment and division operators.  The <a href=""http://htsql.org/doc/ref/syntax.html#syntax-structure"" rel=""nofollow"">described grammar</a> isn't terribly formal, so I've been following the exact output of the Python implementation, which from a cursory glance seems to be a handwritten parser, rather than using a parser generator - for reference the parser generator I'm currently using is <a href=""https://www.irif.univ-paris-diderot.fr/~jch/software/cl-yacc/"" rel=""nofollow""><code>CL-YACC</code></a> with <a href=""https://github.com/djr7C4/cl-lex"" rel=""nofollow""><code>CL-LEX</code></a>.  (FWIW the full thing <a href=""https://github.com/Ferada/cl-htsql"" rel=""nofollow"">is here</a>, although likely a bit outdated.)</p>

<p>The one ambiguity I'm struggling with arises due to <code>""/1""</code> being parsed as <code>'(:COLLECT (:INTEGER ""1""))'</code>, but <code>""/1/2""</code> being parsed as <code>'(:COLLECT (:OPERATOR / (:INTEGER ""1"") (:INTEGER ""2"")))'</code>, i.e. one is the segment separator, the other one is division; <code>""/1//2""</code> again is <code>'(:COLLECT (:OPERATOR / (:INTEGER ""1"") (:COLLECT (:INTEGER ""2""))))'</code>.</p>

<p>The question is thus, how can I deal with this in the grammar specification without resorting to switching to a manual parser?  Would a switch to a different parser generator class (instead of LALR(1)) help?</p>

<p>So far I've tried different variations of the grammar, however the fact that the precedences are fixed for the whole grammar also interferes with both interpretations of the slash.  The other way I tried was to disambiguate in the lexer, i.e. treating the first slash (in each ""group"") differently and returning a different symbol, e.g. <code>DIV</code> - however I couldn't find a good rule and somehow doubt that it exists purely by looking at the lexical structure.</p>

<p>Lastly, I'm tempted to resolve this by diverging from the given parser entirely to make my life easier.  Would you consider that more desirable, in the sense that having a predictable grammar is more easily understood?</p>

<p><strong>Exhibit 1, the Python script to examine the parse tree:</strong></p>

<pre><code>import htsql


application = htsql.HTSQL(""sqlite:///htsql_demo.sqlite"")


global y
y = None


def p(string):
    global y
    with application:
        y = htsql.core.syn.parse.parse(string)
        return y


def l(name):
    result = []
    for c in name:
        if c.isupper() and result:
            result.append(""-"")
        result.append(c)
    return """".join(result)


def keyword(name):
    return "":{}"".format(name.upper())


def n(expression):
    name = expression.__class__.__name__
    name = name[:name.find(""Syntax"")]
    return keyword(l(name))


def t(expression):
    arguments = [n(expression)]
    d = expression.__dict__
    if ""identifier"" in d:
        arguments.append(t(expression.identifier))
    if ""text"" in d:
        arguments.append(""\""{}\"""".format(expression.text))
    if ""symbol"" in d:
        if not isinstance(expression, (htsql.core.syn.syntax.ProjectSyntax, htsql.core.syn.syntax.FilterSyntax, htsql.core.syn.syntax.CollectSyntax, htsql.core.syn.syntax.DetachSyntax)):
            arguments.append(expression.symbol)
    if ""arm"" in d:
        arguments.append(t(expression.arm))
    if ""larm"" in d:
        arguments.append(t(expression.larm))
    if ""rarm"" in d:
        arguments.append(t(expression.rarm))
    if ""arms"" in d:
        arguments.extend(t(x) for x in expression.arms)
    if ""rarms"" in d:
        arguments.extend(t(x) for x in expression.rarms)
    return ""({})"".format("" "".join(arguments))


# t(p(""/school""))
# '(:COLLECT (:IDENTIFIER ""school""))

# t(p(""/'school'""))
# '(:COLLECT (:STRING ""school""))
</code></pre>

<p><strong>Exhibit 2, my current parser, which doesn't deal with this correctly:</strong></p>

<pre><code>(defpackage #:cl-htsql
  (:use #:cl #:alexandria #:cl-lex #:yacc)
  (:import-from #:arnesi #:with-collector))

(eval-when (:compile-toplevel :load-toplevel :execute)
  (defun maybe-intern (name &amp;optional (package NIL package-p))
    ""If NAME is a SYMBOL, return it, otherwise INTERN it.""
    (cond
      ((symbolp name)
       name)
      (package-p
       (intern name package))
      (T
       (intern name))))

  (defmacro define-lexer (name &amp;body patterns)
    ""Shortcut for DEFINE-STRING-LEXER.""
    `(define-string-lexer ,name
       ,@(mapcar
          (lambda (pattern)
            (etypecase pattern
              ((or symbol string)
               (let ((symbol (maybe-intern pattern))
                     (pattern (string pattern)))
                 `(,pattern (return (values ',symbol ',symbol)))))
              (list
               (destructuring-bind (pattern &amp;optional symbol value) pattern
                 (let* ((symbol (or symbol (intern pattern)))
                        (value (or value symbol)))
                   (etypecase symbol
                     (list
                      `(,pattern ,symbol))
                     (symbol
                      `(,pattern (return (values ',symbol ',value))))))))))
          patterns))))

;; parser are results are to be treated immutable
(define-lexer string-lexer
  /
  (""\\|"" \|)
  (""\\&amp;"" &amp;)
  &lt;=
  &gt;=
  ==
  =
  !==
  !=
  !~
  !
  ~
  &lt;
  &gt;
  @
  (""\\?"" ?)
  (""\\."" \.)
  (""\\("" \()
  (""\\)"" \))
  (""\\+"" +)
  -
  (""\\*"" *)
  \:
  (""-?0|[1-9][0-9]*(\\.[0-9]*)?([eE][+-]?[0-9]+)?""
   (return (cond
             ((find #\e $@)
              (values 'float $@))
             ((find #\. $@)
              (values 'decimal $@))
             (T
              (values 'integer $@)))))
  (""([^\""\\.\\?~\'=&lt;&gt;\\(\\)@\\|\\&amp;/:])+"" (return (values 'name $@)))
  (""\'([^\\\']|\\.)*?\'"" (return (values 'string (string-trim ""\'"" $@))))
  (""\""([^\\\""]|\\.)*?\"""" (return (values 'string (string-trim ""\"""" $@)))))

(define-parser *expression-parser*
  (:muffle-conflicts (44 0))
  (:start-symbol query)
  (:terminals (|\|| #+(or)div &amp; ! |.| ? / = != !== !~ ~ &lt; &gt; == &lt;= &gt;= \( \) + - * @ name integer decimal float string))
  (:precedence ((:left @) (:left ~) (:left |.|) (:left + -) (:left * div) (:left = != == !== ~ !~ &lt; &lt;= &gt; &gt;=) (:left !) (:left &amp;) (:left |\||) (:left ?) (:left /)))

  (query
   segment)

  (segment
   (/ segment (lambda (x y) (declare (ignore x)) (if (eq y :skip) '(:skip) `(:collect ,y))))
   skip
   group)

  (skip
   ((constantly :skip)))

  (group
   (\( segment \) (lambda (x y z) (declare (ignore x z)) `(:group ,y)))
   sieve)

  (sieve
   (segment ? segment (lambda (x y z) (declare (ignore y)) `(:filter ,x ,z)))
   or)

  (or
   (segment |\|| segment (lambda (x y z) `(:operator ,y ,x ,z)))
   and)

  (and
   (segment &amp; segment (lambda (x y z) `(:operator ,y ,x ,z)))
   not)

  (not
   (! segment (lambda (x y) `(:prefix ,x ,y)))
   comparison)

  (comparison
   (segment = segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment != segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment ~ segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment !~ segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment == segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment !== segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment &lt; segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment &lt;= segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment &gt; segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment &gt;= segment (lambda (x y z) `(:operator ,y ,x ,z)))
   addition)

  (addition
   (segment + segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment - segment (lambda (x y z) `(:operator ,y ,x ,z)))
   multiplication)

  (multiplication
   (segment * segment (lambda (x y z) `(:operator ,y ,x ,z)))
   (segment / segment (lambda (x y z) (declare (ignore y)) `(:operator / ,x ,z)))
   composition)

  (composition
   (segment |.| segment (lambda (x y z) (declare (ignore y)) `(:compose ,x ,z)))
   attach)

  (attach
   (segment @ segment (lambda (x y z) (declare (ignore y)) `(:attach ,x ,z)))
   detach)

  (detach
   (@ segment (lambda (x y) (declare (ignore x)) `(:detach ,y)))
   term)

  (term
   (name (lambda (x) `(:identifier ,x)))
   (string (lambda (x) `(:string ,x)))
   (number (lambda (x) `(:integer ,x)))
   (integer (lambda (x) `(:integer ,x)))
   (decimal (lambda (x) `(:decimal ,x)))
   (float (lambda (x) `(:float ,x)))))

(defun make-lexer-for-source (source)
  ""Make a lexer for the SOURCE, either a STRING or a STREAM.""
  (etypecase source
    (string (string-lexer source))
    (stream
     (flet ((ignore (c)
              (declare (ignore c))))
       (stream-lexer #'read-line #'string-lexer #'ignore #'ignore)))))

(defun lex-source (source)
  ""Debug helper to lex a SOURCE into a list of tokens.""
  (let ((lexer (make-lexer-for-source source)))
    (loop
      for (x y) = (multiple-value-list (funcall lexer))
      while x
      collect (list x y))))

(define-condition htsql-parse-error (simple-error) ())

(defun translate-yacc-error (error)
  (make-condition
   'htsql-parse-error
   :format-control ""Couldn't parse HTSQL query: ~A.""
   :format-arguments (list error)))

(defun parse-htsql-query (source)
  ""Parse SOURCE into a syntax tree.  The SOURCE may be either a STRING or
a STREAM.""
  (handler-case
      (parse-with-lexer
       (make-lexer-for-source source)
       *expression-parser*)
    (yacc-parse-error (error)
      (error (translate-yacc-error error)))))

;; &gt; (parse-htsql-query ""/1/"")
;; (:OPERATOR / (:COLLECT (:INTEGER ""1"")) :SKIP)
;; &gt; (parse-htsql-query ""/1/2"")
;; (:OPERATOR / (:COLLECT (:INTEGER ""1"")) (:INTEGER ""2""))
</code></pre>
","<p>When you use HTSQL from a Django shell, you have to open a transaction explicitly:</p>

<pre><code>&gt;&gt;&gt; from django.db import transaction
&gt;&gt;&gt; from htsql_django import produce
&gt;&gt;&gt; with transaction.commit_on_success():
...     query = ""/polls_poll{question, total:=sum(polls_choice.votes)}""
...     for row in produce(query):
...        print ""%s: %s"" % (row.question, row.total)
</code></pre>

<p>I'm sorry the documentation isn't clear about that. We may change it in a future release.</p>
"
"<p>I discovered that under heavy load my pyramid web app throws
py-postgresql exceptions like <code>postgresql.exceptions.ProtocolError</code>.
Some search revealed, that py-postgresql is not thread-safe and one
connection can not be used by several threads concurrently.</p>

<p>I tried to make some sort of pooling mechanism, but I still get
ProtocolErrors :(</p>

<p>What am I doing wrong?</p>

<p>First I create number of connection objects:</p>

<pre><code>    for x in range(num_db_connections):
        self.pool.append(Connection(conn_string,x))
</code></pre>

<p>Each object in pool contains <code>db_lock = threading.Lock()</code> and a connection to database <code>self.conn = postgresql.open( conn_string )</code> </p>

<p>Then I try to acquire lock on a connection and do some work with it.
This code can be executed by many threads concurrently, but i think no
two threads can run <code>work</code> on one connection concurrently because of
lock.</p>

<pre><code>    time_start = time.time()
    while time.time() - time_start &lt; self.max_db_lock_wait_time:
        for conn in self.pool:
            acquired = conn.db_lock.acquire(False)
            if acquired:
                try:
                        lst = conn.work()
                finally:
                    conn.db_lock.release()
                return lst
        time.sleep(0.05)
    raise Exception('Could not get connection lock in time')
</code></pre>

<p>Maybe there is flaw in my code, or I misunderstood the nature of
""thread unsafety"" of py-postgresql?
Please, help me!</p>
","<p>I think that probably is some bug in <a href=""https://admin.fedoraproject.org/pkgdb/acls/name/python3-postgresql?_csrf_token=f882c062f7956b949e845f7efa27455d6ed8869f"" rel=""nofollow""><code>python3-postgresql</code></a> package, but it looks like it works after little change. Edit this file (probably <code>/usr/lib64</code> for 64 bit installations):</p>

<pre><code>/usr/lib/python3.2/site-packages/postgresql/protocol/client3.py
</code></pre>

<p>Change (line 514):</p>

<pre><code>element.Startup(**startup), password
</code></pre>

<p>to:</p>

<pre><code>element.Startup(startup), password
</code></pre>

<p>After that I made simple connection (I changed <code>pg_hba.conf</code> host methods to md5) and it looks ok:</p>

<pre><code>[grzegorz@localhost Desktop]$ python3
Python 3.2 (r32:88445, Feb 21 2011, 21:12:33) 
[GCC 4.6.0 20110212 (Red Hat 4.6.0-0.7)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import postgresql
&gt;&gt;&gt; db = postgresql.open(""pq://grzegorz:12345@localhost/grzegorz"")
&gt;&gt;&gt; ps = db.prepare(""SELECT version()"")
&gt;&gt;&gt; ps()
[('PostgreSQL 9.0.4 on i386-redhat-linux-gnu, compiled by GCC gcc (GCC) 4.6.0 20110530 (Red Hat 4.6.0-9), 32-bit',)]
&gt;&gt;&gt; ps = db.prepare(""TABLE t"")
&gt;&gt;&gt; ps()
[(1, 'aaa'), (2, 'bbb'), (3, 'ccc')]
&gt;&gt;&gt;
</code></pre>
"
"<p>I am trying to output some files using NZSQL CLI but not able to output as tab delimited files. Can somebody who has worked on NZ share your thoughts on this below command. </p>

<p>Tried so far :-</p>

<pre><code>nzsql  -o sample.txt -F=  -A -t -c  ""SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5;""
</code></pre>
","<p>To specify tab as the delimiter use $ in conjunction with the -F option.</p>

<pre><code>nzsql  -o sample.txt -F $'\t'  -A -t -c  ""SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5;""
</code></pre>

<p>This is documented in the nzsql -h output.</p>

<pre><code>nzsql -h
This is nzsql, the IBM Netezza SQL interactive terminal.

Usage:
  nzsql [options] [security options] [dbname [username] [password]]

Security Options:
  -securityLevel       Security Level you wish to request (default: preferredUnSecured)
  -caCertFile          ROOT CA certificate file (default: NULL)

Options:
  -a                   Echo all input from script
  -A                   Unaligned table output mode (-P format=unaligned)
  -c &lt;query&gt;           Run only single query (or slash command) and exit
  -d &lt;dbname&gt;          Specify database name to connect to (default: system)
  -D &lt;dbname&gt;          Specify database name to connect to (default: system)
  -schema &lt;schemaname&gt; Specify schema name to connect to (default: $NZ_SCHEMA)
  -e                   Echo queries sent to backend
  -E                   Display queries that internal commands generate
  -f &lt;filename&gt;        Execute queries from file, then exit
  -F &lt;string&gt;          Set field separator (default: ""|"") (-P fieldsep=)
                       For any binary/control/non-printable character use '$'
                       (e.g., nzsql -F $'\t' // for TAB)
...
</code></pre>

<p>If you have a lot of data, I'd recommend using external tables instead as they perform better.</p>

<pre><code>CREATE EXTERNAL TABLE '/tmp/sample.txt' USING (DELIMITER '\t') 
AS SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5;
</code></pre>
"
"<p>I'm pretty new to the node world and trying to migrate our php application to node. To be able to return all article data several different queries have to be done depending on the results of the first query. Currently my data object is empty as it's returned before the two queries run. How can I ""chain"" these queries using a promised based approach. </p>

<p>I found a library <a href=""https://github.com/lukeb-uk/node-promise-mysql"" rel=""nofollow noreferrer"">https://github.com/lukeb-uk/node-promise-mysql</a> which I think could help but I have no idea how to implement it with my code. </p>

<pre><code>exports.getArticleData = function(req, done) {
  pool.getConnection(function(error, connection) {
    if (error) throw error;

    var data = {
        article: {},
        listicles: []
    };

    // Inital query
    connection.query(
        `SELECT article_id, title, is_listicle_article, FROM com_magazine_articles AS article WHERE article_id = ${req
            .params.articleId}`,
        function(error, results) {
            data.article = results;
        }
    );

    // This query should only be excuted if is_listicle_article = true
    if (data.article.is_listicle_article) {
        connection.query(
            `SELECT * FROM com_magazine_article_listicles WHERE article_id = ${req.params
                .articleId}`,
            function(error, results) {
                data.listicle = results;
            }
        );
    }

    // More queries depending on the result of the first one
    // ....
    // ....

    // Callback with the data object
    done(data);

    connection.release();
  });
};
</code></pre>

<p>What would be the best approach to execute queries based on other queries results? Any help is really appreciated. </p>
","<p>Share my working example:</p>

<p>I use this <a href=""https://gist.github.com/matthagemann/30cfee724d047007a031eb12b3a95a23"" rel=""noreferrer"">Promisified MySQL middleware for Node.js</a></p>

<p>read this article <a href=""https://medium.com/gatemill/create-a-mysql-database-middleware-with-node-js-8-and-async-await-6984a09d49f4"" rel=""noreferrer"">Create a MySQL Database Middleware with Node.js 8 and Async/Await</a></p>

<p>here is my database.js</p>

<pre><code>var mysql = require('mysql'); 

// node -v must &gt; 8.x 
var util = require('util');


//  !!!!! for node version &lt; 8.x only  !!!!!
// npm install util.promisify
//require('util.promisify').shim();
// -v &lt; 8.x  has problem with async await so upgrade -v to v9.6.1 for this to work. 



// connection pool https://github.com/mysqljs/mysql   [1]
var pool = mysql.createPool({
  connectionLimit : process.env.mysql_connection_pool_Limit, // default:10
  host     : process.env.mysql_host,
  user     : process.env.mysql_user,
  password : process.env.mysql_password,
  database : process.env.mysql_database
})


// Ping database to check for common exception errors.
pool.getConnection((err, connection) =&gt; {
if (err) {
    if (err.code === 'PROTOCOL_CONNECTION_LOST') {
        console.error('Database connection was closed.')
    }
    if (err.code === 'ER_CON_COUNT_ERROR') {
        console.error('Database has too many connections.')
    }
    if (err.code === 'ECONNREFUSED') {
        console.error('Database connection was refused.')
    }
}

if (connection) connection.release()

 return
 })

// Promisify for Node.js async/await.
 pool.query = util.promisify(pool.query)



 module.exports = pool
</code></pre>

<p>You must upgrade node -v > 8.x </p>

<p>you must use async function to be able to use await.</p>

<p>example:</p>

<pre><code>   var pool = require('./database')

  // node -v must &gt; 8.x, --&gt; async / await  
  router.get('/:template', async function(req, res, next) 
  {
      ...
    try {
         var _sql_rest_url = 'SELECT * FROM arcgis_viewer.rest_url WHERE id='+ _url_id;
         var rows = await pool.query(_sql_rest_url)

         _url  = rows[0].rest_url // first record, property name is 'rest_url'
         if (_center_lat   == null) {_center_lat = rows[0].center_lat  }
         if (_center_long  == null) {_center_long= rows[0].center_long }
         if (_center_zoom  == null) {_center_zoom= rows[0].center_zoom }          
         _place = rows[0].place


       } catch(err) {
                        throw new Error(err)
       }
</code></pre>
"
"<p>I want to export table data in csv format in my app. I have used Alasql library with XLSX.js. It works fine with all modern browsers(Chrome, Firefox ..) not on Safari. </p>
","<p>Sometimes you want to use Headers including blank spaces (Separated By) or... using headers that are reserved words (Deleted), in both cases you can use [] like this:</p>

<pre><code>alasql('SELECT firstName AS FirstName, [Deleted] AS [Erased], Separated AS [Separated By] INTO XLSX(""test.xlsx"", ?) FROM ?', [options, $scope.users]);
</code></pre>
"
"<p>For a Customer we need to import data from an old</p>

<pre><code>Centura SQLBase 7.5.1 
</code></pre>

<p>database. The best way would be to connect directly from .Net to the customers database, but I can't find a driver or .Net Connector to use. </p>

<p>So far I found out that the company behind the product is currently: <a href=""http://www.unify.com/"" rel=""nofollow"">http://www.unify.com/</a>
But before I contact them, is there a freely available driver / connector for SQLBase available?.</p>
","<p>It was hard to find, but eventually I found the download:</p>

<p><a href=""http://support.guptatechnologies.com/supportwiki/index.php/SQLBase_Driver_Packs"" rel=""nofollow"">http://support.guptatechnologies.com/supportwiki/index.php/SQLBase_Driver_Packs</a></p>

<p>You have to query your favorite search engine for ""SQLBase Driver Packs"" instead of "".NET Data Provider"" to get a result.
The Setup contains ODBC/.NET/OLEDB and JDBC drivers.</p>

<p>That said, I could not connect to my database anyway, because the installer (I tried the 9.0.1 and the 10.0.0 setup) is broken.</p>

<p>The was not able to get the 10.0.0 to work but for the 9.x version I manually copied these files:</p>

<pre><code>- SQLBaseUtil.dll
- MFC71.dll
- msvcr71.dll
- msvcp71.dll
</code></pre>

<p>from the installation path to my system32 folder. That worked for Windows XP and Windows 7, but for Windows 7 I also needed to define the path to the ini file in  the connectionstring:</p>

<pre><code>var connectionString = ""data source=ISLAND;"" +
                       ""uid=sysadm;pwd=sysadm;"" +
                       ""ini=C:\\Program Files\\Gupta\\SQLBase901\\sql.ini"";
</code></pre>
"
"<p>From what I can make out NoSQL databases might be a good option for high intensity data read applications, but are a less good fit if you need to do also do a lot data updates and transactionality is very important to you (what with there being no ACID compliance). Right? Too simplistic maybe. </p>

<p>But anyway, supposing I'm partly right at least I'm now concerned about how NoSQL databases maintain a ""read consistent"" view of the data that you're either reading or writing. Or do they? And if they don't, isn't that a really big problem? </p>

<p>I mean, if the data that you're reading (or updating) is changing as you read it then you're potentially going to get an inconsistent/dirty result set. Coming from an Oracle rdbms background, where all this is just handled for you, I find it confusing how the lack of read consistency is anything but a big problem. Could well be though that I'm missing some key point about all this. Can someone set me straight?</p>
","<p>I am a developer on the Oracle NoSQL Database and will answer your question relative to that particular NoSQL system.</p>

<p>The Oracle NoSQL Database API allows the programmer to specify -- with each API call -- the level of read consistency.  The four possible values, ranging from strictest to loosest, are Absolute, Time, Version, and None.  Absolute says to always read from the replication master so that the most current value is returned.  ""Time"" says that the system can return a value from any replica that is at least within a certain time delta of the master (e.g. read the value from any replica that is within 2 seconds of the master).  Every read and write call to the system returns a ""version handle"".  This version handle may be passed into any read call when Consistency.Version is specified and it tells the system to read from any replica which is at least as up to date as that version.  This is useful for Read Modify Write (aka CAS) scenarios.  The last value, Consistency.None says that any replica can be used (i.e. there is no consistency guaranteed).</p>

<p>I hope this is helpful.</p>

<p>Charles Lamb</p>
"
"<p>How can I tell if a Azure SQL Database has <a href=""https://azure.microsoft.com/en-us/blog/query-store-a-flight-data-recorder-for-your-database/"" rel=""nofollow"">QUERY_STORE</a> turned on? </p>

<p>You enable it with this command:</p>

<pre><code>ALTER DATABASE &lt;database_name&gt; SET QUERY_STORE = ON;
</code></pre>

<p>I figure it should be simple to check the database for this, but I have not found the trick.</p>

<p>FYI, I tried this command on a database that had it enabled, but the command just returned null:</p>

<pre><code>SELECT DATABASEPROPERTYEX ('&lt;database_name&gt;', 'QUERY_STORE')
</code></pre>
","<p>This DMV <kbd><strong><a href=""https://msdn.microsoft.com/en-in/library/dn818146.aspx"" rel=""nofollow noreferrer"">sys.database_query_store_options</a></strong></kbd> should allow you to determine if <code>QUERY_STORE</code> is enabled:</p>

<pre><code>SELECT  desired_state_desc ,
        actual_state_desc ,
        readonly_reason, 
        current_storage_size_mb , 
        max_storage_size_mb ,
        max_plans_per_query 
FROM    sys.database_query_store_options ;
</code></pre>

<p>Description of Actual_state_Desc states :  </p>

<p><strong>OFF (0)</strong>   </p>

<blockquote>
  <p>-Not Enabled   </p>
</blockquote>

<p><strong>READ_ONLY (1)</strong>    </p>

<blockquote>
  <p>Query Store may operate in read-only mode even if read-write was specified by the user. For example, that might happen if the database is in read-only mode or if Query Store size exceeded the quota</p>
</blockquote>

<p><strong>READ_WRITE (2)</strong></p>

<blockquote>
  <p>Query store is on and it is capturing all queries  </p>
</blockquote>

<p><strong>ERROR (3)</strong></p>

<blockquote>
  <p>Extremely rarely, Query Store can end up in ERROR state because of internal errors. In case of memory corruption, Query Store can be recovered by requesting READ_WRITE mode explicitly, using the ALTER DATABASE SET QUERY_STORE statement. In case of corruption on the disk, data must be cleared before READ_WRITE mode is requested explicitly.</p>
</blockquote>
"
"<p>I have an <code>INSERT INTO ... ON DUPLICATE KEY UPDATE ...</code> statement that executes fine (but with warnings) in the <code>mysql&gt;</code> prompt:</p>

<pre><code>mysql&gt; INSERT INTO ... ON DUPLICATE KEY UPDATE ... ;
Query OK, 2 rows affected, 2 warnings (0.00 sec)
Warning (Code 1364): Field 'x' doesn't have a default value
</code></pre>

<p>However, when I try to execute the same statement via JDBC the warning shows up as an <code>SQLException</code> and no rows are updated:</p>

<pre><code>java.sql.SQLException: Field 'x' doesn't have a default value
  at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1055)
  at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:956)
  at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3536)
  at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3468)  
  at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1957)
  at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2107)
  at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2648)
  at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2086)
  at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1365)
</code></pre>

<p><strong>Is there a JDBC or mysql connector setting or command to ignore or suppress these warnings?</strong></p>

<p>I'm using MySQL Community Server 5.1.31 with MySQL connector 5.1.8 and Java 1.5.0_24.</p>
","<p>One of your servers is running in strict mode by default and the other not.
If a server runs in strict mode (or you set it in your connection) and you try to insert a NULL value into a column defined as NOT NULL you will get #1364 error. Without strict mode your NULL value will be replaced with empty string or 0.</p>

<p>Example:</p>

<pre><code>CREATE TABLE `test_tbl` (
 `id` int(11) NOT NULL,
 `someint` int(11) NOT NULL,
 `sometext` varchar(255) NOT NULL,
 `somedate` datetime NOT NULL,
 PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8

SET sql_mode = '';
INSERT INTO test_tbl(id) VALUES(1);
SELECT * FROM test_tbl;
+----+---------+----------+---------------------+
| id | someint | sometext | somedate            |
+----+---------+----------+---------------------+
|  1 |       0 |          | 0000-00-00 00:00:00 |
+----+---------+----------+---------------------+
SET sql_mode = 'STRICT_ALL_TABLES';
INSERT INTO test_tbl(id) VALUES(2);
#1364 - Field 'someint' doesn't have a default value 
</code></pre>
"
"<p>I am using the DMO API via .NET to provide an alternative interface to job scheduling functionality on SQL Server 2000 Agent. The working code looks something like this:</p>

<pre><code>using SQLDMO;

internal class TestDmo {
    public void StartJob() {
        SQLServerClass sqlServer = new SQLServerClass();
        sqlServer.Connect(""MyServerName"", ""sql_user_id"", ""p@ssword""); // no trusted/SSPI overload?

        foreach (Job job in sqlServer.JobServer.Jobs) {
            if (!job.Name.Equals(""MyJob"")) continue;

            job.Start(null);
        }
    }
}
</code></pre>

<p>Everything works in the above-listed form (SQL Server authentication with uid/pwd provided) but I would also like to provide an option to authenticate as a trusted user (aka SSPI, Trusted Connection)</p>

<p>Is this possible in the DMO API? If so how? </p>

<p>Note: The SQLServerClass.Connect method does not seem to have any overloads, I already tried to pass null values for the user id and password to no avail and the Googles has not been helpful yet. Any ideas?</p>
","<p>From <a href=""http://msdn.microsoft.com/en-us/library/ms144581.aspx"" rel=""nofollow"">the documentation</a>:</p>

<blockquote>
  <p>object.Connect( [ ServerName ] , [ Login ] , [ Password ] )</p>
  
  <p>[...]</p>
  
  <p>Use the <em>Login</em> and <em>Password</em> arguments to specify values used for SQL Server Authentication. To use Windows Authentication for the connection, set the <strong>LoginSecure</strong> property to TRUE prior to calling the <strong>Connect</strong> method. When <strong>LoginSecure</strong> is TRUE, any values provided in the <em>Login</em> and <em>Password</em> arguments are ignored.</p>
</blockquote>

<p>Thus, you have to set the <code>LoginSecure</code> property to <code>true</code> before calling Connect. Then, it does not matter which values you pass for the last two parameters.</p>
"
"<p>Currently I'm trying to automatically generate a create script for all my SQL jobs of a MS SQL2005 Server.</p>

<ul>
<li>One method I found was done
manually 
<a href=""http://msdn.microsoft.com/en-us/library/ms191450.aspx"" rel=""nofollow noreferrer"">http://msdn.microsoft.com/en-us/library/ms191450.aspx</a></li>
<li>A second method I found could be done
automatically but I don't have direct
access to the SQL server.
<a href=""http://relatedterms.com/thread/1916663/Can%20I%20script%20out%20SQL%20Server%20jobs%20programmatically"" rel=""nofollow noreferrer"">http://relatedterms.com/thread/1916663/Can%20I%20script%20out%20SQL%20Server%20jobs%20programmatically</a></li>
</ul>

<p>Does anyone know a good TSQL statement or a simple program for this?</p>
","<p>There are several commercial tools out there that can create database scripts, e.g.</p>

<ul>
<li>ApexSQL's <a href=""http://www.apexsql.com/sql_tools_script.asp"" rel=""nofollow noreferrer"">SQL Script</a></li>
<li><a href=""http://sqlmanager.net/en/products/mssql/extract"" rel=""nofollow noreferrer"">EMS DB Extract</a></li>
</ul>

<p>Here's an article showing off a free tool - however, it will only script ALL objects from your database: <a href=""http://www.emoreau.com/Entries/Articles/2009/02/Scripting-your-Microsoft-SQL-Server-database-objects.aspx"" rel=""nofollow noreferrer"">Eric Moreau's blog</a>.</p>

<p>If you want to ""roll your own"", have a look at the Server Management Objects (SMO) - those allow you to inspect your database and create scripts from them.</p>

<p>See info <a href=""http://www.apexsql.com/sql_tools_script.asp"" rel=""nofollow noreferrer"">here</a>, <a href=""http://davidhayden.com/blog/dave/archive/2006/11/09/ScriptDatabaseUsingSQLServerManagementObjects.aspx"" rel=""nofollow noreferrer"">here</a> or <a href=""http://www.sqlservercentral.com/articles/SMO/scriptdatabaseobjectswithsmo/2342/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Marc</p>
"
"<p>What are the different database options on Windows Mobile available?
I have used CEDB and EDB for linear dataset needs.</p>

<p>I have heard of SQL server 2005 Mobile edition. But what are the advantages over others (if there is any)</p>
","<p>Also take a look at <a href=""http://sqlite-wince.sourceforge.net/"" rel=""nofollow noreferrer"">SQLite for Windows CE</a>.  There are also .NET bindings available to use it from the Compact Framework.</p>
"
"<p>I am trying to decrypt one database file by using python 3.7. So to decrypt it I have to  use pysqlcipher3 version for python 3.7. To install it I have tried by using both commands:</p>

<pre><code>pip3 install pysqlcipher3
</code></pre>

<p>and </p>

<pre><code>pip install pysqlcipher3
</code></pre>

<p>and both the commands have showed successful installation of pysqlcipher package. But now the issue is when I am trying to import pysqlcipher3 in my python project by using this line: </p>

<pre><code>from pysqlcipher3 import dbapi2 as sqlite
</code></pre>

<p>it displays me this error:</p>

<pre><code>ModuleNotFoundError: No module named 'pysqlcipher3
</code></pre>

<p>I have checked various github projects but none of them provide a clear working solution. The python packages website says to install <strong>libsqlcipher</strong> in your OS but this time the issue is same, no documentation and link regarding for the installation of libsqlcipher for windows 10. So anyone please if can provide me proper steps of installation or any document or any video tutorial regarding the same Or is there any issue with the import statement?</p>
","<p>Since I haven't found a way how to do it I decided to use <a href=""https://github.com/orcasgit/django-fernet-fields"" rel=""nofollow"">django-fernet-fields</a>. </p>

<p>The way it works is that it ciphers individual fields in the database, so one can still open the database and check the tables structure, however individual entries are ciphered. </p>

<p>Additionally, it is very easy to use and integrate. </p>
"
"<p>I've just installed <code>rsqlserver</code> like so (no errors)</p>

<pre><code>install_github('rsqlserver', 'agstudy',args = '--no-multiarch')
</code></pre>

<p>And created a connection to my database:</p>

<pre><code>&gt; library(rClr)
&gt; library(rsqlserver)

Warning message:
multiple methods tables found for dbCallProc 

&gt; drv &lt;- dbDriver(""SqlServer"")
&gt; conn &lt;- dbConnect(drv, url = ""Server=MyServer;Database=MyDB;Trusted_Connection=True;"")
&gt; 
</code></pre>

<p>Now when I try to get data using <code>dbGetQuery</code>, I get this error:</p>

<pre><code>&gt; df &lt;- dbGetQuery(conn, ""select top 100 * from public2013.dim_Date"")

Error in clrCall(sqlDataHelper, ""GetConnectionProperty"", conn, prop) : 
  Type:    System.MissingMethodException
Message: Method not found: 'System.Object System.Reflection.PropertyInfo.GetValue(System.Object)'.
Method:  System.Object GetConnectionProperty(System.Data.SqlClient.SqlConnection, System.String)
Stack trace:
   at rsqlserver.net.SqlDataHelper.GetConnectionProperty(SqlConnection _conn, String prop)

&gt; 
</code></pre>

<p>When I try to fetch results using <code>dbSendQuery</code>, I also get an error.</p>

<pre><code>&gt; res &lt;- dbSendQuery(conn, ""select top 100 * from public2013.dim_Date"")
&gt; df &lt;- fetch(res, n = -1)

Error in clrCall(sqlDataHelper, ""Fetch"", stride) : 
  Type:    System.InvalidCastException
Message: Object cannot be stored in an array of this type.
Method:  Void InternalSetValue(Void*, System.Object)
Stack trace:
   at System.Array.InternalSetValue(Void* target, Object value)
   at System.Array.SetValue(Object value, Int32 index)
   at rsqlserver.net.SqlDataHelper.Fetch(Int32 capacity) in c:\projects\R\rsqlserver\src\rsqlserver.net\src\SqlDataHelper.cs:line 116
</code></pre>

<p>Strangely, the file <code>c:\projects\R\rsqlserver\src\rsqlserver.net\src\SqlDataHelper.cs</code> doesn't actually exist on my computer.</p>

<p>Am I doing something wrong?</p>
","<p>I am agstudy the creator of <code>rsqlserver</code> package. Sorry for the late but I finally I get some time to fix this bug. ( actually it was a not yet implemented feature). I demonstrate here how you can read/write data.frame with missing values in Sql server.</p>

<p>First I create a data.frame with missing values. It is important to distinguish the difference between  numeric and character variables. </p>

<pre><code>library(rsqlserver)
url = ""Server=localhost;Database=TEST_RSQLSERVER;Trusted_Connection=True;""
conn &lt;- dbConnect('SqlServer',url=url)
## create a table with some missing value
dat &lt;- data.frame(txt=c('a',NA,'b',NA),
                  value =c(1L,NA,NA,2))
</code></pre>

<p>My input looks like this :</p>

<pre><code># txt value
# 1    a     1
# 2 &lt;NA&gt;    NA
# 3    b    NA
# 4 &lt;NA&gt;     2
</code></pre>

<p>I insert dat in my data base with the handy function <code>dbWriteTable</code>:
    dbWriteTable(conn,name='T_TABLE_WITH_MISSINGS',
                 dat,row.names=FALSE,overwrite=TRUE)
Then I will read it using 2 methods:</p>

<h3>dbSendQuery</h3>

<pre><code>res = dbSendQuery(conn,'SELECT * 
                  FROM T_TABLE_WITH_MISSINGS')
fetch(res,n=-1)
dbDisconnect(conn)
   txt value
1    a     1
2 &lt;NA&gt;   NaN
3    b   NaN
4 &lt;NA&gt;     2
</code></pre>

<h3>dbReadTable:</h3>

<p>rsqlserver is DBI compliant and implement many convenient functions to deal at least at possible with SQL.</p>

<pre><code>conn &lt;- dbConnect('SqlServer',url=url)
dbReadTable(conn,name='T_TABLE_WITH_MISSINGS')
dbDisconnect(conn)
   txt value
1    a     1
2 &lt;NA&gt;   NaN
3    b   NaN
4 &lt;NA&gt;     2
</code></pre>
"
"<p>I'm trying to debug a very strange discrepency between 2 seperate cosmos db collection that on face value are configured the same.</p>

<p>We recently modified some code that executed the following query.</p>

<p>OLD QUERY</p>

<pre><code>SELECT * FROM c 
WHERE c.ProductId = ""CODE"" 
AND c.PartitionKey = ""Manufacturer-GUID""
</code></pre>

<p>NEW QUERY</p>

<pre><code>SELECT * FROM c
WHERE (c.ProductId = ""CODE"" OR ARRAY_CONTAINS(c.ProductIdentifiers, ""CODE"")) 
AND c.PartitionKey = ""Manufacturer-GUID""
</code></pre>

<p>The introduction of that <code>Array_Contains</code> call in the production environment has tanked the performance of this query from ~3 RU/s ==> ~6000 RU/s. But only in the production environment.</p>

<p>And the cause seems to be that in Production, it's not hitting an index. See output below for the two environments.</p>

<p><strong>DEV CONFIGURATION</strong></p>

<p>Collection Scale: 2000 RU/s</p>

<p>Index Policy: (Notice the Exclude path for ETags)</p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/*"",
            ""indexes"": [
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""String"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Spatial"",
                    ""dataType"": ""Point""
                }
            ]
        }
    ],
    ""excludedPaths"": [
        {
            ""path"": ""/\""_etag\""/?""
        }
    ]
}
</code></pre>

<p><strong>PROD CONFIGURATION</strong></p>

<p>Collection Scale: 10,000 RU/s</p>

<p>Index Policy: (Notice the absense of an Exclude path for ETags compared to DEV)</p>

<pre><code>{
    ""indexingMode"": ""consistent"",
    ""automatic"": true,
    ""includedPaths"": [
        {
            ""path"": ""/*"",
            ""indexes"": [
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""Number"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Range"",
                    ""dataType"": ""String"",
                    ""precision"": -1
                },
                {
                    ""kind"": ""Spatial"",
                    ""dataType"": ""Point""
                }
            ]
        }
    ],
    ""excludedPaths"": []
}
</code></pre>

<p>When comparing the Output results from both environments, DEV is showing an index hit, while PROD is showing an index miss, despite no noticeable difference between the index policies.</p>

<p><strong>Results in DEV</strong></p>

<pre><code>Request Charge:           3.490 RUs
Showing Results:          1 - 1
Retrieved document count: 1
Retrieved document size:  3118 bytes
Output document count:    1
Output document size:     3167 bytes
Index hit document count: 1
</code></pre>

<p><strong>Results in PROD</strong></p>

<pre><code>Request Charge:           6544.870 RUs
Showing Results:          1 - 1
Retrieved document count: 124199
Retrieved document size:  226072871 bytes
Output document count:    1
Output document size:     3167 bytes
Index hit document count: 0
</code></pre>

<p>The only thing I've been able to find online is a reference in the documents to some change that occurred within Cosmos Collection stating that a ""New Index Layout"" is being used for newer collections, but there's no other mention of Index Layouts as a concept that I can find anywhere in the docs.</p>

<p><a href=""https://docs.microsoft.com/en-us/azure/cosmos-db/index-types#index-kind"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/cosmos-db/index-types#index-kind</a></p>

<p>Anyone got any idea where I can go from here in terms of debugging/addressing this issue.</p>
","<p>Currently <strong><a href=""https://feedback.azure.com/forums/263030-documentdb/suggestions/6333414-implement-wildcards-when-searching"" rel=""noreferrer""><code>Azure Cosmosdb</code></a></strong> supports the <strong><code>CONTAINS</code></strong>, <strong><code>STARTSWITH</code></strong>, and <strong><code>ENDSWITH</code></strong> built-in functions which are equivalent to LIKE.</p>

<p>The keyword for LIKE in Cosmosdb is COntains .  </p>

<pre><code>""SELECT * FROM c WHERE CONTAINS(c.pi, '09')""
</code></pre>
"
"<p>I am migrating from SQL Server 2008 To 2014 and I get an error </p>

<blockquote>
  <p>The data types datetime and time are incompatible in the add operator.</p>
</blockquote>

<p>I figured out the solution convert time to DateTime, but I have changes in many stored procedures and views.</p>

<p>Is there any other solution for above problem?</p>
","<p>In your function up() or safeUp() you would add the following code:</p>

<pre><code>$this-&gt;createTable('tbl_post', array(
  ""id"" =&gt; ""pk"",
  ""title"" =&gt; ""VARCHAR(128) NOT NULL"",
  ""content"" =&gt; ""TEXT NOT NULL"",
  ""tags"" =&gt; ""TEXT"",
  ""status"" =&gt; ""INT NOT NULL"",
  ""create_time"" =&gt; ""INT"",
  ""update_time"" =&gt; ""INT"",
  ""author_id"" =&gt; ""INT NOT NULL"",
), ""ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci"");

$this-&gt;addForeignKey('FK_post_author', 'tbl_post', 'author_id', 'tbl_user', 'id', 'CASCADE', 'RESTRICT');

$this-&gt;insert('tbl_lookup', array(
  ""name"" =&gt; ""Draft"",
  ""type"" =&gt; ""PostStatus"",
  ""code"" =&gt; 1,
  ""position"" =&gt; 1,
);
</code></pre>

<p>There is an update() method available as well (insert has been shown above):
<a href=""http://www.yiiframework.com/doc/api/1.1/CDbMigration#update-detail"" rel=""nofollow"">http://www.yiiframework.com/doc/api/1.1/CDbMigration#update-detail</a></p>
"
"<p>How connect to Sybase custdb.db using SwisSQL?
I work with standart Sybase sample.</p>

<p>I input appropriate command line:</p>

<pre><code>""mlsrv12.exe -vcrs -zu+
   -c ""dsn=SQL Anywhere 12 CustDB;uid=ml_server;pwd=sql""
   -x http(port=80)""
</code></pre>

<p>Database work fine - I test it from InteractiveSQL and from Android - client. 
When I try to migrate from this database to Oracle I have trouble.</p>

<pre><code>Host name: 127.0.0.1
Port number: 80
User name: ml_server
Password: sql
Service ID (SID): custdb
Shema(optional): empty
</code></pre>

<p>What I am doing wrong?</p>

<p>Any help appreciated!</p>
",""
"<p>I have the same problem that was discussed <a href=""https://stackoverflow.com/questions/26878467/pygresql-error-from-pg-import-importerror-dll-load-failed-the-specified-mo"">here</a>, but I haven't credit to comment an answer so I start new question. </p>

<p>I have in PATH way to libpq.dll (C:\PostgreSql\lib) but it doesn't solve this problem.</p>

<p>Using Python 2.7.9 32-bit, PostgreSQL 8.4, Win 8</p>

<pre><code>Traceback (most recent call last):
File ""&lt;pyshell#1&gt;"", line 1, in &lt;module&gt;
    import pg
  File ""C:\Python27\lib\site-packages\pg.py"", line 21, in &lt;module&gt;
from _pg import *
ImportError: DLL load failed: The specified module could not be found.
</code></pre>
","<p>Try running this:</p>

<pre><code>sudo apt-get install libpq-dev
</code></pre>

<p>and retry.</p>
"
"<p>I used the official tutorial to create a default instance
<a href=""https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu</a></p>

<p>but now I want to create a named-instance and can't find how to do that</p>
","<p>Yes, that's correct from a remote connection view if you change MyServerABC to MyServerDEF in connection strings.</p>

<p>There are a few more things to consider (@@SERVERNAME will not change by default for example) so have a look here: <a href=""http://msdn.microsoft.com/en-us/library/ms143799.aspx"" rel=""nofollow noreferrer"">How to: Rename a Computer that Hosts a Stand-Alone Instance of SQL Server</a></p>

<p>Often, you'd use MyServerPermanentAlias as a network DNS entry too so the actual server name is irrelevant.</p>
"
"<p>I have the following table:</p>

<pre><code>create table dbo.Link
(
    FromNodeId int not null,
    ToNodeId int not null
)
</code></pre>

<p>Rows in this table represent links between nodes.</p>

<p>I want to prevent inserts or updates to this table from creating a cyclic relationship between nodes.</p>

<p>So if the table contains:</p>

<pre><code>(1,2)
(2,3)
</code></pre>

<p>it should not be allowed to contain any of the following:</p>

<pre><code>(1,1)
(2,1)
(3,1)
</code></pre>

<p>I'm happy to treat (1,1) separately (e.g. using a CHECK CONSTRAINT) if it makes the solution more straightforward.</p>

<p>I was thinking of creating an AFTER INSERT trigger with a recursive CTE (though there may be an easier way to do it).</p>

<p>Assuming this is the way to go, what would the trigger definition be? If there is a more elegant way, what is it?</p>
","<p>The node names inside MATCH can be repeated. In other words, a node can be traversed an arbitrary number of
times in the same query.
An edge name cannot be repeated inside MATCH.
An edge can point in either direction, but it must have an explicit direction.
OR and NOT operators are not supported in the MATCH pattern. MATCH can be combined with other expressions
using AND in the WHERE clause. However, combining it with other expressions using OR or NOT is not
supported.
Find 2 User who are both friends with same User</p>

<pre><code>SELECT Person1.name AS Friend1, Person2.name AS Friend2
FROM user user1, friend friend1, user user2,
friend friend2, user user0
WHERE MATCH(user1-(friend1)-&gt;user0&lt;-(friend2)-user2);
</code></pre>

<p>this pattern can also be expressed as below</p>

<pre><code>SELECT user1.name AS Friend1, user2.name AS Friend2
FROM user user2, friend friend1, user user2,
friend friend2, user user0
WHERE MATCH(user1-(friend1)-&gt;user0 AND user2-(friend2)-&gt;user0);
</code></pre>
"
"<p>Why teamsql is so slow to execute sql commands? 
I've been using tableplus and teamsql clients and teamsql is to much slow than teable plus.</p>
","<p>TeamSQL doesn't have that feature, but you can export your result sets using export options (as CSV, JSON, etc.. )</p>

<p>Right click on the gridview and you'll see ""Export Table"" option;</p>

<p><a href=""https://i.stack.imgur.com/Q6FWg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q6FWg.png"" alt=""enter image description here""></a></p>
"
"<p>When a query is run from the snowsql shell, i get to see the generated query id on the UI. Later on if i have to search for the same in the history, i want to search the query id that i can define or someway to tag the query.</p>

<p>Is it possible to create my own query id or tag when i run the query?</p>
","<p>You first need to specify the non-default date format for your input data.  In the case of the example above:</p>

<pre><code>alter session set date_input_format = 'DD-MON-YY';
</code></pre>

<p>Then</p>

<pre><code>alter session set  TWO_DIGIT_CENTURY_START=2000;
select cast ('05-FEB-00' as date) from dual;
</code></pre>

<p>yields:</p>

<p>2000-02-05</p>
"
"<p>I'm not sure if this is even within the scope of MySQL to be honest or if some php is necessary here to parse the data.  But if it is... some kind of stored procedure is likely necessary.</p>

<p>I have a table that stores rows with a timestamp and an amount.  </p>

<p>My query is dynamic and will be searching based on a user-provided date range.  I would like to retrieve the SUM() of the amounts for each day in a table that are between the date range.  <strong><em>including a 0 if there are no entries for a given day</em></strong></p>

<p>Something to the effect of...</p>

<pre><code>SELECT 
   CASE
     WHEN //there are entries present at a given date
       THEN SUM(amount)
     ELSE 0
   END AS amountTotal,
   //somehow select the day
   FROM  thisTableName T
   WHERE T.timeStamp BETWEEN '$start' AND '$end'
   GROUP BY //however I select the day
</code></pre>

<p>This is a two parter...<br>
<strong>is there a way to select a section of a returned column? Like some kind of regex within mysql?<br>
Is there a way to return the 0's for dates with no rows?</strong></p>
","<p>First of all, isn't your sample code </p>

<pre><code>SELECT SUM(p.Amount), foo.d 
    FROM foo LEFT JOIN ItemTracker_dbo.Payment ON foo.d = p.Datetime 
    WHERE p.ClientId = ClientId 
    GROUP BY 
        foo.d; 
</code></pre>

<p>missing a <code>p</code>  for the alias for table <code>ItemTracker_dbo.Payment</code>  ??
i.e., shouldn;t it read:</p>

<pre><code>SELECT SUM(p.Amount), foo.d 
    FROM foo 
       LEFT JOIN ItemTracker_dbo.Payment p
            ON foo.d = p.Datetime 
WHERE p.ClientId = ClientId 
GROUP BY foo.d; 
</code></pre>

<p>Anyway, the reason you are having this problem is that where clause conditions are not applied until after the outer join is processed (where the rows from the outer side are added back in) So you need to move the where condition into the join condition:</p>

<pre><code>SELECT SUM(p.Amount), foo.d 
    FROM foo 
       LEFT JOIN ItemTracker_dbo.Payment p
            ON foo.d = p.Datetime 
                And p.ClientId = ClientId 
GROUP BY foo.d; 
</code></pre>
"
"<p>With the recent update of <a href=""https://www.postgresql.org/about/news/1855/"" rel=""nofollow noreferrer"">Postres 11</a>. It now supports stored procedures. Transactions are finally supported in stored procedures in Postgres. However, I have been trying to perform a commit within a procedure with no luck.</p>

<pre><code>    CREATE OR REPLACE PROCEDURE test_update(
        IN pid character,
        IN pcategoryname character varying,
        IN pwebcode character varying,
        IN porder numeric,
        IN pupdatedby character varying,
        IN pupdateddate timestamp without time zone,
        IN plang character varying,
        INOUT cresults refcursor)
      LANGUAGE plpgsql
    AS
    $BODY$
    DECLARE

      cnt bigint;
      rtnCode char(1);

    BEGIN
     cresults := 'cur';

        BEGIN
        update test_table
        set  FCATEGORYNAME   = pCategoryName,
             FWEBCODE        = pWebCode,
             FORDER          = pOrder,
             FUPDATEDBY      = pUpdatedBy,
             FUPDATEDDATE    = pUpdatedDate,
             FLANGCODE       = pLang
        where lower(FID) = lower(pId);
        COMMIT;
        end;
      .
      .
      EXCEPTION WHEN ....
      .
      .
     OPEN cresults FOR VALUES ('stringresult'); 

    end;
    $BODY$;
</code></pre>

<p><strong>UPDATED</strong>
Edited so that the Update and Commit is within a block that does not have any exception.</p>

<p>This was previously used to execute the procedure. I was able to fetch the result of the refcursor. Does not work anymore after the commit is added.:</p>

<pre><code>begin;
CALL testupdate('ad3caecb-9235-4945-b37a-9b7ff89fdfe0','aa','138',0,'test','2018/06/29 18:04:03','zh-cn','');
fetch all in cur;
commit;
</code></pre>

<p>but when i execute with:</p>

<pre><code>CALL testupdate('ad3caecb-9235-4945-b37a-9b7ff89fdfe0','aa','138',0,'test','2018/06/29 18:04:03','zh-cn','');
</code></pre>

<p>.. it runs fine and the row got updated, but this way of running doesnt allow me to fetch the result my refcursor.</p>

<p>Is there any way to get the result of my refcursor when excuting a procedure with COMMIT. Any help is greatly appreciated. Thanks</p>
","<p>I <em>think</em> this is because the cast operator <code>::</code> has a higher precedence that the minus sign. </p>

<p>So <code>-32768::smallint</code> is executed as <code>-1 * 32768::smallint</code> which indeed is invalid. </p>

<p>Using parentheses fixes this: <code>(-32768)::smallint</code> or using the SQL standard <code>cast()</code> operator: <code>cast(-32768 as smallint)</code></p>
"
"<p>I used <a href=""https://github.com/SimulatedGREG/electron-vue"" rel=""nofollow noreferrer"">electron-vue</a> to generate the base for my project. I can get my application to launch, however when I attempt to run <code>yarn test</code>, I get the following error:</p>

<pre><code>Child html-webpack-plugin for ""index.html"":
                                   Asset      Size  Chunks             Chunk Names
                              index.html    568 kB       1             
    db649fa959425ff07a09.hot-update.json  44 bytes          [emitted]  
       [0] ./node_modules/html-webpack-plugin/lib/loader.js!./src/index.ejs 1.46 kB {1} [built]
       [1] ./node_modules/lodash/lodash.js 540 kB {1} [built]
       [2] (webpack)/buildin/module.js 517 bytes {1} [built]
19 01 2018 02:30:17.603:INFO [karma]: Karma v1.7.1 server started at http://0.0.0.0:9876/
19 01 2018 02:30:17.605:INFO [launcher]: Launching browser visibleElectron with unlimited concurrency
19 01 2018 02:30:17.618:INFO [launcher]: Starting browser Electron
19 01 2018 02:30:19.579:INFO [Electron 1.7.9 (Node 7.9.0)]: Connected on socket XnzwV7bi3QYEDoyCAAAA with id 16128747
Electron 1.7.9 (Node 7.9.0) ERROR
  Uncaught Error: Could not locate the bindings file. Tried:
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Debug/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/out/Debug/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/Debug/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/out/Release/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/Release/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/default/better_sqlite3.node
    /home/cassius/workspace/tagister/node_modules/better-sqlite3/compiled/7.9.0/linux/x64/better_sqlite3.node
  at webpack:///node_modules/bindings/bindings.js:96:0 &lt;- index.js:30116

Electron 1.7.9 (Node 7.9.0): Executed 0 of 0 ERROR (1.957 secs / 0 secs)
</code></pre>

<p>The bindings do actually exist:</p>

<pre><code>find $HOME/workspace/tagister/node_modules/better-sqlite3/ -name ""*node""
/home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/test_extension.node
/home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/better_sqlite3.node
/home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/obj.target/test_extension.node
/home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/obj.target/better_sqlite3.node
</code></pre>

<p>To get <code>better-sqlite3</code> to work in development mode, I did have to add <code>""postinstall"": ""electron-builder install-app-deps""</code> to my <code>package.json</code>. I don't understand why the tests are failing when the dependent modules are clearly there. </p>

<p>I thought this might have been a bug and added an <a href=""https://github.com/electron/electron/issues/11388"" rel=""nofollow noreferrer"">issue to electron's github</a> but there haven't been any comments on it.</p>

<p>This can be reproduced locally with the following:</p>

<pre><code>git clone https://gitlab.com/djsumdog/tagster.git
git checkout ca712c4c
yarn
yarn test
</code></pre>
","<p>SQL parameters do not need formatting; they are not inserted into the query text, but passed directly to the database. (This is the only practical way to handle blobs, which can literally contain anything.)</p>
"
"<p>I am using <code>SQL Server 2016</code> and <code>SSIS</code>. I have a package in which I am calling a sql server job. </p>

<p>My job have just one step as <code>PowerShell command</code> for transfering all files from one directory to another directory between two servers.</p>

<p>When I run my job from SQL server, it works well.</p>

<p>When I run my package from <code>VisualStudio</code>, every things is ok, but after deploying my project to sql server, when I want to run this package from SQL Server, I have this error : </p>

<blockquote>
  <p>Execute SQL Server Agent Job Task: Error: Failed to lock variable ""RunId"" for read access with error 0xC0010001 ""The variable cannot be found. This occures when an attempt is made to retrive a variable from the Variable collection on a container during execution of package, and the variable is not there. The variable name may have changed or the variable is not being created."". </p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/XSomW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XSomW.png"" alt=""enter image description here""></a></p>

<p>I deleted the <code>Execute sql server agent job task</code> from my package and I redeployed my project and now I do't have this error. It is clear that error is about <code>Execute sql server agent job task</code>.</p>

<p>I have more than 2000 files in source directory and it takes several minutes to transfer all files to destination directory.</p>

<p><strong>Edit1</strong> : I have several Task and component in my package and the last ones is <code>Execute sql server agent job task</code> and i don't have <code>Execute T-SQL Statement Task</code>.</p>
","<p>You get an error because your job step command contains string <code>@backupfilename</code>, which is a variable that is not defined in the context of this command. If I am not mistaken <code>sp_add_jobstep</code> does not support a way how you could pass parameters into this command, so you need to create command string without parameters:</p>

<pre><code>set @path ='BACKUP DATABASE AdventureWorks2012 TO  DISK ='''+ @backupfilename + ''''
</code></pre>

<p>Here I concatenated the string and also added quotes.</p>
"
"<p>I have seen several mentions of an <em>""upsert mode""</em> for dynamic tables based on a unique key in the Flink documentation and on the official Flink blog. However, I do not see any examples / documentation regarding how to enable this mode on a dynamic table.</p>

<p>Examples:</p>

<ul>
<li><p><a href=""https://flink.apache.org/news/2017/04/04/dynamic-tables.html"" rel=""nofollow noreferrer"">Blog post</a>:</p>

<blockquote>
  <p>When defining a dynamic table on a stream via update mode, we can specify a <strong>unique key</strong> attribute on the table. In that case, update and delete operations are performed with respect to the key attribute. The <strong>update mode</strong> is visualized in the following figure.</p>
</blockquote></li>
<li><p><a href=""https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/table/streaming.html"" rel=""nofollow noreferrer"">Documentation</a>:</p>

<blockquote>
  <p>A dynamic table that is converted into an <strong>upsert stream</strong> requires a (possibly composite) <strong>unique key</strong>.</p>
</blockquote></li>
</ul>

<p>So my questions are:</p>

<ul>
<li>How do I specify a unique key attribute on a dynamic table in Flink?</li>
<li>How do I place a dynamic table in update/upsert/""replace"" mode, as opposed to append mode?</li>
</ul>
","<p>Apache Flink's Table API join is an inner equi-join and requires at least one equality predicate. </p>

<p>Joins without predicates are cross products. Flink's Table API does not offer an operator for cross product, because cross products are <strong>very</strong> expensive to compute.  </p>

<p>With Flink's DataSet API, cross products can be computed using cross operator or a map function with a Broadcast set.</p>
"
"<p>I have SQL CLR function based on .net regex function in order to split values by regular expression. In one of the cases, I am using the function to split a value by <code>|</code>. The issue is I have found that one of the values has double <code>||</code>. Since, I am sure that the second value (right value) is a number, I know that the second <code>|</code> is part of the first value (left value).</p>

<p>I have:</p>

<pre><code>||2215
</code></pre>

<p>and that should be split to:</p>

<pre><code>|
2215
</code></pre>

<p>I am splitting using this expression <code>[|]</code>. I think that in order to make it work, I need to use <code>Zero-width negative look ahead assertion.</code> but when I do split by <code>(?![|])[|]</code> I get:</p>

<pre><code>||2215
</code></pre>

<p>and if I try with look behind - <code>(?&lt;![|])[|]</code> I get:</p>

<pre><code>
|2215
</code></pre>

<p>but I need the pipe to be part of the first value. </p>

<p>Could anyone assist me on this? Looking for only regex solution as not being able to change the application right now.</p>

<hr>

<p>Here is the function if anyone need it:</p>

<pre><code>/// &lt;summary&gt;
///     Splits an input string into an array of substrings at the positions defined by a regular expression pattern.
///     Index of each value is returned.
/// &lt;/summary&gt;
/// &lt;param name=""sqlInput""&gt;The source material&lt;/param&gt;
/// &lt;param name=""sqlPattern""&gt;How to parse the source material&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
[SqlFunction(FillRowMethodName = ""FillRowForSplitWithOrder"")]
public static IEnumerable SplitWithOrder(SqlString sqlInput, SqlString sqlPattern)
{
    string[] substrings;
    List&lt;Tuple&lt;SqlInt64, SqlString&gt;&gt; values = new List&lt;Tuple&lt;SqlInt64, SqlString&gt;&gt;(); ;

    if (sqlInput.IsNull || sqlPattern.IsNull)
    {
        substrings = new string[0];
    }
    else
    {
        substrings = Regex.Split(sqlInput.Value, sqlPattern.Value);
    }

    for (int index = 0; index &lt; substrings.Length; index++)
    {
        values.Add(new Tuple&lt;SqlInt64, SqlString&gt;(new SqlInt64(index), new SqlString(substrings[index])));
    }

    return values;
}
</code></pre>
","<p>You should use a negative lookahead here rather than a lookbehind</p>

<pre><code>[|](?![|])
</code></pre>

<p>See the <a href=""http://regexstorm.net/tester?p=%5B%7C%5D%28%3F!%5B%7C%5D%29&amp;i=%E6%85%82%7C%7C2215"" rel=""nofollow noreferrer"">regex demo</a></p>

<p><strong>Details</strong></p>

<ul>
<li><code>[|]</code> - matches a <code>|</code> char</li>
<li><code>(?![|])</code> - a negative lookahead that requires no <code>|</code> char immediately to the right of the current location.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/jwvTu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jwvTu.png"" alt=""enter image description here""></a></p>
"
"<p>I am using MySQL and PHP with 2 application servers and 1 database server.
With the increase of the number of users (around 1000 by now), I'm getting the following error :</p>

<pre><code>SQLSTATE[08004] [1040] Too many connections
</code></pre>

<p>The parameter <code>max_connections</code> is set to <code>1000</code> in <code>my.cnf</code> and <code>mysql.max_persistent</code> is set to <code>-1</code> in <code>php.ini</code>.</p>

<p>There are at most 1500 apache processes running at a time since the <code>MaxClients</code> apache parameter is equal to 750 and we have 2 application servers.</p>

<ul>
<li>Should I raise the <code>max_connections</code> to 1500 as indicated <a href=""https://stackoverflow.com/questions/2229974/sqlstate08004-1040-too-many-connections"">here</a>?</li>
<li>Or should I set <code>mysql.max_persistent</code> to 750 (we use PDO with persistent connections for performance reasons since the database server is not the same as the application servers)?</li>
<li>Or should I try something else?</li>
</ul>

<p>Thanks in advance!</p>
","<p>It sounds like the MySQL server is being shared by too many accounts, as that error means that the server's <code>max_connections</code> setting has been exceeded.  It could be that there are several other high-traffic customers that are taking up the connections (this is a shared MySQL server, I take it?)</p>

<p>I'm fairly sure that PHP automatically closes resources at the end of the request (although it's always best to explicitly close your connections etc. anyway) so I wouldn't think it's your script keeping connections open.</p>

<p>You'll need to keep on at your hosting provider, I'm afraid, or consider moving providers.  Also try to keep a log of when the errors occur and for how long, as you'll need that as evidence if they refuse to acknowledge there's a problem.</p>

<p>You can also refer your provider to this page: <a href=""http://dev.mysql.com/doc/refman/5.1/en/too-many-connections.html"" rel=""nofollow noreferrer"">http://dev.mysql.com/doc/refman/5.1/en/too-many-connections.html</a>.</p>
"
"<p>Is is possible to use Micronaut with JNoSQL ?
JNoSQL is depending on CDI implementatio i.e Weld, Micronaut support many of the same set of annotations, but I do not think it is exposing a full CDI container, so the question is it possible to use some of the JNoSQL goodness with Micronaut ?</p>

<p>Thank you.
Luis Oscar</p>
","<p>If Micronaut implements CDI 2.0 or higher it will work.
However, I think it works with Spring as the engine. Eclipse JNoSQL has the initiative to work with Spring Data soon.</p>
"
"<p>I want to use vagrant, and I defined the following puppet file:
<a href=""http://pastebin.com/GfJK1ziS"" rel=""nofollow"">http://pastebin.com/GfJK1ziS</a></p>

<p>When vagrant tries to install the modules everything works as expected. But when it tries to configure <code>mysql</code>, it always get this error: </p>

<pre><code> Error: Validation of Mysql_grant[${username}@%/${db_name}.*] failed: name must match user and table parameters
</code></pre>

<p>What can I do?</p>

<p>As far as I can tell its due to this line in <code>puppetlabs_mysql</code> module</p>

<p><a href=""https://github.com/puppetlabs/puppetlabs-mysql/commit/07b661dcea926981cf5cd1c703a1c982d6eb6ef1"" rel=""nofollow"">https://github.com/puppetlabs/puppetlabs-mysql/commit/07b661dcea926981cf5cd1c703a1c982d6eb6ef1</a></p>

<p>i don't know what i have to change</p>
","<p>Instead of <code>ensure</code>, use <code>package_name</code>. This worked for me:</p>

<pre><code>class { '::mysql::server':
  package_name =&gt; 'mysql-server-5.6'
}
</code></pre>

<p>If incorrect version of <code>mysql-client</code> is being installed, add this as well:</p>

<pre><code>class { '::mysql::client':
  package_name =&gt; 'mysql-client-5.6'
}
</code></pre>
"
"<p>I am trying to read a Parquet file from Azure Data Lake using the following Pyspark code.</p>

<pre><code>df= sqlContext.read.format(""parquet"")
   .option(""header"", ""true"")
   .option(""inferSchema"", ""true"")
   .load(""adl://xyz/abc.parquet"")
df = df['Id','IsDeleted']
</code></pre>

<p>Now I would like to load this dataframe df as a table in sql dataware house using the following code:</p>

<pre><code>df.write \
  .format(""com.databricks.spark.sqldw"") \
  .mode('overwrite') \
  .option(""url"", sqlDwUrlSmall) \
  .option(""forward_spark_azure_storage_credentials"", ""true"") \
  .option(""dbtable"", ""test111"") \
  .option(""tempdir"", tempDir) \
  .save()
</code></pre>

<p>This creates a table dbo.test111 in the SQL Datawarehouse with datatypes: </p>

<ul>
<li>Id(nvarchar(256),null)</li>
<li>IsDeleted(bit,null)</li>
</ul>

<p>But I need these columns with different datatypes say char(255), varchar(128) in SQL Datawarehouse. How do I do this while loading the dataframe into SQL Dataware house?</p>
","<p>This isn't the issue that you might think.</p>

<p>First, the limit is now 128. (<a href=""https://docs.microsoft.com/en-us/azure/sql-data-warehouse/memory-and-concurrency-limits#gen2-1"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/sql-data-warehouse/memory-and-concurrency-limits#gen2-1</a>)</p>

<p>Second, this is well above the concurrency of the next most concurrent single cluster warehouse. I've often wondered what marketing mistake was made by Microsoft that concurrency is seen as a limitation on ASDW, but rarely mentioned for less concurrent competitors.</p>

<p>Third, the best way to serve thousands of concurrent query users (ie, dashboards) is through PowerBI hybrid queries, and (potentially) Azure Analysis Services. This gives extremely high concurrency and interactivity.</p>

<p>Perhaps the best evidence I can give is that I work with Azure SQL Data Warehouse customers on a daily basis. I often get questions like this when a customer is first exposed to ASDW, but I never get questions about concurrency by the time they're in production. In other words, the issue of ""concurrency"" just isn't important for most customers.</p>
"
"<p>I am building a phone catalog of my organization (an AJAX application which accesses the search.asmx web service).
I'd like to show a list box where the user could select a department (which is stored in managed property <code>Department</code>). To fill the list box with values, I need to somehow select all the distinct values of that property. 
Is it possible through search.asmx web service? </p>

<p>What I've found:</p>

<ul>
<li>an article, <a href=""http://blog.robgarrett.com/2009/09/23/pre-search-facets-in-moss-2007/"" rel=""nofollow noreferrer"">which states that it is possible</a>, but it does not use the web service interface</li>
<li>a Microsoft's <a href=""http://download.microsoft.com/download/8/5/8/858F2155-D48D-4C68-9205-29460FD7698F/%5BMS-SEARCH%5D.pdf"" rel=""nofollow noreferrer"">white paper</a> which states that ""If the protocol client specifies at least one property, it MUST also specify the Path property. If it does not, the protocol server MUST return the status code ""ERROR_BAD_QUERY"".""</li>
</ul>

<p>The two findings are somewhat inconsistent. (and, yes the search really returns ERROR_BAD_QUERY).</p>
","<p>It could be a bracket problem, and you have to prefix your IdOwner. Be careful to prefix your {0} and {1} parameter :</p>

<pre><code>SELECT ROW_NUMBER() OVER ( ORDER BY {0} {1} ) AS RowNum, 
       Cars.Id,Cars.Make, Cars.Model, Color.Name 
FROM (Cars INNER JOIN Color ON Cars.ColorId=Color.Id) 
WHERE Cars.IdOwner={2} 
</code></pre>
"
"<p>When I use the <code>create_view ()</code> method of the <code>sqlalchemy-utils</code> module, everything works just fine the first time I run my script. However, every time after that first call, I encounter this error : </p>

<p><code>sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) table XXX already exists</code>.</p>

<p>I am currently capturing the exception and <code>pass</code> to avoid my script to stop but this seems messy to me.</p>

<p>Is there a way to avoid that behavior ?</p>
","<p>This is the last example in the <a href=""http://sqlalchemy-utils.readthedocs.io/en/latest/data_types.html#module-sqlalchemy_utils.types.encrypted.encrypted_type"" rel=""nofollow noreferrer"">docs</a> that you linked to:</p>

<blockquote>
  <p>The key parameter accepts a callable to allow for the key to change
  per-row instead of being fixed for the whole table.</p>
</blockquote>

<pre><code>def get_key():
    return 'dynamic-key'

class User(Base):
    __tablename__ = 'user'
    id = sa.Column(sa.Integer, primary_key=True)
    username = sa.Column(EncryptedType(
        sa.Unicode, get_key))
</code></pre>
"
"<p>I want to exchange information between ExactOnline and Freshdesk based on deliveries (Exact Online Accounts -> Freshdesk Contacts, Exact Online deliveries -> Freshdesk tickets). </p>

<p>The serial number of delivered goods is not available in either the <code>ExactOnlineREST..GoodsDeliveryLines</code> table nor in <code>ExactOnlineXML..DeliveryLines</code>.</p>

<p>The following query lists all columns that are also documented on <a href=""https://start.exactonline.nl/docs/HlpRestAPIResourcesDetails.aspx?name=SalesOrderGoodsDeliveryLines"" rel=""nofollow"">Exact Online REST API GoodsDeliveryLines</a>:</p>

<pre><code>select * from goodsdeliverylines
</code></pre>

<p>All other fields of the documentation on REST APIs are included in GoodsDeliveryLines, solely serial numbers and batch numbers not.</p>

<p>I've tried - as on ExactOnlineXML tables where there column only come into existence when actually specified - to use:</p>

<pre><code>select stockserialnumbers from goodsdeliverylines
</code></pre>

<p>This raises however an error:</p>

<pre><code>itgensql005: Unknown identifier 'stockserialnumbers'.
</code></pre>

<p>How can I retrieve the serial numbers?</p>
","<p>Payment conditions are referenced from the account on the outstanding items. In order to get the (sales) payment conditions on an account, there are several options.</p>

<ol>
<li><p>Join the <a href=""https://documentation.invantive.com/2017R2/exact-online-data-model/webhelp/invantive-eol-provider-exactonlinerest-crm-accounts.html"" rel=""nofollow noreferrer"" title=""Accounts""><code>Accounts</code></a> table and get the payment condition from there (from the field <code>salespaymentcondition_code_attr</code> and <code>salespaymentcondition_description</code>).</p>

<p>The SQL would look like this then:</p>

<pre><code>select ...
,      act.salespaymentcondition_code_attr
from   aroutstandingitems aom
join   exactonlinexml..accounts act
on     aom.outstandingitems_ar_account_code_attr = act.code_attr
</code></pre></li>
<li><p>Use an Excel function to get the payment condition: <code>I_EOL_ACT_SLS_PAY_CODE</code>.</p>

<p>The formula has two parameters: <code>division_code</code> and <code>account_code_attr</code>. The first is optional.</p>

<p>A valid call to the formula would thus be: <code>=I_EOL_ACT_SLS_PAY_CODE(,""22"")</code> for the payment condition code for account with code 22 in the current Exact Online company. You can incorporate that in your SQL like this:</p>

<pre><code>select ...
,      '=I_EOL_ACT_SLS_PAY_CODE(""' + division_code + '"", ""' + outstandingitems_ar_account_code_attr + '"")'
       pcn_code
from   aroutstandingitems
</code></pre>

<p>That would result in your model on sync receive the formula for retrieving the payment condition code. Remember to check the check box 'Formula' to ensure that the SQL outcome is treated as an Excel formula.</p></li>
<li><p>The same as above, but then using column expressions:</p>

<pre><code>select ...
,      '=I_EOL_ACT_SLS_PAY_CODE(""$C{E,.,.,^,.}"";""$C{E,.,.,^+3,.}"")'
       pcn_code
from   aroutstandingitems
</code></pre>

<p>Remember to check the check boxes 'Formula' and 'Column expression' to ensure that the SQL outcome is treated as an Excel formula with <code>$C</code> column expressions.</p></li>
</ol>

<p>The recommended option is to use column expressions, since these work across the widest range of deployment scenarios such as accountancy with hundreds of companies and the formulas are upgrade safe. SQL statements may need to be adapted with new releases of the Exact Online data model of Invantive.</p>
"
"<pre><code>SELECT *
FROM dbo.staff
WHERE st_position = 'Supervisor' AND st_salary &lt; AVG(st_salary);
</code></pre>

<p>So I'm trying to set a query that outputs a list of all supervisors that have a salary lower than average. 
putting this in I get the following error.</p>

<p>Msg 147, Level 15, State 1, Line 1
An aggregate may not appear in the WHERE clause unless it is in a subquery contained in a HAVING clause or a select list, and the column being aggregated is an outer reference.</p>
","<p>To get the value of average salary, we can use the following query:</p>

<pre><code>SELECT AVG(st_salary) FROM dbo.staff
</code></pre>

<p>Combining it together with the other condition, will give the following query:</p>

<pre><code>SELECT  *
FROM    dbo.staff
WHERE   st_position = 'Supervisor'
AND     st_salary &lt; (SELECT AVG(st_salary) FROM dbo.staff)
</code></pre>
"
"<p>I have a sequence that looks like this:</p>

<pre><code>CREATE SEQUENCE dbo.NextWidgetId
 AS [bigint]
 START WITH 100
 INCREMENT BY 2
 NO CACHE 
GO
</code></pre>

<p>And a table that looks like this:</p>

<pre><code>CREATE TABLE [dbo].[Widget_Sequenced]
(
    [WidgetId] [int] NOT NULL DEFAULT(NEXT VALUE FOR dbo.NextWidgetId),
    [WidgetCost] [money] NOT NULL,
    [WidgetName] [varchar](50) NOT NULL,
    [WidgetCode] [int] NOT NULL,
    [LastChangedBy] [int] NOT NULL,
    [RowVersionId] [timestamp] NOT NULL,

    CONSTRAINT [PK_Widget_Sequenced] 
    PRIMARY KEY CLUSTERED ([WidgetId] ASC) ON [PRIMARY]
) ON [PRIMARY]
</code></pre>

<p>Is there a way to add a new record to this table structure using Entity Framework?</p>

<p>I tried setting <code>StoreGeneratedPattern</code> for <code>WidgetId</code> to <code>computed</code> and I tried it with <code>Identity</code>. Both gave me errors.</p>

<p>I tried this with EF 5. But I could move to EF 6 if it fixes this.</p>
","<p>It's totally possible.<br>
Changing the example from the post you've linked to something like this:</p>

<pre><code>create sequence mainseq as bigint start with 1 increment by 1;

create table mytable (
    id      varchar(20) not null constraint DF_mytblid default 'p' + CAST(next value for mainseq as varchar(10)),
    code    varchar(20) not null
)
</code></pre>

<p>Test:</p>

<pre><code>INSERT INTO MyTable (Code) VALUES ('asdf'), ('cvnb')

SELECT *
FROM MyTable
</code></pre>

<p>Results:</p>

<pre><code>id  code
p1  asdf
p2  cvnb
</code></pre>
"
"<p>I've successfully installed MySQL, Boost and mysql-connector-c++ on my macOS High Sierra 10.13.3 via Homebrew, but i've ran into problems with using those libs in Xcode.<p>
So, my boost and connector libs are located at <code>/usr/local/Cellar/</code>. So i just wrote a simple code sample to check if everything ""works"":</p>

<pre><code>#include &lt;mysql_driver.h&gt;
#include &lt;mysql_error.h&gt;
#include &lt;mysql_connection.h&gt;

int main()
{
    return 0;
}
</code></pre>

<p>and compiled it with </p>

<pre><code>c++ -I /usr/local/Cellar/mysql-connector-
c++/1.1.9_1/include/mysql_connection.h /usr/local/Cellar/mysql-connector-
c++/1.1.9_1/include/mysql_driver.h /usr/local/Cellar/mysql-connector-
c++/1.1.9_1/include/mysql_driver.h main.cpp
</code></pre>

<p>Everything seems to be working, except I get some warnings:</p>

<blockquote>
  <p>clang: warning: treating 'c-header' input as 'c++-header' when in C++ mode, 
      this behavior is deprecated [-Wdeprecated]</p>
  
  <p>clang: warning: treating 'c-header' input as 'c++-header' when in C++ mode, 
  this behavior is deprecated [-Wdeprecated]</p>
</blockquote>

<p>But that's not a huge issue, I think.
And then I try to use those libs in Xcode, so i do this:<p><a href=""https://i.stack.imgur.com/ozrpc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ozrpc.png"" alt=""Xcode 9.2 build settings""></a>
Yet when I try to compile any code with those libs in Xcode, I get this: </p>

<blockquote>
  <p>'boost/scoped_ptr.hpp' file not found with  include; use ""quotes"" instead'</p>
</blockquote>

<p>and many errors alike for boost and mysql-connector-c++. Obviosly, changing all &lt;> to """" in source files is NOT a good idea, futhermore, it's extremely tiring.<p>
How to fix this error?</p>
","<p>This line is wrong:</p>

<pre><code>sqlquery = &amp;sqlbuff;
</code></pre>

<p><code>sqlbuff</code> has type <code>char []</code>, so taking its address gives you a <em>pointer to array</em>, type <code>char (*)[]</code>. You just want a pointer to the first <code>char</code> (type <code>char *</code>), and evaluating the identifier of an array results in such a pointer in most contexts (there are some exceptions like when used with <code>sizeof</code>). So, just write</p>

<pre><code>sqlquery = sqlbuff;
</code></pre>

<p>That said, there's no need to have a separate pointer variable at all. <code>sqlbuff</code> evaluates to <code>char *</code>, and a pointer is convertable to its <code>const</code>-qualified  counterpart implicitly. Just pass <code>sqlbuff</code> directly where a <code>const char *</code> is expected and it will work.</p>

<hr>

<p>Another thing should be mentioned although not directly related to your question: You should <strong>never construct SQL queries from user input using string operations</strong>. Attackers can easily inject own SQL code this way. Read the documentation of your SQL client library and look for <em>prepared statements</em> and <em>parameter binding</em> and use this everywhere user input needs to be a parameter in a SQL query.</p>

<hr>

<p>And one more hint: If you find yourself writing code like</p>

<pre><code>sprintf(""%s"", ""foobar"");
</code></pre>

<p>e.g., you're giving a constant string for a <code>%s</code> conversion, you're doing it wrong. Just make your constant string part of your format-string.</p>
"
"<p>Is there any product which can be queries using JDBC (normal SQL), it sees whether all the tables in the query are in CACHED tables, and use the cache, otherwise fallback to the back-end database.</p>

<p>I am aware of two products: Oracle In Memory Database (IMDB) Cache, and VMware SQLFire.</p>

<p>I'm not familiar with none of them, so I want to know is it possible to query IMDB cache on non-cached tables, so it falls-back to underlying database?</p>

<p>Is there any other products which support this feature?</p>
","<p>One straightforward way to accomplish this is as follows:</p>

<pre><code>SELECT DISTINCT (MovieID)
FROM Movies m
WHERE Format='DVD'
AND NOT EXISTS (
    SELECT * FROM Movies mm WHERE mm.MovieID=m.MovieID AND mm.Format='Blu-ray'
)
</code></pre>

<p>This is pretty much a translation of English description of the problem to SQL syntax.</p>
"
"<p>We have a huge Oracle database and I frequently fetch data using SQL Navigator (v5.5). From time to time, I need to stop code execution by clicking on the <code>Stop</code> button because I realize that there are missing parts in my code. The problem is, after clicking on the <code>Stop</code> button, it takes a very long time to complete the stopping process (sometimes it takes hours!). The program says <code>Stopping...</code> at the bottom bar and I lose a lot of time till it finishes. </p>

<p>What is the rationale behind this? How can I speed up the stopping process? Just in case, I'm not an admin; I'm a limited user who uses some views to access the database.</p>
","<p>Two things need to happen to stop a query:</p>

<ol>
<li>The actual Oracle process has to be notified that you want to cancel the query</li>
<li>If the query has made any modification to the DB (DDL, DML), the work needs to be rolled back.</li>
</ol>

<p>For the first point, the Oracle process that is executing the query should check from time to time if it should cancel the query or not. Even when it is doing a long task (big HASH JOIN for example), I think it checks every 3 seconds or so (I'm looking for the source of this info, I'll update the answer if I find it). Now is your software able to communicate correctly with Oracle? I'm not familiar with SLQ Navigator but I suppose the cancel mechanism should work like with any other tool so I'm guessing you're waiting for the second point:</p>

<p>Once the process has been notified to stop working, it has to undo everything it has already accomplished in this query (all statements are atomic in Oracle, they can't be stopped in the middle without rolling back). Most of the time in a DML statement the rollback will take longer than the work already accomplished (I see it like this: Oracle is optimized to work forward, not backward). If you are in this case (big DML), you will have to be patient during rollback, there is not much you can do to speed up the process.</p>

<p>If your query is a simple SELECT and your tool won't let you cancel, you could kill your session (needs admin rights from another session) -- this should be instantaneous.</p>
"
"<p>I need to process data from a SQLite database using some Java code. The problem is that the database is retrieved from an external service and thus it is only available as an <code>InputStream</code>.</p>

<p>I would really like to avoid saving the db to the file system before starting to process the data, but I don't see a way of doing it.</p>

<p>I am currently using <a href=""http://sqljet.com/"" rel=""nofollow"">SqlJet</a>, as it provides an option to create an in-memory database. However there seems to be no way to turn an <code>InputStream</code> / byte array into a database.</p>

<p>There's nothing forcing me to use <a href=""http://sqljet.com/"" rel=""nofollow"">SqlJet</a>, it can be easily replaced by any other tool if it provides the needed functionality.</p>
","<p>Use the <a href=""http://www.h2database.com/"" rel=""nofollow"">H2</a> in <a href=""http://www.h2database.com/html/tutorial.html"" rel=""nofollow"">TCP server mode</a>. In one of your VMs start it like so:</p>

<pre><code> Server server = Server.createTcpServer(args).start();
</code></pre>

<p>Other VM can connect using JDBC:</p>

<pre><code>JDBC driver class: org.h2.Driver
Database URL: jdbc:h2:tcp://localhost/~/test 
</code></pre>
"
"<p>Today I had to use a SQLite database for the first time and I really wondered about the display of a <code>DATETIME</code> column like <code>1411111200</code>. Of course, internally it has to be stored as some integer value to be able to do math with it. But who wants to see that in a grid output, which is clearly for human eyes?</p>

<p>I even tried two programs, SQLiteStudio and SQLite Manager, and both don't even have an option to change this (at least I couldn't find it).</p>

<p>Of course with my knowledge about SQL it didn't take long to find out what the values mean - this query displays it like I expected:</p>

<pre><code>select datetime(timestamp, 'unixepoch', 'localtime'), * from MyTable
</code></pre>

<p>But that's very uncomfortable when working with a GUI Tool. So why? Just because? Unix nerds? Or did I just get a wrong impression because I accidentally tried the only 2 Tools which are bad?</p>

<p>(I also appreciate comments on which tools to use or where I can find the hidden settings.)</p>
","<p>There is no need for the join and you need to add a HAVING clause</p>

<pre><code>SELECT name, SUM(qty) AS Total 
FROM t1 
GROUP BY name 
HAVING SUM(qty) BETWEEN 500 AND 1500
ORDER BY name
</code></pre>
"
"<p>I am using <a href=""https://sqlkata.com/docs"" rel=""nofollow noreferrer"">SqlKata</a> to creating dynamic SQL queries. I have a list of conditions, stored in my database, which are generated according to my business rules. this is my code sample:</p>

<pre><code>var list = new List&lt;Query&gt;();
foreach(var rule in rules){
    var q = new Query()
       .Where(x=&gt; x.Where(""Price"", ""&lt;"", rule.Price).OrWhere(""GoodsType"", ""="", rule.Type));
    list.Add(q);
}
</code></pre>

<p>Now I want to join this list item together but none of <strong>Where()</strong> extensions overload accepts <code>Query</code> type parameters. Is there a way to join the where clauses together?</p>

<p>this is A VERY LITTLE PART OF expected query which I need to generate.</p>

<pre><code>select * from ship_schedule where Path = @path and scheduleDate= @Date
AND (FD.IssueType ='O' OR fd.Path!='ILMTOP' OR  (fd.Path='ILMTOP' AND F.carrier !='MAL'))
AND (FD.IssueType ='O' OR fd.Path!='TOPILM' OR  (fd.Path='ILMTOP' AND F.carrier !='MAL'))
</code></pre>

<p>I need to create the second line of the query to the end.</p>
","<p>The <code>Where</code> method is additive and calling it multiple times will add multiple conditions to the query, so you don't need to build the list of conditions by yourself.</p>

<pre><code>var query = new Query(""ship_schedule"").Where(""Path"", path);

foreach(var rule in rules) {
  // loop over rules and append them to the query

  if(col == null) {

    query.WhereNull(col);

  } else {

    query.Where(q =&gt; 
      q.Where(""Price"", ""&lt;"", rule.Price)
        .OrWhere(""GoodsType"", ""="", rule.Type)
    )

  }
}
</code></pre>

<h2>Other ways</h2>

<p>using the <code>When</code> method  </p>

<pre><code>query.When(condition, q =&gt; q.Where(...));
</code></pre>

<p>using the <code>WhereIf</code> method</p>

<pre><code>query.WhereIf(condition, ""Id"", ""="", 10);
</code></pre>
"
"<p>I used the <a href=""http://mysqlbackupnet.codeplex.com/wikipage?title=Documentation%20for%20MySqlBackup.NET%20V1.5"" rel=""nofollow noreferrer"">following code</a> for backup the MYSQL database.</p>

<pre><code>    private void button2_Click(object sender, EventArgs e)
    {
        string file = ""D:\\backup.sql"";
        //string conn = ""server=localhost;user=root;pwd=qwerty;database=test;"";
        String str = @""server=192.168.1.219;database=abc;userid=sha;password='123';"";
        MySqlBackup mb = new MySqlBackup(str);
        mb.ExportInfo.FileName = file;
        mb.Export();
    }
</code></pre>

<p>my stack trace is following - </p>

<pre><code>A first chance exception of type 'System.NullReferenceException' occurred in MySqlBackup.dll
System.Transactions Critical: 0 : &lt;TraceRecord xmlns=""http://schemas.microsoft.com/2004/10/E2ETraceEvent/TraceRecord"" Severity=""Critical""&gt;&lt;TraceIdentifier&gt;http://msdn.microsoft.com/TraceCodes/System/ActivityTracing/2004/07/Reliability/Exception/Unhandled&lt;/TraceIdentifier&gt;&lt;Description&gt;Unhandled exception&lt;/Description&gt;&lt;AppDomain&gt;TestAppMysqlDBConnect.vshost.exe&lt;/AppDomain&gt;&lt;Exception&gt;&lt;ExceptionType&gt;System.NullReferenceException, mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&lt;/ExceptionType&gt;&lt;Message&gt;Object reference not set to an instance of an object.&lt;/Message&gt;&lt;StackTrace&gt;   at MySql.Data.MySqlClient.MySqlBackup.ExportExecute()
   at MySql.Data.MySqlClient.MySqlBackup.Export()
   at TestAppMysqlDBConnect.Form1.button2_Click(Object sender, EventArgs e) in C:\Users\Shashika\Documents\Visual Studio 2010\Projects\TestAppMysqlDBConnect\TestAppMysqlDBConnect\Form1.cs:line 52
   at System.Windows.Forms.Control.OnClick(EventArgs e)
   ..
</code></pre>

<p>But there is a exception and is says there is a null references exception. when i pass the data to the database through the C# program. it was successfully inserted there were no exception. this exception only occur in when i try to backup the database through the C# program. i used 2 Dll files that was in above link. those are - 
      MySql.Data.dll
      MySqlBackup.dll</p>

<p>I can not solve this exception. Please help.</p>

<p><img src=""https://i.stack.imgur.com/onTon.jpg"" alt=""Exception""></p>
","<p>According to <a href=""https://www.codeproject.com/Articles/256466/MySqlBackup-NET#basicusage"" rel=""nofollow noreferrer"">this documentation section</a>, you can specify it in the <code>MySqlBackup.ExportInfo</code> using the <code>List&lt;string&gt;</code> property called <code>TablesToBeExportedList</code>.</p>

<p>So, something like this should work:</p>

<pre><code>string constring = ""server=localhost;user=root;pwd=1234;database=test1;"";
string file = ""Y:\\backup.sql"";
using (MySqlConnection conn = new MySqlConnection(constring))
{
    using (MySqlCommand cmd = new MySqlCommand())
    {
        using (MySqlBackup mb = new MySqlBackup(cmd))
        {
            cmd.Connection = conn;
            conn.Open();
            mb.ExportInfo.TablesToBeExportedList = new List&lt;string&gt; {
                ""Table1"",
                ""Table2""
            };
            mb.ExportToFile(file);
        }
    }
}
</code></pre>
"
"<p>I created an external table using <code>polybase</code> with </p>

<pre><code>CREATE EXTERNAL TABLE [ext].[gendertable]
( 
        gender_id TINYINT NOT NULL,
        gender VARCHAR(16) NOT NULL
) 
WITH 
( 
    LOCATION = '/MovieDB/gender.csv', 
    DATA_SOURCE = AzureBlobHDP, 
    FILE_FORMAT = csvformat0
);
GO
</code></pre>

<p>The data source is <code>HADOOP</code>. Is there a way to import this table without defining data type <em>again</em> for every column? I search for some code <a href=""https://docs.microsoft.com/en-us/azure/sql-data-warehouse/load-data-wideworldimportersdw"" rel=""nofollow noreferrer"">like this</a>:</p>

<pre><code>CREATE TABLE [mov].[gendertable]
WITH
( 
    DISTRIBUTION = REPLICATE,
    CLUSTERED COLUMNSTORE INDEX
)
AS
SELECT * FROM [ext].[gendertable]
</code></pre>

<p>Of course this code fails for me, since I do not use an Azure SQL DW (I get a Syntax error). I use SQL Server 2019 on a VM. </p>

<p>My question is there is a <code>SQL</code> expression, such that I do not have to declare the data type for each column?</p>
","<p>SQL Server 2019 is still in CTP and this is a new feature. If you have found a bug in it you should report it to Microsoft so it is fixed before release (<a href=""https://feedback.azure.com/forums/908035-sql-server/suggestions/37034101-inline-scalar-udf-bug-multiple-columns-are-specif"" rel=""nofollow noreferrer"">done for you here</a>).</p>

<p>On previous versions if you try and manually inline it you will see the same error (as in the cut down example below). This is because of the limitation <a href=""https://www.itprotoday.com/aggregates-outer-reference"" rel=""nofollow noreferrer"">discussed here</a>.</p>

<pre><code>WITH T(m_id,fra, til) AS
(
SELECT 1, GETDATE(), GETDATE()
)
SELECT *
FROM T
CROSS APPLY
        (SELECT     
             SUM(DATEDIFF(D, e.FRADATO, til))
         FROM 
             dbo.mlr_eos_avl e 
) CA(result)
</code></pre>

<p>Until this inline case is fixed by Microsoft you can use</p>

<pre><code>WITH INLINE = OFF
</code></pre>

<p>in the scalar UDF definition to disable inlining of the UDF</p>
"
"<p>I cannot seem to get the query with declare statement working in SAP HANA. Below I've put the original working T-SQL version and the HANA version output from the SQL converter. I've tried several versions and combinations, but every time I get errors which you also find below. Anybody willing to give me an ""how to"" so I can copy this? I also spelled out the SAP Documentations, but nothing there what could help me.
Your help would be very appreciated.</p>

<p>The T-SQL Code:</p>

<pre><code>DECLARE @NumAtCardDuplicate VARCHAR(50)
SET @NumAtCardDuplicate =
  (SELECT TOP 1 DocNum
  FROM TEST_RSCA.OPCH 
  WHERE CardCode = 'S100424'
  AND NumAtCard = '118 120 266 805')

IF @NumAtCardDuplicate IS NOT NULL
  SELECT 'Invoice number already used in entry ' + @NumAtCardDuplicate + '!'
ELSE
  SELECT '118 120 266 805'
</code></pre>

<p>The translated HANA query:</p>

<pre><code>NumAtCardDuplicate varchar(50);

SELECT 
  (SELECT TOP 1 ""DocNum"" 
  FROM TEST_RSCA.OPCH 
  WHERE ""CardCode"" = 'S100424' 
  AND ""NumAtCard"" = '118 120 266 805') 
INTO NumAtCardDuplicate FROM DUMMY;

temp_var_0 integer;

SELECT :NumAtCardDuplicate INTO temp_var_0 FROM DUMMY;

IF :temp_var_0 IS NOT NULL THEN 
  SELECT 'Invoice number already used in entry ' || :NumAtCardDuplicate || '!' 
  FROM DUMMY;
ELSE 
  SELECT '118 120 266 805' 
FROM DUMMY;
END IF;
</code></pre>

<p>The Errors I get:</p>

<pre><code>Could not execute 'NumAtCardDuplicate varchar(50)' in 1 ms 989 s . 
SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near ""NumAtCardDuplicate"": line 1 col 1 (at pos 1) 

Could not execute 'SELECT (SELECT TOP 1 ""DocNum"" FROM TEST_RSCA.OPCH WHERE ""CardCode"" ='S100424' AND ""NumAtCard"" = ...' in 3 ms 578 s . 
SAP DBTech JDBC: [337] (at 119): INTO clause not allowed for this SELECT statement: line 4 col 67 (at pos 119) 

Could not execute 'temp_var_0 integer' in 1 ms 701 s . 
SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near ""temp_var_0"": line 1 col 1  (at pos 1) 

Could not execute 'SELECT :NumAtCardDuplicate INTO temp_var_0 FROM DUMMY' in 1 ms 976 s . 
SAP DBTech JDBC: [467]: cannot use parameter variable: NUMATCARDDUPLICATE: line 4294967295 col 4294967295 (at pos 4294967295) 

Could not execute 'IF :temp_var_0 IS NOT NULL THEN SELECT 'Invoice number already used in entry ' || ...' in 1 ms 560 s . 
SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near ""IF"": line 1 col 1 (at pos 1) 

Could not execute 'ELSE SELECT '118 120 266 805' FROM DUMMY' in 1 ms 338 s . 
SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near ""ELSE"": line 1 col 1 (at pos 1) 

  SAP DBTech JDBC: [257]: sql syntax error: incorrect syntax near ""END"": line 1 col 1 (at pos 1) 
</code></pre>

<p>Duration of 7 statements: 13 ms </p>
","<p>You could just use <code>MAX()</code> on all columns like below if each row has a single value:</p>

<pre><code>CREATE TABLE #data
(
    col1 INT,
    col2 INT,
    col3 INT
);

INSERT INTO #data
(
    col1,
    col2,
    col3
)
VALUES
(1, NULL, NULL),
(NULL, 2, NULL),
(NULL, NULL, 3);

SELECT MAX(d.col1) AS col1,
       MAX(d.col2) AS col2,
       MAX(d.col3) AS col3
FROM #data AS d;

DROP TABLE #data;
</code></pre>
"
"<p>In current project we need to find cheapest paths in almost fully connected graph which can contain lots of edges per vertex pair.  </p>

<p>We developed <a href=""http://orientdb.com/docs/2.1/Extend-Server.html"" rel=""nofollow"">a plugin</a> containing functions</p>

<ol>
<li>for special traversal this graph to lower reoccurences of similar paths while <code>TRAVERSE</code> execution. We will refer it as <code>search()</code></li>
<li>for special effective extraction of desired information from results of such traverses. We will refer it as <code>extract()</code></li>
<li>for extracting best <code>N</code> records according to target parameter without costly <code>ORDER BY</code>. We will refer it as <code>best()</code></li>
</ol>

<p>But resulted query still has unsatisfactory performance on full data.</p>

<p>So we decided to modify <code>search()</code> function so it could watch best edges first and prune paths leading to definitely undesired result by using current state of <code>best()</code> function.<br>
Overall solution is effectively a flexible implementation of <a href=""https://en.wikipedia.org/wiki/Branch_and_bound"" rel=""nofollow"">Branch and Bound method</a></p>

<p>Resulting query (omitting <code>extract()</code> step) should look like </p>

<pre><code>SELECT best(path, &lt;limit&gt;) FROM (
   TRAVERSE search(&lt;params&gt;) FROM #&lt;starting_point&gt;
   WHILE &lt;conditions on intermediate vertixes&gt;
  ) WHERE &lt;conditions on result elements&gt; 
</code></pre>

<p>This form is very desired so we could adapt conditions under <code>WHILE</code> and <code>WHERE</code> for our current task. The <code>path</code> field is generated by <code>search()</code> containing all information for <code>best()</code> to proceed.</p>

<p>The trouble is that <code>best()</code> function is executed strictly after <code>search()</code> function, so <code>search()</code> can not prune non-optimal branches according to results already evaluated by <code>best()</code>.</p>

<p>So the Question is:<br>
Is there a way to pipeline results from <code>TRAVERSE</code> step to <code>SELECT</code> step in the way that older paths were <code>TRAVERSE</code>d with <code>search()</code> after earlier paths handled by <code>SELECT</code> with <code>best()</code>?</p>
","<p>You could use this query</p>

<pre><code>SELECT FROM (
MATCH {CLASS:v1, AS:v1, WHERE: (@rid=#29:0)}.outE('e1'){AS:e1}.inV('e1'){AS:v2} RETURN v1, e1.p, v2 
) 
</code></pre>

<p>I hope that is clear enough.</p>
"
"<p>This is my model Riders:</p>

<pre><code>&lt;?php

namespace backend\models;

use Yii;

class Riders extends \yii\db\ActiveRecord
{
public static function tableName()
{
    return 'riders';
}

public function rules()
{
    return [
        [['cagories_category_id', 'rider_firstname', 'rider_no_tlpn', 'rider_ucinumber', 'countries_id', 'rider_province', 'rider_city', 'rider_dateofbirth', 'rider_gender'], 'required'],
        [['user_id', 'countries_id'], 'integer'],
        [['rider_dateofbirth', 'cagories_category_id'], 'safe'],
        [['rider_gender', 'rider_status'], 'string'],
        [['rider_firstname', 'rider_lastname', 'rider_nickname', 'rider_province', 'rider_city'], 'string', 'max' =&gt; 45],
        [['rider_email', 'rider_sponsor', 'rider_birthcertificate_url', 'rider_parental_consent_url'], 'string', 'max' =&gt; 100],
        [['rider_no_tlpn'], 'string', 'max' =&gt; 15],
        [['rider_ucinumber'], 'string', 'max' =&gt; 11]
    ];
}

/**
 * @inheritdoc
 */
public function attributeLabels()
{
    return [
        'rider_id' =&gt; 'rider_id',
        'cagories_category_id' =&gt; 'Category Name',
        'user_id' =&gt; 'User Team',
        'rider_firstname' =&gt; 'Rider Firstname',
        'rider_lastname' =&gt; 'Rider Lastname',
        'rider_nickname' =&gt; 'Rider Nickname',
        'rider_email' =&gt; 'Rider Email',
        'rider_no_tlpn' =&gt; 'Rider No Tlpn',
        'rider_ucinumber' =&gt; 'Rider Ucinumber',
        'countries_id' =&gt; 'Country Name',
        'rider_province' =&gt; 'Rider Province',
        'rider_city' =&gt; 'Rider City',
        'rider_sponsor' =&gt; 'Rider Sponsor',
        'rider_dateofbirth' =&gt; 'Rider Dateofbirth',
        'rider_gender' =&gt; 'Rider Gender',
        'rider_birthcertificate_url' =&gt; 'Rider Birthcertificate Url',
        'rider_parental_consent_url' =&gt; 'Rider Parental Consent Url',
        'rider_status' =&gt; 'Rider Status',
    ];
}

/**
 * @return \yii\db\ActiveQuery
 */
public function getRegistrations()
{
    return $this-&gt;hasMany(Registrations::className(), ['riders_rider_id' =&gt; 'rider_id']);
}

/**
 * @return \yii\db\ActiveQuery
 */
public function getCagoriesCategory()
{
    return $this-&gt;hasOne(Categories::className(), ['category_id' =&gt; 'cagories_category_id']);
}

/**
 * @return \yii\db\ActiveQuery
 */
public function getUser()
{
    return $this-&gt;hasOne(User::className(), ['id' =&gt; 'user_id']) -&gt; from(user::tableName() . 'ud');
}

/**
 * @return \yii\db\ActiveQuery
 */
public function getUserDesc()
{
    return $this-&gt;hasOne(UserDesc::className(), ['desc_id' =&gt; 'user_id']) -&gt; from(['ud' =&gt; userDesc::tableName()]);
}

/**
 * @return \yii\db\ActiveQuery
 */
public function getCountries()
{
    return $this-&gt;hasOne(Countries::className(), ['id' =&gt; 'countries_id']);
}
</code></pre>

<p>}</p>

<p>This my Controller actionIndex:</p>

<pre><code>$searchModel = new RidersSearch();
$dataProvider = $searchModel-&gt;search(Yii::$app-&gt;request-&gt;queryParams);

$totalCount = Yii::$app-&gt;db-&gt;createCommand('SELECT COUNT(*) FROM riders WHERE user_id = :user_id',
    [':user_id' =&gt; Yii::$app-&gt;user-&gt;identity-&gt;id])-&gt;queryScalar();

$dataProvider = new SqlDataProvider([
    'sql' =&gt; 'SELECT * FROM riders WHERE user_id = :user_id',
    'params' =&gt; [':user_id' =&gt; Yii::$app-&gt;user-&gt;identity-&gt;id],
    'totalCount' =&gt; $totalCount,
    'key' =&gt; 'rider_id',
    'pagination' =&gt; [
        'pageSize' =&gt; 10,
    ],
    'sort' =&gt; [
        'attributes' =&gt; [
            'cagories_category_id',
            'rider_id',
            'rider_firstname',
            'rider_email:email',
            'rider_no_tlpn',
        ]
    ]
]);

$models = $dataProvider-&gt;getModels();

return $this-&gt;render('index', [
    'searchModel' =&gt; $searchModel,
    'dataProvider' =&gt; $dataProvider,
]);
</code></pre>

<p>This is my view index:</p>

<pre><code>&lt;?= GridView::widget([
    'dataProvider' =&gt; $dataProvider,
    // 'filterModel' =&gt; $searchModel,
    'columns' =&gt; [
        ['class' =&gt; 'yii\grid\SerialColumn'],
        [
            'label' =&gt; 'Category Name',
            'attribute'=&gt;'cagories_category_id',
            'value' =&gt; 'cagoriesCategory.category_name', &lt;---Can't work again
        ],
        [
            'label' =&gt; 'BIB',
            'attribute'=&gt;'rider_id',
        ],
        'rider_firstname',
        'rider_email:email',
        'rider_no_tlpn',

        ['class' =&gt; 'yii\grid\ActionColumn'],
    ],
]); ?&gt;
</code></pre>

<p>Before I use sqldataprovider, it can call from model function have relation, after use sqldataprovider can't work. How to get relation table value???
then before use it, i can to merge <code>rider_firstname</code> and <code>rider_lastname</code> with <code>return $model-&gt;rider_firstname . "" "" . rider_lastname;</code> after use sqldataprovider can't work too?? </p>
","<p>Turns out unlike what was mentioned in the question I followed, Yii accepts the <code>attributes</code> param as an array of values and not an array with one CSV value.</p>

<p>So:</p>

<pre><code>'sort' =&gt; array(
    'attributes' =&gt; array ( 'enabled', 'store_name', 'rating' ),
    'defaultOrder' =&gt; array('store_name'=&gt;false)
),
</code></pre>

<p>And not:</p>

<pre><code>'sort' =&gt; array(
    'attributes' =&gt; array ( 'enabled, store_name, rating' ),
    'defaultOrder' =&gt; array('store_name'=&gt;false)
),
</code></pre>
"
"<p>I am configuring sql2java the first time myself. I extracted the zip-archive and imported all files into an eclipse java project. I don't know if this is correct, because when I run the ant build file through the eclipse ant function (""Run as ant build..."") there is no output on the console. </p>

<p>I don't know where the problem is located, sql2java, ant, eclipse? Its a fresh and clean install of eclipse galileo. </p>

<p>How can I get sql2java / ant to work?
How do I get any information that can help me to locate the problem?
Is there a way to use the eclipse's ant installation to run the build file from console?</p>

<p>Any help would be appreciated. Thanks in advance.</p>
","<p>the problem may be related to anthome definitions, generally it should be the same with your eclipse plugin directory.</p>

<ol>
<li>goto windows/preferences menu</li>
<li>on the left tree-menu choose ant/Runtime</li>
<li>on right screen edit your anthome variable, set to your eclipse installition ant plugin directory 
    forexample :  ..\eclipse-jee-galileo win32\eclipse\plugins\org.apache.ant_1.7.1.v20090120-1145</li>
</ol>

<p>hope it works </p>
"
"<p>I'm trying to get sqlalchemy-continuum to work alongside flask-sqlalchemy and flask-migrate. My <code>__init__.py</code> file looks like this:</p>

<pre><code>import os

from flask import Flask

def create_app():
    """"""Create and configure an instance of the Flask application.""""""
    app = Flask(__name__, instance_relative_config=True)

    app.config.from_mapping(
        SQLALCHEMY_DATABASE_URI='postgres+psycopg2://{}:{}@{}:{}/{}'.format(
            os.environ['POSTGRES_USER'],
            os.environ['POSTGRES_PASSWORD'],
            os.environ['POSTGRES_HOST'],
            os.environ['POSTGRES_PORT'],
            os.environ['POSTGRES_DB']
        ),
        SQLALCHEMY_TRACK_MODIFICATIONS=False
    )

    try:
        os.makedirs(app.instance_path)
    except OSError:
        pass

    from .models import db, migrate
    db.init_app(app)
    migrate.init_app(app, db)

    return app
</code></pre>

<p>My models.py file looks like this:</p>

<pre><code>import sqlalchemy
from flask_sqlalchemy import SQLAlchemy
from flask_migrate import Migrate
from sqlalchemy_continuum import make_versioned


db = SQLAlchemy()
migrate = Migrate()

make_versioned(user_cls=None)

class User(db.Model):
    __versioned__ = {}
    __tablename__ = 'user'

    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)
    password = db.Column(db.String(20), unique=True, nullable=False)

    def __repr__(self):
        return '&lt;User {} - {}&gt;'.format(self.username, self.email)

sqlalchemy.orm.configure_mappers()
</code></pre>

<p>I then run the following flask-migrate commands to initialise and migrate the database:</p>

<pre><code>flask db init
flask db migrate
flask db upgrade
</code></pre>

<p>The output of the flask db upgrade command seems to show the correct tables being created:</p>

<pre><code>INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.autogenerate.compare] Detected added table 'transaction'
INFO  [alembic.autogenerate.compare] Detected added table 'user'
INFO  [alembic.autogenerate.compare] Detected added table 'user_version'
INFO  [alembic.autogenerate.compare] Detected added index 'ix_user_version_end_transaction_id' on '['end_transaction_id']'
INFO  [alembic.autogenerate.compare] Detected added index 'ix_user_version_operation_type' on '['operation_type']'
INFO  [alembic.autogenerate.compare] Detected added index 'ix_user_version_transaction_id' on '['transaction_id']'
</code></pre>

<p>In the python shell I can do the following:</p>

<pre><code>&gt;&gt;&gt; from test_flask.__init__ import create_app
&gt;&gt;&gt; from test_flask.models import db, User

&gt;&gt;&gt; app = create_app()
&gt;&gt;&gt; with app.app_context():
...     user = User(username='devuser', email='devuser@gmail.com', 
password='devpassword')
...     db.session.add(user)
...     db.session.commit()
</code></pre>

<p>This seems to work fine but when I attempt to access an element in the versions attribute using:</p>

<pre><code>&gt;&gt;&gt; user.versions[0]
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/dynamic.py"", line 254, in __getitem__
  attributes.PASSIVE_NO_INITIALIZE).indexed(index)
File ""/usr/local/lib/python3.6/site-packages/sqlalchemy/orm/dynamic.py"", line 359, in indexed
  return list(self.added_items)[index]
IndexError: list index out of range
</code></pre>

<p>The command:</p>

<pre><code>&gt;&gt;&gt; user.versions
</code></pre>

<p>returns:</p>

<pre><code>&lt;sqlalchemy.orm.dynamic.AppenderQuery object at 0x7f6515d3a898&gt;
</code></pre>

<p>This doesn't seem to be the expected behaviour of the versions attribute, as specified in the <a href=""https://sqlalchemy-continuum.readthedocs.io/en/latest/intro.html#versions-and-transactions"" rel=""nofollow noreferrer"">sqlalchemy-continuum docs</a>. Any ideas as to what I've done wrong?</p>
","<p>If you can get the user from some global scope (like flask.g or flask.request), you can create a plugin, implementing the <code>transaction_args()</code> method:</p>

<pre class=""lang-py prettyprint-override""><code>from sqlalchemy_continuum.plugins import Plugin


class UserPlugin(Plugin):

    def transaction_args(self, uow, session):
        return {
            'user_id': get_user_from_globals()
        }
</code></pre>

<p>See <a href=""https://github.com/kvesteri/sqlalchemy-continuum/blob/master/sqlalchemy_continuum/plugins/flask.py"" rel=""nofollow noreferrer"">plugins/flask.py</a> or <a href=""https://github.com/M4d40/sqlalchemy-continuum/blob/111d943c13d3d3b3a35b008aada6bee8358a0945/sqlalchemy_continuum/plugins/pyramid_plugin.py"" rel=""nofollow noreferrer"">pyramid_plugin.py</a>. You should return primary key value of your User model.</p>

<p>You can save additional data with <a href=""https://sqlalchemy-continuum.readthedocs.io/en/latest/plugins.html#module-sqlalchemy_continuum.plugins.transaction_meta"" rel=""nofollow noreferrer"">TransactionMeta plugin</a> in a separate table. There is no documented API for extending the Transaction class itself. It is created by <code>TransactionFactory</code>, but that could change in later versions. It is probably a good idea to keep additional data separate.</p>
"
"<p>Display_Info is a SQL stored procedure,has three input parameters and three output parameters. info_Data(serialized information data may also contain unicode and null values) one of the output parameters is of type NVARCHAR(1000) previously. Since the larger the size of info_Data now it is changed of type NVARCHAR(MAX). When it was like NVARCHAR(1000) there was no problem in executing the stored procedure in client application, but after changing it to NVARCHAR(MAX) the client application is throwing the error like ""At least one parameter contained a type that was not supported."". SQL stored procedure design is as shown below.</p>

<pre><code>Create Display_Info @channel NVARCHAR(100)
    ,@infoType INT
    ,@locationId NVARCHAR(50)
    ,@Id BIGINT OUTPUT
    ,@infoData NVARCHAR(MAX) OUTPUT
    ,@infoStatus TINYINT OUTPUT
AS
...
</code></pre>

<p>The way the client application executing the stored procedure is,</p>

<pre><code>try
{
SACommand conncmd;
CheckConnection();
conncmd.setConnection(&amp;mConn);
std::wstring cmdText = COMMAND_TEXT(""ReadMessage"");
conncmd.setCommandText(cmdText.c_str());
conncmd.Param(""channel"").setAsString() = SAString(channel.c_str(), (int)channel.length());
conncmd.Param(""infoType"").setAsNumeric() = SANumeric((sa_int64_t)type);
conncmd.Param(""locationId"").setAsString() = SAString(locationId.c_str(), (int)locationId.length());
conncmd.Execute(); 
std::wstring Id = conncmd.Param(COMMAND_TEXT(""Id"")).asString();
infodata = conncmd.Param(COMMAND_TEXT(""info_Data"")).asString();
}
catch (SAException &amp;e)
{
std::string errorMessage = (mb_twine)e.ErrText();
std::cout &lt;&lt; ""\n"" &lt;&lt;errorMessage;
}
</code></pre>

<p><strong>Sample Input/output:</strong></p>

<p>infoData serialized input : Total length <strong>5191</strong></p>

<pre><code>(((Protocol Buffers is a method of serializing structured data. It is useful in developing programs to communicate with each other over a wire or for storing data. The method involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.Google developed Protocol Buffers for use internally and has made protocol compilers for C++, Java and Python available to the public under a free software, open source license. Various other language implementations are also available, including C#, JavaScript, Go, Perl, PHP, Ruby, and Scala.[1]The design goals for Protocol Buffers emphasized simplicity and performance. In particular, it was designed to be smaller and faster than XML.[2] Third parties have reported that Protocol Buffers outperforms the standardized Abstract Syntax Notation One with respect to both message size and decoding performance.[3]Protocol Buffers is widely used at Google for storing and interchanging all kinds of structured information. The method serves as a basis for a custom remote procedure call (RPC) system that is used for nearly all inter-machine communication at Google.[4]Protocol Buffers is very similar to the Apache Thrift protocol (used by Facebook for example), except that the public Protocol Buffers implementation does not include a concrete RPC protocol stack to use for defined services.A software developer defines data structures (called messages) and services in a proto definition file (.proto) and compiles it with protoc. This compilation generates code that can be invoked by a sender or recipient of these data structures. For example, example.proto will produce example.pb.cc and example.pb.h, which will define C++ classes for each message and service that example.proto defines.Canonically, messages are serialized into a binary wire format which is compact, forwards-compatible, and backwards-compatible, but not self-describing (that is, there is no way to tell the names, meaning, or full datatypes of fields without an external specification). There is no defined way to include or refer to such an external specification (schema) within a Protocol Buffers file. The officially supported implementation includes an ASCII serialization format,[5] but this format  though self-describing  loses the forwards-and-backwards-compatibility behavior, and is thus not a good choice for applications other than debugging.Though the primary purpose of Protocol Buffers is to facilitate network communication, its simplicity and speed make Protocol Buffers an alternative to data-centric C++ classes and structs, especially where interoperability with other languages or systems might be needed in the future.A schema for a particular use of protocol buffers associates data types with field names, using integers to identify each field. (The protocol buffer data contains only the numbers, not the field names, providing some bandwidth / storage savings compared with systems that include the field names in the data.)//polyline.protomessage Point {  required int32 x = 1;    required int32 y = 2;   optional string label = 3;   }      message Line {     required Point start = 1;    required Point end = 2;      optional string label = 3;   }      message Polyline {     repeated Point point = 1;       optional string label = 2;    }    The ""Point"" message defines two mandatory data items, x and y. The data item label is optional. Each data item has a tag. The tag is defined after the equal sign. For example, x has the tag 1.        The Line and ""Polyline"" messages, which both use Point, demonstrate how composition works in Protocol Buffers. Polyline has a repeated field, which behaves like a vector.        This schema can subsequently be compiled for use by one or more programming languages. Google provides a compiler called protoc which can produce output for C++, Java or Python. Other schema compilers are available from other sources to create language-dependent output for over 20 other languages.[6]        For example, after a C++ version of the protocol buffer schema above is produced, a C++ source code file, polyline.cpp, can use the message objects as follows:        // polyline.cpp#include polyline.pb.h  // generated by calling protoc polyline.proto        Line* createNewLine(const std::string&amp; name) {      // create a line from (10, 20) to (30, 40)        Line* line = new Line;       line-&gt;mutable_start()-&gt;set_x(10);         line-&gt;mutable_start()-&gt;set_y(20);        line-&gt;mutable_end()-&gt;set_x(30);          line-&gt;mutable_end()-&gt;set_y(40);         line-&gt;set_label(name);           return line;        }                Polyline* createNewPolyline() {          // create a polyline with points at (10,10) and (20,20)            Polyline* polyline = new Polyline;           Point* point1 = polyline-&gt;add_point();             point1-&gt;set_x(10);            point1-&gt;set_y(10);              Point* point2 = polyline-&gt;add_point();             point2-&gt;set_x(20);               point2-&gt;set_y(20);              return polyline;              }
</code></pre>

<p>When, <strong>NVARCHAR(1000)</strong>, infoData value : Total length - <strong>1003</strong></p>

<pre><code>(((Protocol Buffers is a method of serializing structured data. It is useful in developing programs to communicate with each other over a wire or for storing data. The method involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.Google developed Protocol Buffers for use internally and has made protocol compilers for C++, Java and Python available to the public under a free software, open source license. Various other language implementations are also available, including C#, JavaScript, Go, Perl, PHP, Ruby, and Scala.[1]The design goals for Protocol Buffers emphasized simplicity and performance. In particular, it was designed to be smaller and faster than XML.[2] Third parties have reported that Protocol Buffers outperforms the standardized Abstract Syntax Notation One with respect to both message size and dec
</code></pre>

<p>when <strong>NVARCHAR(4000)</strong>, infoData : Total length - <strong>4084</strong></p>

<pre><code>(((Protocol Buffers is a method of serializing structured data. It is useful in developing programs to communicate with each other over a wire or for storing data. The method involves an interface description language that describes the structure of some data and a program that generates source code from that description for generating or parsing a stream of bytes that represents the structured data.Google developed Protocol Buffers for use internally and has made protocol compilers for C++, Java and Python available to the public under a free software, open source license. Various other language implementations are also available, including C#, JavaScript, Go, Perl, PHP, Ruby, and Scala.[1]The design goals for Protocol Buffers emphasized simplicity and performance. In particular, it was designed to be smaller and faster than XML.[2] Third parties have reported that Protocol Buffers outperforms the standardized Abstract Syntax Notation One with respect to both message size and decoding performance.[3]Protocol Buffers is widely used at Google for storing and interchanging all kinds of structured information. The method serves as a basis for a custom remote procedure call (RPC) system that is used for nearly all inter-machine communication at Google.[4]Protocol Buffers is very similar to the Apache Thrift protocol (used by Facebook for example), except that the public Protocol Buffers implementation does not include a concrete RPC protocol stack to use for defined services.A software developer defines data structures (called messages) and services in a proto definition file (.proto) and compiles it with protoc. This compilation generates code that can be invoked by a sender or recipient of these data structures. For example, example.proto will produce example.pb.cc and example.pb.h, which will define C++ classes for each message and service that example.proto defines.Canonically, messages are serialized into a binary wire format which is compact, forwards-compatible, and backwards-compatible, but not self-describing (that is, there is no way to tell the names, meaning, or full datatypes of fields without an external specification). There is no defined way to include or refer to such an external specification (schema) within a Protocol Buffers file. The officially supported implementation includes an ASCII serialization format,[5] but this format  though self-describing  loses the forwards-and-backwards-compatibility behavior, and is thus not a good choice for applications other than debugging.Though the primary purpose of Protocol Buffers is to facilitate network communication, its simplicity and speed make Protocol Buffers an alternative to data-centric C++ classes and structs, especially where interoperability with other languages or systems might be needed in the future.A schema for a particular use of protocol buffers associates data types with field names, using integers to identify each field. (The protocol buffer data contains only the numbers, not the field names, providing some bandwidth / storage savings compared with systems that include the field names in the data.)//polyline.protomessage Point {  required int32 x = 1;    required int32 y = 2;   optional string label = 3;   }      message Line {     required Point start = 1;    required Point end = 2;      optional string label = 3;   }      message Polyline {     repeated Point point = 1;       optional string label = 2;    }    The ""Point"" message defines two mandatory data items, x and y. The data item label is optional. Each data item has a tag. The tag is defined af
</code></pre>

<p>when <strong>NVARCHAR(MAX) :</strong> 
with same infoData input
After executing the command, </p>

<pre><code>conncmd.Execute(); // after this statement 
</code></pre>

<p>it throws an <strong>error</strong> like</p>

<pre><code>At least one parameter contained a type that was not supported.
</code></pre>

<p>From the error it is understood very clear that, this type will not be supported anymore. Also While explicitly executing the stored procedure in SQL Server Management Studio. It is working fine, Got the complete infoData without any truncation.</p>

<pre><code>USE [TestDB]
GO

DECLARE @return_value int,
        @Id bigint,
        @infoData nvarchar(max),
        @infoStatus tinyint

EXEC    @return_value = ""DisplayInfo""
        @channel = N'telephoneMessage',
        @infoType = 1,
        @locationId = N'F6C8B935',
        @Id = @Id OUTPUT,
        @infoData = @infoData OUTPUT,
        @infoStatus = @infoStatus OUTPUT

SELECT  @Id as N'@PayloadId',
        @infoData as N'@MessageData',
        @infoStatus as N'@Status'

SELECT  'Return Value' = @return_value

GO
</code></pre>

<p>I have also noted that <a href=""https://stackoverflow.com/questions/11131958/what-is-the-maximum-characters-for-the-nvarcharmax"">What is the maximum characters for the NVARCHAR(MAX)?</a> which is saying that ""The max size for a column of type NVARCHAR(MAX) is 2 GByte of storage"". But I don't understand then Why in this case it is showing the NVARCHAR(MAX) as <strong>Type not supported</strong>. I have mentioned the SSMS version I am using so that it may helps to fix the error accurately.</p>

<p>SQL server Management Studio 2008 R2. V 10.50.2550.0: 
SQLAPI++ - 3.8.3</p>

<p>Help me get the complete info_Data as it is without any loss or truncation.</p>

<p>Thanks in Advance.</p>
","<p>My first impression is that SQLApi++ is great. Here is a bit of background.</p>

<p>I have been using ADO for a long time, but it's starting to give some COM errors for some users, without any helpful information. Also, msado??.tlb is not backwards-compatible so you have to be careful about all your users being on the same version. I understand this might not apply to you, but I figured I would share this anyway.</p>

<p>I started looking into SQLApi++ a few days ago and have almost only good things to say. The only draw-back I have found so far is that there is no way to know how many rows you get back without going to the result set. Also, it isn't free. On the positive side, the API is intuitive, the documentation is good, and the examples are useful. It is blazingly fast by comparison to ADO. Instead of copy/pasting, take a look at <a href=""http://www.sqlapi.com/Examples/step4.cpp"" rel=""nofollow"">http://www.sqlapi.com/Examples/step4.cpp</a>. </p>
"
"<p>I need to design a stored procedure for  running some functions on Netteza database from Aginity workbench 4.3  on win7. </p>

<pre><code>CREATE OR REPLACE PROCEDURE my_pro(int)
RETURNS integer EXECUTE AS CALLER
LANGUAGE NZPLSQL AS
BEGIN_PROC

DECLARE t int; 
BEGIN
     t := 0 ;
     WHILE t &lt;= 1 loop
         EXECUTE IMMEDIATE  'select 1';
         t := t + 1;
     END LOOP;
  END;
END_proc;

exec my_pro(0)
</code></pre>

<p>But I got null in result. </p>

<p>Did I miss something here ?</p>

<p>Thanks </p>
","<p>Are the table structures exactly the same?  If so you could potentially use Set Operators, though the performance might not be the best.  Something along the lines of:</p>

<pre><code>Select * 
  from (Select * From TableA
       MINUS
       Select * from TableB) A
       Join
       (select * from TableB
       MINUS
       Select * from TableA) ON *common unique field if there is one*
</code></pre>

<p>Each of the MINUS sub queries will give you the records in the First table which an exact match can't be found for in the second.  If there's a common unique identifier in the two tables then you could then join the results of the two sub-queries to get the result you're expecting.</p>
"
"<p>I wrote a function</p>

<pre><code>app :: Request -&gt; H.Session H.Postgres IO Response
</code></pre>

<p>which accepts web requests and builds responses (consulting the database as needed). To actually send out the responses I made a wrapper</p>

<pre><code>runApp :: H.Postgres -&gt; H.SessionSettings -&gt; Application
runApp pg sess req respond =
  respond =&lt;&lt; H.session pg sess (app req)
</code></pre>

<p>I pass this function to <a href=""http://hackage.haskell.org/package/warp-3.0.2.3/docs/Network-Wai-Handler-Warp.html"" rel=""nofollow"">Warp</a>s <code>runSettings</code> to loop forever and handle requests:</p>

<pre><code>runSettings appSettings $ runApp pgSettings sessSettings
</code></pre>

<p>However this is really bad because it creates a new session for every request which defeats the purpose of the connection pool and prepared statements. </p>

<p>I would like to call <code>runSettings</code> inside <code>H.session</code> rather than the other way around. However <code>runSettings</code> has a signature <code>Settings -&gt; Application -&gt; IO ()</code> and once inside <code>IO</code> I have lost access to the session. Is there a way to get back inside <code>Session b m r</code>?</p>

<p><em>This is a repost of a question from a private email.</em></p>
","<p>Yes, in your example you create a new session for every request, which is unacceptable.</p>

<p>First of all, <a href=""http://hackage.haskell.org/package/hasql-0.1.6/docs/Hasql.html#t:Session"" rel=""nofollow""><code>Session</code> is just and alias to the reader monad transformer</a>, which gives you a direct access to the pool. So you can always do:</p>

<pre><code>session postgresSettings sessionSettings $ do
  -- session' :: H.Session b m r -&gt; m r
  session' &lt;- flip runReaderT &lt;$&gt; ask
  let runApp request respond = 
        respond =&lt;&lt; session' (app request)
  liftIO $ do
    -- run warp here 
</code></pre>

<p>Secondly, <code>ReaderT</code> has a <a href=""http://hackage.haskell.org/package/monad-control-0.3.3.0/docs/Control-Monad-Trans-Control.html#g:3"" rel=""nofollow""><code>MonadBaseControl</code></a> instance, which is intended for similar patterns.</p>
"
"<p>I have two tables,</p>

<pre><code>USER

USER_ID | USER_NAME
--------------------
659    |  John  
660    |  Andrew 
661    |  Bianca
--------------------


USER_ADDRESS

USER_ID |TYPE |    ADDRESS
------------------------------
659     | HOME |    New York
659     | WORK |    New Jersey
660     | HOME |    San Francisco
660     | WORK |    Fremont
------------------------------
</code></pre>

<p>Output should be,</p>

<pre><code>USER_ID | USER_NAME | HOME_ADDRESS | WORK_ADDRESS 
--------------------------------------------------
659   |  John    |   New York      | New Jersey
660   |  Andrew  |   San Francisco | Fremont
</code></pre>

<p>How do I get the above output in a select query? Thanks in advance!</p>
","<p>My guess is that you've actually simply renamed the table to <code>[dbo.tablename]</code> and its fully qualified name is <code>[dbname].[dbo].[dbo.tablename]</code>. This happens when you right-click to rename a table name in SSMS and I'd imagine that WinSQL is doing the same thing (though I don't know why you're using that tool when SSMS is included). When you right-click, it takes away the schema name which makes you believe you need to fully qualify the new name, but you don't. </p>

<p>You should be safe to right-click and rename the table name to <strong><em>just</em></strong> the new table name.</p>

<p>To be sure, though, you can run:</p>

<pre><code>select * 
    from sys.schemas
 where name = 'dbo.dbo';
</code></pre>

<p>just to confirm that you've not created a new schema.</p>

<p><strong>EDIT</strong></p>

<p>Just for the sake of completeness I'll incorporate the comment made by @billinkc:</p>

<p>Run this query to get the exact schema of the table:</p>

<pre><code>select 
    s.name as SchemaName, 
        t.name as TableName
from sys.schemas s
    join sys.tables t 
    on s.schema_id = t.schema_id
where t.name = 'tablename'
</code></pre>
"
"<p>I have set of custom activities, which are used in complex workflows.</p>

<p>I would like to make them (custom activities) persistable without having workflow in an idle state.  It should be kind of failover system, so whenever something goes wrong during execution of a workflow it can be either:</p>

<ul>
<li>paused by a user (in anytime) and resumed later from the bookmark/point it was paused (for example user has noticed that an external system is down and he wants to pause the workflow for time being).</li>
<li>in case of an unhandled exception we can restart execution from last bookmark/point in time</li>
<li>stop of WorkflowApplication host can happen anytime and we can restart execution from last bookmark/point in time</li>
</ul>

<p>I've worked for few days with workflow persistance, but I am not sure if I can achieve my goal with it. Why?</p>

<ul>
<li>I could use blocking bookmarks in each custom activity, but blocking a workflow and restarting it just for purpose of having it persisted doesn't look promising.</li>
<li>I could use notblocking bookmarks, but I was not able to see them in database and resume from it. </li>
</ul>

<p>Can you please advice me, it workflow bookmarks are the way to go here?</p>

<p>I see some light in notblocking bookmarks, but I cannot persist them and resume later on. Can you please give me some hints how to persist a nonblocking bookmark for later resume?</p>

<p><strong>Edit:</strong></p>

<p>In wf3 there was an attribute <code>PersistOnClose</code> which would be enough for my requirement. 
in wf4 it was replaced with <code>Persist</code> activity, which could also be useful, however I do not want to have extra activities in my already complex workflows.</p>

<p>Ideally, it would be great to be able to execute <code>context.RequestPersist(callback)</code> from <code>NativeActivityContext</code> , however this method is internal (and everything what is inside it is not visible outside of original assembly.</p>
","<p>This delete behavior is configurable. For example, if you are using the WorkflowServiceHost and are using code to configure your service host, you could set the SqlWorkflowInstanceStoreBehavior.InstanceCompletionAction to InstanceCompletionAction.DeleteNothing as in the following example:</p>

<pre><code>WorkflowServiceHost host = new WorkflowServiceHost(workflow, baseAddress);
SqlWorkflowInstanceStoreBehavior persistenceBehavior = new SqlWorkflowInstanceStoreBehavior(connString);
persistenceBehavior.InstanceCompletionAction = InstanceCompletionAction.DeleteNothing;
host.Description.Behaviors.Add(persistenceBehavior);
</code></pre>

<p>For more information on features of the SQL workflow instance store and how you can configure it, check out this <a href=""http://msdn.microsoft.com/en-us/library/ee383994%28VS.100%29.aspx"" rel=""nofollow noreferrer"">MSDN article</a>.</p>
"
"<p>I try to do a simple select for some duplicates but my sql statement always end up with this error: </p>

<blockquote>
  <p>Command not properly ended</p>
</blockquote>

<p>What I have is a table like this</p>

<pre><code>EAN              STR
=============    ====
8030524096397    AAAA
8030524096397    BBBB
8030524096731    XXXX
8030524096731    YYYY
8030524096324    CCCC
</code></pre>

<p>My select is actually simple</p>

<pre><code>SELECT EAN, COUNT(*) FROM ean GROUP BY ean HAVING COUNT(*) &gt; 1;
</code></pre>

<p>Reults:</p>

<pre><code>EAN               COUNT(*)
=============     ========
8030524096397        2
8030524096731        2
</code></pre>

<p>Everything is fine until here! Now I want the <code>STR</code> of the duplicates and try this</p>

<pre><code>SELECT * FROM EAN E 
    INNER JOIN ( SELECT EAN, COUNT(*) FROM ean GROUP BY ean HAVING COUNT(*) &gt; 1 )  
R ON 
E.EAN = R.EAN;
</code></pre>

<p>But this results this error.</p>

<p>It exactly says this:</p>

<pre><code>SELECT * FROM EAN E
INNER JOIN ( SELECT EAN, COUNT(*) FROM ean GROUP BY ean HAVING COUNT(*) &gt; 1 )  R ON
^
Error: Command not properly ended
</code></pre>

<p>What am I doing wrong?</p>

<p><strong>Information to DB:</strong> Gupta Centura SQLBase 7.0.1</p>
","<p>I don't think SQLBase 7.01 supports proper ANSI <code>JOIN</code> syntax (aside:  what a good reason to use a more modern product).  The error indicates a problem on the <code>INNER JOIN</code>.</p>

<p>Here are two possible solutions.</p>

<p>First, yucky archaic <code>join</code> syntax:</p>

<pre><code>SELECT *
FROM EAN E,
     ( SELECT EAN, COUNT(*) as cnt FROM ean GROUP BY ean HAVING COUNT(*) &gt; 1 )  
R 
WHERE E.EAN = R.EAN;
</code></pre>

<p>Second, <code>IN</code>:</p>

<pre><code>SELECT *
FROM EAN E
WHERE E.EAN IN ( SELECT EAN FROM ean GROUP BY ean HAVING COUNT(*) &gt; 1 )  
</code></pre>
"
"<p>Does anyone know the location of a 64 bit version of ""Osm2pgsql.exe"" for windows? I can only find a 32 bit version.</p>
","<p>The <code>type ""geometry"" does not exist</code> part clearly states that you have no postgis extension installed.</p>

<p>Maybe you missed/forgot following part of the tutorial on creating extension in postgres db?</p>

<blockquote>
  <p>substitute your username for <em>username</em> below</p>
</blockquote>
"
"<p>I'm in learning about add view programmatically. But, I'm in confusing now. 
I have data, there are idpatient, idheader. Patient can have more than one ID header. When I input idpatient, it will add listview (with custom adapter) programmatically. The number of listview is same with the number of ID header.</p>

<p>I want to set in each listview with data patient group by ID header.. 
So far, I add search view when loop adding listview, but when I input one ID header to seacrh view, all of listview will view the same data according to ID header in search view..</p>

<p>I'm sorry for the long explanation. Can anybody help me to solve this problem? 
Thanks in advance</p>

<p><a href=""https://i.stack.imgur.com/GJD0Zm.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GJD0Zm.jpg"" alt=""enter image description here""></a></p>

<p>This is My Adapter :</p>

<pre><code>/**
 * Created by RKI on 11/9/2016.
 */
  public class AdapterHistory extends BaseAdapter implements Filterable,   View.OnClickListener {
    private Activity activity;
    LayoutInflater inflater;
    HistoryHeaderActivity main;
    public int count = 0;
    Context context;
    public ModelHistory product;
    ArrayList&lt;ModelHistory&gt; mStringFilterList;
    ModelHistory tempValues = null;
    ValueFilter valueFilter;
    public Cart cart;

    public AdapterHistory(HistoryHeaderActivity main, ArrayList&lt;ModelHistory&gt; arraylist) {
        this.main = main;
        this.main.historyModel = arraylist;
        mStringFilterList = arraylist;
    }

    @Override
    public int getCount() {
        return main.historyModel.size();
    }

    @Override
    public Object getItem(int position) {
        return main.historyModel.get(position);
    }

    @Override
    public long getItemId(int position) {
        return 0;
    }

    public View getView(final int position, View convertView, ViewGroup parent) {
        int pos = position;
        final Cart carts = CartHelper.getCart();
        View vi = convertView;
        ViewHolderItem holder = new ViewHolderItem();
        if (convertView == null) {
            LayoutInflater inflater = (LayoutInflater) main.getSystemService(Context.LAYOUT_INFLATER_SERVICE);
            vi = inflater.inflate(R.layout.list_history, null);
            holder.h_id = (TextView) vi.findViewById(R.id.id_header_);
            holder.h_type = (TextView) vi.findViewById(R.id.servicetype_);
            holder.h_qty = (TextView) vi.findViewById(R.id.qty_);
            holder.h_ps_id = (TextView) vi.findViewById(R.id.patient_id);
            holder.h_ps_name = (TextView) vi.findViewById(R.id.patient_);
            holder.h_dokid = (TextView) vi.findViewById(R.id.doctor_id_);
            holder.h_dokname = (TextView) vi.findViewById(R.id.doctor_);
            holder.h_item = (TextView) vi.findViewById(R.id.item_);
            holder.h_date = (TextView) vi.findViewById(R.id.date_);
            holder.checkToCart = (CheckBox) vi.findViewById(R.id.checkBox);
            holder.checkToCart.setTag(position);
            holder.checkToCart.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {

                @Override
                public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {
                    int getPosition = (Integer) buttonView.getTag();  // Here we get the position that we have set for the checkbox using setTag.
                    tempValues.setSelected(isChecked);
                }
            });
            vi.setTag(holder);
            vi.setTag(R.id.checkBox, holder.checkToCart);
        } else {
            holder = (ViewHolderItem) vi.getTag();
        }
        if (main.historyModel.size() &lt;= 0) {
            holder.h_date.setText(""No Data"");

        } else {
            /*** Get each Model object from Arraylist ****/
            tempValues = null;
            tempValues = (ModelHistory) main.historyModel.get(position);
            holder.h_id.setText(tempValues.getH_id());
            holder.h_type.setText(tempValues.getH_service());
            holder.h_qty.setText(tempValues.getH_qty());
            holder.h_ps_name.setText(tempValues.getH_p_name());
            holder.h_ps_id.setText(tempValues.getH_p_id());
            holder.h_dokid.setText(tempValues.getH_d_id());
            holder.h_dokname.setText(tempValues.getH_d_name());
            holder.h_item.setText(tempValues.getH_item());
            holder.h_date.setText(tempValues.getH_date());
            holder.checkToCart.setTag(position);
            holder.checkToCart.setOnCheckedChangeListener(new CompoundButton.OnCheckedChangeListener() {
                @Override
                public void onCheckedChanged(CompoundButton buttonView, boolean isChecked) {
                    main.historyModel.get(position).setSelected(buttonView.isChecked());
                }
            });
            holder.checkToCart.setChecked(main.historyModel.get(position).isSelected());
            for (pos = 1; pos &lt;= 1; pos++) {
                main.u_pname = tempValues.getH_p_name();
                main.u_pid = tempValues.getH_p_id();
                main.u_service = tempValues.getH_service();
                main.up_iddetail = tempValues.getH_id();
            }
        }
        return vi;
    }

    @Override
    public void onClick(View v) {

    }

    public static class ViewHolderItem {
        TextView h_id, h_type, h_ps_id, h_ps_name, h_dokid, h_dokname, h_item, h_qty, h_date;
        CheckBox checkToCart;
    }

    private List&lt;TransactionsItem&gt; getCartItems(Cart cart) {
        List&lt;TransactionsItem&gt; cartItems = new ArrayList&lt;&gt;();
        Map&lt;Saleable, Integer&gt; itemMap = cart.getItemWithQuantity();

        for (Map.Entry&lt;Saleable, Integer&gt; entry : itemMap.entrySet()) {
            TransactionsItem cartItem = new TransactionsItem();
            cartItem.setProduct((ModelInventory) entry.getKey());
            cartItem.setQuantity(entry.getValue());
            cartItems.add(cartItem);
        }

        return cartItems;
    }

    @Override
    public Filter getFilter() {
        if (valueFilter == null) {
            valueFilter = new ValueFilter();
        }
        return valueFilter;
    }

    private class ValueFilter extends Filter {
        @Override
        protected FilterResults performFiltering(CharSequence constraint) {
            FilterResults results = new FilterResults();

            if (constraint != null &amp;&amp; constraint.length() &gt; 0) {
                List&lt;ModelHistory&gt; filterList = new ArrayList&lt;ModelHistory&gt;();
                for (int i = 0; i &lt; mStringFilterList.size(); i++) {
                    if ((mStringFilterList.get(i).getH_id().toUpperCase())
                            .contains(constraint.toString().toUpperCase())) {

                        ModelHistory country = new ModelHistory(
                                mStringFilterList.get(i).getH_id(),
                                mStringFilterList.get(i).getH_p_id(),
                                mStringFilterList.get(i).getH_date(),
                                mStringFilterList.get(i).getH_p_name(),
                                mStringFilterList.get(i).getH_d_id(),
                                mStringFilterList.get(i).getH_d_name(),
                                mStringFilterList.get(i).getH_item(),
                                mStringFilterList.get(i).getH_qty(),
                                mStringFilterList.get(i).getH_service());
                        filterList.add(country);
                    }
                }
                results.count = filterList.size();
                results.values = filterList;
            } else {
                results.count = mStringFilterList.size();
                results.values = mStringFilterList;
            }
            return results;
        }

        @Override
        protected void publishResults(CharSequence constraint,
                                      FilterResults results) {
            main.historyModel = (ArrayList&lt;ModelHistory&gt;) results.values;
            notifyDataSetChanged();
        }
    }
}
</code></pre>

<p>This is My Activity :</p>

<pre><code>import com.mobileproject.rki.mobile_his_receipt.view.adapter.AdapterHistory;
import org.w3c.dom.*;
import java.io.Serializable;
import java.util.*;

public class HistoryHeaderActivity extends AppCompatActivity implements SearchView.OnQueryTextListener {

    static final String URL = ""http://.../GetDataInv"";
    static final String KEY_TABLE = ""Table""; // parent node
    static final String KEY_REG_ID = ""Sales_Aptk_ID"", KEY_DATE = ""Sales_Aptk_Date"",
            KEY_PATIENT_ID = ""Sales_Aptk_Patient_ID"", KEY_SERVICE_ID = ""Sales_Aptk_Type"",
            KEY_PATIENT_NAME = ""Sales_Aptk_Patient_Name"",
            KEY_DOCTOR_ID = ""Sales_Aptk_Doctor_ID"", KEY_DOCTOR_NAME = ""Sales_Aptk_Doctor_Name"",
            KEY_ITEM_ID = ""Sales_Aptk_Detail_Item_ID"", KEY_DETAIL_UNIT = ""Sales_Aptk_Detail_Unit"",
            KEY_ITEM_QTY = ""Sales_Aptk_Detail_Qty"";
    static final String KEY_ID_HEADER = ""Sales_Aptk_ID"";

    static final String KEY_TABLE_INV = ""Table""; // parent node
    static final String KEY_ITEM_ID_INV = ""Item_ID"", KEY_ITEM_NAME = ""Item_Name"",
            KEY_MAX_STOCK = ""Item_Max_Stock"";

    public static final int DIALOG_DOWNLOAD_DATA_PROGRESS = 0, DIALOG_NO_DATA = 1,
            DIALOG_DOWNLOAD_FULL_PHOTO_PROGRESS = 2;

    Element e;
    final Context context = this;
    public List&lt;ModelInventory&gt; invModels;
    private ProgressDialog mProgressDialog;
    public static String id, up_iddetail, up_user, u_pid, u_pname, u_service, xml;
    public ArrayList&lt;ModelInventory&gt; invModel = new ArrayList&lt;ModelInventory&gt;();
    public ArrayList&lt;ModelHistory&gt; historyModel = new ArrayList&lt;ModelHistory&gt;();
    public ArrayList&lt;ModelIDHeader&gt; headerModel = new ArrayList&lt;ModelIDHeader&gt;();
    public Cart cart;
    private Menu menu;
    AdapterHistory hstAdpt, idhstAdpt;
    private ProgressDialog progressDialog;
    public ModelInventory productInv;
    int mPosition, invPosition;
    EditText p_id;
    ListView listHistory;
    XMLParser parser;
    LinearLayout lm;
    LinearLayout.LayoutParams params;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_history_header);

        lm = (LinearLayout) findViewById(R.id.linearMain);
        params = new LinearLayout.LayoutParams(
                LayoutParams.WRAP_CONTENT, LayoutParams.WRAP_CONTENT);

        p_id = (EditText) findViewById(R.id.patientID);
        listHistory = (ListView) findViewById(R.id.listhistory);

        hstAdpt = new AdapterHistory(HistoryHeaderActivity.this, historyModel);
        listHistory.setAdapter(hstAdpt);


        listHistory.setOnItemClickListener(new AdapterView.OnItemClickListener() {
            @Override
            public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) {
                mPosition = position;
                invPosition = position;
            }
        });

        SharedPreferences login2 = getSharedPreferences(""USERLOGIN"", 0);
        String doktername = login2.getString(""userlogin"", ""0"");
        up_user = doktername;

        p_id.addTextChangedListener(new TextWatcher() {

            public void afterTextChanged(Editable s) {
            }

            public void beforeTextChanged(CharSequence s, int start,
                                          int count, int after) {
            }

            public void onTextChanged(CharSequence s, int start,
                                      int before, int count) {
                onGetHistory();
            }
        });
    }

    /**
     * Check Form Input
     *
     * @return
     */
    private boolean isFormValid() {
        String aTemp = p_id.getText().toString();

        if (aTemp.isEmpty()) {
            Toast.makeText(this, ""Please Input Patient ID.."", Toast.LENGTH_SHORT).show();
            return false;
        } else {
            id = aTemp.toString();
        }

        return true;
    }

    protected void dismissDialogWait() {
        if (progressDialog != null) {
            if (progressDialog.isShowing()) {
                progressDialog.dismiss();
            }
        }
    }

    public void onGetHistory() {
        listHistory.setAdapter(null);
        if (isFormValid()) {
            GetDataHistoryTask syncTask = new GetDataHistoryTask();
            syncTask.execute(new IntroductingMethod());

            GetDataHeaderTask syncTaskHeader = new GetDataHeaderTask();
            syncTaskHeader.execute(new IntroductingMethod());
        }
    }

    private class GetDataHistoryTask extends AsyncTask&lt;IntroductingMethod, String, String&gt; {

        @Override
        protected String doInBackground(IntroductingMethod... params) {
            IntroductingMethod REGService = params[0];
            return REGService.getHistoryData(id);
        }

        @Override
        protected void onPostExecute(String result) {
            dismissDialogWait();

            if (result != null) {
                try {
                    ArrayList&lt;HashMap&lt;String, String&gt;&gt; menuItems = new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
                    if (Build.VERSION.SDK_INT &gt; 9) {
                        StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
                        StrictMode.setThreadPolicy(policy);
                    }
                    XMLParser parser = new XMLParser();
                    Document doc = parser.getDomElement(result); // getting DOM element

                    NodeList nl = doc.getElementsByTagName(KEY_TABLE);
                    // looping through all item nodes &lt;item&gt;
                    for (int i = 0; i &lt; nl.getLength(); i++) {
                        // creating new HashMap
                        HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
                        Element e = (Element) nl.item(i);

                        ModelHistory add = new ModelHistory();
                        add.setH_id(parser.getValue(e, KEY_REG_ID));
                        add.setH_p_name(parser.getValue(e, KEY_PATIENT_NAME));
                        add.setH_p_id(parser.getValue(e, KEY_PATIENT_ID));
                        add.setH_service(parser.getValue(e, KEY_SERVICE_ID));
                        add.setH_date(parser.getValue(e, KEY_DATE));
                        add.setH_detail_unit(parser.getValue(e, KEY_DETAIL_UNIT));
                        add.setH_d_id(parser.getValue(e, KEY_DOCTOR_ID));
                        add.setH_d_name(parser.getValue(e, KEY_DOCTOR_NAME));
                        add.setH_item(parser.getValue(e, KEY_ITEM_ID));
                        add.setH_qty(parser.getValue(e, KEY_ITEM_QTY));
                        historyModel.add(add);
                    }
                    ShowAllContentHistory();
                } catch (Exception e) {
                    Toast.makeText(HistoryHeaderActivity.this.getApplicationContext(),
                            ""Koneksi gagal. Silahkan coba kembali."", Toast.LENGTH_LONG).show();
                }
            } else {
                Toast.makeText(HistoryHeaderActivity.this.getApplicationContext(),
                        ""Koneksi gagal. Silahkan coba kembali."", Toast.LENGTH_LONG).show();
            }
        }
    }

    private class GetDataHeaderTask extends AsyncTask&lt;IntroductingMethod, String, String&gt; {

        @Override
        protected String doInBackground(IntroductingMethod... params) {
            IntroductingMethod REGService = params[0];
            return REGService.getHistoryHeaderData(id);
        }

        @Override
        protected void onPostExecute(String result) {
            dismissDialogWait();

            if (result != null) {
                try {
                    ArrayList&lt;HashMap&lt;String, String&gt;&gt; menuItems = new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
                    if (Build.VERSION.SDK_INT &gt; 9) {
                        StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
                        StrictMode.setThreadPolicy(policy);
                    }
                    XMLParser parser = new XMLParser();
                    Document doc = parser.getDomElement(result); // getting DOM element

                    NodeList nl = doc.getElementsByTagName(KEY_TABLE);
                    // looping through all item nodes &lt;item&gt;
                    for (int i = 0; i &lt; nl.getLength(); i++) {
                        // creating new HashMap
                        HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();
                        Element e = (Element) nl.item(i);

                        ModelIDHeader add = new ModelIDHeader();
                        add.setIdSalesHeader(parser.getValue(e, KEY_ID_HEADER));
                        headerModel.add(add);
                    }
                    if(result.contains(""&lt;Sales_Aptk_Patient_ID&gt;""+id+""&lt;/Sales_Aptk_Patient_ID&gt;"")){
                    Log.d(""NilaiID "", id);
                        AddNewList();
                    }
                    else{
                        Log.d(""Kenapayahh"", id);
                    }
                } catch (Exception e) {
                    Toast.makeText(HistoryHeaderActivity.this.getApplicationContext(),
                            ""Koneksi gagal. Silahkan coba kembali."", Toast.LENGTH_LONG).show();
                }
            } else {
                Toast.makeText(HistoryHeaderActivity.this.getApplicationContext(),
                        ""Koneksi gagal. Silahkan coba kembali."", Toast.LENGTH_LONG).show();
            }
        }
    }


    public void AskUpdate(View v) {
        if (hstAdpt.getCount() == 0) {
            Toast.makeText(HistoryHeaderActivity.this.getApplicationContext(),
                    ""Tidak ada rekaman pasien."", Toast.LENGTH_LONG).show();
        } else {

            SharedPreferences id_header = getSharedPreferences(""IDSALESAPTK"", 0);
            SharedPreferences.Editor editorID = id_header.edit();
            editorID.putString(""idsalesaptk"", up_iddetail);
            editorID.commit();

            SharedPreferences sendPref2 = getSharedPreferences(""DATAPATIENTNAME"", 0);
            SharedPreferences.Editor editor2 = sendPref2.edit();
            editor2.putString(""datapatientname"", u_pname);
            editor2.commit();
            editor2.clear();

            SharedPreferences sendPref3 = getSharedPreferences(""DATAPATIENTID"", 0);
            SharedPreferences.Editor editor3 = sendPref3.edit();
            editor3.putString(""datapatientid"", u_pid);
            editor3.commit();
            editor3.clear();

            SharedPreferences regID = getSharedPreferences(""DATAREGID"", 0);
            String reg_ID = regID.getString(""dataregid"", ""0"");
            SharedPreferences sendPref4 = getSharedPreferences(""DATAREGID"", 0);
            SharedPreferences.Editor editor4 = sendPref4.edit();
            editor4.putString(""dataregid"", reg_ID);
            editor4.commit();
            editor4.clear();

            SharedPreferences sendPref1 = getSharedPreferences(""DATASERVICE"", 0);
            SharedPreferences.Editor editor1 = sendPref1.edit();
            editor1.putString(""dataservice"", u_service);
            editor1.commit();
            editor1.clear();
            new LoadingDataAsync().execute();
        }
    }


    public class LoadingDataAsync extends AsyncTask&lt;String, Void, Void&gt; {

        @Override
        protected Void doInBackground(String... params) {
            updateDetail();
            return null;
        }

        protected void onPostExecute(Void unused) {
            dismissDialog(DIALOG_DOWNLOAD_DATA_PROGRESS);
            removeDialog(DIALOG_DOWNLOAD_DATA_PROGRESS);
        }

        protected void onPreExecute() {
            super.onPreExecute();
            showDialog(DIALOG_DOWNLOAD_DATA_PROGRESS);
        }
    }

    public void updateDetail() {
        ArrayList&lt;ModelHistory&gt; candidateModelArrayList = new ArrayList&lt;ModelHistory&gt;();
        for (ModelHistory model : historyModel) {
            if (model.isSelected()) {
                candidateModelArrayList.add(model);
            }
        }

        ArrayList&lt;HashMap&lt;String, String&gt;&gt; menuItems = new ArrayList&lt;HashMap&lt;String, String&gt;&gt;();
        if (Build.VERSION.SDK_INT &gt; 9) {
            StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder().permitAll().build();
            StrictMode.setThreadPolicy(policy);
        }
        parser = new XMLParser();
        xml = parser.getXmlFromUrl(URL);
        Document doc = parser.getDomElement(xml);

        NodeList nl = doc.getElementsByTagName(KEY_TABLE_INV);
        for (int i = 0; i &lt; nl.getLength(); i++) {
            e = (Element) nl.item(i);

            ModelInventory add = new ModelInventory();
            add.setItem_ID(parser.getValue(e, KEY_ITEM_ID_INV));
            add.setItem_Name(parser.getValue(e, KEY_ITEM_NAME));
            add.setItem_Max_Stock(parser.getValue(e, KEY_MAX_STOCK));
            invModel.add(add);
        }
        invModels = new ArrayList&lt;ModelInventory&gt;();
        invModels = invModel;
        ArrayAdapter&lt;ModelInventory&gt; adptInv = new ArrayAdapter&lt;ModelInventory&gt;(context, android.R.layout.simple_spinner_dropdown_item, invModels);
        ArrayAdapter&lt;ModelHistory&gt; adptChkItem = new ArrayAdapter&lt;ModelHistory&gt;(context, android.R.layout.simple_spinner_dropdown_item, candidateModelArrayList);
        if (adptInv.isEmpty()) {
            Toast.makeText(HistoryHeaderActivity.this, ""Empty Patient"", Toast.LENGTH_SHORT).show();
        }

        cart = CartHelper.getCart();
        for (mPosition = 0; mPosition &lt; adptChkItem.getCount(); mPosition++) {
            ModelHistory historyItem = (ModelHistory) adptChkItem.getItem(mPosition);
            for (int j = mPosition; j &lt; adptInv.getCount(); j++) {
                ModelInventory inventoryItem = (ModelInventory) adptInv.getItem(j);
                if (candidateModelArrayList.get(mPosition).getH_item().equals(inventoryItem.getItem_ID())) {
                    if (cart.getProducts().toString().contains(inventoryItem.getItem_Name())) {
                    } else {
                        productInv = (ModelInventory) (Serializable) adptInv.getItem(j);
                        int qty = Integer.parseInt(historyItem.getH_qty());
                        cart.add(productInv, qty);
                    }
                } else {
                }
            }
        }

        Intent i = new Intent(getBaseContext(), CartActivity.class);
        i.putExtra(""PersonID"", ""try"");
        startActivity(i);
    }

    @Override
    protected Dialog onCreateDialog(int id) {
    }


    @Override
    public boolean onQueryTextSubmit(String query) {
        return false;
    }

    @Override
    public boolean onQueryTextChange(String newText) {
        hstAdpt.getFilter().filter(newText);
        idhstAdpt.getFilter().filter(newText);
        return false;
    }

    public void AddNewList(){
        ArrayAdapter&lt;ModelIDHeader&gt; adptIDHeader = new ArrayAdapter&lt;ModelIDHeader&gt;(context, android.R.layout.simple_spinner_dropdown_item, headerModel);

        for(int count =0; count&lt; adptIDHeader.getCount(); count++){
            idhstAdpt = new AdapterHistory(HistoryHeaderActivity.this, historyModel);
            ModelIDHeader idHeader = (ModelIDHeader) adptIDHeader.getItem(count);

            Button btn = new Button(this);
            btn.setId(count);
            btn.setText(idHeader.getIdSalesHeader());
            lm.addView(btn);

            SearchView search = new SearchView(this);
            search.setQuery(idHeader.getIdSalesHeader(), false);
            search.setOnQueryTextListener(this);
            lm.addView(search);

            ListView tv = new ListView(this);
            tv.setId(count);
            tv.setLayoutParams(params);
            tv.setDividerHeight(2);
            tv.setAdapter(idhstAdpt);
            lm.addView(tv);

        }
    }

    public void ShowAllContentHistory() {
        listHistory = (ListView) findViewById(R.id.listhistory);
        hstAdpt = new AdapterHistory(HistoryHeaderActivity.this, historyModel);
        listHistory.setAdapter(hstAdpt);
        listHistory.setOnItemClickListener(new AdapterView.OnItemClickListener() {

            @Override
            public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) {

            }
        });
    }

    @Override
    public boolean onCreateOptionsMenu(Menu menu) {
        MenuInflater inflater = getMenuInflater();
        inflater.inflate(R.menu.activity_history_actions, menu);
        return super.onCreateOptionsMenu(menu);

    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        int id = item.getItemId();

        if (id == R.id.action_logout) {
            new AlertDialog.Builder(this)
                    .setMessage(""Are you sure you want to exit?"")
                    .setCancelable(false)
                    .setPositiveButton(""Yes"", new DialogInterface.OnClickListener() {
                        public void onClick(DialogInterface dialog, int id) {
                            ClearPrefs();
                            logout();
                            Intent intent = new Intent(HistoryHeaderActivity.this, LoginActivity.class);
                            startActivity(intent);
                        }
                    })
                    .setNegativeButton(""No"", null)
                    .show();
            return true;
        }
        if (id == R.id.action_home) {
            Intent intent = new Intent(HistoryHeaderActivity.this, MainActivity.class);
            startActivity(intent);
            return true;
        }
        return super.onOptionsItemSelected(item);
    }

    @Override
    public void onBackPressed() {
        Intent intent = new Intent(HistoryHeaderActivity.this, MainActivity.class);
        startActivity(intent);
    }

    public void ClearPrefs() {
    }

    public void logout() {
        SharedPreferences preferences5 = getSharedPreferences(""IDLOGIN"", Context.MODE_PRIVATE);
        SharedPreferences.Editor editor5 = preferences5.edit();
        editor5.clear();
        editor5.commit();
    }
}
</code></pre>
","<p>It seems the phrases that are being used are:</p>

<ul>
<li><p>Spread your data out amongst many containers for best performance</p></li>
<li><p>Modeling your data via Entities </p></li>
<li><p>Process your queries in parallel for best performance</p></li>
<li><p>Caching data in the service hosted middle tier</p></li>
</ul>

<p>This would imply that we have to start thinking like OO modellers rather than in a relational mind-set. Performance seems to rely on the ability to massively parallelise an object query in a smiliar way to creating a LINQ query that can take advantage of parallelisation.</p>
"
"<p>I'm trying to read a FIC file that is not encrypted (indeed, the data can almost be read when displaying it). I want to convert this file to a more convenient format, CSV, XML, SQL, etc...</p>

<p>When I try to open it with Windev Express 19, I have an error telling me that the file is password protected. But If there really was a password, the file would be encrypted (I think).</p>

<p>If someone has any idea about what could be the problem. Or any suggestion, I'd be glad.</p>
","<p>Here is how do to it ^^ :</p>

<p>1 : You can get the OleDb Provider for hyperfileSql files from this <a href=""http://www.pcsoft.fr/st/telec/modules-communs-15/wx15_63j.htm"" rel=""nofollow"">page</a>.</p>

<p>2 : Here is a simple exemple of the code used to extract data :</p>

<pre><code>string connectionString = @""Provider=PCSOFT.HFSQL;Initial Catalog=C:\MyDataFolder"";
string sql = @""SELECT * FROM MyTable""; //MyTable = The .FIC file

DataTable table = new DataTable();

using (OleDbConnection connection = new OleDbConnection(connectionString))
{
       using (OleDbDataAdapter adapter = new OleDbDataAdapter(sql, connection))
       {
              adapter.Fill(table); //Fill the table with the extracted data
       }
}

gridControl1.DataSource = table; //Set the DataSource of my grid control
</code></pre>

<p>For other connection strings : visite the <a href=""http://doc.pcsoft.fr/fr-FR/?9000059"" rel=""nofollow"">page</a></p>
"
"<p>I'm trying to switch one of my VBS scripts from using SQLOLEDB to ODBC driver. So long all works like expected - all but one thing:</p>

<p>When fetching <em>SERVERPROPERTY(""is_clustered"")</em> from an MSSQL instance the resulting value is different using each driver.</p>

<p>Here's the output of an example script (script follows below):</p>

<blockquote>
<pre><code>C:\&gt; cscript test.vbs

Provider: sqloledb

is_clustered (name): is_clustered
is_clustered (type): 12
is_clustered (value): 0

Driver: (SQL Server)

is_clustered (name): is_clustered
is_clustered (type): 204
C:\test.vbs(33, 1) Microsoft VBScript runtime error: Type mismatch
</code></pre>
</blockquote>

<p>Does anyone know what  I'm doing wrong or what I'm missing in my code?</p>

<p>Oh, yes, the code... here's the example script itself:</p>

<pre><code>Option Explicit

Dim RS, CONN1, CONN2

Set RS        = CreateObject(""ADODB.Recordset"")

Set CONN1      = CreateObject(""ADODB.Connection"")
CONN1.ConnectionTimeout = 2
CONN1.Provider = ""sqloledb""
CONN1.Properties(""Integrated Security"").Value = ""SSPI""
CONN1.Properties(""Data Source"").Value = ""HOSTNAME\INST01""
CONN1.Open

WScript.echo ""Provider: sqloledb"" &amp; vbLf
RS.Open ""SELECT SERVERPROPERTY('IsClustered') AS is_clustered"", CONN1
WScript.echo ""is_clustered (name): "" &amp; RS.fields(0).Name
WScript.echo ""is_clustered (type): "" &amp; RS.fields(0).Type
WScript.echo ""is_clustered (value): "" &amp; RS(""is_clustered"") &amp; vbLf
RS.Close

Set CONN2      = CreateObject(""ADODB.Connection"")
CONN2.ConnectionTimeout = 2
CONN2.ConnectionString = ""driver={SQL Server};"" &amp; _
                         ""server=HOSTNAME\INST01;"" &amp; _
                         ""Trusted_Connection=yes""
CONN2.Open

WScript.echo ""Driver: (SQL Server)"" &amp; vbLf

RS.Open ""SELECT SERVERPROPERTY('IsClustered') AS is_clustered"", CONN2
WScript.echo ""is_clustered (name): "" &amp; RS.fields(0).Name
WScript.echo ""is_clustered (type): "" &amp; RS.fields(0).Type
WScript.echo ""is_clustered (value): "" &amp; RS(""is_clustered"")
RS.Close
</code></pre>

<p>Many Thanks in advance!</p>

<p>BR,
Marcel</p>
","<p>Since there is no solution, and ADO (while not <em>dead</em>) is done, the hack <strong>is</strong> the answer.</p>

<p>Never issue a <code>MERGE</code> statement without a leading comment line:</p>

<pre><code>--Dummy leading comment line to thwart ADO from mangling MERGE on Windows XP/2003R2
MERGE Users
USING (VALUES ...
</code></pre>
"
"<p>Having a table with jsonb column containing some array,
what is best way to select records containing specific tag with <code>rom-sql</code>?</p>

<p>Example of query.
<a href=""https://www.db-fiddle.com/f/u4CFkUUpnHZj67j1RJ5YRe/0"" rel=""nofollow noreferrer"">https://www.db-fiddle.com/f/u4CFkUUpnHZj67j1RJ5YRe/0</a></p>

<pre><code>CREATE TABLE posts (
  id INT,
  tags JSONB
);
INSERT INTO posts (id, tags) VALUES (1, '[""cats"", ""dogs""]');
INSERT INTO posts (id, tags) VALUES (2, '[""dogs""]');

SELECT * FROM posts WHERE tags @&gt; '[""cats""]';
</code></pre>

<p>So, how to build this query with <code>rom-sql</code>? </p>
","<p>Complete example: <a href=""https://github.com/gotar/sinatra-rom"" rel=""nofollow"">https://github.com/gotar/sinatra-rom</a></p>

<p>after you add </p>

<pre><code>require 'bundler/setup'
require 'rom/sql/rake_task'

task :setup do
  # Load ROM related stuff. You don't need to specify manually connection
end
</code></pre>

<p>to Rakefile you will get few Raketasks (rake -T) to list them, </p>

<p>and then</p>

<pre><code>$ rake db:create_migration[any_name]
</code></pre>

<p>and in file it will create, you can add your migration. </p>

<p>Thats all</p>
"
"<p>I am trying to build a query for a tableau dashboard that is connected to Google BigQuery. We have tables for each month of data, but I want to present the last 30 days of data at any given time (so it will have to go across multiple tables). The current query I have gives the error ""Timestamp literal or explicit conversion to timestamp is required."" I've been looking around for some help on how to convert to timestamp but haven't found anything helpful. This is my code.</p>

<pre><code>    SELECT
      DATE(date_time) AS date,
    FROM
      TABLE_QUERY(myTable, ""date(concat(left(table_id,4),'-',right(table_id,2),'-','01')) &gt;= '2017-06-01'"")
    WHERE 
        DATE(date_time) &gt;= DATE_ADD(day,-30, current_date()) 
        and   DATE(date_time) &lt;= current_date()
    ORDER BY 
        date
</code></pre>

<p>Any help would as to how to get it to work will be greatly appreciated. </p>

<p>Note: we are using legacy SQL</p>
","<blockquote>
  <p>is it the problem with the data or the problem with the split ??  </p>
</blockquote>

<p>To help in troubleshooting - I would recommend running same logic in BigQuery Standard SQL  </p>

  

<pre class=""lang-sql prettyprint-override""><code>#standardSQL
SELECT 
  ColA,
  SPLIT(ColA, '|')[SAFE_OFFSET(0)] AS part1,
  SPLIT(ColA, '|')[SAFE_OFFSET(1)] AS part2
FROM TabA
</code></pre>
"
"<p>I'm trying to add some fields to an existing report that runs just fine.  To add these fields, I need to join to another table.  The problem is the field is a different type on each table</p>

<pre><code>Table A / FieldA = Varchar (20)
Table B / FieldB = Decimal (19,0)
</code></pre>

<p>This is the join I have:</p>

<pre><code>inner join TableA ta on ta.FieldA = b.FieldB 
</code></pre>

<p>With this join, I get a <code>SELECT Failed [3754]  Precision error in FLOAT type constant or during implicit conversions</code>.</p>

<p>I'm thinking I have to use a CAST statement like this:</p>

<pre><code>inner join TableA ta on ta.FieldA = cast(b.FieldB as Varchar (20))
</code></pre>

<p>When I run the report now,  I don't get any results and I'm expecting at least 1 row.</p>

<p>Any help with the inner join would be greatly appreciated.  Thanks.</p>
","<p>This returns the same result as Standard SQL's <code>LAG(END_dt) OVER (PARTITION BY CUST_ID ORDER BY END_dt</code>, i.e. the previous row's <code>END_dt</code> (or NULL for the 1st row per CUST_ID).</p>

<p>When you switch to <code>FOLLOWING</code> instead of <code>PRECEDING</code> it's the next row, <code>LEAD</code> in Standard SQL.</p>

<p>Both <code>LAG</code> and <code>LEAD</code> are finally implemented in TD16.10.</p>

<p>As you simply want to find gaps and you don't access the actual difference you can also simplify it to:</p>

<pre><code>SELECT DISTINCT CUST_ID
FROM table
QUALIFY
   STRT_dt - 
   MIN(END_dt)
   OVER (PARTITION BY CUST_ID
         ORDER BY END_dt
         ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) &gt; 1
</code></pre>
"
"<pre><code>ALTER PROCEDURE [dbo].[sp_helptext3] 
    (@ProcName VARCHAR(256))
AS
    SET NOCOUNT ON
BEGIN
    DECLARE @path VARCHAR(MAX)
    DECLARE @filepath_loction VARCHAR(MAX)

    SET @filepath_loction = '\\SOLVERLAPT292\tempfolder\new.txt'

    EXEC @path = sp_helptext2 @ProcName 

    SET @filepath_loction = @path
    SET @sql = 'bcp ""' + @sql + '"" queryout ""' +  @filename +'"" -c -r""''\n"" 
-t""'',''"" -S '+@@servername+' -T'

    SELECT @filepath_loction
END
</code></pre>

<p>Output I got it because not convert into text file format I get this error:</p>

<blockquote>
  <p>Msg 214, Level 16, State 201, Procedure xp_cmdshell, Line 1<br>
  Procedure expects parameter 'command_string' of type 'varchar'.</p>
</blockquote>
","<p>What I believe you are missing is that the total script stops. Your <code>pause</code> command at the moment holds the script temporarily, but not the full script. </p>

<p>You should use a label to jump to instead. In the below script I added the label <code>:error_happened</code> to jump to in case of an error. In case there is no error, then the command <code>goto :EOF</code> will skip past the <code>:error_happened</code> label.</p>

<pre><code>bcp.exe ""select * from OrderXpress.dbo.Customers where CustId &lt; 1000"" queryout ""D:\Customer.dat"" -S localhost -U sa -P Sa12345 -E -n  
IF ERRORLEVEL 1 GOTO error_happened

bcp.exe OrderXpress.dbo.Customers out ""D:\Customer2.dat"" -S localhost -U sa -P Sa12345 -E -n 
IF ERRORLEVEL 1 GOTO error_happened

bcp.exe OrderXpress.dbo.Orders out ""D:\Orders.dat"" -S localhost -U sa -P Sa12345 -E -n 
IF ERRORLEVEL 1 GOTO error_happened

goto :EOF

:error_happened
echo,
echo An error has occurred. Script stopped
echo
pause
</code></pre>

<p>A remark must be made however on the errorlevel that comes out of BCP. In case a data row is not exported correctly, then I am not sure on what happens with the errorlevel. I know that on import a row error does not lead to problems. See also: <a href=""https://groups.google.com/forum/#!topic/microsoft.public.sqlserver.tools/qzpWuZzJnr4"" rel=""nofollow noreferrer"">https://groups.google.com/forum/#!topic/microsoft.public.sqlserver.tools/qzpWuZzJnr4</a></p>

<p>I have a workaround using the -e parameter to check for errors. The script would then be:</p>

<pre><code>bcp.exe ""select * from OrderXpress.dbo.Customers where CustId &lt; 1000"" queryout ""D:\Customer.dat"" -e errors.txt -S localhost -U sa -P Sa12345 -E -n  
IF ERRORLEVEL 1 GOTO error_happened
call :check_errorfile

bcp.exe OrderXpress.dbo.Customers out ""D:\Customer2.dat"" -e errors.txt -S localhost -U sa -P Sa12345 -E -n 
IF ERRORLEVEL 1 GOTO error_happened
call :check_errorfile

bcp.exe OrderXpress.dbo.Orders out ""D:\Orders.dat"" -e errors.txt -S localhost -U sa -P Sa12345 -E -n 
IF ERRORLEVEL 1 GOTO error_happened
call :check_errorfile

goto :EOF

:check_errorfile
if exist errors.txt (
    FOR %%A IN (errors.txt) DO (
        if %%~zA GTR 0 (
        goto error_happened
        ) else (
        del errors.txt
        )
    )
)
exit /b

:error_happened
echo,
echo An error has occurred. Script stopped
echo
pause
</code></pre>
"
"<p>I have installed in my PC Docker. I have installed SQlServer for linux and instance run correctly.</p>

<p>I need to connect to Docker instance with SSMS.
In the picture you can see my personal config.</p>

<p>For installation I have follow this link
<a href=""https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-docker"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-docker</a></p>

<p>Thanks in advance</p>

<p><a href=""https://i.stack.imgur.com/iSy4G.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iSy4G.jpg"" alt=""enter image description here""></a></p>

<p>When I lunch the server I have this message </p>

<pre><code>PS C:\Users\Daniele&gt; docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS
NAMES
PS C:\Users\Daniele&gt; docker run  microsoft/mssql-server-linux
This is an evaluation version.  There are [171] days left in the evaluation period.
2017-05-23 07:19:15.12 Server      Setup step is copying system data file 'C:\templatedata\master.mdf' to '/var/opt/mssq
l/data/master.mdf'.
2017-05-23 07:19:15.19 Server      Setup step is copying system data file 'C:\templatedata\mastlog.ldf' to '/var/opt/mss
ql/data/mastlog.ldf'.
2017-05-23 07:19:15.19 Server      Setup step is copying system data file 'C:\templatedata\model.mdf' to '/var/opt/mssql
/data/model.mdf'.
2017-05-23 07:19:15.21 Server      Setup step is copying system data file 'C:\templatedata\modellog.ldf' to '/var/opt/ms
sql/data/modellog.ldf'.
2017-05-23 07:19:15.22 Server      Setup step is copying system data file 'C:\templatedata\msdbdata.mdf' to '/var/opt/ms
sql/data/msdbdata.mdf'.
2017-05-23 07:19:15.24 Server      Setup step is copying system data file 'C:\templatedata\msdblog.ldf' to '/var/opt/mss
ql/data/msdblog.ldf'.
2017-05-23 07:19:15.34 Server      Microsoft SQL Server 2017 (CTP2.1) - 14.0.600.250 (X64)
        May 10 2017 12:21:23
        Copyright (C) 2017 Microsoft Corporation. All rights reserved.
        Developer Edition (64-bit) on Linux (Ubuntu 16.04.2 LTS)
2017-05-23 07:19:15.34 Server      UTC adjustment: 0:00
2017-05-23 07:19:15.35 Server      (c) Microsoft Corporation.
2017-05-23 07:19:15.35 Server      All rights reserved.
2017-05-23 07:19:15.35 Server      Server process ID is 4116.
2017-05-23 07:19:15.35 Server      Logging SQL Server messages in file '/var/opt/mssql/log/errorlog'.
2017-05-23 07:19:15.36 Server      Registry startup parameters:
         -d /var/opt/mssql/data/master.mdf
         -l /var/opt/mssql/data/mastlog.ldf
         -e /var/opt/mssql/log/errorlog
2017-05-23 07:19:15.37 Server      SQL Server detected 1 sockets with 2 cores per socket and 2 logical processors per so
cket, 2 total logical processors; using 2 logical processors based on SQL Server licensing. This is an informational mes
sage; no user action is required.
2017-05-23 07:19:15.38 Server      SQL Server is starting at normal priority base (=7). This is an informational message
 only. No user action is required.
2017-05-23 07:19:15.39 Server      Detected 3944 MB of RAM. This is an informational message; no user action is required
.
2017-05-23 07:19:15.39 Server      Using conventional memory in the memory manager.
2017-05-23 07:19:15.63 Server      Buffer pool extension is already disabled. No action is necessary.
2017-05-23 07:19:15.76 Server      InitializeExternalUserGroupSid failed. Implied authentication will be disabled.
2017-05-23 07:19:15.76 Server      Implied authentication manager initialization failed. Implied authentication will be
disabled.
2017-05-23 07:19:15.77 Server      Successfully initialized the TLS configuration. Allowed TLS protocol versions are ['1
.0 1.1 1.2']. Allowed TLS ciphers are ['ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES128-GCM
-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RS
A-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:AES256-GCM-SHA38
4:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA'].
2017-05-23 07:19:15.81 Server      The maximum number of dedicated administrator connections for this instance is '1'
2017-05-23 07:19:15.81 Server      Node configuration: node 0: CPU mask: 0x0000000000000003:0 Active CPU mask: 0x0000000
000000003:0. This message provides a description of the NUMA configuration for this computer. This is an informational m
essage only. No user action is required.
2017-05-23 07:19:15.82 Server      Using dynamic lock allocation.  Initial allocation of 2500 Lock blocks and 5000 Lock
Owner blocks per node.  This is an informational message only.  No user action is required.
2017-05-23 07:19:15.83 Server      In-Memory OLTP initialized on lowend machine.
2017-05-23 07:19:15.88 Server      Database Instant File Initialization: enabled. For security and performance considera
tions see the topic 'Database Instant File Initialization' in SQL Server Books Online. This is an informational message
only. No user action is required.
2017-05-23 07:19:15.89 Server      Query Store settings initialized with enabled = 1,
2017-05-23 07:19:15.90 spid7s      Starting up database 'master'.
2017-05-23 07:19:15.91 Server      Software Usage Metrics is disabled.
2017-05-23 07:19:16.12 spid7s      The tail of the log for database master is being rewritten to match the new sector si
ze of 4096 bytes.  3584 bytes at offset 418304 in file /var/opt/mssql/data/mastlog.ldf will be written.
2017-05-23 07:19:16.18 spid7s      Converting database 'master' from version 862 to the current version 868.
2017-05-23 07:19:16.19 spid7s      Database 'master' running the upgrade step from version 862 to version 863.
2017-05-23 07:19:16.21 spid7s      Database 'master' running the upgrade step from version 863 to version 864.
2017-05-23 07:19:16.23 spid7s      Database 'master' running the upgrade step from version 864 to version 865.
2017-05-23 07:19:16.23 spid7s      Database 'master' running the upgrade step from version 865 to version 866.
2017-05-23 07:19:16.24 spid7s      Database 'master' running the upgrade step from version 866 to version 867.
2017-05-23 07:19:16.25 spid7s      Database 'master' running the upgrade step from version 867 to version 868.
2017-05-23 07:19:16.42 spid7s      Buffer pool extension is already disabled. No action is necessary.
2017-05-23 07:19:16.42 spid7s      Resource governor reconfiguration succeeded.
2017-05-23 07:19:16.43 spid7s      SQL Server Audit is starting the audits. This is an informational message. No user ac
tion is required.
2017-05-23 07:19:16.43 spid7s      SQL Server Audit has started the audits. This is an informational message. No user ac
tion is required.
2017-05-23 07:19:16.49 spid7s      SQL Trace ID 1 was started by login ""sa"".
2017-05-23 07:19:16.50 spid7s      Server name is '29e275920103'. This is an informational message only. No user action
is required.
2017-05-23 07:19:16.50 spid7s      The NETBIOS name of the local node that is running the server is '29e275920103'. This
 is an informational message only. No user action is required.
2017-05-23 07:19:16.52 spid19s     Password policy update was successful.
2017-05-23 07:19:16.52 spid22s     Always On: The availability replica manager is starting. This is an informational mes
sage only. No user action is required.
2017-05-23 07:19:16.53 spid9s      Starting up database 'mssqlsystemresource'.
2017-05-23 07:19:16.53 spid7s      Starting up database 'msdb'.
2017-05-23 07:19:16.54 spid22s     Always On: The availability replica manager is waiting for the instance of SQL Server
 to allow client connections. This is an informational message only. No user action is required.
2017-05-23 07:19:16.55 spid9s      The resource database build version is 14.00.600. This is an informational message on
ly. No user action is required.
2017-05-23 07:19:16.57 spid9s      Starting up database 'model'.
2017-05-23 07:19:16.88 spid7s      The tail of the log for database msdb is being rewritten to match the new sector size
 of 4096 bytes.  512 bytes at offset 52736 in file /var/opt/mssql/data/MSDBLog.ldf will be written.
2017-05-23 07:19:16.94 spid7s      Converting database 'msdb' from version 862 to the current version 868.
2017-05-23 07:19:16.95 spid7s      Database 'msdb' running the upgrade step from version 862 to version 863.
2017-05-23 07:19:16.98 spid19s     A self-generated certificate was successfully loaded for encryption.
2017-05-23 07:19:17.00 spid7s      Database 'msdb' running the upgrade step from version 863 to version 864.
2017-05-23 07:19:17.01 spid19s     Server is listening on [ 0.0.0.0 &lt;ipv4&gt; 1433].
2017-05-23 07:19:17.02 Server      Server is listening on [ 127.0.0.1 &lt;ipv4&gt; 1434].
2017-05-23 07:19:17.02 Server      Dedicated admin connection support was established for listening locally on port 1434
.
2017-05-23 07:19:17.02 spid9s      The tail of the log for database model is being rewritten to match the new sector siz
e of 4096 bytes.  2048 bytes at offset 75776 in file /var/opt/mssql/data/modellog.ldf will be written.
2017-05-23 07:19:17.02 spid19s     SQL Server is now ready for client connections. This is an informational message; no
user action is required.
2017-05-23 07:19:17.05 spid7s      Database 'msdb' running the upgrade step from version 864 to version 865.
2017-05-23 07:19:17.05 spid7s      Database 'msdb' running the upgrade step from version 865 to version 866.
2017-05-23 07:19:17.06 spid9s      Converting database 'model' from version 862 to the current version 868.
2017-05-23 07:19:17.06 spid9s      Database 'model' running the upgrade step from version 862 to version 863.
2017-05-23 07:19:17.07 spid7s      Database 'msdb' running the upgrade step from version 866 to version 867.
2017-05-23 07:19:17.08 spid7s      Database 'msdb' running the upgrade step from version 867 to version 868.
2017-05-23 07:19:17.09 spid9s      Database 'model' running the upgrade step from version 863 to version 864.
2017-05-23 07:19:17.10 spid9s      Database 'model' running the upgrade step from version 864 to version 865.
2017-05-23 07:19:17.10 spid9s      Database 'model' running the upgrade step from version 865 to version 866.
2017-05-23 07:19:17.10 spid9s      Database 'model' running the upgrade step from version 866 to version 867.
2017-05-23 07:19:17.11 spid9s      Database 'model' running the upgrade step from version 867 to version 868.
2017-05-23 07:19:17.22 spid9s      Polybase feature disabled.
2017-05-23 07:19:17.22 spid9s      Clearing tempdb database.
2017-05-23 07:19:17.71 spid9s      Starting up database 'tempdb'.
2017-05-23 07:19:17.97 spid9s      The tempdb database has 1 data file(s).
2017-05-23 07:19:17.99 spid22s     The Service Broker endpoint is in disabled or stopped state.
2017-05-23 07:19:17.99 spid22s     The Database Mirroring endpoint is in disabled or stopped state.
2017-05-23 07:19:18.00 spid22s     Service Broker manager has started.
2017-05-23 07:19:18.14 spid7s      Recovery is complete. This is an informational message only. No user action is requir
ed.
</code></pre>
","<p>I had the same issue. Luckily in my case the linked instance had been set up to listen to another static tcp port. So I could use that as a workaround.</p>

<p>If you have to option to change the server you are linking to, you can set the tcp port in <em>SQL Server Configuration manager - Network configuration - Protocols for INST2 - TCP/IP - properties - IP Addresses - TCP port.</em></p>

<p>After that I could add the linked server like this. Port number is 1435.</p>

<pre><code>EXEC master.dbo.sp_addlinkedserver @server = N'Server20_inst2', @srvproduct=N'', @provider=N'SQLNCLI', @datasrc=N'192.168.1.112,1435', @catalog=N'Test'

EXEC master.dbo.sp_addlinkedsrvlogin 
 @rmtsrvname=N'Server20_inst2', @useself=N'False', @locallogin=NULL, @rmtuser=N'sa', @rmtpassword='##########'
</code></pre>
"
"<p>I am using SQL.js for SQLite in my chrome app , I am loading external db file to perform query , now i want to save my changes to local storage to make it persistent , it is already define here-</p>

<p><a href=""https://github.com/kripken/sql.js/wiki/Persisting-a-Modified-Database"" rel=""nofollow"">https://github.com/kripken/sql.js/wiki/Persisting-a-Modified-Database</a></p>

<p>i am using the same way as defined in the article-</p>

<pre><code>function toBinString (arr) {
    var uarr = new Uint8Array(arr);
    var strings = [], chunksize = 0xffff;
    // There is a maximum stack size. We cannot call String.fromCharCode with as many arguments as we want
    for (var i=0; i*chunksize &lt; uarr.length; i++){
        strings.push(String.fromCharCode.apply(null, uarr.subarray(i*chunksize, (i+1)*chunksize)));
    }
    return strings.join('');
}

function toBinArray (str) {
    var l = str.length,
            arr = new Uint8Array(l);
    for (var i=0; i&lt;l; i++) arr[i] = str.charCodeAt(i);
    return arr;
}
</code></pre>

<p>save data to storage -</p>

<pre><code>var data =toBinString(db.export());
chrome.storage.local.set({""localDB"":data});
</code></pre>

<p>and to get data from storage-</p>

<pre><code>chrome.storage.local.get('localDB', function(res) {
    var data = toBinArray(res.localDB);
      //sample example usage
      db = new SQL.Database(data);
      var result = db.exec(""SELECT * FROM user"");
});
</code></pre>

<p>Now when i make a query , i am getting this error -</p>

<blockquote>
  <p>Error: file is encrypted or is not a database</p>
</blockquote>

<p>is there any differnces for storing values in chrome.storage and localStorage ? because its working fine using localStorage, find the working example here-</p>

<p><a href=""http://kripken.github.io/sql.js/examples/persistent.html"" rel=""nofollow"">http://kripken.github.io/sql.js/examples/persistent.html</a></p>

<p>as document sugggested here -
<a href=""https://developer.chrome.com/apps/storage"" rel=""nofollow"">https://developer.chrome.com/apps/storage</a> </p>

<p>we don't need to use stringify and parse in chrome.storage API unlike localStorage, we can directly saves object and array.</p>

<p>when i try to save result return from db.export without any conversion, i am getting this error-</p>

<blockquote>
  <p>Cannot serialize value to JSON</p>
</blockquote>

<p>So please help me guys what will be the approach to save db export in chrome's storage, is there anything i am doing wrong?</p>
","<p><code>sql.Database()</code> needs to be called as a constructor (<em>e.g.</em> <code>db = new sql.Database();</code>). Note the <code>new</code>, as in the <a href=""https://github.com/kripken/sql.js/#usage"" rel=""nofollow noreferrer"">Usage example</a>.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>var sql = window.SQL;
var db = new sql.Database();

sqlstr = ""CREATE TABLE hello (a int, b char);"";
sqlstr += ""INSERT INTO hello VALUES (0, 'hello');""
sqlstr += ""INSERT INTO hello VALUES (1, 'world');""
db.run(sqlstr);

var res = db.exec(""SELECT * FROM hello"");

console.log(res);</code></pre>
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;script src=""https://rawgit.com/kripken/sql.js/master/js/sql.js"" type=""text/javascript""&gt;&lt;/script&gt;</code></pre>
</div>
</div>
</p>
"
"<p>To select N records per category one can do:</p>

<pre><code>SELECT category, category_id, value FROM
(
    SELECT category, value, row_number() OVER (PARTITION by category) as category_id
    FROM myTable
)
WHERE  category_id &lt; N;
</code></pre>

<p>The inner SELECT will first partition the records per category and assign each record per category an id called category_id.
The outer query will then use the category_id to limit the number of records it queries per category.</p>

<p>This is <strong>extremely inefficient on BIG tables</strong> as it will be going through assigning ids to all the records even though we are only interested in N records per category. </p>

<p>The following does not work on the sql engine that I am working with - not sure if it works on any engine at all.</p>

<pre><code>SELECT category, value, row_number() OVER (PARTITION by category) as category_id
FROM myTable
WHERE category_id &lt; N
</code></pre>

<p>Does anyone know of any other ways of achieving this with a better time complexity?</p>

<p>More thoughts:</p>

<p>Time profiling the following algorithm against above query might provide more insights as to how the query runs behind the scene:</p>

<pre><code>   1. SELECT DISTINCT(category) FROM myTable
   2. FOREACH category SELECT N rows
</code></pre>

<hr>

<p>more info:
my data is physically partitioned by <code>category</code>, being able to explicitly leverage that would be useful</p>
","<p>As @Lamak mentioned in a comment, you cannot avoid sorting all rows in the table, but not for the reason stated. A sort is required to determine distinct categories by which the result set should be partitioned, and, in the absence of explicit ordering within each partition, row numbers are easily determined as a side effect of the category sort.</p>

<p>How the query runs ""behind the scenes"", or, if using the correct term, its execution plan is determined by the presence (or absence) of an index that might help avoid that category sort. If you had a covering index on <code>(category, value)</code>, and whatever other columns you need in the result, your query would run much more efficiently.</p>

<p>In that latter case the simplified algorithm might look more like this:</p>

<ol>
<li>Read the pre-sorted records containing all necessary columns, including row numbers, from the index.</li>
<li>Discard records with row number greater than <code>n</code>.</li>
</ol>

<p>Your ""ideal"" query</p>

<blockquote>
<pre><code>SELECT category, value, row_number() OVER (PARTITION by category) as
category_id FROM myTable WHERE category_id &lt; N
</code></pre>
</blockquote>

<p>probably wouldn't run in any SQL database, because the <code>SELECT</code> list is processed <em>after</em> the <code>WHERE</code> clause predicates, so <code>category_id</code> is unknown when the predicates are evaluated.</p>
"
"<p>I have the following batch code which executes all SQL files of a folder.</p>

<pre><code>for /r %%G in (*.sql) do sqlcmd /S localhost /d superDB /U sa /P topsecret -i""%%G""
pause
</code></pre>

<p>But now I want to execute only the files which start with a two digit number in name like:</p>

<pre><code>01_file.sql
02_file.sql
05_file.sql
43_file.sql
</code></pre>

<p>But not files like:</p>

<pre><code>optional_01_file.sql 
XYZ04_file.sql
</code></pre>

<p>Can someone tell me how to change the code to filter only files with a file name which starts with 2 digits?</p>
","<p>Findstr's rudimentary RegEx capabilities are sufficient to match the files.<br>
<s>Un</s><strong>Tested:</strong></p>

<pre><code>:: Q:\Test\2018\07\21\SO_51456661.cmd
@echo off
for /r %%G in (*.sql) do (
    Echo=%%~nG|Findstr ""^[0-9][0-9]_"" 2&gt;&amp;1&gt;Nul &amp;&amp; (
        echo sqlcmd /S localhost /d superDB /U sa /P topsecret -i""%%G""
    )
)
</code></pre>

<p>In a folder with these files</p>

<pre><code>&gt; dir /B *.sql
01_file.sql
0815_file.sql
55_file.sql
abc0855_file.sql
</code></pre>

<p>I get these commands (only echoed for demonstration)</p>

<pre><code>&gt; SO_51456661.cmd
sqlcmd /S localhost /d superDB /U sa /P topsecret -i""Q:\Test\2018\07\21\01_file.sql""
sqlcmd /S localhost /d superDB /U sa /P topsecret -i""Q:\Test\2018\07\21\55_file.sql""
</code></pre>
"
"<p>When creating a new postgresql instance on the azure portal or even the CLI, the username is automatically made to be like user@databasename so i can't use it on a database string, funny enough Azure has an entire section with connection strings, for example the <strong>node.js</strong> example is </p>

<pre><code>postgres://{your_username}:{your_password}@{host_name}:5432/{your_database}?ssl=true
</code></pre>

<p>So if we substitute the values we would end up with something like this</p>

<pre><code>postgres://user@databasename:password@host.azure.com:5432/database_name?ssl=true
</code></pre>

<p>Can you see the problem? two <code>@</code>, this is completely wrong, <code>node.js</code> can't parse this, I have the same problem on <code>ruby</code>, this is not a valid url.</p>

<p>How can I change the username to remove the @part ?</p>
","<p>In this case the OP solved the problem by upgrading Postgre tier and implementing vnet rules to allow traffic.</p>
"
"<p>I use <code>mysql-client 5.55.9999</code> and I would like to know how to prevent the mysql-client to save passwords to <code>~/.mysql_history</code>? Let's say, I create a new user:</p>

<pre><code>CREATE USER ""username""@""host"" IDENTIFIED BY ""thePassword"";
</code></pre>

<p>This statement is being put exactly as it is into <code>~/.mysql_history</code>. This is what I don't want. I remember in earlier versions of mysql-client, rows including passwords have not been put into the history file, but I don't remember what the exact option in the settings was.</p>
","<p>The <code>$</code> are part of shell variables, which are unintentionally get replaced. You have to escape the <code>$</code> character to keep it in the string as a literal <code>$</code>.</p>

<pre><code>$ echo ""$1$Hat7oFty$mA.L2vsQdD3MxvxAuDFKp0""
.L2vsQdD3MxvxAuDFKp0
$ echo ""\$1\$Hat7oFty\$mA.L2vsQdD3MxvxAuDFKp0""
$1$Hat7oFty$mA.L2vsQdD3MxvxAuDFKp0
</code></pre>
"
"<p>INT(10) unsigned in mysql has limit up to 4b+ and when use with getLong of row, it throws the following error:</p>

<pre><code>java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Long
    at com.github.jasync.sql.db.RowData$DefaultImpls.getLong(RowData.kt:48) ~[jasync-common-0.9.23.jar:?]
    at com.github.jasync.sql.db.general.ArrayRowData.getLong(ArrayRowData.kt:5) ~[jasync-common-0.9.23.jar:?]
    at com.richdevt.hthookserver.repository.ProjectRepository.findProjectEnvironment(ProjectRepository.kt:29) ~[classes/:?]
    at com.richdevt.hthookserver.repository.ProjectRepository$findProjectEnvironment$3.invokeSuspend(ProjectRepository.kt) ~[classes/:?]
    at kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith(ContinuationImpl.kt:32) [kotlin-stdlib-1.3.21.jar:1.3.21-release-158 (1.3.21)]
    at kotlinx.coroutines.DispatchedTask.run(Dispatched.kt:233) [kotlinx-coroutines-core-1.1.1.jar:?]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute$$$capture(AbstractEventExecutor.java:163) [netty-common-4.1.33.Final.jar:4.1.33.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java) [netty-common-4.1.33.Final.jar:4.1.33.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) [netty-common-4.1.33.Final.jar:4.1.33.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:495) [netty-transport-4.1.33.Final.jar:4.1.33.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:905) [netty-common-4.1.33.Final.jar:4.1.33.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-common-4.1.33.Final.jar:4.1.33.Final]
    at java.lang.Thread.run(Thread.java:748) [?:1.8.0_152
</code></pre>

<p>Why is the conversion not possible?</p>
","<p>The type <code>INT(10) UNSIGNED</code> is in the size of int (4 bytes) and unsigned: <a href=""https://stackoverflow.com/questions/10879345/what-is-the-maximum-size-of-int10-in-mysql"">What is the maximum size of int(10) in Mysql</a></p>

<p>However, Java has no such type (ie the maximum value is bigger than the maximum Java int). On the other hand, the type returned from MySQL is INT as defined here: <a href=""https://dev.mysql.com/doc/refman/8.0/en/integer-types.html"" rel=""nofollow noreferrer"">https://dev.mysql.com/doc/refman/8.0/en/integer-types.html</a> without additional info about unsigned. In such case <a href=""https://github.com/jasync-sql/jasync-sql"" rel=""nofollow noreferrer"">jasync-sql</a> converts it to a regular Int type. More details are here: <a href=""https://github.com/jasync-sql/jasync-sql/issues/102"" rel=""nofollow noreferrer"">https://github.com/jasync-sql/jasync-sql/issues/102</a></p>

<p>As a workaround for such case it is possible to do a widening conversion like suggested here: <a href=""https://stackoverflow.com/questions/9578639/best-way-to-convert-a-signed-integer-to-an-unsigned-long"">Best way to convert a signed integer to an unsigned long?</a></p>
"
"<p>mysql supports passwordless login using stored local authentication credentials in a file named .mylogin.cnf (see <a href=""http://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html"" rel=""nofollow"">here</a> for more details). </p>

<p>for example: </p>

<pre><code>mysql --login-path=local
</code></pre>

<p>My questions is: how to do this in perl using DBD::mysql?</p>
","<p>So I finally found it. Of course, it was locating the libraries and include files properly.</p>

<p>So I went into the cpan directory on my system</p>

<pre><code>/Users/robert/.cpan/build/DBD-mysql-4.048-0
</code></pre>

<p>After checking just where the brew installations were for both openssl and mysql, I woke up.</p>

<pre><code>sudo perl Makefile.PL --libs=""-L/usr/local/opt/openssl/lib -lssl -lcrypto -L/usr/local/lib -L/usr/local/Cellar/mysql/8.0.12/lib -lmysqlclient""
</code></pre>

<p>Now. that worked because after looking in the mysaql lib directory, I see that the library is 'mysqlclient' and not mysql. As you see above.</p>

<p>Now that worked doing the Makefile.PL thing as above.</p>

<p>Then 'make' worked. As above. Not showing any errors... as above</p>

<p>Then <strong>sudo make test worked</strong> - this time.</p>

<pre><code>All tests successful.
Files=73, Tests=9, 11 wallclock secs ( 0.24 usr  0.15 sys +  8.25 cusr  1.23 csys =  9.87 CPU)
Result: PASS
</code></pre>

<p>Finally - make install. Worked</p>

<pre><code>zeus:DBD-mysql-4.048-0 robert$ sudo make install
""/usr/local/Cellar/perl/5.28.0/bin/perl"" -MExtUtils::Command::MM -e 'cp_nonempty' -- mysql.bs blib/arch/auto/DBD/mysql/mysql.bs 644
Manifying 3 pod documents
Files found in blib/arch: installing files in blib/lib into architecture dependent library tree
Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/auto/DBD/mysql/mysql.bundle
Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/Bundle/DBD/mysql.pm
Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql.pm
Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql/INSTALL.pod
Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql/GetInfo.pm
Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/Bundle::DBD::mysql.3
Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/DBD::mysql::INSTALL.3
Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/DBD::mysql.3
Appending installation info to /usr/local/Cellar/perl/5.28.0/lib/perl5/5.28.0/darwin-thread-multi-2level/perllocal.pod
zeus:DBD-mysql-4.048-0 robert$ pwd
/Users/robert/.cpan/build/DBD-mysql-4.048-0
</code></pre>

<p>Well finally. Now why did I want this? I can't remember.</p>
"
"<p>Consider this method in an angular service:</p>

<pre><code>    select : function(table) {
        window.sqlitePlugin.openDatabase({
            name: 'smartLab.db',
            location: 'default'
        }, function success(db) {
            var defer = window.Q.defer();

            console.dir(defer);

            db.executeSql('SELECT * FROM ' + table, [], function success(rows) {
                var resp = [];

                for(var i = 0; i &lt; rows.rows.length; i++) {
                    resp.push(rows.rows.item(i));
                }

                defer.resolve(resp);

            }, defer.reject)
        }, function error(err) {

        })
    }
</code></pre>

<p>I am calling it from another service in the following way:</p>

<pre><code>DatabaseService.select(TBL_NAME).then(function(rows) {
  // logic goes here
});
</code></pre>

<p>The error I get is: </p>

<pre><code>Uncaught TypeError: Cannot read property 'then' of undefined
</code></pre>

<p>I have never used Q before but am familiar with promises, anything jump out as being inherently wrong?  Clearly something is...</p>
","<p>You are never returning the promise from the defered object (click <a href=""https://github.com/kriskowal/q/blob/master/README.md#using-deferreds"" rel=""nofollow noreferrer"">here</a> for more)</p>

<pre><code>select : function(table) {
    var defer = window.Q.defer();

    window.sqlitePlugin.openDatabase({
        name: 'smartLab.db',
        location: 'default'
    }, function success(db) {


        console.dir(defer);

        db.executeSql('SELECT * FROM ' + table, [], function success(rows) {
            var resp = [];

            for(var i = 0; i &lt; rows.rows.length; i++) {
                resp.push(rows.rows.item(i));
            }

            defer.resolve(resp);

        }, defer.reject)
    }, function error(err) {

    });

    return defer.promise;
}
</code></pre>
"
"<p>I am trying to run a large script that creates a table, and then inserts almost 15,000 rows into it.  The table gets created just fine, and then at the 833 INSERT, I get an error:</p>

<pre><code>Error: Query was empty (1065)
</code></pre>

<p>Here is my 833rd INSERT statement (the one that is failing):</p>

<pre><code>INSERT INTO CLASSCODE (CLASS_CODE, CLASS_CODE_NAME, RATE_GROUP, PROGRAM_NM, ST_CODE, EFF_DT, EXP_DT) VALUES (10255, ""Funeral Directors - incl PL other than Crematory  - 10255"", 3, ""Service"", ""AZ"", 19980801, NULL);
</code></pre>

<p>I can't see any syntax errors or differences between this line, and one that works.  FOr reference, here is an example of an INSERT statement that works just fine:</p>

<pre><code>INSERT INTO CLASSCODE (CLASS_CODE, CLASS_CODE_NAME, RATE_GROUP, PROGRAM_NM, ST_CODE, EFF_DT, EXP_DT) VALUES (10425, ""Frame Shop - Picture/Posters                      - 10425"", 2, ""Retail"", ""AZ"", 19980801, NULL);
</code></pre>

<p>The part that puzzles me is that the error sounds like something that would happen if I was populating the new row using data from another SELECT statement, which was coming up empty.  That is not the case, though, as my INSERT statements are all using static data.</p>

<p>My table definition looks like this:</p>

<pre><code>CREATE TABLE CLASSCODE (
      CLASS_CODE INTEGER NOT NULL, 
      CLASS_CODE_NAME VARCHAR(60) NOT NULL, 
      RATE_GROUP SMALLINT NOT NULL, 
      PROGRAM_NM VARCHAR(20) NOT NULL, 
      ST_CODE CHAR(2), 
      EFF_DT DATE, 
      EXP_DT DATE) 
</code></pre>

<p>I am running this script in the GUI MySQL Query Browser.</p>

<p>Could it be something to do with the number of rows I'm trying to insert?  Do I need to periodically commit?  Is there something simple that I am just overlooking?</p>

<p>Thanks!</p>
","<p>The most common scenario to run into these in a script file is when you have a double semi-colon somewhere:</p>

<pre><code>INSERT INTO CLASSCODE 
(CLASS_CODE, CLASS_CODE_NAME, RATE_GROUP, PROGRAM_NM, ST_CODE, EFF_DT, EXP_DT)
VALUES
(10255, ""Funeral Directors - incl PL other than Crematory  - 10255"", 3, ""Service"", ""AZ"", 19980801, NULL)
;;
</code></pre>

<p>I'd do a quick search through the script and see if there's a <code>;;</code> in there around line 833.</p>
"
"<p>Hi this is a pretty basic question, but I am not able to find a definitive answer to it. </p>

<p>What does IF evaluate to for NULL in SQL ? What will happen in following code if fieldXYZ were NULL</p>

<pre><code>IF fieldXYZ=SomeValue
//do something
ELSE 
//do something else
</code></pre>

<p>Update:</p>

<p>I have found one result which suggests that NULL can be considered as falsy</p>

<p><a href=""https://technet.microsoft.com/en-us/library/ms182587(v=sql.110).aspx"" rel=""nofollow"">https://technet.microsoft.com/en-us/library/ms182587(v=sql.110).aspx</a></p>
","<p>First of all, I think your <a href=""http://www.techonthenet.com/oracle/loops/if_then.php"" rel=""nofollow""><code>if..else</code></a> statement is incomplete as you are not defining any expression for your column <code>fieldXYZ</code>. Generally, when you write an <code>if..else</code> statement, you would want to write an expression that compares your column value with something. You can explicitly check for null values if you want to handle them. Here is a gist of that syntax:</p>

<pre><code>--This handles null check explicitly
IF fieldXYZ IS NOT NULL AND fieldXYZ = 'somevalue'
//do something
ELSE 
//do something else
</code></pre>

<p>Also, if the <code>fieldXYZ</code> is passed a <code>NULL</code> value in the above statement, then the first condition is not met, so the <code>else</code> condition applies immediately in that scenario.Hope this helps!</p>
"
"<p>I want to connect a MySQL database on my machine with a Vapor 3 app.<br>
My current <code>configure.swift</code> file looks as follows:</p>

<pre><code>try services.register(FluentMySQLProvider())

...

let mysqlConfig = MySQLDatabaseConfig(
    username: ""dev"",
    password: """",
    database: ""test""
)
let mysql = MySQLDatabase(config: mysqlConfig)

var databases = DatabasesConfig()
databases.add(database: mysql, as: .mysql)
services.register(databases)
</code></pre>

<p>This works just fine. However, since I need to add my Model to the migration configuration, I also need to add:</p>

<pre><code>var migrations = MigrationConfig()
migrations.add(model: Posts.self, database: .mysql)
services.register(migrations)
</code></pre>

<p>When running the app this time, I'm seeing an error saying:</p>

<blockquote>
  <p>Full authentication not supported over insecure connections.</p>
</blockquote>

<p>After some research, it seems that this error can be overcome by changing the password logic from <code>caching_sha2_password</code> to <code>mysql_native_password</code>.<br>
However, that leaves me with the error saying:</p>

<blockquote>
  <p>Unrecognized basic packet.</p>
</blockquote>

<p>How do I fix this?</p>
","<p>From MySQL 8 if you want to use it on <code>localhost</code> (unsecured connection) then you need to disable the MySQL transport layer security. Use <code>unverifiedTLS</code> for <code>transport</code> in <code>MySQLDatabaseConfig</code> initializer.</p>

<p>Your <code>MySQLDatabaseConfig</code> initializer should look something like this:</p>

<pre><code>let config = MySQLDatabaseConfig(
    hostname: ""127.0.0.1"",
    port: 3306,
    username: ""dev"",
    password: """",
    database: ""test"",
    transport: MySQLTransportConfig.unverifiedTLS
)
</code></pre>

<p>It should work fine with this configuration.</p>
"
"<p>I created alerts to notify us when our SQL Database DTU usage peaks, but the alerts are still in a ""Not activated"" status:</p>

<p><img src=""https://i.stack.imgur.com/2uEd8.png"" alt=""Azure Alert Notifications""></p>

<p>The alert details are also showing a ""Not activated"" state:</p>

<p><img src=""https://i.stack.imgur.com/2AQ5S.png"" alt=""Azure Alert Details""></p>

<p>But the alerts are set to ""Enabled"":</p>

<p><img src=""https://i.stack.imgur.com/DyXTb.png"" alt=""Azure Alert Enabled""></p>

<p><strong>How do I go about activating them?</strong></p>
","<p>To find a list of all the applicable/available metrics that can be set on a particular resource, there is a powerShell Command. 
You can use this powershell command to get a list of all such metrics with their logical names.</p>

<p><strong>Get-AzureRmMetricDefinition</strong></p>

<p>For example, if you want to lookup for a list of metrics for alerts for your elastic pool, you can simple use this command,</p>

<p><strong>Get-AzureRmMetricDefinition -ResourceId ""ElasticPoolResourceId""</strong></p>

<p>You can provide the resourceID of your elastic Pool as a paramter here. And it will give you a list of all the applicable metrics for setting up alerts.</p>

<p>Hope this helps!</p>
"
"<p>There's a command to optimize and repair all databases</p>

<pre><code>mysqlcheck --user=root --password=PASSWORD_FOR_ROOT --auto-repair --optimize --all-databases
</code></pre>

<p>How can I skip the database from this process? --skip-database=DATABASE_NAME seems don't work</p>
","<p>It's the <code>--optimize</code> which uses all that space. It basically rebuilds all your tables to regain unused space (data that was deleted), if you've configured your server with <a href=""https://dev.mysql.com/doc/refman/8.0/en/innodb-multiple-tablespaces.html"" rel=""nofollow noreferrer"">innodb_file_per_table</a></p>

<p>This is rarely necessary, you can skip that.</p>

<p>That said, you really do this on <em>all your databases</em>? This should also not be necessary. I only have one database where I do a weekly check, and the purpose of this host is solely to verify that my backups work. Once a week a backup is restored on this host, then the mysqlcheck verifies that all tables work. And that's it. My boss would fire me, if I would do this on servers in production :)</p>
"
"<p>I'm trying to implement an incredibly basic use of SQLite. I have a <code>Button</code> and an <code>EditText</code>. I want to store the contents of the <code>EditText</code> <code>OnClick</code>. </p>

<p>I'm following this: <a href=""https://developer.xamarin.com/guides/android/application_fundamentals/data/part_3_using_sqlite_orm/"" rel=""nofollow noreferrer"">https://developer.xamarin.com/guides/android/application_fundamentals/data/part_3_using_sqlite_orm/</a></p>

<p>and <a href=""https://developer.xamarin.com/guides/xamarin-forms/application-fundamentals/databases/"" rel=""nofollow noreferrer"">https://developer.xamarin.com/guides/xamarin-forms/application-fundamentals/databases/</a></p>

<p>I cannot get passed the following starting code without getting the subsequent errors: <code>var db = new SQLiteConnection (dbPath);</code></p>

<p>Error: </p>

<blockquote>
  <p>The type initializer for 'SQLite.SQLiteConnection' threw an exception.</p>
</blockquote>

<p>Inner Exception:</p>

<blockquote>
  <p>System.Exception: This is the 'bait'.  You probably need to add one of
  the SQLitePCLRaw.bundle_* nuget packages to your platform project.<br>
  at SQLitePCL.Batteries_V2.Init () [0x00000] in
  &lt;9baed10c674b49e0b16322f238b8ecc1>:0    at
  SQLite.SQLiteConnection..cctor () [0x00000] in
  /Users/vagrant/git/src/SQLite.cs:169 }</p>
</blockquote>

<p>I've installed the NuGet package on both PCL and Android projects. I see the following packages installed:</p>

<pre><code>SQLitePCLRaw.provider.e_sqlite3.android
SQLitePCLRaw.lib.e_sqlite3.android
</code></pre>

<p>I've tried installing:</p>

<pre><code>SQLitePCLRaw.bundle_e_sqlite3
</code></pre>

<p>As mentioned, the code is the most basic implementation possible:</p>

<pre><code>try
{
    string dbPath = Path.Combine(System.Environment.GetFolderPath(System.Environment.SpecialFolder.Personal), ""TestDB-DEV.db3"");

    var db = new SQLiteConnection(dbPath);
    db.CreateTable&lt;PersonName&gt;();
}
</code></pre>

<p>I've spent a couple days on this and tried numerous resources like: <a href=""https://stackoverflow.com/questions/41344485/xamarin-sqlite-this-is-the-bait"">https://forums.xamarin.com/discussion/87289/sqlite-net-pcl-bait-issue</a> but ultimately no success.</p>

<p>Unfortunately, nonsense like ""it just works"", ""not sure what I did"", ""clean/rebuild"" are the only answers I've seen, e.g. previous link, other SO posts like <a href=""https://stackoverflow.com/questions/41344485/xamarin-sqlite-this-is-the-bait"">Xamarin SQLite &quot;This is the &#39;bait&#39;&quot;</a></p>

<p>Here is my <code>package.config</code> for the Android project:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;packages&gt;
  &lt;package id=""sqlite-net-pcl"" version=""1.4.118"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""SQLitePCLRaw.bundle_green"" version=""1.1.5"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""SQLitePCLRaw.core"" version=""1.1.5"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""SQLitePCLRaw.lib.e_sqlite3.android"" version=""1.1.5"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""SQLitePCLRaw.provider.e_sqlite3.android"" version=""1.1.5"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.Animated.Vector.Drawable"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.Design"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.v4"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.v7.AppCompat"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.v7.CardView"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.v7.MediaRouter"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.v7.RecyclerView"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Android.Support.Vector.Drawable"" version=""23.3.0"" targetFramework=""monoandroid60"" /&gt;
  &lt;package id=""Xamarin.Forms"" version=""2.4.0.282"" targetFramework=""monoandroid60"" /&gt;
&lt;/packages&gt;
</code></pre>

<p>Here is the package.config for the PCL project:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;packages&gt;
  &lt;package id=""sqlite-net-pcl"" version=""1.4.118"" targetFramework=""portable45-net45+win8+wpa81"" /&gt;
  &lt;package id=""SQLitePCLRaw.bundle_green"" version=""1.1.5"" targetFramework=""portable45-net45+win8+wpa81"" /&gt;
  &lt;package id=""SQLitePCLRaw.core"" version=""1.1.5"" targetFramework=""portable45-net45+win8+wpa81"" /&gt;
  &lt;package id=""Xamarin.Forms"" version=""2.3.4.247"" targetFramework=""portable45-net45+win8+wpa81"" /&gt;
&lt;/packages&gt;
</code></pre>
","<p>I hate to put myself in the ""I don't know how I fixed it"" boat, but that's what happened. I started clean and copy+pasted the code and repulled Nuget packages and everything just worked. Maybe I overlooked something initially, maybe had a version mismatch, I cannot say. However, I tried adding the dependencies mentioned by Trevor and the problem still existed, so I don't think I was missing anything. </p>
"
"<p>I'm working on a <code>Django Project</code> on my localhost and I would like to use a distant <code>MySQL Database</code>.</p>

<p>My localhost IP is : <strong>172.30.10.XX</strong></p>

<p>My MySQL distant server is : <strong>172.30.10.XX</strong></p>

<p>In my <strong>Django settings.py</strong> file, I wrote :</p>

<pre><code>DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.mysql',
        'NAME': 'DatasystemsEC',
        'USER': 'root',
        'PASSWORD': '*****',
        'HOST': '172.30.10.XX',
        'PORT': '3306',
        'OPTIONS': {
            'init_command': 'SET innodb_strict_mode=1',
        },
    }
}
</code></pre>

<p>My Database name is : DatasystemsEC</p>

<p>But, when I run : <code>python manage.py migrate</code>, I get this error :</p>

<pre><code>Traceback (most recent call last):
  File ""manage.py"", line 22, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 367, in execute_from_command_line
    utility.execute()
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/__init__.py"", line 359, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/base.py"", line 294, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/base.py"", line 342, in execute
    self.check()
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/base.py"", line 374, in check
    include_deployment_checks=include_deployment_checks,
  File ""/usr/local/lib/python2.7/site-packages/django/core/management/commands/migrate.py"", line 61, in _run_checks
    issues = run_checks(tags=[Tags.database])
  File ""/usr/local/lib/python2.7/site-packages/django/core/checks/registry.py"", line 81, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/usr/local/lib/python2.7/site-packages/django/core/checks/database.py"", line 10, in check_database_backends
    issues.extend(conn.validation.check(**kwargs))
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/mysql/validation.py"", line 9, in check
    issues.extend(self._check_sql_mode(**kwargs))
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/mysql/validation.py"", line 13, in _check_sql_mode
    with self.connection.cursor() as cursor:
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 231, in cursor
    cursor = self.make_debug_cursor(self._cursor())
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 204, in _cursor
    self.ensure_connection()
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 199, in ensure_connection
    self.connect()
  File ""/usr/local/lib/python2.7/site-packages/django/db/utils.py"", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 199, in ensure_connection
    self.connect()
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py"", line 171, in connect
    self.connection = self.get_new_connection(conn_params)
  File ""/usr/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py"", line 263, in get_new_connection
    conn = Database.connect(**conn_params)
  File ""/Library/Python/2.7/site-packages/MySQLdb/__init__.py"", line 81, in Connect
    return Connection(*args, **kwargs)
  File ""/Library/Python/2.7/site-packages/MySQLdb/connections.py"", line 193, in __init__
    super(Connection, self).__init__(*args, **kwargs2)
django.db.utils.OperationalError: (2003, ""Can't connect to MySQL server on '172.30.10.XX' (61)"")
</code></pre>

<p>On MySQL with phpmyadmin, I have :</p>

<p><a href=""https://i.stack.imgur.com/9SNMD.png"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p>I assume I need to configure a new user in order to register my MacOSX localhost ?</p>

<p>So I created a new user named <code>osx</code> with all granted privileges. But it still doesn't work.</p>

<p>Thank you if you could help me</p>
","<p>You can do</p>

<pre><code>class model_name(models.Model):
   id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=True)
   user = models.ForeignKey(User)
   foo = models.CharField(max_length=51)

   def save(self, *args, **kwargs):
       if self.__class__.objects.filter(user=self.user).count()&gt;=10:
           return None
       return super(model_name, self).save(*args, **kwargs)
       #return super().save(*args, **kwargs) python3.x
</code></pre>

<p>You can do the same thing if you are using <code>forms</code> by updating <code>Form.clean</code> method</p>

<pre><code>def clean(self):
    super(MyForm, self).clean()
    if model_name.objects.filter(user=self.cleaned_data['user']).count()&gt;=10:
           raise forms.ValidationError(""You have exceeded limit."")
</code></pre>
"
"<p>I am developing a simple application for Motorola MC32N0 on WinCE7.0 using VS2008 SP1 having .net Compact Framework 3.5 and SQL Server CE 3.5SP1. </p>

<p>Whenever I try to read from database I get this error.</p>

<blockquote>
  <p>A native exception has occurred in projectName.exe. Select Quit and then restart this program or select details for more information.</p>
</blockquote>

<p>When I go to details I get this</p>

<blockquote>
  <p>ExceptionCode: 0xc0000005<br>
  ExceptionAddress: 0xc0000000<br>
  Reading: 0xc0000000<br>
  at NativeMethods.GetKeyInfo(IntPtrpTx, String pwszBase Table, IntPtr prgDbKeyInfo, Int32 cDbKeyInfo,IntPtr pError)<br>
  at SqlCeDataReader.FillMetaData(SqlCeDataReader reader, Int32 resultType) . . .</p>
</blockquote>

<p>it continues. This is the database read code</p>

<pre><code>public List&lt;Users&gt; SelectByUserName(string UserName)
{
    var list = new List&lt;Users&gt;();
    using (var command = EntityBase.CreateCommand(Transaction))
    {
        if (UserName != null)
        {
            command.CommandText = ""SELECT * FROM Users WHERE UserName=@UserName"";
            command.Parameters.Add(""@UserName"", SqlDbType.NVarChar);
            command.Parameters[""@UserName""].Value = UserName;
        }
        else
            command.CommandText = ""SELECT * FROM Users WHERE UserName IS NULL"";

        using (var reader = command.ExecuteReader())
        {
            list = fetchData(reader);
        }
    }
    return list;
}
</code></pre>

<p>When it reaches this <code>command.ExecuteReader()</code> the exception occurs.</p>

<p>After a lot of searching some guys are saying that this might be a mismatch of the SQL Server CE version but how do I fix that? One file is copied to folder when application is deployed. How to check it's version or change the version of SQL Server CE being used?</p>
","<p>From MSDN: [The Dispose method] Releases the resources used by the DbDataReader and calls Close.</p>

<p>So it seems sensible to call dispose once you're finished with the reader. Better still, wrap it all in a Using block and forget all about it.</p>
"
"<p>I have MySQL function with <strong>2 parameters</strong> namely <strong>user_id</strong> and <strong>post_id</strong></p>

<p>Here's my function:</p>

<pre><code>CREATE FUNCTION isliked(pid INT, uid INT)
RETURN TABLE
AS
RETURN (EXISTS (SELECT 1 FROM likedata ld WHERE post_id = pid AND user_id = uid
       )) as is_liked
END
</code></pre>

<p>I tried to call it with below query:</p>

<pre><code>SELECT posts.id, posts.title, isliked(111,123)
FROM posts
</code></pre>

<p>It returns the following error:</p>

<pre><code>You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'RETURN TABLE
AS
RETURN (EXISTS (SELECT 1 FROM likedata ld WHERE post_id = pid AN' at line 2
</code></pre>

<p>It should be return results like this <a href=""http://sqlfiddle.com/#!9/91040/5"" rel=""nofollow noreferrer"">http://sqlfiddle.com/#!9/91040/5</a>
I'm new to sql, any help will be great, thanks in advance</p>
","<p>You are over complicating the code, there is no need to dynamically generate sql code here and it will not work anyway.</p>

<p>Just create a function that takes a field value and a json field value as parameter and you do not need dynamic sql:</p>

<pre><code>DROP FUNCTION IF EXISTS get_select_expr;
CREATE FUNCTION get_select_expr (field_name VARCHAR(255), json_field_name varchar (255))
RETURNS VARCHAR(255) DETERMINISTIC
RETURN json_unquote(json_extract(field_name, concat(
    SUBSTRING_INDEX(json_unquote(JSON_SEARCH(field_name, 
    'one', 'pref.', json_field_name, '', NULL, '$.f[*].q')), 
    '.', 2), '.', 'value')));

SELECT get_select_expr(my_table.response, 'field_1') AS field_1 FROM my_table;
</code></pre>
"
"<p>I am new to phpmysqli. Here is what I have and what am trying to achieve: I will update this based on the recommendations; 
<a href=""https://i.stack.imgur.com/hJryA.png"" rel=""nofollow noreferrer"">Database sample data</a></p>

<p>I want to display the data on one page with separate tables for each student based on their sid. This is what I have tried so far;</p>

<pre><code>&lt;?php
include_once 'dbcon.php';

$results = $MySQLiconn-&gt;query('SELECT * FROM activitybook');

$students = [];

foreach ( $results-&gt;fetch_array() as $activity ) {
    $students[$activity['sid']][] = $activity;
}

foreach($students as $sid=&gt;$activities) {
    foreach($activities as $activity) {
         echo
                    ""&lt;table&gt;&lt;tr&gt;
                        &lt;th&gt;SID&lt;/th&gt;
                        &lt;th&gt;Date&lt;/th&gt;
                        &lt;th&gt;FName&lt;/th&gt;
                        &lt;th&gt;LName&lt;/th&gt;
                        &lt;th&gt;activity&lt;/th&gt;
                        &lt;th&gt;time&lt;/th&gt;
                        &lt;th&gt;score&lt;/th&gt;
                        &lt;/tr&gt;
                &lt;tr&gt;
                    &lt;td&gt;"" . $sid . ""&lt;/td&gt;
                    &lt;td&gt;"" . $activity['fname'] . ""&lt;/td&gt;
                    &lt;td&gt;"" . $activity['lname'] . ""&lt;/td&gt;
                    &lt;td&gt;"" . $activity['activity'] .  ""&lt;/td&gt;
                    &lt;td&gt;"" . $activity['atime'] .  ""&lt;/td&gt;
                    &lt;td&gt;"" . $activity['ascore'] .  ""&lt;/td&gt;
                &lt;/tr&gt;&lt;/table&gt;"";
    }
}
?&gt;
</code></pre>

<p><a href=""https://i.stack.imgur.com/BR3rH.png"" rel=""nofollow noreferrer"">This is what I get</a></p>

<p>What am trying to achieve is separate tables for each <code>sid</code>.
<a href=""https://i.stack.imgur.com/bGz1L.png"" rel=""nofollow noreferrer"">This is the sample of what I want to archive</a></p>
","<blockquote>
  <p>and I can't figure out what I'm missing.</p>
</blockquote>

<p>Missing? Nothing. Extra? :)</p>

<p><code>mysqli_fetch_array</code> will take <code>$result</code>, which already knows about <code>$conn</code>; so <code>mysqli_fetch_array($result)</code> is correct, and adding <code>$conn</code> confuses it.</p>
"
"<p>When parsing in a SQL query like:</p>

<pre><code>SELECT g.A, g.B, g.C FROM dbo.Goat g inner join dbo.Badger b on g.A=b.A
</code></pre>

<p>and iterating through TSqlParserToken there is a Property called TokenType.
I get the following:</p>

<pre><code>SELECT   Select
     WhiteSpace
g    Identifier
.    Dot
A    Identifier
,    Comma
     WhiteSpace
g    Identifier
.    Dot
B    Identifier
,    Comma
     WhiteSpace
g    Identifier
.    Dot
C    Identifier
     WhiteSpace
FROM     From
     WhiteSpace
dbo  Identifier
.    Dot
Goat     Identifier
     WhiteSpace
g    Identifier
     WhiteSpace
inner    Inner
     WhiteSpace
join     Join
     WhiteSpace
dbo  Identifier
.    Dot
Badger   Identifier
     WhiteSpace
b    Identifier
     WhiteSpace
on   On
     WhiteSpace
g    Identifier
.    Dot
A    Identifier
=    EqualsSign
b    Identifier
.    Dot
A    Identifier 
</code></pre>

<p>I understand that in order for the parser to return these values it need to have an awareness of the underlying schema. Is it possible for me to feed the parser with the underlying objects so it can return a more aware set of tokens?</p>

<p>The TSqlTokenType enum also has values such as Table,Schema, View  so Im sure this must be possible. Its just the documentation is scarce.
Dan</p>
","<p>RegEx won't get the job done - definitely... you will need a real parser - like <a href=""http://www.sqlparser.com/sql-parser-dotnet.php"" rel=""nofollow"">this one</a> (commercial).</p>
"
"<p>Note that the output has been ""stylized"" so it reads better here on SO.</p>

<p>What I've got...</p>

<pre><code>(sql/format 
  (-&gt; 
    (sqlh/select :*) 
    (sqlh/from :event) 
    (sqlh/merge-where [:in :field_id [""1673576"", ""1945627"", ""1338971""]]) 
    (sqlh/merge-where [:in :layer [""fha.abs"" ""fha.rank"" ""fha.true-color""]])
    (sqlh/merge-order-by :field_id)
    (sqlh/merge-order-by :layer)
    (sqlh/merge-order-by :event_date)
    (sqlh/limit 5)))
=&gt;
[""SELECT * 
  FROM event 
  WHERE ((field_id in (?, ?, ?)) AND (layer in (?, ?, ?))) 
  ORDER BY field_id, layer, event_date 
  LIMIT ?""
 ""1673576""
 ""1945627""
 ""1338971""
 ""fha.abs""
 ""fha.rank""
 ""fha.true-color""
 5]
</code></pre>

<p>What I want...</p>

<pre><code>(sql/format 
  (-&gt; 
    (sqlh/select :*) 
    (sqlh/from :event) 
    (sqlh/merge-where [:in :field_id [""1673576"", ""1945627"", ""1338971""]]) 
    (sqlh/merge-where [:in :layer [""fha.abs"" ""fha.rank"" ""fha.true-color""]])
    ;;; this doesn't work, but is conceptually what I'm looking for
    (sqlh/merge-order-by [:field_id :layer :event_date])
    (sqlh/limit 5)))
=&gt;
[""SELECT * 
  FROM event 
  WHERE ((field_id in (?, ?, ?)) AND (layer in (?, ?, ?))) 
  ORDER BY (field_id, layer, event_date) 
  LIMIT ?""
 ""1673576""
 ""1945627""
 ""1338971""
 ""fha.abs""
 ""fha.rank""
 ""fha.true-color""
 5]
</code></pre>

<p>How can I get HoneySQL to emit SQL that treats my order by clause as the compound key that the table itself is using as the Primary Key?</p>

<p>It seems HoneySQL should be able to do this as it ""does the right thing"" when presented the same challenge in a where clause like...</p>

<pre><code>(sql/format
  (-&gt;
    (sqlh/select :*)
    (sqlh/from :event)
    (sqlh/merge-where [:= [:field_id :layer :event_date] [""1338971"" ""fha.abs"" (c/from-string ""2011-08-02T10:54:55-07"")]])))
=&gt;
[""SELECT * FROM event WHERE (field_id, layer, event_date) = (?, ?, ?)""
 ""1338971""
 ""fha.abs""
 #object[org.joda.time.DateTime 0xe59f807 ""2011-08-02T17:54:55.000Z""]]
</code></pre>
","<p>First you need to look at the format behavior on <code>order-by</code></p>

<pre><code>(sql/format {:order-by [:c1 :c2]}) 
=&gt; [""ORDER BY c1, c2""]
(sql/format {:order-by [[:c1 :desc] :c2]})
=&gt; [""ORDER BY c1 DESC, c2""]
</code></pre>

<p>that is the struct about order-by which will be generated.</p>

<p>If you look at the macro <code>defhelper</code>  it will do two things. </p>

<ol>
<li>defrecord for the spec type</li>
<li>define a function to call the mutimethod </li>
</ol>

<p><code>
(do
  (defmethod
    build-clause
    :order-by
    [_ m fields]
    (assoc m :order-by (collify fields)))
  (defn order-by [&amp; args__14903__auto__]
    (let [[m__14904__auto__ args__14903__auto__] (if
                                                   (plain-map?
                                                     (first
                                                       args__14903__auto__))
                                                   [(first
                                                      args__14903__auto__)
                                                    (rest
                                                      args__14903__auto__)]
                                                   [{}
                                                    args__14903__auto__])]
      (build-clause :order-by m__14904__auto__ args__14903__auto__)))
  (alter-meta! #'order-by assoc :arglists '([fields] [m fields])))
</code></p>

<p>The <code>collify</code> is very simple. </p>

<pre><code> (defn collify [x]
     (if (coll? x) x [x]))
</code></pre>

<p>So , we need to look at <code>defn order-by</code>  function . 
When you call <code>(sqlh/merge-order-by {} [:a :b])</code>,</p>

<p><code>args__14903__auto__ = '({} [:a :b])</code></p>

<p>The first <code>if</code> will create two var <code>m__14904__auto__ = {}</code> and <code>args__14903__auto__ = (rest args__14903__auto__) = ([:a :b])</code>.</p>

<p>So, I guess the merge-order-by function is wrong.</p>

<p>I solve your problem like this.</p>

<pre><code>(sql/format
  (-&gt;
    (sqlh/select :*)
    (sqlh/from :event)
    (sqlh/merge-where [:in :field_id [""1673576"", ""1945627"", ""1338971""]])
    (sqlh/merge-where [:in :layer [""fha.abs"" ""fha.rank"" ""fha.true-color""]])
    (sqlh/merge-order-by [:field_id :desc] :layer :event_date)
    (sqlh/limit 5)))
</code></pre>
"
"<p>Based on an example for forking, I build up this little script:</p>

<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sqlanydb
import os

def child():
    conn = sqlanydb.connect(uid='dba', pwd='sql', eng='somedb_IQ', dbn='somedb')
    curs = conn.cursor()
    curs.execute(""""""SELECT * FROM foobaa;"""""")
    os.exit(0)

def parent():
   while True:
      newpid = os.fork()
      if newpid == 0:
         child()
      else:
         pids = (os.getpid(), newpid)
         print ""parent: %d, child: %d"" % pids
      if raw_input( ) == 'q': break

parent()
</code></pre>

<p>The intention is to do the database action inside a seperate process (big goal later is to run a huge number of queries at the same time). </p>

<p>But when running the script, I'm getting: </p>

<pre><code>parent: 20580, child: 20587
Traceback (most recent call last):
  File ""connectiontest.py"", line 25, in &lt;module&gt;
    parent()
  File ""connectiontest.py"", line 19, in parent
    child()
  File ""connectiontest.py"", line 8, in child
    conn = sqlanydb.connect(uid='dba', pwd='sql', eng='somedb_IQ', dbn='somedb')
  File ""/usr/local/lib/python2.6/dist-packages/sqlanydb.py"", line 461, in connect
    return Connection(args, kwargs)
  File ""/usr/local/lib/python2.6/dist-packages/sqlanydb.py"", line 510, in __init__
    self.handleerror(*error)
  File ""/usr/local/lib/python2.6/dist-packages/sqlanydb.py"", line 520, in handleerror
    eh(self, None, errorclass, errorvalue)
  File ""/usr/local/lib/python2.6/dist-packages/sqlanydb.py"", line 342, in standardErrorHandler
    raise errorclass(errorvalue)
sqlanydb.OperationalError: Failed to initialize connection object
</code></pre>

<p>What did I might miss? </p>
","<p>Since Sybase IQ is based on Sybase ASA, are you sure that you're using the proper keys for the credentials? This (albeit, old) documentation looks like it wants DSN and DSF instead of ENG and DBN.</p>

<p><a href=""http://dcx.sybase.com/1101/en/dbprogramming_en11/python-writing-open.html"" rel=""nofollow"">http://dcx.sybase.com/1101/en/dbprogramming_en11/python-writing-open.html</a></p>
"
"<p>The window user details is different from the Sql Server user I log in. So I had tried to use pyodbc connect to the database using the username(Admin_JJack) and password. But the connection show fails for the Window User(Jack) and I don't know where goes wrong.</p>

<p>my connection string :</p>

<pre><code>connection = pyodbc.connect(
    ""Driver={""SQL Driver""};""
    ""Server= ""ServerName"";""
    ""Database=""DatabaseName"";""
    ""UID=""UserName"";""
    ""PWD=""Password"";""
    ""Trusted_Connection=yes""
)
</code></pre>

<blockquote>
  <p>pyodbc.InterfaceError: ('28000', ""[28000] [Microsoft][SQL Server Native Client 11.0][SQL Server]Login failed for user 'Jack'. (18456) (SQLDriverConnect);</p>
</blockquote>

<p>How to connect to the database using sql server authentication ?</p>
","<p>Some details on SQL Server mixed authentication. To enable do, the following:</p>

<ol>
<li>Connect to DB server (presumably via Windows Authentication</li>
<li>Right click for properties on the server icon</li>
<li>On the properties dialog go to Security</li>
<li>Select ""SQL Server and Windows Authentication mode"". Save.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/rzYgR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rzYgR.png"" alt=""enter image description here""></a></p>
"
"<p>Not a duplicate of <a href=""https://stackoverflow.com/questions/6092390/user-defined-table-type-insertion-sometimes-causing-conversion-error"" title=""User-Defined Table Type insertion sometimes causing conversion error"">User-Defined Table Type insertion sometimes causing conversion error</a></p>

<p>I have a user defined table type:</p>

<pre><code>CREATE TYPE [dbo].[udtImport] AS TABLE(
    Name    varchar(256)    null
    Code    varchar(32) null
    StartDate   varchar(256)    null
    EndDate varchar(256)    null
    DateCreated datetime    null
</code></pre>

<p>The DateCreated field is populated in my DB layer using <code>DateTime.Now(),</code> while all other fields are from an imported table.</p>

<p>When I import a file with the date fields populated I get a SQL error: </p>

<blockquote>
  <p>Conversion failed when converting date and/or time from character
  string.</p>
</blockquote>

<p>Intercepting the generated code using SQL Profiler shows this:</p>

<pre><code>DECLARE @p1 dbo.udtImport;
INSERT INTO @p1
VALUES
  ( N'Kit A1', 
    N'A002',  
    '2016-04-02 00:00:00.000',
    '2016-10-10 00:01:00.000', 
    '2018-10-22 16:08:28.6823468' );

exec impSaveImport @ImportList=@p1
</code></pre>

<p>impSaveImport is a stored procedure that has just one parameter: the table type var and does a straight insert to table [Import]. No logic, no triggers, no reference to other tables.  </p>

<p>Executing this code in SSMS shows the same error as expected.<br>
Trimming the last 4 digits off the last DateTime field causes the insert query to succeed.</p>

<p>So far so good.</p>

<p>When I import a file with the StartDate and EndDate fields empty, I get no error, and data is successfully inserted into the Import table.</p>

<p>When I intercept the successful insert using profiler I get this:</p>

<pre><code>DECLARE @p1 dbo.udtImport;
   INSERT INTO @p1
        VALUES
          ( N'Kit A1', 
            N'A002',  
            null,
            null, 
            '2018-10-22 16:34:11.5243245' );

        exec impSaveImport @ImportList=@p1
</code></pre>

<p>Keep in mind this query SUCCESSFULLY insert one row into the table Import.</p>

<p>When I run this latest query in SSMS I get the same conversion error as before,</p>

<p><strong>but it ran without error from within my MVC app!</strong> </p>

<p>This last part has me stumped.</p>

<p>How can it be?</p>

<p>Project is using MVC with SQL2016.</p>
","<p>You could use <code>DATETIME2</code>:</p>

<pre><code>CREATE TYPE [dbo].[udtImport] AS TABLE(
    Name    varchar(256)    null,
    Code    varchar(32) null,
    StartDate   varchar(256)    null,  -- should be datetime2 format
    EndDate varchar(256)    null,      -- should be datetime2 format
    DateCreated datetime2    null);

DECLARE @p1 dbo.udtImport;
INSERT INTO @p1(Name, Code, StartDate, EndDate, DateCreated)
VALUES
  ( N'Kit A1', 
    N'A002',  
    '2016-04-02 00:00:00.000',
    '2016-10-10 00:01:00.000', 
    '2018-10-22 16:08:28.6823468' );
</code></pre>

<p><strong><a href=""https://dbfiddle.uk/?rdbms=sqlserver_2017&amp;fiddle=1cefe235953d2330670186fead36c420"" rel=""nofollow noreferrer"">db&lt;>fiddle demo</a></strong></p>
"
"<p>Does DeepSQL require a new MySQL installation or can I add it to my existing environment?</p>
","<p>(Disclaimer: I work at Deep) DeepSQL is installed as a pluggable storage engine for MySQL. We currently support MySQL 5.5 and 5.6 as well as Percona Server 5.5 and 5.6. The DeepSQL plugin is designed to install into the binary distributions from mysq.com as well as the the ones from the Ubuntu and IUS repositories. Details can be found at <a href=""http://deepis.com/product-documentation/deepsql-installation-guide-rel-330"" rel=""nofollow"">http://deepis.com/product-documentation/deepsql-installation-guide-rel-330</a></p>
"
"<p>It seems like this should be tremendously easy, but I've had no luck thus far.</p>
","<p>I'd like to try break this down, and work the problem through. There are a number of things to address.</p>

<h3>Logic</h3>

<p>How are you going to control uniqueness across the tables? There might be an ALBERT in both students <em>and</em> administration. It might even be the same person, a student and employee of the facility. I've been such a person.</p>

<h3>Suggested SQL</h3>

<p>There are no joins between your tables, this would turn the result into a cartesion product, and I'm pretty sure you could return true to ALBERT logging in with NIKOLA's password.</p>

<p>I think you probably meant to use a set operator</p>

<pre><code>select ...
from student 
where ...
union all
select ...
from admin
where ...
</code></pre>

<p>The UNION <em>ALL</em> means there is no need to check for uniqueness, no need for an <a href=""http://www.grassroots-oracle.com/2009/07/union-all-performance-accuracy.html"" rel=""noreferrer"">extra sort</a>.</p>

<h3>Password protection</h3>

<p>To be able to UPPER a password means you're storing it clear text. People today should inherit enough digital fluency that passwords <strong>should not</strong> be stored clear text. Ever.</p>

<p>See this article for an example of how to set up custom authentication in APEX by hashing passwords. A rare one amongst a disappointing number that did not hash passwords. This one also hashes with the username and some salt, which is better.
<a href=""http://barrybrierley.blogspot.com.au/2015/05/custom-authentication-in-apex.html"" rel=""noreferrer"">http://barrybrierley.blogspot.com.au/2015/05/custom-authentication-in-apex.html</a></p>

<p>It also starts to cover your needs regarding user type.</p>

<p>I'm sure there was an example in the APEX documentation, but I couldn't find it.</p>

<h2>User Type - Authorisation</h2>

<p>Once you have established you have a valid user, you can determine what type of user they are, then control their access to various components using <a href=""https://docs.oracle.com/database/apex-18.1/HTMDB/providing-security-through-authorization.htm#HTMDB12004"" rel=""noreferrer"">Authorization Schemes</a>.</p>

<p>For a more flexible system, I would abstract this further and use authorisation schemes to control <em>privileges</em> to certain components, and allocate these to <em>business roles</em>, which are in turn granted to <em>users</em>.</p>

<p>This serves your ""multiple types of people"".</p>

<h2>'Record Exists' check</h2>

<p>From early on I learned something at AskTom regarding checking for existence of rows, which seems to hold up well across all versions</p>

<pre><code>declare
  ln_exists pls_integer;
begin
  select count(*)
  into ln_exists
  from dual
  where exists (
     select null 
     from your_table -- whatever you're looking for
     where id = p_id
  );
  return ln_exists = 1; -- true if exists
 end;
</code></pre>

<p>Oracle knows just how to spend the least effort in solving this problem.</p>

<p>Many other variations just select too many rows from the database.</p>

<h2>Shared applications</h2>

<p>You <em>can</em> actually define multiple entry points using different authentication to the same application.
<a href=""http://www.grassroots-oracle.com/2014/04/shared-authentication-across-multiple-apex-apps.html"" rel=""noreferrer"">http://www.grassroots-oracle.com/2014/04/shared-authentication-across-multiple-apex-apps.html</a></p>
"
"<p>running into some issues trying to figure out an Azure Function (node.js-based) can connect to our mysql database (also hosted on Azure). We're using mysql2 and following tutorials pretty much exactly (<a href=""https://docs.microsoft.com/en-us/azure/mysql/connect-nodejs"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/azure/mysql/connect-nodejs</a>, and similar) Here's the meat of the call:
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const mysql = require('mysql2');
const fs = require('fs');

module.exports = async function (context, req) {
    context.log('JavaScript HTTP trigger function processed a request.');

    if (req.query.fname || (req.body &amp;&amp; req.body.fname)) {
        
        context.log('start');
        var config = {
            host:process.env['mysql_host'],
            user: process.env['mysql_user'],
            password: process.env['mysql_password'],
            port:3306,
            database:'database_name',
            ssl:{
            ca : fs.readFileSync(__dirname + '\\certs\\cacert.pem')
            },
            connectTimeout:5000
        };

        const conn = mysql.createConnection(config);
        /*context.log(conn);*/

        conn.connect(function (err) {
            context.log('here'); 
            if (err) { 
                context.error('error connecting: ' + err.stack);
                context.log(""shit is broke"");
                throw err;
            }
            console.log(""Connection established."");  
            
        });

        context.log('mid');
        conn.query('SELECT 1+1',function(error,results,fields) {
            context.log('here');
            context.log(error);
            context.log(results);
            context.log(fields);
        });</code></pre>
</div>
</div>
</p>

<p>Basically, running into an issue where the conn.connect(function(err)... doesn't return anything - no error message, no logs, etc. conn.query works similarly.</p>

<p>Everything seems set up properly, but I don't even know where to look next to resolve the issue. Has anyone come across this before or have advice on how to handle?</p>

<p>Thanks!!
Ben</p>
","<p>Yes, this should be possible using the <a href=""https://docs.microsoft.com/en-us/python/azure/python-sdk-azure-install?view=azure-python"" rel=""nofollow noreferrer"">Azure Python SDK</a>.</p>

<p>This might be what you are looking for <a href=""https://docs.microsoft.com/en-us/python/api/azure-mgmt-rdbms/azure.mgmt.rdbms.mysql.operations.servers_operations.serversoperations?view=azure-python"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/python/api/azure-mgmt-rdbms/azure.mgmt.rdbms.mysql.operations.servers_operations.serversoperations?view=azure-python</a></p>
"
"<p>using  <code>global sql parser (gsp)</code> for extracting column and sorting type from order sql query and extract and or from where condition</p>

<pre><code>SELECT employee_id, dept, name, age, salary
FROM employee_info
WHERE dept = 'Sales' and ID=1
ORDER BY salary, age DESC,ID;
</code></pre>

<p>I can extracting column name but can extract order type</p>

<blockquote>
  <p>1- how can extract order type?</p>
  
  <p>2- how can extract and , or from where sql?</p>
</blockquote>
","<p>If <em>pSqlstmt</em> is <em>gsp_selectStatement</em> * then you can do something like this:</p>

<pre><code>if(pSqlstmt-&gt;orderbyClause != nullptr)
{       
    string sortType;
    int colNumOrderBy = pSqlstmt-&gt;orderbyClause-&gt;items-&gt;length;

    for(int i = 0; i &lt; colNumOrderBy; i++)
    {
        gsp_orderByItem *field = reinterpret_cast&lt;gsp_orderByItem *&gt;(gsp_list_celldata(pSqlstmt-&gt;orderbyClause-&gt;items-&gt;head));

        //get order by column name
        char *sortCol = gsp_node_text(reinterpret_cast&lt;gsp_node*&gt;(field-&gt;sortKey)); 

        if(field-&gt;sortToken== nullptr)
        {
            //ERROR
        }
        else
        {
            //find out sorting type (ASC/DESC)
            sortType = sortType.substr(0,field-&gt;sortToken-&gt;nStrLen);
        }

        free(sortCol);
        pSqlstmt-&gt;orderbyClause-&gt;items-&gt;head = pSqlstmt-&gt;orderbyClause-&gt;items-&gt;head-&gt;nextCell;
    }

}
</code></pre>
"
"<p>I have a simple SQL query in Elasticsearch which I know returns less than 100 rows of results. How can I get all these results at once (i.e., without using scroll)? I tried the <code>limit n</code> clause but it works when <code>n</code> is less than or equal to 10 but doesn't work when <code>n</code> is great than 10.</p>

<p>The Python code for calling the Elasticsearch SQL API is as below.</p>

<pre><code>import requests
import json

url = 'http://10.204.61.127:9200/_xpack/sql'
headers = {
   'Content-Type': 'application/json',
}
query = {
    'query': '''
        select
            date_start,
            sum(spend) as spend
       from
           some_index
       where
           campaign_id = 790
           or
           campaign_id = 490
       group by
           date_start
   '''
}
response = requests.post(url, headers=headers, data=json.dumps(query))
</code></pre>

<p>The above query returns a cursor ID. I tried to feed the cursor ID into the same SQL API but it doesn't gave me more result.</p>

<p>I also tried to translated the above SQL query to native Elasticsearch query using the SQL translate API and wrapped it into the following Python code, but it doesn't work either. I still got only 10 rows of results.</p>

<pre><code>import requests
import json


url = 'http://10.204.61.127:9200/some_index/some_doc/_search'
headers = {
    'Content-Type': 'application/json',
}
query = {
    ""size"": 0,
    ""query"": {
        ""bool"": {
            ""should"": [
                {
                    ""term"": {
                        ""campaign_id.keyword"": {
                            ""value"": 790,
                            ""boost"": 1.0
                        }
                    }
                },
                {
                    ""term"": {
                        ""campaign_id.keyword"": {
                            ""value"": 490,
                            ""boost"": 1.0
                        }
                    }
                }
            ],
            ""adjust_pure_negative"": True,
            ""boost"": 1.0
        }
    },
    ""_source"": False,
    ""stored_fields"": ""_none_"",
    ""aggregations"": {
        ""groupby"": {
            ""composite"": {
                ""size"": 1000,
                ""sources"": [
                    {
                        ""2735"": {
                            ""terms"": {
                                ""field"": ""date_start"",
                                ""missing_bucket"": False,
                                ""order"": ""asc""
                            }
                        }
                    }
                ]
            },
            ""aggregations"": {
                ""2768"": {
                    ""sum"": {
                        ""field"": ""spend""
                    }
                }
            }
        }
    }
}
response = requests.post(url, headers=headers, data=json.dumps(query)).json() 
</code></pre>
","<p>With <a href=""https://www.elastic.co/products/stack/elasticsearch-sql"" rel=""nofollow noreferrer"">elasticsearch-sql</a>, <code>LIMIT 100</code> should translate to <code>""size"": 100</code> in <a href=""https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-from-size.html"" rel=""nofollow noreferrer"">traditional query DSL</a>. This will return up to 100 matching results.</p>

<p>Given this request:</p>

<pre><code>POST _xpack/sql/translate
{
  ""query"":""SELECT FlightNum FROM flights LIMIT 100""
}
</code></pre>

<p>The translated query is:</p>

<pre><code>{
  ""size"": 100,
  ""_source"": {
    ""includes"": [
      ""FlightNum""
    ],
    ""excludes"": []
  },
  ""sort"": [
    {
      ""_doc"": {
        ""order"": ""asc""
      }
    }
  ]
}
</code></pre>

<p>So syntax-wise, <code>LIMIT N</code> should do what you expect it to. As to why you're not seeing more results, this is likely something specific to your index, your query, or your data.</p>

<p>There is a setting <code>index.max_result_window</code> which can cap the size of a query, but it defaults to 10K and also should return an error rather than just limiting the results.</p>
"
"<p>I have this and it seems to be valid syntax:</p>

<pre><code>SELECT
  A.email,
  A.handle as foo
FROM
  (
    user_table A
    INNER JOIN (
      klass_table K
      LEFT JOIN user_table B ON (B.x = A.y)
    )
  )
</code></pre>

<p>but if I re-use the alias (user_table A x2):</p>

<pre><code>SELECT
  A.email,
  A.handle as foo
FROM
  (
    user_table A
    INNER JOIN (
      klass_table K
      LEFT JOIN user_table A ON (A.x = K.y)
    )
  )
</code></pre>

<p>then I will get this error:</p>

<blockquote>
  <p>Not unique table/alias: 'A'</p>
</blockquote>

<p>Can anyone explain the logic of how aliasing works in this case? If it's the same table, why does it need a different alias? Note these are nonsense queries - I am more concerned about the semantics/syntax rules here.</p>
","<p>This:</p>

<pre><code>SELECT
  A.email,
  A.handle as foo
FROM
  (
    user_table A
    INNER JOIN (
      klass_table K
      LEFT JOIN user_table B ON (B.x = A.y)
    )
  )
</code></pre>

<p>selects 2 columns, but from which table or subquery? <br/>
What is <code>A</code>? <br/>
<code>A</code> is an alias for <code>user_table</code> but it exists only inside this subquery:</p>

<pre><code>  (
    user_table A
    INNER JOIN (
      klass_table K
      LEFT JOIN user_table B ON (B.x = A.y)
    )
  )
</code></pre>

<p>Outside of this subquery it does not exist unless you alias this whole subquery like:</p>

<pre><code>  (
    user_table A
    INNER JOIN (
      klass_table K
      LEFT JOIN user_table B ON (B.x = A.y)
    )
  ) A
</code></pre>

<p>Of course this <code>A</code> is not the same as the previous <code>A</code>.<br/>
The 1st <code>A</code> was an alias for the table <code>user_table</code> but 
the 2nd <code>A</code>is an alias for the subquery.  </p>
"
"<p>What is the right way to color-code rows in a <code>QTableView</code>?</p>

<p>I'm developing a spreadsheet application that should color-code its rows based on a specific value set in one of the columns. I use <code>QSqlRelationalTableModel</code> and <code>QSqlRelationalDelegate</code>; because, the value that should determine the color is a foreign key.</p>

<p>Why can't it be as simple as the following? Any ideas?</p>

<pre><code>model-&gt;setData( model-&gt;index( index.row(), index.column() ), 
                QBrush(Qt::red),
                Qt::BackgroundRole );
</code></pre>
","<p>So that you do not show the unreadable text of the PDF you must return an empty string when they ask for the <code>Qt::DisplayRole</code> role and return the icon when they ask for the <code>Qt::DecorationRole</code> role. I also recommend reading the icon once only if it is unique, which I show below:</p>

<p><strong>*.h</strong></p>

<pre><code>private:
    QIcon icon;
</code></pre>

<p><strong>*.cpp</strong></p>

<pre><code>RelationalTableModelWithIcon::RelationalTableModelWithIcon(QObject *parent, QSqlDatabase db):
    QSqlRelationalTableModel(parent, db)
{
    icon =  QIcon("":/icons/Art/Icons/Iynque-Flat-Ios7-Style-Documents-Pdf.ico"");
}
QVariant RelationalTableModelWithIcon::data(const QModelIndex &amp;index, int role) const
{
    if(index.column() == 3){
        if(role == Qt::DisplayRole)
            return """";
        else if ( role == Qt::DecorationRole)
            return icon;
    }
    return QSqlRelationalTableModel::data(index, role);
}
</code></pre>
"
"<p>I have a alter query </p>

<pre><code>ALTER TABLE Table_name  ADD(coumn1  DOUBLE,coumn2  DOUBLE);
</code></pre>

<p>I am using </p>

<pre><code>this.namedParameterJdbcTemplate.execute(alterTableSQL, namedValues, new PreparedStatementCallback&lt;Boolean&gt;() {
    @Override
    public Boolean doInPreparedStatement(PreparedStatement ps) throws SQLException, DataAccessException {
        return ps.execute();
    }
});
</code></pre>

<p>While executing the statement, it is throwing me the following exception.</p>

<pre><code>org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [ALTER TABLE &lt;sample&gt;  ADD (
 coumn1  DOUBLE,
 coumn2  DOUBLE
);]; nested exception is org.h2.jdbc.JdbcSQLException: Syntax error in SQL statement ""ALTER TABLE TEMPLATE_1  ADD ([*]
 coumn1  DOUBLE,
 coumn2  DOUBLE
); ""; expected ""identifier""; SQL statement:
ALTER TABLE template_1  ADD (
 coumn1  DOUBLE,
 coumn2  DOUBLE
); [42001-160]
</code></pre>

<p>How should I proceed on this?</p>
","<p>OVER is a MariaDB keyword:
<a href=""https://mariadb.com/kb/en/library/window-functions-overview/"" rel=""nofollow noreferrer"">https://mariadb.com/kb/en/library/window-functions-overview/</a></p>

<p>Rename that column to something else. I would also strongly advise using the same database in all your environments, otherwise your tests will detect bugs that don't happen in production, or won't detect bugs that happen in production.</p>
"
"<p>I'm trying to use the query engine 
<a href=""http://www.thomasfrank.se/sqlike.html"" rel=""nofollow"">SQLike</a> and am struggling with the basic concept.</p>

<p>The JSON I'm using as my data source comes from my PHP code, like so:</p>

<pre><code>var placesJSON=&lt;? echo json_encode($arrPlaces) ?&gt;;
</code></pre>

<p>Here's a sample JSON:</p>

<pre><code>var placesJSON=[{""id"":""100"",""name"":""Martinique"",""type"":""CTRY""},{""id"":""101"",""name"":""Mauritania"",""type"":""CTRY""},{""id"":""102"",""name"":""Mauritius"",""type"":""CTRY""},{""id"":""103"",""name"":""Mexico"",""type"":""CTRY""},{""id"":""799"",""name"":""Northern Mexico"",""type"":""SUBCTRY""},{""id"":""800"",""name"":""Southern Mexico"",""type"":""SUBCTRY""},{""id"":""951"",""name"":""Central Mexico"",""type"":""SUBCTRY""},{""id"":""104"",""name"":""Micronesia, Federated States"",""type"":""CTRY""},{""id"":""105"",""name"":""Moldova"",""type"":""CTRY""}];
</code></pre>

<p>I understand (via this <a href=""http://www.thomasfrank.se/SQLike/"" rel=""nofollow"">reference</a>) that I first need to unpack my JSON like so:</p>

<pre><code>var placesData = SQLike.q(
       {
       Unpack: placesJSON,
       Columns: ['id','name','type']
       }
    )
</code></pre>

<p>And the next step would be to query the results like so:</p>

<pre><code>var selectedPlaces = SQLike.q(
               {
               Select: ['*'],
               From: placesData,
               OrderBy: ['name','|desc|']
               }
</code></pre>

<p>Lastly, to display the results in the browser I should use something like:</p>

<pre><code>  document.getElementById(""myDiv"").innerHTML=selectedPlaces[0].name
</code></pre>

<p>This doesn't work. The error I get is: selectedPlaces[0].name is undefined.  </p>

<p>I'm pretty sure I'm missing out on something very simple. Any hints?</p>
","<pre><code>return this.userData = username
                    ^^^
</code></pre>

<p><code>=</code> is assignment and since you are returning an assignment it will be true and every record will be returned.</p>

<pre><code>return this.userData === username;
                     ^^^
</code></pre>
"
"<p>I am using sql-interactive-mode to connect to 2 databases: MySQL and SQLite. I created yasnippets for mysql in <code>yasnippets/sql-interactive-mode</code> folder. For example to add a column in MySQL I use the following snippet:</p>

<pre><code># -*- mode: snippet -*-
# name: Add column
# key: addcol
# --
ALTER TABLE $1 ADD COLUMN \`$2\` $3;
</code></pre>

<p>But SQLite uses different syntax. How can I create different yasnippets for different databases?</p>
","<p>As explained <a href=""https://capitaomorte.github.io/yasnippet/snippet-development.html#sec-3-2"" rel=""nofollow"">here</a> you can add arbitrary Emacs Lisp code (enclosed in backquotes) to <code>yasnippet</code> snippets that will be evaluated when they expand. In <code>sql-mode</code> and <code>sql-interactive-mode</code> there is a variable called <code>sql-product</code> that you can check to determine which type of database (<code>mysql</code>, <code>sqlite</code>, <code>postgres</code>, etc.) you are currently working with.</p>

<p>That's basically all you need to know. Here is an example of how to modify your <code>addcol</code> snippet:</p>

<pre><code># ...
ALTER TABLE $1 `(if (eq sql-product 'mysql) ""ADD"" ""FROB"")` COLUMN \`$2\` $3;
</code></pre>

<p>This will expand to</p>

<pre><code>ALTER TABLE $1 ADD COLUMN \`$2\` $3;
</code></pre>

<p>for <code>mysql</code> and</p>

<pre><code>ALTER TABLE $1 FROB COLUMN \`$2\` $3;
</code></pre>

<p>for other types of databases.</p>
"
"<p>In our system, we have two similar(but not same) databases. So I built these sqlalchemy models:</p>

<pre><code># base.py
Base = declarative_base()

class T1(Base):
    __tablename__ = 't1'
    id = Column(Integer, primary_key=True)
    name = Column(String)

# production1.py
from . import base

class T1(base.T1):
    status1 = Column(String)

# production2.py
from . import base

class T1(base.T1):
    status2 = Column(String)

# sessions.py
engine1 = create_engine(**production1_params)
session1 = scoped_session(sessionmaker(bind=engine1))
engine2 = create_engine(**production2_params)
session2 = scoped_session(sessionmaker(bind=engine2))
</code></pre>

<p>Then I can access different database by:</p>

<pre><code>import production1, production2
session1().query(production1.T1)
session2().query(production2.T2)
</code></pre>

<p>Now, I want to build our API system by graphql. First, I inherit from <code>SQLAlchemyConnectionField</code> to support database switching.</p>

<pre><code>class SwitchableConnectionField(SQLAlchemyConnectionField):
    def __init__(self, type, *args, **kwargs):
        kwargs.setdefault('db_type', String())
        super

    @classmethod
    def get_query(self, model, info, sort=None, **args):
        session = get_query(args['db_type'])
        query = session.query(model)
        ...
</code></pre>

<p>But when I want to define my nodes, I found the definitions must be:</p>

<pre><code>import production1, production2

class Production1Node(SQLAlchemyObjectType):
    class Meta:
        model = production1,T1
        interfaces = (Node,)

class Production2Node(SQLAlchemyObjectType):
    class Meta:
        model = production2.T1
        interfaces = (Node,)
</code></pre>

<p>There are two nodes definitions to support different databases. But I want to do something like:</p>

<pre><code>import base

class ProductionNode(SQLAlchemyObjectType):
    class Meta:
        model = base.T1
        interfaces = (Node,)
</code></pre>

<p>So that I can switch similar model at run time. However, even though I try to inherit from <code>Node</code>, I can't implement it. Does anyone know what should I do?</p>
","<p>Based on the error message, it seems that <code>project.models.site</code> (imported in the second snippet with <code>from project.models import site as site_model</code>) is a Python module rather than a subclass of <code>db.Model</code> or similar.  Did you perhaps mean to import <code>Site</code> (uppercase) instead of <code>site</code>?</p>
"
"<p>First, some background info, maybe someone suggests some better way then I try to do. I need to export SQLite database into text file. For that I have to use C++ and chosen to use CppSQLite lib.</p>

<p>That I do is collecting create queries and after that export every table data, the problem is that there are tables like <code>sqlite_sequence</code> and <code>sqlite_statN</code>. During import I cannot create these tables because these are special purpose, so the main question, would it affect stability if these tables are gone?</p>

<p>Another part of question. Is there any way to export and import SQLite database using CppSQLite or any other SQLite lib for C++? </p>

<p>P.S. Solution to copy database file is not appropriate in this particular situation.</p>
","<p>Searching through the <a href=""https://github.com/neosmart/CppSQLite/blob/master/CppSQLite3.cpp"" rel=""nofollow noreferrer"">source code</a> shows that <code>sqlite3_exec()</code> is called only from here:</p>

<pre><code>int CppSQLite3DB::execDML(const char* szSQL);
</code></pre>

<p>But the callback is not supported; if you want to read returned data, you must use the query object.</p>
"
"<p>I've got problem with sorting entries in JqGrid. Orderby seem to not work. I set breakpoint in code and I noticed, that orderby doesn't change order of elements. Any idea what could be wrong?</p>

<p>I'm using LINQ to SQL with MySQL (DbLinq project).</p>

<p>My action code:</p>

<pre><code>public ActionResult All(string sidx, string sord, int page, int rows)
        {
            var tickets = ZTRepository.GetAllTickets().OrderBy(sidx + "" "" + sord).ToList();
            var rowdata = (
                from ticket in tickets
                select new {
                    i = ticket.ID,
                    cell = new String[] {
                        ticket.ID.ToString(), ticket.Hardware, ticket.Issue, ticket.IssueDetails, ticket.RequestedBy, ticket.AssignedTo, ticket.Priority.ToString(), ticket.State
                    }
                }).ToArray();

            var jsonData = new
            {
                total = 1, // we'll implement later 
                page = page,
                records = tickets.Count(),
                rows = rowdata
            };

            return Json(jsonData, JsonRequestBehavior.AllowGet);
        }
</code></pre>
","<p>You probably want to Skip first then Take.  (Doing the take first then the skip doesn't make much sense.)</p>
"
"<p>I'm on windows 10 with docker version 1.9.1 using docker toolbox
I wanted to put up a quick postgres container, something I've done before with a dockerfile I had laying around.</p>

<pre><code>FROM postgres

ADD create-db.sql /tmp/
ADD drop_create_table.sql /tmp/
ADD db.sql /tmp/

ADD create-db.sh /docker-entrypoint-initdb.d/
</code></pre>

<p>It's pretty simple.
and when i run the resulting image. it starts fine.
However at the end it says:</p>

<blockquote>
  <p>...</p>
  
  <p>server started ALTER ROLE</p>
  
  <p>/docker-entrypoint-sh: running</p>
  
  <p>/docker-entrypoint-initdb.d/create-db.sh :No such file or directory</p>
</blockquote>

<p>If I try to do <code>docker run -it &lt;imagename&gt; //bin/bash</code> I can see that the file is indeed there:</p>

<blockquote>
  <p>root@xxxx:/docker-entrypoint-initdb.d# ls </p>
  
  <p>create-db.sh</p>
</blockquote>

<p>but whenever I run it it tells me it's not.<br>
The container promptly stops when it doesn't find the file, so I can't try to ssh into the running container.</p>
","<p><code>pg_ctl</code> does not have an option <code>-E</code>. If you want to run <code>initdb</code> through <code>pg_ctl</code>, you need to pass the <code>initdb</code> options using <code>-o</code> e.g. <code>pg_ctl initdb -D ... -o ""-E=UTF8""</code> </p>

<p>But it's a lot easier to call <code>inidb</code> directly:</p>

<pre><code>initdb -D=D:\testdata -E=UTF8 -U=postgres
</code></pre>
"
"<p>So I'm trying to create a table in SQL and then insert values into it. However, I seem to be getting this error:</p>

<blockquote>
  <p>[Error Code: -12101, SQL State: 42000]  Syntax error, 'CHECK'  assumed
  missing</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>[Error Code: -12233, SQL State: 42000]  The number of insert values is
  not the same as the number of object columns</p>
</blockquote>

<p>Here is my SQL code:</p>

<pre><code>CREATE TABLE Server(
Nummer INTEGER NOT NULL
PRIMARY KEY(Nummer) 
);
INSERT INTO Server(Nummer)
VALUES (1,2,3,4,5);
</code></pre>

<p>So I want to create a table named <code>Server</code> which has a primary key named <code>nummer</code>. Nummer then has the values 1,2,3,4,5</p>

<p>UPDATE--------------------------------------------------------------------</p>

<p>So my new code is:</p>

<pre><code>CREATE TABLE Server(
    Nummer INTEGER NOT NULL,
    PRIMARY KEY(Nummer), 
);

INSERT INTO Server(Nummer)
    VALUES (1);
INSERT INTO Server(Nummer)
    VALUES (2);
INSERT INTO Server(Nummer)
    VALUES (3);
INSERT INTO Server(Nummer)
    VALUES (4);
INSERT INTO Server(Nummer)
    VALUES(5);
</code></pre>

<p>I solved the check problem by simply putting a comma after every statement in the create section.</p>

<p>But I got a new problem which is this error code:</p>

<blockquote>
  <p>[Error Code: -12101, SQL State: 42000]  Syntax error, IDENTIFIER
  IDENTIFIER  assumed missing</p>
</blockquote>
","<p>If you want only one row, and your DBMS's version is <code>10.1+</code> then use <code>fetch first</code>:</p>

<pre><code>CREATE VIEW HighestValue AS
    SELECT s.dept, d.name, SUM(s.quantity) TotalQuantity
    FROM sale s INNER JOIN
         dept d
         ON d.number = s.dept
    GROUP BY s.dept, d.name
    ORDER BY TotalQuantity
    FETCH FIRST 1 ROW ONLY;
</code></pre>
"
"<p>I have this data:</p>

<pre><code>Id  Name     amount    Comments 
-------------------------------
1     n1     421762    Hello    
2     n2        421    Bye      
3     n2        262    null     
4     n2       5127    ''  
</code></pre>

<p>Each name may or may not have extra rows with null or empty comments.</p>

<p>How can I group by name and sum(amount) such that it ignores/absorbs the null or empty comments in the grouping and shows me only 2 groups.</p>

<p>Output I want:</p>

<pre><code>Id   Name     sum(amount)   Comments 
------------------------------------
1     n1         421762     Hello    
2     n2           5180     Bye 
</code></pre>

<p>I can't figure this out.  </p>

<p>I hoped that would ignore the null/empty values but I always end up with 4 groups</p>

<pre><code>select id, name, sum(amount), comments 
from table 
group by id, name, comments
</code></pre>
","<p>The reason for that is because you have provided count with a column value which is <code>null</code>. Instead, use <code>count(*)</code>:</p>

<pre><code>SELECT COUNT(*) FROM Table_Name WHERE [Current_Status] IS NULL
</code></pre>

<hr>

<p>Sample data:</p>

<pre><code>current_status
--------------
Processed
Null
Not Processed
Null
</code></pre>

<p>And the difference between two queries:</p>

<p><strong>count(current_status)</strong></p>

<pre><code>SELECT count(current_status) FROM table_name WHERE current_status IS NULL 

0
</code></pre>

<p><strong>count(*)</strong></p>

<pre><code>SELECT count(*) FROM table_name WHERE current_status IS NULL 

2
</code></pre>
"
"<p>Assume that we have a table with an unique clustered index. We query a row in that table using index key value. SQL Server will use index seek to find related data pages.</p>

<p>But how does SQL Server read those data pages from physical disk after having their file_id:page_id? How does it know the position of disk sector/cluster?</p>
","<p>SQL Server does not deal with disk sector/cluster.
It reads from <code>files</code> that are logically divided into <code>pages</code> (8Kb).
<code>Extent</code> is 8 consecutive pages.</p>

<p>Every table has <code>IAM</code> page(s) associated with it where all the extents allocated to this table are listed (it's a bitmap page where 1 means extent is allocated to the object an 0 means it is not).</p>

<p>SQL Server maintains internal pointers to the first IAM page and the
first data page of a heap. Those  pointers can be found in the system view <code>sys.system_internals_allocation_units</code>.</p>

<p>In case of clustered index all the data lives in pages as usual, and it still can be read using IAM page(s), but it can be also accessed in ""ordered"" way using <code>binary tree</code> that is index.</p>

<p>This means that over the data (index leaf pages) server builds additional searching structure that also consists of pages, the root page directs you to lower index levels in base of clustered index key.</p>

<p>Conclusion: all addresses presented in the index pages consist of <code>file_id:page_id</code>, not of cluster/sector.</p>

<p>Here you can find how database structures are organized:</p>

<p><a href=""https://books.google.it/books?id=FZlCAwAAQBAJ&amp;pg=PT332&amp;lpg=PT332&amp;dq=%22The%20structure%20is%20called%20a%20heap%20because%20the%20data%20is%20not%20organized%20in%20any%20order;%20rather%22&amp;source=bl&amp;ots=S6RYRc19Q3&amp;sig=FpxLMmHIdCrwd3nWsZj1ZA1pmTc&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj9o9vVoIvXAhWE2hoKHVMJC-EQ6AEIKzAB#v=onepage&amp;q=%22The%20structure%20is%20called%20a%20heap%20because%20the%20data%20is%20not%20organized%20in%20any%20order%3B%20rather%22&amp;f=false"" rel=""nofollow noreferrer"">Inside Microsoft SQL Server 2008 T-SQL Querying</a></p>
"
"<p>I have set up a SQL Server on an EC2 instance that is NOT on a domain.</p>

<p>I need the SQL Server Agent to run SSIS scripts, do back ups, send DB Mail, . I have set up a sql server user with read / write db permissions, and tried to follow the instructions here </p>

<p><a href=""https://docs.microsoft.com/en-us/sql/ssms/agent/configure-a-user-to-create-and-manage-sql-server-agent-jobs"" rel=""nofollow noreferrer"">https://docs.microsoft.com/en-us/sql/ssms/agent/configure-a-user-to-create-and-manage-sql-server-agent-jobs</a></p>

<p>After doing some research the Job needs to run on a Proxy, which requires a Credential. When I try to create a new credential I click on the elipsis next to Identity and I get the error</p>

<p>""The program cannot open the required dialog box because it cannot determine whether the computer named 'BLAHBLAHBLAH' is joined to a domain. Close this message and try again.""</p>

<p>To be clear I only need the account to run SSIS jobs, back ups and send the occasional mail.</p>

<p>What am I doing wrong?</p>

<p>Thanks</p>
","<p>In OrmLite the <code>[AutoIncrement]</code> attribute is for specifying an Auto Incrementing primary key. So it can't be be used on a non-Primary Key column.</p>
"
"<p>Using <code>F#</code> , <code>FsSql</code> and <code>PostGres</code>  </p>

<p>So I'm using this function</p>

<pre><code>let getSqlParameter value = 
    let uniqueKey = Guid.NewGuid().ToString(""N"")
    let key = (sprintf ""@%s"" uniqueKey)
    (key,Sql.Parameter.make(key,value))
</code></pre>

<p>to get me a parameter of anything I pass in dynamically </p>

<p>Which I then append to a query and I get something like this </p>

<pre><code>select * from (select * from mytable) as innerQuery where @a29c575b69bb4629a9971dac2808b445 LIKE '%@9e3485fdf99249e5ad6adb6405f5f5ca%' 
</code></pre>

<p>Then I take a collection of these and pass them off </p>

<p><code>Sql.asyncExecReader connectionManager query parameters</code></p>

<p>The problem that I'm having is that when I don't run this through my parameterization engine, it works fine. When I do, it doesn't work. It just returns empty sets. </p>

<p>The only thing I can think of is that the column names can't be parameterized. This is a problem because they're coming from the client. Is there a way to do this? </p>
","<p>This is because you're using the checked-cast operator <code>:&gt;</code>, which has the compiler verify that your cast is valid. In this case, since you know the data in your <code>obj</code> is an <code>int64</code>, you can tell the compiler that you know better by using the unsafe-cast operator <code>:?&gt;</code>.</p>
"
"<p>I am trying to run a docker container from <a href=""https://hub.docker.com/r/microsoft/mssql-server-linux/"" rel=""nofollow noreferrer"">microsoft/mssql-server-linux</a></p>

<pre><code>sudo docker run -d -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=SecertP@ssW0rd!' -p 1433:1433 --name TestName-SqlServer microsoft/mssql-server-linux
</code></pre>

<p>The container does not start and when I  inspcts the logs:</p>

<pre><code>docker logs e2
</code></pre>

<p>I see following errors:</p>

<blockquote>
  <p>2018-10-16 09:06:42.54 spid19s     Error: 17182, Severity: 16, State: 1.
  2018-10-16 09:06:42.54 spid19s     TDSSNIClient initialization failed with error 0xffffffff, status code 0x80. Reason: Unable to initialize SSL support.  </p>
  
  <p>2018-10-16 09:06:42.55 spid19s     Error: 17182, Severity: 16, State: 1.
  2018-10-16 09:06:42.55 spid19s     TDSSNIClient initialization failed with error 0xffffffff, status code 0x1. Reason: Initialization failed with an infrastructure error. Check for previous errors.  </p>
  
  <p>2018-10-16 09:06:42.56 spid19s     Error: 17826, Severity: 18, State: 3.</p>
  
  <p>2018-10-16 09:06:42.56 spid19s     Could not start the network library because of an internal error in the network library. To determine the cause, review the errors immediately preceding this one in the error log.  </p>
  
  <p>2018-10-16 09:06:42.56 spid19s     Error: 17120, Severity: 16, State: 1.<br>
  2018-10-16 09:06:42.56 spid19s     SQL Server could not spawn FRunCommunicationsManager thread. Check the SQL Server error log and the operating system error log for information about possible related problems.</p>
</blockquote>
",""
"<p>I am using Oracle Database 12.2 and ORDS 18.3. Is it possible to make an HTTP POST request with a raw body (no parameters using <code>application/x-www-form-urlencoded</code> or <code>multipart/form-data</code>) to ORDS, knowing that ORDS is used only for its PL/SQL Gateway part, not the REST Data Services part.</p>

<p>In this scenario, ORDS in standalone mode (with Jetty) is the HTTP server. The HTTP client that makes the request is some external program (Postman, Java, ...).</p>

<p>The idea is to send JSON data but it could be binary data like an image or something else.</p>

<p>It looks like the PL/SQL Gateway is ""only"" able to invoke procedures using parameters (or no parameter at all but also of course no body). I am able to make a request with a dummy parameter in multipart form and somehow simulate what I want (tested with <code>VARCHAR2</code>, not <code>CLOB</code> nor <code>BLOB</code>) but I wanted to know if the possibility exists.</p>
","<p>To answer my own question, it is not possible. We have to use a parameter and the MIME type <code>application/x-www-form-urlencoded</code> (or <code>multipart/form-data</code>).</p>
"
"<p>Please help me figure this one out. I've already tried deleting and re-creating data files, flushing tables, restarting the database and entire server.</p>

<p>InnoDB is in Force Recovery = 4</p>

<pre><code>&gt;mysql -u root
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 3
Server version: 5.5.16 MySQL Community Server (GPL)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql&gt; use wotstats_info_gold;
Database changed
mysql&gt; show tables;
Empty set (0.00 sec)

mysql&gt; create table player_team_role ( id int(11) ) ;
ERROR 1050 (42S01): Table '`wotstats_info_gold`.`player_team_role`' already exists
mysql&gt; drop table player_team_role;
ERROR 1051 (42S02): Unknown table 'player_team_role'
mysql&gt; flush tables;
Query OK, 0 rows affected (0.00 sec)

mysql&gt; create table player_team_role ( id int(11) ) ;
ERROR 1050 (42S01): Table '`wotstats_info_gold`.`player_team_role`' already exists
mysql&gt;
</code></pre>
","<p>Should <code>fb-celebotd</code> (with a dash) be <code>fb_celebotd</code> (with an underscore)?  If the table name has a dash, then you'll have to quote the table name: <code>""fb-celebotd""</code>.  Otherwise the dash is treated as a minus sign and tries to subtract celebotd from fb (both unknown).</p>
"
"<p>I attempted to apply the solution posted on <a href=""https://stackoverflow.com/questions/2876789/case-insensitive-for-sql-like-wildcard-statement"">How can I search (case-insensitive) in a column using LIKE wildcard?</a>, but I'm given the error </p>

<blockquote>
  <p>1253 - COLLATION 'utf8_general_ci' is not valid for CHARACTER SET 'latin1'</p>
</blockquote>

<p><strong>How can I apply case insensitivity to a <code>latin1</code> table?</strong></p>
","<pre><code>SELECT  *
FROM    mytable
WHERE   mycolumn = CAST('CamelCaseWord' AS CHAR CHARACTER SET latin1) COLLATE latin1_general_ci
</code></pre>

<p>See <a href=""http://sqlfiddle.com/#!2/d76b2/2"" rel=""nofollow"">SQLFiddle</a>.</p>
"
"<p>I'm using the <a href=""https://mongoosejs.com/"" rel=""nofollow noreferrer"">Mongoose ODM wrapper</a> for NodeJS and I'm concerned about injection attacks. Let's assume I have the following schema:</p>

<pre class=""lang-js prettyprint-override""><code>const UserSchema = new mongoose.Schema({ userName: String, password: String });
</code></pre>

<p>If I were to perform a login request that looks like the following:</p>

<pre class=""lang-js prettyprint-override""><code>router.post('/login', (request, response) =&gt; {

    const userName = request.body.userName;
    const password = request.body.password;

    User.findOne({ userName: userName }, function (error, user) {
        // ... check password, other logic
    });
});
</code></pre>

<p>I would be open to an injection attack with the following JSON payload which will always find a user:</p>

<pre class=""lang-js prettyprint-override""><code>{
    ""email"": { ""$gte"": """" },
    ""password"": { ""$gte"": """" }
}
</code></pre>

<p>I'm not concerned about the password as it is hashed if a user is found which prevents any actual log in but I want to make sure my input is sanitized so that an attacker wouldn't even make it to that point.</p>

<p>I'm aware of the <a href=""https://www.npmjs.com/package/mongo-sanitize"" rel=""nofollow noreferrer"">mongo-sanitize</a> NPM package referenced in a similar <a href=""https://stackoverflow.com/questions/28710345/sanitize-user-input-in-mongoose"">StackOverflow post</a> which appears to remove all JSON keys that begin with '$'. I plan on using this anyway but I will never allow the user to submit raw, unparsed JSON. Is it good practice in that case to just call toString() on the userName assuming I do the correct <code>null</code> checks? </p>

<pre class=""lang-js prettyprint-override""><code>const userName = request.body.userName.toString();
</code></pre>

<p>That would eliminate the query from being executed but it doesn't feel very secure. I assume the following is a better approach as it tries to convert <code>userName</code> to a <code>String</code>:
</p>

<pre><code>User.findOne({ userName: { ""$eq"": userName } }, function (error, user) {
     // ... other logic
});
</code></pre>

<p>I can't find anything concerning this in the in the <a href=""https://mongoosejs.com/docs/api.html#model_Model.findOne"" rel=""nofollow noreferrer"">Model.findOne() documentation</a> which leads me to believe I'm overlooking something.</p>

<p>Any insight would be appreciated.</p>

<p><strong>Other References:</strong></p>

<ol>
<li><a href=""https://blog.websecurify.com/2014/08/hacking-nodejs-and-mongodb.html"" rel=""nofollow noreferrer"">https://blog.websecurify.com/2014/08/hacking-nodejs-and-mongodb.html</a></li>
<li><a href=""https://ckarande.gitbooks.io/owasp-nodegoat-tutorial/content/tutorial/a1_-_sql_and_nosql_injection.html"" rel=""nofollow noreferrer"">https://ckarande.gitbooks.io/owasp-nodegoat-tutorial/content/tutorial/a1_-_sql_and_nosql_injection.html</a></li>
</ol>
","<p>Both JPQL injection and SQL injection are examples of the broader category of <a href=""https://en.wikipedia.org/wiki/Code_injection"" rel=""nofollow noreferrer"">Code Injection</a>.</p>

<p><em>Any</em> language that is parsed at runtime is susceptible to Code Injection. </p>

<p>JPQL or <a href=""https://www.objectdb.com/java/jpa/query"" rel=""nofollow noreferrer"">Java Persistence Query Language</a> is similar to SQL in syntax, and in the fact that it is written as strings and parsed at runtime.</p>

<blockquote>
  <p>Building queries by passing JPQL query strings directly to the createQuery method, as shown above, is referred to in JPA as dynamic query construction because the query string can be built dynamically at runtime.</p>
</blockquote>

<p>When the description says ""built dynamically at runtime"" they mean your code formats the JPQL query as a Java string, then submits the string to be parsed and executed. Therefore your code has an opportunity to combine fixed strings with variable content.</p>

<p>Here's an example of using parameters safely to combine a variable with a JPQL statement. This comes from <a href=""https://www.objectdb.com/java/jpa/query/parameter"" rel=""nofollow noreferrer"">https://www.objectdb.com/java/jpa/query/parameter</a></p>

<p><strong>SAFE:</strong></p>

<pre><code>TypedQuery&lt;Country&gt; query = em.createQuery(
    ""SELECT c FROM Country c WHERE c.name = :name"", Country.class);
return query.setParameter(""name"", name).getSingleResult();
</code></pre>

<p>Here's the same query written in an unsafe way, combining the variable directly into the string.</p>

<p><strong>UNSAFE:</strong></p>

<pre><code>TypedQuery&lt;Country&gt; query = em.createQuery(
    ""SELECT c FROM Country c WHERE c.name = '"" + name + ""'"", Country.class);
</code></pre>

<p>Don't use string concatenation to form JPQL queries if you can avoid it. That's how unsafe content sneaks into your JPQL.</p>
"
"<p>I've been scratching my head over this for the past month now and I still can't figure out what is going on.</p>

<p>The problem is that I have a very serious memory leak on a C++ application running on Windows Server 2008, compiled using Visual Studio 2005. This is a managed project. The application starts at around 5-6MB (according to Task Manager) and starts to exhibit symptoms of failure around the ~200MB mark. I know Task Manager is a crude tool, but given the scale of the leak it seems OK to use.</p>

<p>I've narrowed the problem to MySQL Database interaction. If the application does not interact with the database, no memory is leaked.</p>

<p>All database interactions use mysql++. I've followed the build instructions in the man pages on tangentsoft.net.</p>

<p>We've evaluated the code for thread safety (that is, we ensured that each thread only uses mysqlpp object from that thread and no other) and checked to make sure all destructors are called for any dynamically generated objects created using 'new'.</p>

<p>Looking on the internet I keep seeing various reports from users of the mysqlpp class that indicate there is a leak somewhere. In particular, there was a discussion about how the Win C API would leak when mysqlpp was used:</p>

<p><a href=""http://www.phpmarks.com/6-mysql-plus/ffd713579bbb1c3e.htm"" rel=""nofollow"">http://www.phpmarks.com/6-mysql-plus/ffd713579bbb1c3e.htm</a></p>

<p>This discussion seems to conclude in a fix, however, when I try the fixes in my application it still leaks.</p>

<p>I implemented a version of the application cited in the thread above, but with some of the advice from the man pages added:</p>

<pre><code>int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
{
    while (true)
    {
        //Initialise MySQL API
        mysql_library_init(0, NULL, NULL);

        Sleep(50);        
        //Connect to Database.
        mysqlpp::Connection c;        
        c.connect(""myDatabase"",""localhost"",""username"",""password"");

        Sleep(50);
        //Disconnect from Database
        c.disconnect();

        Sleep(50);
        //Free memory allocated to the heap for this thread
        c.thread_end();

        Sleep(50);
        //Free any memory allocated by MySQL C API
        mysql_library_end();

        Sleep(50);
    }
    return 1;
}
</code></pre>

<p>I added the Sleep(50) just to throttle each stage of the loop, so that each function has time to ""settle down"". I know it probably isn't necessary but at least this way I can eliminate that as a cause.</p>

<p>Nevertheless, this program leaks quite rapidly (~1mb per hour).</p>

<p>I've seen similar questions to mine asked in a few places, with no conclusions made :(</p>

<p>So i'm not alone with this issue. It occurs to me that the mysqlpp class has a reputation for usefulness and so must be quite robust. Given that is the case, I still can't see what i've done wrong. Does anyone have some experience of mysqlpp with Visual Studio 2005 that might shed light on the problem? </p>

<p>Cheers,
Adam.</p>

<p><strong>EDIT</strong></p>

<p>I created another example using a pointer, just in case c was being duplicated in the loop:</p>

<pre><code>//LEAKY
int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
{
    mysqlpp::Connection * c;

    while (true)
    {
        mysql_library_init(0, NULL, NULL);

        c = new mysqlpp::Connection;

        Sleep(50);

        c-&gt;connect(""myDatabase"",""localhost"",""username"",""password"");

        Sleep(50);
        c-&gt;disconnect();

        Sleep(50);
        c-&gt;thread_end();

        Sleep(50);
        mysql_library_end();

        Sleep(50);

        delete c;
        c = NULL;
    }       
    return 1;
}
</code></pre>

<p>This also leaks. I then created a control example based on this code, which doesn't leak at all:</p>

<pre><code>//NOT LEAKY
int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
{
    char * ch;

    while (true)
    {
        mysql_library_init(0, NULL, NULL);

        //Allocate 4000 bytes
        ch = new char [4000];

        Sleep(250);

        mysql_library_end();

        delete ch;
        ch = NULL;

    }
return 1;
}
</code></pre>

<p>Note that I also left the calls to the MySQL C API here to prove that it isn't the cause of the leak. I then created an example using a pointer but without the calls to connect/ disconnect:</p>

<pre><code>//NOT LEAKY
int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
{

    mysqlpp::Connection * c;

    while (true)
    {
        mysql_library_init(0, NULL, NULL);

        c = new mysqlpp::Connection;
        Sleep(250);

        mysql_library_end();

        delete c;
        c = NULL;
    }

return 1;
}
</code></pre>

<p>This doesn't leak.</p>

<p>So the difference is just the use of the mysqlpp::connect / disconnect methods. I'll dig into the mysqlpp class itself and try to see whats up.</p>

<p>Cheers,
Adam.</p>

<p><strong>EDIT</strong></p>

<p>Here is an example of the leaky code where checks are made.</p>

<pre><code>//LEAKY
int _tmain(int argc, TCHAR* argv[], TCHAR* envp[])
{
    mysqlpp::Connection * c;

    while (true)
    {
        mysql_library_init(0, NULL, NULL);

        c = new mysqlpp::Connection;

        Sleep(50);

        if ( c-&gt;connect(""myDatabase"",""localhost"",""username"",""password"") == false )
        {
            cout &lt;&lt; ""Connection Failure"";
            return 0;
        }

        Sleep(50);
        c-&gt;disconnect();

        Sleep(50);
        c-&gt;thread_end();

        Sleep(50);
        mysql_library_end();

        Sleep(50);

        delete c;
        c = NULL;
    }       
return 1;
}
</code></pre>

<p>Cheers,
Adam.</p>
","<p>mysqlpp::String has <code>operator int()</code> so your code snippet should work.  What problem are you having with it?</p>

<p>If you want to be more explicit, you can use mysqlpp::String's <a href=""http://tangentsoft.net/mysql++/doc/html/refman/classmysqlpp_1_1String.html#a12"" rel=""nofollow noreferrer"">conv</a> function:</p>

<pre><code>int i = futureItemsets[j].conv&lt;int&gt;(0);
timeFrameItemsets.push_back(i);
</code></pre>
"
"<p>I wanted to open in IntelliJ a NonStop/SQLMX database using the driver com.tandem.t4jdbc.</p>

<p>After the configuration the connection is successful, I can also open the table with the double click, but it looks like the column are not immediately recognized and the label is loaded instead of the name in the column header. </p>

<p>When I connect I have this error:</p>

<pre><code>java.lang.NullPointerException
  at com.tandem.t4jdbc.SQLMXResultSetMetaData.getColumnLabel(SQLMXResultSetMetaData.java:145)
  at com.intellij.database.remote.jdbc.impl.RemoteResultSetMetaDataImpl.getColumnLabel(RemoteResultSetMetaDataImpl.java:69)
  at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:346)
  at sun.rmi.transport.Transport$1.run(Transport.java:200)
  at sun.rmi.transport.Transport$1.run(Transport.java:197)
  at java.security.AccessController.doPrivileged(Native Method)
  at sun.rmi.transport.Transport.serviceCall(Transport.java:196)
  at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:568)
  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:826)
  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.lambda$run$0(TCPTransport.java:683)
  at java.security.AccessController.doPrivileged(Native Method)
  at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:682)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
  at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:276)
  at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:253)
  at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:162)
  at java.rmi.server.RemoteObjectInvocationHandler.invokeRemoteMethod(RemoteObjectInvocationHandler.java:227)
  at java.rmi.server.RemoteObjectInvocationHandler.invoke(RemoteObjectInvocationHandler.java:179)
  at com.sun.proxy.$Proxy199.getColumnLabel(Unknown Source)
  at sun.reflect.GeneratedMethodAccessor351.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.intellij.execution.rmi.RemoteUtil.invokeRemote(RemoteUtil.java:179)
  at com.intellij.execution.rmi.RemoteUtil.access$300(RemoteUtil.java:39)
  at com.intellij.execution.rmi.RemoteUtil$RemoteInvocationHandler.invoke(RemoteUtil.java:275)
  at com.sun.proxy.$Proxy200.getColumnLabel(Unknown Source)
  at com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil$ResultSetWrapper.&lt;init&gt;(MetaDataUtil.java:86)
  at com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil$RemoteResultSetWrapper.&lt;init&gt;(MetaDataUtil.java:224)
  at com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil$ResultSetWrapper.wrap(MetaDataUtil.java:96)
  at com.intellij.database.introspection.jdbcMetadataWrappers.ClosableIt$ResultSetDelegateIt.&lt;init&gt;(ClosableIt.java:253)
  at com.intellij.database.introspection.jdbcMetadataWrappers.ClosableIt$ResultSetClosableIt.&lt;init&gt;(ClosableIt.java:184)
  at com.intellij.database.introspection.jdbcMetadataWrappers.UserDefinedTypeIt.&lt;init&gt;(UserDefinedTypeIt.java:17)
  at com.intellij.database.introspection.jdbcMetadataWrappers.DatabaseMetaDataWrapper.userDefinedTypes(DatabaseMetaDataWrapper.java:649)
  at com.intellij.database.introspection.GenericIntrospector.introspectTypesInSchema(GenericIntrospector.java:451)
  at com.intellij.database.introspection.GenericIntrospector.lambda$introspectSchemasByCatalogs$7(GenericIntrospector.java:214)
  at java.lang.Iterable.forEach(Iterable.java:75)
  at com.intellij.database.introspection.GenericIntrospector.lambda$forEachSchemaInCatalog$15(GenericIntrospector.java:249)
  at com.intellij.database.introspection.GenericIntrospector.forEachCatalog(GenericIntrospector.java:244)
  at com.intellij.database.introspection.GenericIntrospector.forEachSchemaInCatalog(GenericIntrospector.java:249)
  at com.intellij.database.introspection.GenericIntrospector.introspectSchemasByCatalogs(GenericIntrospector.java:211)
  at com.intellij.database.introspection.GenericIntrospector.introspectSchemas(GenericIntrospector.java:200)
  at com.intellij.database.introspection.GenericIntrospector.lambda$null$0(GenericIntrospector.java:157)
  at org.jetbrains.dekaf.core.BaseFacade.inSession(BaseFacade.java:125)
  at com.intellij.database.introspection.GenericIntrospector.lambda$introspectAuto$1(GenericIntrospector.java:151)
  at com.intellij.database.model.impl.BaseModel.modify(BaseModel.java:114)
  at com.intellij.database.model.impl.BaseModel.modify(BaseModel.java:99)
  at com.intellij.database.model.impl.BaseModel.modify(BaseModel.java:84)
  at com.intellij.database.introspection.GenericIntrospector.introspectAuto(GenericIntrospector.java:151)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.introspectDatabases(DatabaseModelLoader.java:419)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.lambda$null$2(DatabaseModelLoader.java:312)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.withFacade(DatabaseModelLoader.java:533)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.lambda$introspect$3(DatabaseModelLoader.java:295)
  at com.intellij.database.dataSource.DataSourceSyncManager.lambda$null$0(DataSourceSyncManager.java:40)
  at com.intellij.database.dataSource.DatabaseConnectionManager$Executor.perform(DatabaseConnectionManager.java:363)
  at com.intellij.database.dataSource.DatabaseConnectionManager$Executor.lambda$sync$2(DatabaseConnectionManager.java:302)
  at com.intellij.database.dataSource.AsyncUtil.withAsyncFriendly(AsyncUtil.java:158)
  at com.intellij.database.dataSource.DatabaseConnectionManager$Executor.sync(DatabaseConnectionManager.java:298)
  at com.intellij.database.dataSource.DatabaseConnectionManager$Builder.sync(DatabaseConnectionManager.java:112)
  at com.intellij.database.dataSource.DataSourceSyncManager.lambda$static$1(DataSourceSyncManager.java:39)
  at com.intellij.database.dataSource.DataSourceSyncManager$SyncProcessor$1.perform(DataSourceSyncManager.java:242)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.introspect(DatabaseModelLoader.java:292)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.lambda$run$0(DatabaseModelLoader.java:272)
  at com.intellij.database.dataSource.LocalDataSource.performBatch(LocalDataSource.java:1184)
  at com.intellij.database.dataSource.DatabaseModelLoader$IntrospectionSession.run(DatabaseModelLoader.java:270)
  at com.intellij.database.dataSource.DataSourceSyncManager$SyncProcessor.performSync(DataSourceSyncManager.java:244)
  at com.intellij.database.dataSource.AsyncUtil.lambda$null$6(AsyncUtil.java:55)
  at com.intellij.database.dataSource.AsyncUtil.lambda$underProgress$14(AsyncUtil.java:127)
  at com.intellij.openapi.progress.impl.CoreProgressManager.a(CoreProgressManager.java:543)
  at com.intellij.openapi.progress.impl.CoreProgressManager.executeProcessUnderProgress(CoreProgressManager.java:488)
  at com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress(ProgressManagerImpl.java:94)
  at com.intellij.database.dataSource.AsyncUtil.underProgress(AsyncUtil.java:133)
  at com.intellij.database.dataSource.AsyncUtil.underProgress(AsyncUtil.java:127)
  at com.intellij.database.dataSource.AsyncUtil.lambda$captureIndicator$7(AsyncUtil.java:55)
  at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)
  at com.intellij.openapi.application.impl.ApplicationImpl$1.run(ApplicationImpl.java:315)
  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745) (no stack trace).
java.lang.NullPointerException
</code></pre>
","<p>This might help you, althought this is more standard SQL and im not sure how much variation comes into sqlmx</p>

<pre><code>SELECT 
    TableName = t.NAME,
    TableSchema = s.Name,
    RowCounts = p.rows
FROM 
    sys.tables t
INNER JOIN 
    sys.schemas s ON t.schema_id = s.schema_id
INNER JOIN      
    sys.indexes i ON t.OBJECT_ID = i.object_id
INNER JOIN 
    sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id
WHERE 
    t.is_ms_shipped = 0
GROUP BY
    t.NAME, s.Name, p.Rows
ORDER BY 
    s.Name, t.Name
</code></pre>

<p>Obviously this is an example, replace example data and table info with yours</p>
"
"<p>I have started HyperSQL like this:</p>

<pre><code>java -cp hsqldb.jar org.hsqldb.server.Server --database.0 file:/data/db --dbname.0 some_db 
</code></pre>

<p>Then if I try to connect to it like:</p>

<pre><code>DriverManager.getConnection(""jdbc:hsqldb:hsql://localhost/some_db"", ""SA"", """");
</code></pre>

<p>Everything works fine. Now when I add new user like:</p>

<pre><code>CREATE USER new_user PASSWORD ""some_password"" ADMIN;
</code></pre>

<p>I can not connect to HyperSQL server using new user data (also after restarting):</p>

<pre><code>   DriverManager.getConnection(""jdbc:hsqldb:hsql://localhost/some_db"", ""new_user"", ""some_password"");
</code></pre>

<p>Any suggestions?</p>
","<p>Why don't you just try it out locally? I can't seem to find whether it has a limit on the amount of login attempts, but you could try it out with a local instance of the database and just doing something like:</p>

<pre><code>for (int i = 1; i &lt;= 1000; i++) {
    System.out.println(""Trying to log in, try number "" + i);
    DriverManager.getConnection(""jdbc:hsqldb:hsql://localhost/fsdb"", ""SA"", ""some_password"");
    try {
        Thread.sleep(100);
    } catch(InterruptedException ex) {
        System.out.println(""Sleep was interrupted."");
        Thread.currentThread().interrupt();
    }
}
</code></pre>

<p><strong>Edit:</strong> your database server should probably not be open to connections from all external IP's anyways. </p>

<p>So if there's no reason to allow all external IP's to make connections to the database server, you should make a whitelist with the help of <a href=""http://hsqldb.org/doc/guide/listeners-chapt.html#lsc_acl"" rel=""nofollow"">this guide</a>.</p>
"
"<p>I created a <code>SQLite3</code> database and protected it with a password (""test"") thanks to the application <code>DB browser for SQLite</code>.
In order to connect to my database via <code>Python</code>, I need to provide the password but I can't figure out how to do that. I tried the following code:</p>

<pre><code>conn=sqlite3.connect(""mydatabase.db"", Password=""test"")
cur=conn.cursor()
</code></pre>

<p><strong>EDIT:</strong></p>

<p>My <code>SQLite3</code> database has been encrypted with <code>SQLCipher</code> (see image).
<a href=""https://i.stack.imgur.com/kSXYf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kSXYf.png"" alt=""enter image description here""></a>
If I run the following code:</p>

<pre><code>conn=sqlite3.connect(""mydatabase.db"")
cur=conn.cursor()
</code></pre>

<p>I get this error:</p>

<pre><code>sqlite3.DatabaseError: file is encrypted or is not a database
</code></pre>

<p>How can I pass the password in order to connect with my <code>db</code> via Python?</p>

<p><strong>EDIT 2</strong></p>

<p>Here a brief summary of what I try to achieve. I am developing an application with <code>Python 3</code> requiring a pre-populated database but this database needs to be protected with a password.
After extensive research, it seems complicated to connect an encrypted <code>SQLite3</code> database via <code>Python 3</code>. A library calls <code>pysqlcipher</code> exists but only for Python 2.7. My next question will be maybe too broadly and I apology in advance. Is there another portable database that exists allowing me to protect it with a password and still get access to Python?
Another idea that I have in mind in order to troubleshoot my problem is to use the <code>zipfile</code> library. This <a href=""https://www.sqlite.org/zipfile.html"" rel=""nofollow noreferrer"">link</a> mentions that the <code>zipfile</code> module does not support encryption but its not clear if encryption refers to the <code>SQLite3</code> database or to the zip file. The idea would be to zip my unprotected <code>DB</code> into a protected zip file as it seems I can do that (<a href=""https://docs.python.org/3/library/zipfile.html#zipfile.ZipFile.setpassword"" rel=""nofollow noreferrer"">link</a>).
The goal of this edit is to get new ideas on how to solve my problem. Thanks</p>
","<p>You need the <a href=""https://www.zetetic.net/sqlcipher/"" rel=""nofollow noreferrer"">SQLCipher</a> module to read that database. The default SQLite3 module has no support for that. See <a href=""https://github.com/sqlitebrowser/sqlitebrowser/wiki/Encrypted-Databases"" rel=""nofollow noreferrer"">https://github.com/sqlitebrowser/sqlitebrowser/wiki/Encrypted-Databases</a></p>
"
"<p>We're using <a href=""https://github.com/jinzhu/gorm"" rel=""nofollow noreferrer"">gorm</a> and I'd like to be able to specify database specific annotations. For convenience, in development/test we use an sqlite3 database, then MySQL in production.</p>

<p>Unfortunately sqlite3 doesn't accept <code>CHARACTER SET</code> and <code>COLLATE</code> keywords. Which means that the following breaks:</p>

<pre><code>type User struct {
    Name  string `gorm:""primary_key;type:varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci""`
}
</code></pre>

<p>Has anyone found a work around for this? I'd rather not use mysql in test and I'd also rather not manually manage the columns.</p>
","<p>The usual pattern for what you're trying to achieve uses <a href=""https://golang.org/pkg/sync/#WaitGroup"" rel=""nofollow noreferrer""><code>WaitGroup</code></a>.</p>

<p>I think the problem you're facing is that <code>i</code> is captured by each goroutine and it keeps getting incremented by the outer loop. Your inner loop starts at <code>i</code> and since the outer loop has continued, each goroutine starts at 5.</p>

<p>Try passing the iterator as parameter to the goroutine so that you get a new copy each time.</p>

<pre><code>func someFunction(){
    ....
    gos := 5
    var wg sync.WaitGroup
    wg.Add(gos)

    for i:=0; i&lt; gos; i++ {
        go func(n int) {
            defer wg.Done()
            for j:=n; j&lt;len(tables); j+=gos {
                r, err := db.Exec(tables[j])

                fmt.Println(r)

                if err != nil {
                    methods.CheckErr(err, err.Error())
                }
            }
        }(i)
    }
    wg.Wait();     
}
</code></pre>

<p>I'm not sure what you're trying to achieve here, each goroutine does <code>db.Exec</code> on all the tables above the one it started with so the first one treats all the tables, the second one treats all but the first one and so on. Is this what you intended?</p>
"
"<p>I have tried to prepare <code>sqlite3_prepare_v2</code> and <code>sqlite3_step</code> the following two statements:</p>

<pre><code>@""delete from students;

@""insert into students select * from buffer.students;""
</code></pre>

<p>with <code>'buffer'</code> being the database being attached as the source.</p>

<p>However, this does not guarantee both two statements were executed in bundle. </p>
","<p>You can get the path to your database using:</p>

<pre><code>NSString *path = [[NSBundle mainBundle] pathForResource:@""someDB"" ofType:@""sqlite""];
</code></pre>

<p>Change the parameters to match your database. And then provide that path to your framework so you can access it.</p>
"
"<p>I am trying to connect to mysqlshell client from my linux command line
when I try to connect, using</p>

<pre><code>mysqlsh mysql -h IP -u user -ppassword -e ""show databases""
mysqlsh mysql -h IP:port -u user -ppassword -e ""show databases""
mysqlsh mysql -h IP:port -u -port 3306 user -ppassword -e ""show databases""
</code></pre>

<p>I get the below error.</p>

<p>Conflicting options: provided host differs from the host in the URI.</p>

<p>Could you please help with this?</p>
","<p>The proper format of <code>mysqlsh</code> is a bit different:</p>

<pre><code>mysqlsh --user=user --password=password --port=5721 --host &lt;hostname or IP&gt;
</code></pre>
"
"<p>I am using <a href=""https://github.com/rbock/sqlpp11"" rel=""nofollow noreferrer"">Roland Bock's sqlpp11</a> library for <code>mysql</code> queries and <a href=""https://github.com/HowardHinnant/date"" rel=""nofollow noreferrer"">Howard Hinnant's date</a> library for operation on date in my project. </p>

<p>And getting the following error in one of my update query.</p>

<pre><code>/usr/local/include/sqlpp11/rhs_wrap.h: In instantiation of struct sqlpp::rhs_wrap_t&lt;date::year_month_day, false&gt;:
/usr/local/include/sqlpp11/assignment.h:63:12:   required from struct sqlpp::assignment_t&lt;sqlpp::column_t&lt;changestreet::Goals, changestreet::Goals_::GoalEndDate&gt;, date::year_month_day&gt;
sqlOperations/sqlppDbConnection.cpp:286:65:   required from bool setEmergencyFundGoal(T1, T2, T2, T3, T3) [with T1 = int; T2 = const char*; T3 = double]
main.cpp:705:113:   required from here
/usr/local/include/sqlpp11/rhs_wrap.h:119:43: error: no type named _traits in class date::year_month_day
     using _traits = typename Expr::_traits;
</code></pre>

<p>And here is that update statement</p>

<pre><code>auto efGoal = db_cs.run(update(g).set(g.goalAmount = emergencyFund,
                                                  g.goalEndDate = contributionEndDate, // Line number 286
                                                  g.goalContributionStartDate = currentDate(),
                                                  g.goalContributionEndDate = contributionEndDate,
                                                  g.goalInitialContribution = initialContribution,
                                                  g.goalMaximumAchievableAmount = emergencyFund,
                                                  g.goalCreatedOn = currentDateTime(),
                                                  g.goalUpdatedOn = currentDateTime()
                                                  ).where(g.goalName == goalName
                                                          and g.goalType == goalType
                                                          and g.usersUserId == userId)
                                    );
</code></pre>

<p>And here is the value that is being used in rhs</p>

<pre><code>auto contributionEndDate = lastDateOfMonth(currentDate(), date::months{contributionTenure}) ;
</code></pre>

<p>Here is the definition of contribution <code>lastDateOfMonth()</code> function.</p>

<pre><code>date::year_month_day lastDateOfMonth(date::year_month_day givenDate, date::months monthsNum) {
    date::year_month_day newDate = year_month_day{givenDate} + monthsNum;
    newDate = newDate.year()/newDate.month()/last;
    return newDate;
}
</code></pre>

<p>And the <code>currentDate()</code> function</p>

<pre><code>date::year_month_day currentDate() {
    auto currentTime = system_clock::now();
    auto currentDate = floor&lt;days&gt;(currentTime);
    return currentDate;
}
</code></pre>

<p>Column name <code>goal_end_date</code> is of type <code>DATE</code> in mysql table structure. </p>
","<p>I needed to change the mysql datatype from <code>DATE</code> to <code>DATETIME</code> and after that <code>system_clock::time_point</code> type variable assignment variables works like charm.</p>

<p>It will also work with mysql <code>DATE</code> dataype with variable assignment type <code>system_clock::day_point</code>. </p>

<p>P.S : I needed to modify the two given functin with return type of <code>syste_clock::time_point</code>.</p>
"
"<p>We're trying to use <a href=""https://github.com/dyatchenko/ServiceBrokerListener"" rel=""nofollow noreferrer"" title=""SqlDependencyEx"">SqlDependencyEx</a> to track updates made to a database table.</p>

<p>This is how we create an update listener:</p>

<pre><code>var listener = new SqlDependencyEx(ConfigUtils.GetConnectionString(),
              ConfigUtils.GetDbName(), ""codeTriggers"", 
              listenerType: SqlDependencyEx.NotificationTypes.Update);
</code></pre>

<p>Here is the update statement which we manually trigger in sql management studio:</p>

<pre><code>UPDATE codeTriggers
SET type = 'clearListsCache'
WHERE type = 'clearCache'
</code></pre>

<p><strong>PROBLEM</strong></p>

<p>We do not get any notifications.
What are we missing?</p>

<p>When we define our listener as follows (without specifying any type of notification type):</p>

<p><code>var listener = new SqlDependencyEx(ConfigUtils.GetConnectionString(), ConfigUtils.GetDbName(), ""codeTriggers"");</code></p>

<p>and we run the same update statement (updating the column to a different value then actual), then we get Insert and Delete notifications.</p>

<p>Is this expected behavior?</p>
","<p>Problem solved, heres the code:</p>

<pre><code>private const string CONNECTION_STRING = ""Server=LFTCMCPTP83;Database=Database;Trusted_Connection=True;MultipleActiveResultSets=true; Integrated Security=false;User ID=used_id;Password=password"";
private const string DATABASE_NAME = ""db_name"";
private const string TABLE_NAME = ""table_name"";
private const string SCHEMA_NAME = ""dbo"";
private SqlDependencyEx sqlDependency = new SqlDependencyEx(CONNECTION_STRING, DATABASE_NAME,
TABLE_NAME, SCHEMA_NAME);
listener.TableChanged += (o, args) =&gt;
{
   //Code...
};
listener.Start();
listener.Stop();
</code></pre>

<p>Best Regards</p>
"
"<p>SQL/MP is an SQL dialect used on proprietary HP NonStop SQL database systems which go back to the 1980ies... (Who has ever worked on a system that is as old as they are, huh? :))</p>

<p>SQL/MP is only somewhat ANSI compliant, for example <code>CURRENT_DATE</code> does not exist. I have however a hard time figuring out what functions <em>do</em> exist that can help me to get the current date in my queries.</p>

<p>Can anyone help me with this dinosaur of an SQL language?</p>
","<p>Try CURRENT or CURRENT_TIMESTAMP depending on the context.</p>

<p>The HP NonStop SQL/MP Reference Manual is excellent for this sort of thing - one advantage of using an older product, the documentation is good.</p>
"
"<p>the documentation seems to be somewhat out of date (last edit was in 2007), and a quick look through related forums shows a few references to this problem:</p>

<p><a href=""https://rt.cpan.org/Public/Bug/Display.html?id=90700"" rel=""nofollow"">https://rt.cpan.org/Public/Bug/Display.html?id=90700</a></p>

<p>is anyone using SQLFairy to visualize a modern version of SQLServer? Is this supported/meant to work at all?</p>
",""
"<p>I'm writing a <a href=""https://www.liquibase.org/javadoc/liquibase/change/custom/CustomSqlChange.html"" rel=""nofollow noreferrer"">CustomSqlChange</a> for the first time and want to test the outcome by running it on my current database. Of course I could start up the application and execute all change sets via liquibase (including the one that executes my <code>CustomSqlChange</code>), but that takes a lot of time.</p>

<p>Is there a way to manually execute the java class implementing <code>CustomSqlChange</code> from my IDE (IntelliJ) as if it would be from liquibase? Could one maybe even debug that execution?</p>
","<p>You can create a separate changelog file, where only your's custom change will be included. Point Liquibase to use it instead of base one. This will give you ability to debug it as well.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;databaseChangeLog .....&gt;
    &lt;changeSet id=""custom-change"" author=""author"" runOnChange=""true"" &gt;
        &lt;customChange param=""..."" /&gt; 
    &lt;/changeSet&gt;
&lt;/databaseChangeLog&gt;
</code></pre>
"
"<p>I cannot seem to get the CommandTimeout to work in the code below.  I set the timeout value to 1 second just to test to ensure it works but it seems to be ignored.  The SELECT statement takes about 15 seconds to run and it runs to completion.  I am expecting a timeout exception to occur after 1 second.  I have searched the internet to find an example of how to do this and I have not been able to find anything.  What am I doing wrong?</p>

<pre><code>DbProviderFactory factory = DbProviderFactories.GetFactory(""System.Data.SqlClient"");
DataSet dataSet = new DataSet();
DbCommand command = factory.CreateCommand();
command.Connection = factory.CreateConnection();
command.Connection.ConnectionString = ""Server=localhost;Database=MyDatabase;User Id=;Password=****"";
command.CommandType = CommandType.Text;
command.CommandText = ""SELECT * FROM table"";
command.CommandTimeout = 1;

using (DbDataAdapter adapter = factory.CreateDataAdapter())
{
    adapter.SelectCommand = command;
    adapter.Fill(dataSet);
}
</code></pre>

<p><br/>
<strong>UPDATE:</strong>
I wrote a SELECT command that takes longer than 30 seconds to run.  This also does not timeout.  I even commented out my CommandTimeout line and it still ran to completion.  So, it seems that even the default CommandTimeout of 30 seconds is being ignored.  I am stumped...any help would be greatly appreciated.</p>

<p><br/>
<strong>UPDATE:</strong>
I think I might have figured it out.  It seems when I wrote a really complex SELECT statement the command timeout exception occurred.  I can only guess that what is happening it that, with my simple SELECT command, it is taking less than 1 second to execute the command but then it takes an additional 15 seconds to load the data set.  Am I on the right track here?  Does this seem likely?</p>
","<p>You can't break normal string literals across multiple lines, also your closing quote is misplaced:</p>

<pre><code>SqlConnection con = new SqlConnection(""Data Source=local;Initial Catalog=DB;User ID=sa;Password=pass"");
</code></pre>

<p>Or use a verbatim literal, which you <em>can</em> break across multiple lines:</p>

<pre><code>SqlConnection con = new SqlConnection(
    @""Data Source=local;
      Initial Catalog=DB;
      User ID=sa;
      Password=pass"");
</code></pre>

<p>That said, your code is vulnerable to <a href=""http://en.wikipedia.org/wiki/SQL_injection"" rel=""nofollow"">SQL injection</a> attacks. For your own sake, and the sake of your users, you really should use parameterized queries instead of concatenating your SQL queries like that. </p>

<p>Here's a quick example:</p>

<pre><code>using(var con = new SqlConnection(...))
{
    var cmd = new SqlCommand(""select * from Table1 where ID = @ID"", con);
    con.Open();
    cmd.Parameters.AddWithValue(""@ID"", LbLID.Text.Trim());
    var da = new SqlDataAdapter(cmd);
    var dt = new DataTable();
    da.Fill(dt);
    lblS1.Text = dt.Rows[0][4].ToString();
    lblS1.DataBind();
}
</code></pre>

<p>Some other tips: You should avoid using <code>select *</code> queries, since your database schema might change, and that would break any existing code. It would be better to select only the column you're interested in and make a simple call to <a href=""http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlcommand.executescalar%28v=vs.110%29.aspx"" rel=""nofollow""><code>ExecuteScalar</code></a>.</p>
"
"<p>I am trying to install Microsoft Drivers for PHP for SQL Server using the document provided by Microsoft. Link is <a href=""https://github.com/Microsoft/msphpsql"" rel=""nofollow noreferrer"">https://github.com/Microsoft/msphpsql</a></p>

<p>It provides installation steps for Ubuntu and Redhat but not Amazon Linux. To install Microsoft Drivers on Amazon, I followed steps provided for Redhat ( not sure if it is correct). When I run the command </p>

<pre><code>sudo ACCEPT_EULA=Y yum install msodbcsql mssql-tools
</code></pre>

<p>got the below error</p>

<pre><code>Error: Package: msodbcsql-13.1.7.0-1.x86_64 (packages-microsoft-com-prod)
           Requires: unixODBC &gt;= 2.3.1
           Available: unixODBC-2.2.14-14.7.amzn1.i686 (amzn-main)
               unixODBC = 2.2.14-14.7.amzn1
</code></pre>

<p>The error clearly says that to install msodbcsql, unixODBC version should be >= 2.3.1. But the updated/latest unixODBC package available for amazon is unixODBC-2.2.14.</p>

<p>I need some help to install Microsoft Drivers for PHP on Amazon Linux so that I can use <em>Sqlsrv</em> PHP functions to connect SQL server.</p>

<p>PHP7, Apache, and SQL server are already set up.</p>
","<p>After googling and trying all the ways, I found the answer to my own question.</p>

<p>Instead of using</p>

<pre><code>sudo ACCEPT_EULA=Y yum install msodbcsql mssql-tools
</code></pre>

<p>I used</p>

<pre><code>sudo ACCEPT_EULA=Y yum install msodbcsql-13.0.1.0-1 mssql-tools-14.0.2.0-1
</code></pre>

<p>and the issue fixed.</p>
"
"<p>I had configured my Java application to connect through multiple routers to the InnoDB cluster so that if one of my router stops working other router can take up the routing task (I have 6 router configured )</p>

<p>Now  Can some one help me in finding through which router request has been sent in a multirouter setuP ?</p>
","<p>The client should always check for errors.  This is a necessity for any system, because network errors, power outages, etc, can occur in any configuration.</p>

<p>When the client discovers a connection failure (failure to connect / dropped connection), it should start over by reconnecting and replaying the transaction it is in the middle of.</p>

<p>For transaction integrity, the client <em>must</em> be involved in the process; recovery <em>cannot</em> be provide by any proxy.</p>
"
"<p>I'm using MySQL Server 7.0 on Windows Server 2008 and am trying to return the result of a GROUP_CONCAT in a function . General SQL is as follows:</p>

<pre><code>DELIMITER ;

DROP FUNCTION IF EXISTS MyFunction;
DELIMITER $$

CREATE FUNCTION MyFunction(MyVar INT)
    RETURNS VARCHAR(255)
BEGIN
    SELECT @MyRetVar = GROUP_CONCAT(MyColumn)
    FROM   MyTable
 WHERE  MyID = MyVar;
    RETURN @MyRetVar;
END$$

DELIMITER ;
</code></pre>

<p>This yields the following result:</p>

<blockquote>
  <p>ERROR 1415 (0A000): Not allowed to
  return a result set from a function</p>
</blockquote>

<p>I checked the manual (<a href=""http://dev.mysql.com/doc/refman/5.0/en/group-by-functions.html"" rel=""nofollow"">http://dev.mysql.com/doc/refman/5.0/en/group-by-functions.html</a>) and read</p>

<blockquote>
  <p>The result type is TEXT or BLOB unless
  group_concat_max_len is less than or
  equal to 512, in which case the result
  type is VARCHAR or VARBINARY.</p>
</blockquote>

<p>I changed the value of group_concat_max_len from its default value to 512 and also 256 in My.ini (and restarted the MySQL service). I've verified the change using</p>

<blockquote>
  <p>mysql> show variables like ""%concat%"";</p>
</blockquote>

<p>Any help is appreciated!</p>
","<p>I think it's actually your debugging <code>SELECT</code> calls.</p>

<p>From the <a href=""http://dev.mysql.com/doc/refman/5.0/en/create-procedure.html"" rel=""nofollow"">docs</a>:</p>

<blockquote>
  <p>Statements that return a result set can be used within a stored procedure but not within a stored function. This prohibition includes SELECT statements that do not have an INTO var_list clause...</p>
</blockquote>
"
"<p>In database fields are characters to html entities converted, so if I try to search for words like <em>bung</em> or <em>rzte</em> nothing is found. I tried to add in my SQL query <code>COLLATE utf8_general_ci</code> but I get there following error: <code>COLLATION 'utf8_general_ci' is not valid for CHARACTER SET 'latin1'</code></p>

<p>My fields what I search are in utf8_general_ci encoding.</p>

<p>Is there possible to make such search or to convert by submit some letters to html entities?</p>
","<p>something like this?</p>

<pre><code>var field = ""&lt;a href='/Default.aspx?Email=""+dr[""id""]+""'&gt;""+ dr[""First_Name""].ToString()+""&lt;/a&gt;"";
</code></pre>
"
"<p>I've a MySQL master-slave replication setup where the slave is marked as read-only=ON. I've some stored procedures that I want to route through ProxySQL to the slave and prefer not to run on the master. Each stored procedure logs into an exception-logging table using an exception-logging stored procedure if any exception arises. I can set the definer for this as root or some other user with SUPER privilege. But that would break replication when the same set of primary key auto increment values would sync from the master for this exception table.</p>

<p>Would using replicate-ignore-table make sense? How safe is that with ROW based replication? Please suggest if there's any better way of doing this.</p>
","<p>Port 6032 is for the administrative CLI.
Instead, you would instead want to connect to port 6033 which listens to all traffic and does load balancing towards the backend PXC nodes.
Good luck!</p>
"
"<p>I have a custom sitemapprovider which loads pages from the database.</p>

<blockquote>
  <p><strong>Pages</strong> (pageid, fk_pageid (parent), title, url, show_in_menu)</p>
</blockquote>

<p>I would like to globalize/localize the <code>title</code> of the page. What's the best method? </p>
","<p>Have a look at <code>mysqldump</code>.</p>

<pre><code>mysqldump db_name tbl_name &gt; backupfile.sql
</code></pre>

<p>will dump the a db / table and overwrite backupfile.sql if it exists.</p>

<p>Use <code>rsync</code> or <code>scp</code> to copy it to another host if needed.</p>
"
"<p>I'm using the Aster Basket_Generator function to calculate a basket from a table of purchases (retail_purchases).  I can create the basket without issue using the following code:</p>

<pre><code>SELECT order_number, gsi_sku1, gsi_sku2, Count(1) 
FROM basket_generator(
    ON retail_purchases 
    PARTITION BY order_number 
    BASKET_SIZE(2) 
    BASKET_ITEM('gsi_sku')
    ACCUMULATE('order_number')
) 
WHERE gsi_sku1 in (11001788, 12002389) 
GROUP BY 1, 2, 3; 
LIMIT 10;
</code></pre>

<p>What I'd like to do additionally, is to calculate the average value of each basket.  Ideally that would be returned to me as one column, but I'd be completely satisfied with the average sale price for each item in the basket.</p>

<p>I've tried the following:</p>

<pre><code>SELECT order_number, gsi_sku1, gsi_sku2, avg(sales_amt), Count(1) 
FROM basket_generator(
    ON retail_purchases 
    PARTITION BY order_number 
    BASKET_SIZE(2) 
    BASKET_ITEM('gsi_sku')
    ACCUMULATE('order_number', 'sales_amt')
) 
WHERE gsi_sku1 in (11001788, 12002389) 
GROUP BY 1, 2, 3; 
LIMIT 10;
</code></pre>

<p>But the avg(sales_amt) column doesn't seem to be returning the correct value.  What's the recommended way to calculate basket aggregates when using the Aster Basket_Generator analytic function?</p>
","<p>There is a good research study based on a lab study which compares the time to develop and execute analytic functions in both the Teradata Aster discovery platform and Hadoop/Hive. The systems were run side by side to show what workloads are appropriate for each system.  There is a good working example of a ""day in the life"" of an analyst and the time/effort required.(disclosure: i work for Teradata which acquired Aster Data 2 years ago) <a href=""http://www.asterdata.com/resources/assets/ESG-Lab-Validation-Teradata-Aster-MapReduce-Platform.pdf"" rel=""nofollow"">http://www.asterdata.com/resources/assets/ESG-Lab-Validation-Teradata-Aster-MapReduce-Platform.pdf</a> </p>
"
"<p>It's pretty hard to understand the nuget packages to install in a Xamrin solution when searching around the web, there are dozens of packages, dozens of different solutions.</p>

<p>Right now, our solution has 2 projects, an Android one and a PCL one. Our model and data access is defined in the PCL. Our platform implementation is defined in the Android one.</p>

<p>We need SQLite, SQLite.Net (for the data annotations and table relations), and SQLiteExtentions for the *withchildren methods.</p>

<p>We are stuck in old versions because anytime we try to update anything, our frail magical way in which our installed packages are working together just falls apart. We do need to upgrade or find a way to add SQLCipher to this weird snafu of nuget packages.</p>

<p>Our current packages installed, which works:</p>

<p><strong>Android project</strong></p>

<ul>
<li><a href=""https://www.nuget.org/packages/Mono.Data.Sqlite.Portable/"" rel=""nofollow noreferrer"">Mono.Data.Sqlite.Portable 1.0.3.5</a> (not sure why...) </li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net.Core-PCL"" rel=""nofollow noreferrer"">SQLite.Net.Core-PCL 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net.Platform.XamarinAndroidN"" rel=""nofollow noreferrer"">SQLite.Net.Platform.XamarinAndroidN 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net-PCL"" rel=""nofollow noreferrer"">SQLite.Net-PCL 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLiteNetExtensions"" rel=""nofollow noreferrer"">SQLiteNetExtensions 1.3.0</a> (required for GetWithChildren etc.)</li>
<li><a href=""https://www.nuget.org/packages/SQLitePCL.raw"" rel=""nofollow noreferrer"">SQLitePCL.raw 0.9.3</a></li>
</ul>

<p><strong>PCL Project</strong> (model definition and data access methods)</p>

<ul>
<li><a href=""https://www.nuget.org/packages/Mono.Data.Sqlite.Portable/"" rel=""nofollow noreferrer"">Mono.Data.Sqlite.Portable 1.0.3.5</a> (not sure why...) </li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net.Core-PCL"" rel=""nofollow noreferrer"">SQLite.Net.Core-PCL 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net.Platform.XamarinAndroidN"" rel=""nofollow noreferrer"">SQLite.Net.Platform.XamarinAndroidN 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLite.Net-PCL"" rel=""nofollow noreferrer"">SQLite.Net-PCL 3.1.1</a></li>
<li><a href=""https://www.nuget.org/packages/SQLiteNetExtensions"" rel=""nofollow noreferrer"">SQLiteNetExtensions 1.3.0</a> (required for GetWithChildren etc.)</li>
<li><a href=""https://www.nuget.org/packages/SQLitePCL.raw"" rel=""nofollow noreferrer"">SQLitePCL.raw 0.9.3</a></li>
</ul>

<p>Currently, if we update the SQLiteExtensions to 2.0, a bunch of other SQLite nuget packages are installed, breaking the frail stability of our data access code (fails on the *WithChildren methods with:</p>

<pre><code>Severity    Code    Description Project File    Line    Suppression State
Error   CS1929  'SQLiteConnection' does not contain a definition for         
'GetWithChildren' and the best extension method overload 
'ReadOperations.GetWithChildren&lt;TEntity&gt;(SQLiteConnection, object, bool)' 
requires a receiver of type 'SQLiteConnection'  
</code></pre>

<p>We'd also need to incorporate SQLiteCipher and no combinations of packages can be made to work with our solution.</p>

<p>Our Android platform specific implementation:</p>

<pre><code>#region Usings

using OURPCLLib.DataAccess;
using Serilog;
using SQLite.Net;
using SQLite.Net.Interop;
using SQLite.Net.Platform.XamarinAndroid;
using System;
using System.IO;

#endregion Usings

public class AndroidSQLiteDatabase : SQLiteDatabaseAccess
{
    protected ISQLitePlatform SQLitePlatform
    {
        get { return new SQLitePlatformAndroidN(); }
    }

    protected override SQLiteConnection GetConnection()
    {
        var conn = new SQLiteConnection(
            SQLitePlatform,
            ""dbpathforus.sqlite"",
            SQLiteOpenFlags.ReadWrite |
            SQLiteOpenFlags.FullMutex |
            SQLiteOpenFlags.ProtectionCompleteUnlessOpen |
            SQLiteOpenFlags.Create |
            SQLiteOpenFlags.SharedCache);

        return conn;
    }

}
</code></pre>

<p>The (simplified) base data access class in the PCL:</p>

<pre><code>#region Usings

using OURPCLLib.DataAccess.Entities;
using SQLite.Net;
using SQLite.Net.Attributes;
using SQLite.Net.Interop;
using SQLiteNetExtensions.Extensions;
using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Linq.Expressions;

#endregion Usings

public abstract class SQLiteDatabaseAccess
{
    protected abstract SQLiteConnection GetConnection();

    // Example of one of the many methods accessing the DB using SQLite.Net
    public bool Any&lt;TEntity&gt;(Expression&lt;Func&lt;TEntity, bool&gt;&gt; expression)
       where TEntity : class, IBaseEntity, new()
    {        
        using (var currentConnection = this.GetConnection())
        {
            return currentConnection.Table&lt;TEntity&gt;().Where(expression).FirstOrDefault() != null;
        }
    }

    // Example of one of the methods accessing the DB using SQLiteExtentions
    public TEntity GetWithChildren&lt;TEntity&gt;(int id, bool recursive = false)
        where TEntity : class, IBaseEntity, new()
    {
        using (var currentConnection = this.GetConnection())
        {
            return currentConnection.GetWithChildren&lt;TEntity&gt;(id, recursive);
        }
    }
}
</code></pre>

<p>Anyone can help us as to how to use SQLite with SQLIte.net, SQLiteExtentions and SQLIte cipher on a project like ours? (data access in a pcl and connection implementation in an android project?</p>
","<p>I was considering closing this question but felt that leaving it and the answer would be more useful to others.  The SQLite JSON1 documentation does in fact explain how to do such things though the explanations are not immediaately apparent.  The SQL to issue in this instance would be</p>

<pre><code>SELECT users.name FROM users,json_each(users.phone,'$.cell') WHERE json_each.value = '234567890'
</code></pre>
"
"<p>I wrote a few utPL/SQL Test on a PL/SQL Package, put them into a maven project and let them execute by jenkins. I wonder if there is a way to get rid of the test packages created in the database? It feels a bit weird that the test artefacts stay in the database.</p>

<p>I would either consider a maven goal within the utPL/SQL Plugin to delete the created test packages or having a seperate goal, where I can execute PL/SQL to drop the packages. I would also appreciate other ideas.</p>
","<p>I have just recently gone through the same pain. Most utPLSQL examples out there are for utPLSQL v2. It transpires appears that the assertions have been deprecated, and have been replaced by ""Expects"". I found <a href=""http://www.oraclethoughts.com/utplsql/what-is-the-equivalent-of-utassert-eqtable-utplsql-v2-v3/"" rel=""nofollow noreferrer"">a great blog post by Jacek Gebal</a> that describes this. I've tried to put this and other useful links <a href=""https://documentation.red-gate.com/display/DDFO/Unit+testing"" rel=""nofollow noreferrer"">a page about how unit testing fits into Redgate's Oracle DevOps pipeline</a> (I work for Redgate and we often get asked how to best implement automated unit testing for Oracle).</p>
"
"<p>I have a WCF Data Service and I intend to use some <strong>session-based table functions</strong> (that creates temporary tables that are usable in the current session) upon <em>insert</em> or <em>update</em>.</p>

<p>I tried to use the <code>SaveChanges</code> method like this:</p>

<pre><code>public partial class MyContext: DbContext
{
    public override int SaveChanges()
    {          
        var res = SetValues(true);
        var s = Database.SqlQuery&lt;string&gt;(""SELECT [Key] FROM  TempContextView"").ToList();
        System.IO.File.AppendAllText(@""c:\Temp\session.txt"", $""SIZE S: {s.Count}, script res: {res}"");
        foreach (var element in s)
        {            
            System.IO.File.AppendAllText(@""c:\Temp\session.txt"", $""RES: {element}""); //never reached
        }
        return base.SaveChanges();
    }

    public int SetValues(bool insert)
    {
        System.IO.File.AppendAllText(@""c:\Temp\session.txt"", ""SetV: "" + insert);
        return Database.ExecuteSqlCommand(insert ? ""INSERT INTO TempContextView ([Key],[Value]) VALUES('Flag', '1')"" : ""DELETE FROM TempContextView WHERE[Key] = 'Flag'"");
    }
}
</code></pre>

<p>The <em>TempContextView</em> is a <em>view</em> that provides a <em>temporary table</em> created by a function:</p>

<pre><code>SELECT  TOP (32) [Key], Value
FROM    Schema1.getContextTable()
ORDER BY [Key]

function [Schema1].[getContextTable]()
  RETURNS @Context TABLE([Key] varchar(126), [Value] varchar(126))
  WITH SCHEMABINDING
as...
</code></pre>

<p>However, when I select the values from the table that is created by the function, it returns nothing (the query size is 0, yet the insert returns 1).</p>

<p>Does it mean, I can't use EF with sessions? Or every EF function uses <em>its own context</em>?
As the session table is used by other triggers, I need to have the proper key value.</p>

<p>What should I do about this? Any hint if EF is able to use these type of functionality?</p>

<p><strong>UPDATE:</strong></p>

<p>I have learned that EF uses <code>exec sp_reset_connection</code> before each executed command, and it <em>resets all the temporary variables and tables</em>.</p>

<p>So I tried to create a transaction to force EF to execute the commands in one session:</p>

<pre><code>  using (var scope = new TransactionScope(TransactionScopeOption.RequiresNew))
  {
    Database.ExecuteSqlCommand(""INSERT INTO TempContextView ([Key],[Value]) VALUES('Flag', '1')""); //session #1?
    base.SaveChanges(); //session #2? :(
    scope.Complete();
  }
</code></pre>

<p>It still creates new sessions, so I can't really merge the two commands. </p>

<p><a href=""https://i.stack.imgur.com/7P7rv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7P7rv.png"" alt=""enter image description here""></a></p>

<p>Any suggestions?</p>
","<p>A <code>'I/O Error: Socket closed'</code> exception means that the connection between the JDBC client and the server was closed for some reason.</p>

<p>My advice will be use some connection pool library if you are not using any pool.</p>

<p>The reason could be one of the following:</p>

<ol>
<li>You tried to execute a query on a connection that has been closed.  </li>
<li>May be OS terminated the connection because of a network problem.   </li>
<li>Database server is shut down.</li>
<li>Because the result is too long to be read or the query is taking lot of time to execute</li>
</ol>
"
"<p>As per the <a href=""http://docs.hangfire.io/en/latest/configuration/configuring-logging.html"" rel=""nofollow noreferrer"">hangfire documentation</a> </p>

<blockquote>
  <p>Starting from Hangfire 1.3.0, you are not required to do anything, if
  your application already uses one of the following libraries through
  the reflection (so that Hangfire itself does not depend on any of
  them). Logging implementation is automatically chosen by checking for
  the presence of corresponding types in the order shown below.</p>
  
  <p>Serilog<br>
  NLog<br>
  Log4Net<br>
  EntLib<br>
  Logging<br>
  Loupe<br>
  Elmah  </p>
</blockquote>

<p>I have ASP.NET Core 2.0 application that is using <code>Serilog</code> like below</p>

<pre><code>public class Program
{
    public static void Main(string[] args)
    {
        BuildWebHost(args).Run();
    }

    public static IWebHost BuildWebHost(string[] args) =&gt;
        WebHost.CreateDefaultBuilder(args)
        .UseStartup&lt;Startup&gt;()
        .UseApplicationInsights()
        .UseUrls(""http://*:40006"")
        .ConfigureAppConfiguration((hostingContext, config) =&gt;
        {
           // removed for bravity
        })
        .ConfigureLogging((hostingContext, logging) =&gt;
        {                
                Log.Logger = new LoggerConfiguration()
                .ReadFrom.Configuration(hostingContext.Configuration)
                .CreateLogger();

                logging.AddSerilog();

        })
        .Build();
}
</code></pre>

<p>The application is configured to use Hangfire. During background job processing if there is any exception occurs, Hangfire re-tries the job 10 times with increasing delay as expected, and it shows the exceptions in dashboard.</p>

<p><strong>Issue</strong><br>
The Hangfire dashboard shows the exception on UI however it does not log the exception into configured Serilog sink.</p>

<p>Note: The Hangfire dashboard shows exception but it formats the exception  <a href=""https://stackoverflow.com/questions/49245955/hanfire-not-logging-custom-exception-with-metadata"">see here</a> which hides critical information about the exception.
I think if it logs the exception, Serilog logger would log the complete exception.</p>
","<p>Sorry.</p>

<p>I just realized someone renamed [Key] column in one of hangfire tables.
Is all working.</p>
"
"<p>I'm struggling with the following situation:  I need to extract that '10000' after  </p>

<pre><code>...name=""stateid""&gt; &lt;from&gt;10000&lt;/from&gt;
</code></pre>

<p>I've managed to extract some that I need from the first node audittrail </p>

<pre><code>SELECT ref.value ('@datetime', 'nvarchar(364)') as [LastTimeActioned],
ref.value ('@changedby', 'nvarchar(364)') as [Actioned_By]
FROM Audit 
CROSS APPLY audit.Xml.nodes ('/audittrail') R(ref)


&lt;audittrail id=""137943"" datetime=""29-Feb-2016 15:42:06"" changedby=""quality"" type=""update""&gt;
   &lt;fields&gt;
     &lt;field id=""10022"" name=""Content Control""&gt;
      &lt;mergedValue&gt;dsad&lt;/mergedValue&gt;
      &lt;from /&gt;
      &lt;to&gt;dsad&lt;/to&gt;
     &lt;/field&gt;
     &lt;field id=""10027"" name=""Document Controller""&gt;
      &lt;mergedValue&gt;quality&lt;/mergedValue&gt;
      &lt;from /&gt;
      &lt;to&gt;quality&lt;/to&gt;
    &lt;/field&gt;
    &lt;field id=""10028"" name=""Document Owner""&gt;
      &lt;mergedValue&gt;quality&lt;/mergedValue&gt;
      &lt;from /&gt;
      &lt;to&gt;quality&lt;/to&gt;
    &lt;/field&gt;
    &lt;field id=""10029"" name=""Document Type""&gt;
      &lt;mergedValue&gt;Contract/Agreement&lt;/mergedValue&gt;
      &lt;from /&gt;
      &lt;to&gt;Contract/Agreement&lt;/to&gt;
    &lt;/field&gt;
    &lt;field id=""10067"" name=""StateId""&gt;
      &lt;from&gt;10000&lt;/from&gt;
      &lt;to&gt;10000&lt;/to&gt;
    &lt;/field&gt;
    ....
  &lt;/fields&gt;
&lt;/audittrail&gt;
</code></pre>
","<p>There are several problems that can cause such behavior :</p>

<ul>
<li><blockquote>
  <p>This error occurs when you try to use a special character in a SQL
  statement. If a special character other than $, _, and # is used in
  the name of a column or table, the name must be enclosed in double
  quotations.</p>
</blockquote></li>
<li><blockquote>
  <p>This error may occur if you've pasted your SQL into your editor from
  another program. Sometimes there are non-printable characters that may
  be present. In this case, you should try retyping your SQL statement
  and then re-execute it.</p>
</blockquote></li>
<li><blockquote>
  <p>This error occurs when a special character is used in a SQL WHERE
  clause and the value is not enclosed in single quotations.</p>
</blockquote></li>
</ul>
"
"<p>I created the local service. My server name is like <code>sqlitesync.com</code>. In my database folder created the <code>configuration (web.config)</code> and <code>WCF web service (AuroraSyncService.svc)</code>. When I test this service in browser it shows nothing. </p>

<p>i followed this tutorial <a href=""http://sqlite-sync.com/documentation/installation/"" rel=""nofollow noreferrer"">Link</a></p>

<p>When start the service <a href=""http://sqlitesync.com/sqlitesyncservice/AuroraSyncService.svc"" rel=""nofollow noreferrer"">http://sqlitesync.com/sqlitesyncservice/AuroraSyncService.svc</a> in browser it shows the empty like <img src=""https://i.stack.imgur.com/GtOfC.png"" alt=""enter image description here""></p>

<p>When I create my own (<code>index.html</code>) <code>http://sqlitesync.com/index.html</code> it works fine</p>

<p><img src=""https://i.stack.imgur.com/Wjxww.png"" alt=""http://sqlitesync.com/index.html""></p>

<p>I don't know how to solve this also I am new to SQL Server.</p>

<p>Anybody provide a solution to solve this..</p>

<p>configuration code:</p>

<pre><code>&lt;connectionStrings&gt;
&lt;add name=""AuroraSyncCn"" connectionString=""Data Source=(local);Initial Catalog=SQLite-Sync.com;User ID=sa;Password=pass"" providerName=""System.Data.SqlClient"" /&gt;
</code></pre>

<p></p>

<p><img src=""https://i.stack.imgur.com/mz36K.png"" alt=""enter image description here""></p>
","<p>Create those stored procedure in a separate schema (use <code>CREATE SCHEMA</code>) and then GRANT or REVOKE execution right on all stored procedures in that schema for the users in charge.</p>
"
"<p>Today my university lecturer in a databases class brought up PSP. </p>

<p>I have used the LAMP stack and am into open source, so I have a bias against Oracle. I understand however, that Oracle databases are used in industry and so it is good to learn how to use it.</p>

<p>However when we covered PL/SQL Server pages (Stored procedures in an Oracle database that output HTML and can be access with a web browser) I cringed. Is my bias getting in the way too much, and there are advantages to using it compared to using something like PHP and dynamic SQL queries? How popular is PSP in web applications using Oracle?</p>
","<p><a href=""http://download.oracle.com/docs/cd/B19306_01/appdev.102/b14251/adfns_psp.htm"" rel=""noreferrer"">PL/SQL Server Pages</a> (PSPs) are (sort of) a variant of the more general-purpose <a href=""http://download.oracle.com/docs/cd/B19306_01/appdev.102/b14251/adfns_web.htm"" rel=""noreferrer"">PL/SQL Web Toolkit</a> (also known as OWA; it was called the Oracle Web Agent in early days).</p>

<p>There is not much buzz around PSPs, so I assume they are used by very few. The PL/SQL Web Toolkit (OWA), however, is another matter altogether. Lots and lots of people use this for building (data-centric) web applications with the power of PL/SQL.</p>

<p>Also, in 2006 or thereabouts, Oracle released its free (no-cost option) for the database called <a href=""http://apex.oracle.com/i/index.html"" rel=""noreferrer"">Oracle Application Express</a> (Apex), which is built on top of OWA, and is now (at the time of this writing) in version 4.1.</p>

<p>Apex gives you a browser-based IDE for developing PL/SQL web applications (powered by OWA). Apex also includes a whole bunch of built-in features such as session management, authentication and authorization, interactive reports, flash charts, page templates, navigation, Ajax support, and much, much more.</p>

<p>So if you are interested in looking into web development with Oracle, I suggest you forget about PSPs and look into Apex instead.</p>

<p>In terms of popularity, according to <a href=""http://www.oracle.com/technetwork/developer-tools/apex/apex--introduction-157585.ppt"" rel=""noreferrer"">this Powerpoint</a> presentation from Oracle from 2010, Apex had at that time:</p>

<ul>
<li>80,000 downloads per year</li>
<li>4,000,000 page views per week on apex.oracle.com</li>
<li>Third largest discussion forum on forums.oracle.com</li>
<li>100 consulting companies delivering Apex services</li>
<li>60 blogs about Apex</li>
<li>Used as the user interface for the Oracle Store and Oracle Audit Vault</li>
</ul>
"
"<p>I am facing a very strange issue of executing query into mysql manager tool</p>

<p>i exported my database from phpmyadmin using wamp server and now on client's server i am trying to run that query and it gives me that error.</p>

<p>any one help me how to resolve this issue?there is no blank space in this query.</p>

<p>any help would be appreciated.
<img src=""https://i.stack.imgur.com/KQqJw.png"" alt=""enter image description here""></p>
","<p>Is  numberOfRows.$_SESSION always 20?</p>

<h2>follow up</h2>

<p>ok, now I understand your problem as it's better described in the comments :))</p>

<p>what you miss is <strong>SQL_CALC_FOUND_ROWS</strong></p>

<p>use it like that:</p>

<pre><code>SELECT SQL_CALC_FOUND_ROWS * FROM tab_a limit 50;
SELECT FOUND_ROWS();
</code></pre>

<p>if you omit it , it will always return 50, but if you include it than the actual count as it would be without the limit</p>
"
"<p>We are migrating data from Oracle to Hadoop and there is a requirement 
 to continue  use the existing reporting tool(Crystal Report) to generate reports from Hadoop (instead of Oracle)</p>

<p>In the current scenario we are using an Oracle Stored PROC to do few aggregations /logic.
Now with the above requirement and migrated data, The Options that we considered are </p>

<ol>
<li><p>Use <code>HPLSQL</code> (instead of Oracle <code>PLSQL</code>) and call it from Crystal Reports.However it appears that there are challenges with this approach because unlike Oracle Stored Procs, 
<code>HPLSQL</code> Stored Procs are not registered in the DB catalouge and hence Crystal reports may  be unable to find / access those <code>HPLSQL</code> STored PROCS.</p></li>
<li><p>Create the custom aggregation /logic in java and expose it as a webservice  which can be invoked /consumed from Crystal Reports</p></li>
</ol>

<p>The help/guidance needed here is to find out whether </p>

<p>a) Did someone successfully called <code>HPLSQL</code> Stored Procs form an external tool/ reports like Crystal over <code>ODBC/JDBC</code>. If yes, can you share the details?</p>

<p>b) Is there  a better option other than the above mentioned two options to  achieve the requirement?</p>

<p>c) Are there any challenges in using webservices to fetch data and run reports</p>

<p>Thanks in advance and any help is really appreciated. </p>
","<p>What does that logging procedure do so that it might fail? Is it not a pure <code>INSERT</code> + <code>COMMIT</code>?</p>

<p>Anyway: the simplest (and probably the worst) option is to use exception handler section, e.g.</p>

<pre><code>your_procedure is
begin
  do something;
exception
  when others then null;
end;
</code></pre>

<p>It would be OK if you really don't care whether something <em>bad</em> happened or not, but - what's the purpose, then? You think you did something, Oracle doesn't complain, the procedure does nothing and you have no idea what's going on.</p>

<p>Therefore, you'd better make sure that this ""logging"" procedure doesn't fail.</p>
"
"<p>I am working on an <a href=""https://imodeljs.github.io/iModelJs-docs-output/"" rel=""nofollow noreferrer"">iModel.js</a> project that involves an OpenPlant schema. The iModel file has been uploaded to the <a href=""https://imodeljs.github.io/iModelJs-docs-output//learning/imodelhub/"" rel=""nofollow noreferrer"">iModelHub</a> using a MicroStation bridge. I have been able to successfully;</p>

<ol>
<li>Get an IMS access token</li>
<li>Open an iModelDb by passing in the access token, the CONNECT project id and the iModel id</li>
<li>Run ECSql queries against the iModelDb object using the <a href=""https://imodeljs.github.io/iModelJs-docs-output/reference/imodeljs-backend/imodels/imodeldb/executequery/"" rel=""nofollow noreferrer"">executeQuery</a> command.</li>
</ol>

<p>I am trying to get a list of all the Process Piping and Process Equipment classes and display them in a pane. I am doing a <code>SELECT * FROM Bis.Element GROUP BY ECClassId</code>. This gives me a list of all the elements and corresponding classIDs but, this list seems to be incomplete. I only get 4 process equipment classes (<code>CentrifugalPump</code>, <code>Drum</code>, <code>Vessel</code>, and <code>Tank</code>) and 5 process piping classes (<code>ConcentricPipeReducer</code>, <code>Nozzle</code>, <code>GateValve</code>, <code>Pipeline</code>, <code>PipeRun</code>, and <code>ThreeWayValve</code>). </p>

<p>How do I get a list of all the classes? </p>
","<p>It looks like you are only querying the instance data of the iModel. The resulting limited list of classes contains the actual elements that exist in the iModel file. The file contains physical (or functional) elements for only those 4 process equipment class and 5 process piping class types, which is why those are the ones you see.</p>

<p>Luckily, each iModel file has an <code>ECDbMeta ECSchema</code>. An <code>ECDbMeta ECSchema</code> is a schema that provides information ABOUT the schemas themselves (that in turn give the instance data its meaning). All class definitions inside the schemas exist in the iModel as their own separate dataset. This information exists for the sole purpose making the context of the instance data portable. </p>

<p>For example, when you do a <code>SELECT * FROM Bis.Element</code>, you get a list of all the <code>Bis.Elements</code> that currently exist within the iModel. As you noticed, all these elements have their own class Ids. A <code>Nozzle</code> is just another <code>Bis.Element</code> but it is of the <code>Nozzle</code> subclass type that is defined in the <code>ProcessPipingFunctional</code> schema. </p>

<p>You can query the <code>ECDbMeta ECSchema</code>, to get a list of all classes defined within Process Piping and Piping equipment schemas.  Here are some sample <code>SELECT</code> statements:</p>

<blockquote>
  <p>""SELECT Name AS name, Alias AS alias, ECInstanceId AS id FROM ECDbMeta.ECSchemaDef ORDER BY Name"" </p>
</blockquote>

<p>The above statement will give you a list of all the schemas imported into the iModel you are querying. Once you get this list, you can get the <code>schema.id</code> for the process and piping schemas you are looking for. You can then use those ids to run the following query:</p>

<blockquote>
  <p>""SELECT Name AS name FROM ECDbMeta.ECClassDef WHERE ECClassDef.Schema.Id=? ORDER BY Name"" </p>
</blockquote>

<p>This query will give you the list of all the classes defined within those schemas. </p>

<p>For more information on the metaschemas: <a href=""https://imodeljs.github.io/iModelJs-docs-output/learning/ecdbmeta.ecschema/"" rel=""nofollow noreferrer"">https://imodeljs.github.io/iModelJs-docs-output/learning/ecdbmeta.ecschema/</a></p>
"
"<p>I am not a DBA but i am using postgresql for production server and i am using postgresql 10 database. I am using Bigsql and i started replication of my production server to other server and on replication server everything is working but on my production server their is no space left. And after du command on my production server i am getting that <code>pg_wal</code> folder have 17 gb file and each file is of 16 mb size.</p>

<p>After some google search i change my postgresql.conf file as:</p>

<pre><code>wal_level = logical
archive_mode = on
archive_command = 'cp -i %p /etc/bigsql/data/pg10/pg_wal/archive_status/%f'
</code></pre>

<p>i install postgresql 10 from Bigsql and did above changes.</p>

<p>After changes the dir /pg_wal/archive_status had 16 gb of log. So my question is that should i delete them manually or i have to wait for system delete them automatically. 
And is that if i write archive_mode to on should that wal file getting removed automatically??</p>

<p>Thanks for your precious time.</p>
","<p>This depends on how you do your backups and whether you'd ever need to restore the database to some point in time.</p>

<p>Only a full offline filesystem backup (offline meaning with database turned off) or an on-line logical backup with pg_dumpall will not need those files for a restore.</p>

<p>You'd need those files to restore a filesystem backup created while the database is running. Without them the backup will fail to restore. Though there exist backup solutions that copy needed WAL files automatically (like <a href=""http://www.pgbarman.org/"" rel=""nofollow noreferrer"">Barman</a>).</p>

<p>You'd also need those files if your replica database will ever fall behind the master for some reason. Or you'd need to restore the database to some past point-in-time.</p>

<p>But these files compress pretty well - should be less than 10% size after compression - you can write your archive_command to compress them automatically instead of just copying.</p>

<p>And you should delete them eventually from the archive. I'd recommend to not delete them until they're at least a month old and also at least 2 full successful backups are done after creating them.</p>
"
"<p>I am trying to convert a date that's stored as a string to a date, e.g.</p>

<p>YYYYMMDD (string) to YYYY-MM-DD (date) </p>

<p>As far as I know there is no conversion function that checks input format and output format, I tried manual logic, e.g. </p>

<pre><code>CASE 
  WHEN CHAR_LENGTH(TRIM(some_string_date)) = 8
  THEN
    CAST(
      SUBSTRING(TRIM(some_string_date) FROM 1 FOR 4)
      || '-'
      || SUBSTRING(TRIM(some_string_date) FROM 5 FOR 2)
      ||'-'
      || SUBSTRING(TRIM(some_string_date) FROM 7 FOR 2)
    as DATE) 
  ELSE
    NULL 
END
</code></pre>

<p>However this is not accepted by Apache SQL Validator, does anyone see problem here?</p>
","<p>Not directly answering the question, but maybe related, date literals are declared with <code>DATE</code> keyword, e.g. you can see examples in the tests in Beam tests: <a href=""https://github.com/apache/beam/blob/b06e28bf9605f232c270993728361bd0fc2ee08a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/BeamSqlDslSqlStdOperatorsTest.java#L559"" rel=""nofollow noreferrer"">one</a>, <a href=""https://github.com/apache/beam/blob/bdd0081b49f8e7df6733dc8e8bc90dda3efc6621/sdks/java/extensions/sql/jdbc/src/test/java/org/apache/beam/sdk/extensions/sql/jdbc/BeamSqlLineTest.java#L90"" rel=""nofollow noreferrer"">two</a> and in Calcite <a href=""https://calcite.apache.org/docs/reference.html#scalar-types"" rel=""nofollow noreferrer"">docs</a>.</p>

<p><strong>Update:</strong></p>

<p>What seems to happen is Calcite adds some indirection when doing <code>CASE</code>. Casting the strings to dates works as expected in general. For example, if  input rows have schema <code>(INT f_int, VARCHAR f_string)</code> and dates are in <code>'YYYYMMDD'</code> (e.g. <code>(1, '2018')</code>, then this works:</p>

<pre><code>SELECT f_int,
   CAST(
    SUBSTRING(TRIM(f_string) FROM 1 FOR 4)
      ||'-'
      ||SUBSTRING(TRIM(f_string) FROM 5 FOR 2)
      ||'-'
      ||SUBSTRING(TRIM(f_string) FROM 7 FOR 2) as DATE)
  FROM PCOLLECTION
</code></pre>

<p>Even directly casting the <code>'YYYYMMDD'</code> works:</p>

<pre><code>SELECT f_int,
   CAST(f_string AS DATE)
FROM PCOLLECTION
</code></pre>

<p>You can see all supported date formats <a href=""https://github.com/apache/beam/blob/06128f27d1780f25c23ca65cc7ace693a78dac80/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/interpreter/operator/BeamSqlCastExpression.java#L45"" rel=""nofollow noreferrer"">here</a>.</p>

<p>But as soon as you wrap it in <code>'CASE ... ELSE NULL'</code>, then Beam/Calcite seem to infer that the expression type is now a <code>'String'</code>. This means that  <code>'THEN CAST(... AS DATE)'</code> succeeds and returns a 'Date', but then it's converted to <code>'String'</code> when wrapped in <code>'CASE'</code>. Then, when returning the result in my test it seems to try to cast it back to <code>'Date'</code>, but the string format now is not <code>'YYYYMMDD'</code> but some other default format. Unfortunately that format is not in the list of supported, so it fails.</p>

<p><strong>Workaround:</strong></p>

<p>As soon as you change <code>'ELSE NULL'</code> to something that's known to be a <code>'Date'</code>, e.g. <code>'ELSE DATE ""2001-01-01""'</code> then it works again as Beam/Calcite don't seem to go though <code>'String'-&gt;'Date'-&gt;'String'-&gt;'Date'</code> path and this works:</p>

<pre><code>SELECT f_int,
  CASE WHEN CHAR_LENGTH(TRIM(f_string)) = 8
    THEN CAST (
       SUBSTRING(TRIM(f_string) FROM 1 FOR 4)
       ||'-'
       ||SUBSTRING(TRIM(f_string) FROM 5 FOR 2)
       ||'-'
       ||SUBSTRING(TRIM(f_string) FROM 7 FOR 2) AS DATE)
    ELSE DATE '2001-01-01'
  END
FROM PCOLLECTION
</code></pre>

<p>I filed <a href=""https://issues.apache.org/jira/browse/BEAM-5789"" rel=""nofollow noreferrer"">BEAM-5789</a> to track a better solution.</p>

<p><strong>Update 2:</strong></p>

<p>So, while Calcite generates the plan telling Beam what to do, it's Beam that actually casts/parses the dates in this case. There's an effort to use Calcite's built-in implementations of basic operations instead of re-implementing everything in Beam:
<a href=""https://github.com/apache/beam/pull/6417"" rel=""nofollow noreferrer"">https://github.com/apache/beam/pull/6417</a> . After this pull request is merged, this <code>CASE ... ELSE NULL</code> path should work automatically, if I'm reading it right (I assume <a href=""https://github.com/apache/calcite-avatica/blob/3cfafde9f5f0f1fc3ddc60b5a4db19762c73b96b/core/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java#L374"" rel=""nofollow noreferrer"">this class</a> will be used for handling date/time values). It will still go through strings, probably unnecessarily, but it should just work.</p>
"
"<p>I am using Dapper with <a href=""https://github.com/andygjp/ExpressionToSql/tree/master/src/ExpressionToSql"" rel=""nofollow noreferrer"">ExpressionToSQL</a> nuget package. I have a query as below:</p>

<pre><code>Set = new Table{Schema = ""Sch"",Name = ""tbl1""};
Columns = x =&gt; new { x.Id};
Conditions = x =&gt; x.Name== request.Name &amp;&amp; x.Date == request.Date;

Where&lt;TQueryIn, object&gt; query = Sql.Select(Columns, Set).Where(Conditions);
...
connection.QueryAsync(query);
</code></pre>

<p>Where the value for <code>request.Name</code> is ""T1"" and <code>request.Date</code> is ""20121020"".</p>

<p>When I run the above-mentioned query, I get the following query:</p>

<pre><code>SELECT a.[Id] FROM [Sch].[tbl1] AS a WHERE a.[Name] = @Name AND a.[Date] = @Date
</code></pre>

<p>As you see the <code>request.Name</code> and <code>request.Date</code>'s values are replaced with <code>@Name</code> and <code>@Date</code>!</p>

<p>What I need to see is:</p>

<pre><code>SELECT a.[Id] FROM [Sch].[tbl1] AS a WHERE a.[Name] = 'T1' AND a.[Date] = '20121020'
</code></pre>

<p>Definitely this is not a problem with Dapper and it is a problem with ExpressionToSQL package.  </p>

<p>Is there any way to overcome this issue, and convert <code>Expression&lt;Func&lt;T,bool&gt;&gt;</code> to a string value with above mentioned output?</p>
","<p>I'm not familiar with Dapper, but after looking at the tests in <a href=""https://github.com/andygjp/ExpressionToSql/blob/master/tests/ExpressionToSql.Dapper.UnitTests/ExtensionTests.cs"" rel=""nofollow noreferrer"">ExpressionToSQL</a> it seems that you can provide the parameters on the second argument of <code>QueryAsync</code>, therefore, I think this should work:</p>

<pre><code>Set = new Table{Schema = ""Sch"",Name = ""tbl1""};
Columns = x =&gt; new { x.Id};
Conditions = x =&gt; x.Name== request.Name &amp;&amp; x.Date == request.Date;

Where&lt;TQueryIn, object&gt; query = Sql.Select(Columns, Set).Where(Conditions);
...
connection.QueryAsync(query, request);
</code></pre>

<p>Also found this, may be useful:
<a href=""https://dapper-tutorial.net/parameter-anonymous"" rel=""nofollow noreferrer"">https://dapper-tutorial.net/parameter-anonymous</a></p>
"
"<p>I would like to compose a query this way:</p>

<pre><code>$query = $orm-&gt;table();
              -&gt;where('foo_id', $foo['id'])
              -&gt;like('foo_name', '%DP') 
              -&gt;fetch();
</code></pre>

<p>The error is:</p>

<blockquote>
  <p>PDOException: SQLSTATE[42S02]: 
      Base table or view not found: 1146 Table 'database_name.like' doesn't exist in /var/www/webapp/app/vendor/morris/lessql/src/LessQL/Database.php:117</p>
</blockquote>
","<p>You can make <code>LIKE</code> through the <code>where()</code> call:</p>

<pre><code>$query = $orm-&gt;table()
          -&gt;where('foo_id', $foo['id'])
          -&gt;where('foo_name LIKE ?', array('%DP')) 
          -&gt;fetch();
</code></pre>
"
"<p>Given a simple table like this: </p>

<pre><code>var1 | var2
  0  |  3
  2  |  4
  6  |  5
</code></pre>

<p>I would need to return the lowest value (var1, as a multiple of 2) for which either a record does not exist (in this case, 4) or var2 is equal to 5 (in this case, 6). I'd need to do it as a single query, but unfortunately I'm new to MySQL functions.</p>

<p>I tried to create a function like this:</p>

<pre><code>DELIMITER //

CREATE FUNCTION Prova ( starting_value INT ) returns int;
BEGIN
   DECLARE testNum INT DEFAULT 0;
  test_loop : LOOP
    IF (SELECT db.tabella WHERE var1 = testNum AND var2 &lt;= 5) OR WHERE NOT EXISTS (
    SELECT * FROM db.tabella WHERE var1 = 'testNum'
) LIMIT 1; THEN
    RETURN testNum;
    END IF;
SET testNum = testNum + 2;
SELECT testNum; 
END LOOP; 

END;
</code></pre>

<p>but it doesn't seem to work.
Thank you.</p>
","<p>you could use the <code>CASE</code> in mysql (see it's doc <a href=""https://dev.mysql.com/doc/refman/5.7/en/case.html"" rel=""nofollow noreferrer"">MySQL CASE Syntax</a>)</p>

<p>I create a MySQL table test here to test and some inserts, see bellow:</p>

<pre><code>CREATE TABLE test (
  id INT,
  hp INT
);
INSERT INTO test (id, hp) VALUES (1, 1);
INSERT INTO test (id, hp) VALUES (2, 0);
INSERT INTO test (id, hp) VALUES (3, 0);
INSERT INTO test (id, hp) VALUES (4, 1);
INSERT INTO test (id, hp) VALUES (5, 0);
INSERT INTO test (id, hp) VALUES (6, 1);
INSERT INTO test (id, hp) VALUES (7, 0);
INSERT INTO test (id, hp) VALUES (8, 1);
INSERT INTO test (id, hp) VALUES (9, 1);
INSERT INTO test (id, hp) VALUES (10, 0);
INSERT INTO test (id, hp) VALUES (11, 0);
INSERT INTO test (id, hp) VALUES (12, 1);
</code></pre>

<p>For what I understand you want to return the list of some table when <code>hp</code> is <code>0 or 1</code>, but when the <code>hp</code> filter is not used you want bring all data regardless of the <code>hp</code> field. Here is a way to do that, see bellow:</p>

<pre><code>SET @hp_from_html = null;

SELECT * FROM test 
WHERE 
 (CASE
    WHEN @hp_from_html is null then true
    ELSE hp = @hp_from_html
  END);
</code></pre>

<p>You can try this code here: <a href=""https://www.db-fiddle.com/f/7qXi81NodZy8wZAidwhqQx/2"" rel=""nofollow noreferrer"">MySQLFiddle</a></p>

<p>I hope that works for you.</p>
"
"<p>I'm trying to create a JSF based web application in netbeans, and was struck with caching sha2 password error, i replaced the JDBC drivers(8.0.13 version) and it sorted out those issues.
Now when i run i application the browser shows this error</p>

<p>An Error Occurred:
Cannot open file:C:\Users\Siva\AppData\Roaming\NetBeans\8.2\config\GF_4.1.1\domain1/config/keystore.jks [Keystore was tampered with, or password was incorrect]</p>

<p>i searched online and tried out some solutions but couldnt get it sorted,
what am i missing?</p>
","<p>You need sum() aggregation and you are getting error becuase your group by clause was putting wrong place - it should be outside of subquery</p>

<pre><code>select SUBSTRING (name, 1, CHARINDEX ('*' , name) - 1) as name, sum(duration) as duration
FROM (
           SELECT b.""identity"" || '*' || session_id || '-' || a.user_id AS ""name""
             ,MIN(time) AS ""start""
             ,MAX(time) AS ""last""
           ,((DATEDIFF('milliseconds', MIN(time), MAX(time))::FLOAT / 1000  ) / 60) AS ""duration""

     FROM abc_app_production.all_events a ,abc_app_production.users b
     where    a.user_id = b.user_id
     AND b.user_type IS NOT NULL
     AND b.""identity"" IS NOT NULL
     AND b.""identity"" NOT IN ('Shubham')
     AND time &gt;= convert(datetime,'2018-10-01') AND time &lt;= convert(datetime,'2018-11-01') 
     group by b.""identity"" || '*' || session_id || '-' || a.user_id
)X group by SUBSTRING (name, 1, CHARINDEX ('*' , name) - 1)
</code></pre>
"
"<p>I am testing configuration of availability groups(AG) with windows Server 2016 with sql server vNext CTP 1.4 within a clusterless environment. After having been to a recent conference, I have had learnt that this clusterless AG configuration is possible with the recent version of both windows OS and vNext. I have setup everything correctly (viz. HADR enable, two nodes pinging each other, endpoints, certificates, logins permissions) and within the correct parameters. Have even successfully executed the tsql script for creating the AG using CLUSTER_TYPE=None on the primary node of this AG. However, it seems that I am unable to join the secondary replica onto this AG and encountering the below screenshot error.  Also, whilst creating the AG I have noticed that the secondary replica is not connected. I get the below error whilst joining the secondary to this AG. </p>

<p><img src=""https://i.stack.imgur.com/E9vj9.png"" alt=""enter image description here""></p>
","<p>I have now managed to rectify the error within my availability group creation by myself after having re-read through the MS docs again. Basically the error was within the AG group TSQL script I had been using to create the this Clusterless AG. Essentially it is (I think) very important to create the AG with just the syntax (CLUSTER_TYPE=NONE) within the CREATE AVAILABILITY GROUP ....TSQL and nothing else viz. (DB_FAILOVER=ON/OFF etc) and after executing the create availability group on node1..... hop over to node2(your read only replica) and execute the  join syntax </p>

<p>ALTER AVAILABILITY GROUP <strong>AGNAME</strong> JOIN WITH (CLUSTER_TYPE = NONE);</p>

<p>Hope it helps in future for someone who's trying to scale read-only AG using the AG'less and/or cluster'less environment. </p>
"
"<p>As an example:</p>

<p>I have two tables in firebird:</p>

<p>TB_CUSTOMER </p>

<ul>
<li>IDCUSTOMER (autoincrement generator)</li>
<li>CUSTOMERNAME</li>
</ul>

<p>TB_PHONE</p>

<ul>
<li>IDPHONE</li>
<li>IDCUSTOMER (foreing key from TB_CUSTOMER)</li>
<li>PHONE</li>
</ul>

<p>I have a registration form developed in Delphi. The table data TB_PHONE are handled using a dbgrid. I can not assign the value of the field IDCUSTOMER in TB_PHONE, because it was not generated by the Firebird generator. How can I make the relationship between the tables? I want to implement it without first saving the table data TB_CUSTOMER. I'm using datamodules with IBDAC.</p>

<p>Any sugest?</p>
","<blockquote>
  <p>I want to implement it without first saving the table data TB_CUSTOMER</p>
</blockquote>

<p>There's your problem.  You need the primary key from the master table before you can save the detail.  That's just the way it works.  But if what you want is to make sure that the values get saved together, you can do that as a transaction.  In Firebird, you can do it like this:</p>

<ul>
<li>Begin a transaction. Exactly how you do that depends on which DB library you're using to access your Firebird database.</li>
<li>Run an <a href=""http://www.firebirdsql.org/refdocs/langrefupd21-insert.html#langrefupd21-insert-returning"" rel=""nofollow"">INSERT INTO ... RETURNING statement</a> to insert the row into your master table and retrieve the generated value as a single operation.</li>
<li>Use the generated PK value to fill in the FK value on your detail table.</li>
<li>Insert the detail row.</li>
<li>Commit the transaction.</li>
</ul>
"
"<p>My team and I are working on a Wordpress site together. We have one person doing data entry on a live server and the rest of us are working locally. What we would like to do is have our local solutions point to the live server database so that we can develop around the actual database content that has been entered.</p>

<p>I have added our IP address as an ""Access Host"" to the Remote MySQL section of our hosting account and confirmed that I am able to connect to the database remotely(using the live server's IP). </p>

<p>Since the <code>home</code>/<code>site</code> url are pointing to our live server domain in the database, it is my understanding that we will need to edit our <code>hosts</code> file in order to mask our local URL as the live URL.</p>

<p>The local URL I use is <a href=""http://localhost:8888/wordpress"" rel=""nofollow noreferrer"">http://localhost:8888/wordpress</a> so I added this line to my hosts file:</p>

<pre><code>127.0.0.1 ourliveserver.com www.ourliveserver.com
</code></pre>

<p>But when I try to access <a href=""http://ourliveserver.com/wordpress"" rel=""nofollow noreferrer"">http://ourliveserver.com/wordpress</a> I am getting <code>ERR_CONNECTION_REFUSED</code></p>

<p>Can anyone tell me where I went wrong?</p>
","<p>Did you try <code>php artisan config:clear</code> to clear the cached config?</p>

<p>Maybe you have cached the old config file on your testing server.</p>
"
"<p>I am working my way around this problem: <a href=""https://stackoverflow.com/questions/48464936/create-multiple-results-from-single-row-joining-either-2-or-3-tables-based-off-c"">Create multiple results from single row joining either 2 or 3 tables based off conditional of table1 results?</a> and while I wish I could take <code>Strawberry's</code> advice, I can't, so I am trying now to do more in C# rather than through the DB, but also trying to be smart about how much CPU I utilize. </p>

<p>For scale, table1 may have 50,000 records which have those 20 <code>codetype</code> fields which have to be evaluated before they can be matched to table2, which has about 2,000 rows, or table3, which could have 200,000 rows. To avoid hammering the DB, I am going to store what is possible in memory, limit results by date as much as possible, but I want to avoid 2,000 foreach loops per 20 codetype matches.</p>

<p>To start off, I am getting the results I need from table2 and loading them into a C# DataTable stored as the variable named <code>descriptionLookup</code>:</p>

<pre><code>id,    description
13    Item 13 Description
15    Item 15 Description
17    Item 17 Description
18    Item 18 Description
21    Item 21 Description
28    Item 28 Description
45    Item 45 Description
</code></pre>

<p>And table3 as <code>lookupTable</code>:</p>

<pre><code>id,  table2id
1    15
33   17
21   28
</code></pre>

<p>doing a simple (not showing all surrounding code, just relevant):</p>

<pre><code>var rawData = new DataTable();
using (OdbcCommand com = new OdbcCommand(""SELECT id, description from table2"", conn))
{
    using (OdbcDataReader reader = com.ExecuteReader())
    {
        rawData.Load(reader);
        conn.Close();
        return rawData;
    }

}
</code></pre>

<p>I then have that assigned to a variable that called the function. Now I have to deal with table1:</p>

<pre><code>codeid1,codeid2,codeid3,...codeid20 ... codetype1,codetype2,codetype3,.....codetype20
18      13      1          33           0         0         1              1
13      21      45         0            0         1         0              0
</code></pre>

<p>Using a foreach row, I need to evaluate each codetype column for a 1 or a 0. When codetype=1 I need to grab the associated codeid, and then do a lookup from the data I am holding in memory as <code>descriptionLookup</code> to see what the table2id is that matches the <code>id</code> in <code>lookuptable</code> and then use that to lookup the description of the associated field in table2.</p>

<p>If codetype is 0, I just need to match <code>codeid</code> with the associated description field in table2.</p>

<p>I am looking at how to lay this out, and all I can think of is:</p>

<pre><code>DataTable descriptionLookup= DB.ExecuteQuery(""SELECT id, description from table2"");
DataTable lookupTable= DB.ExecuteQuery(""SELECT id, table2id from table3"");
DataTable mainData= DB.ExecuteQuery(""SELECT * from from table1"");

foreach (var row in mainData)
{
    var manDataId = row.GetColumn(""id"");
    var subroutine = new Dictionary&lt;string, string&gt;();
    for (var index = 1; index &lt; 20; index++)
    {
        string description;
        if (row.GetColumn(""codetype"" + index) == ""1"")
        {
            int idLookup = row.GetColumn([""codeid"" +index]);
            foreach (var row2 in lookupTable)
            {
                if (row3.GetColumn(""id"") == idLookup)
                {
                    descriptionId = row3.GetColumn(""table2id"");
                    foreach (var row2 in descriptionLookup)
                    {
                        if (row.GetColumn(""id"") == descriptionId)
                        {
                        description = row2.GetColumn(""description"").ToString();
                        }
                    }
                }
            }
        }elseif (row.GetColumn(""codetype"" + index) == ""0"")
        {
            descriptionId = row.GetColumn([""codeid"" +index]);
            foreach (var row2 in descriptionLookup)
            {
                if (row.GetColumn(""id"") == descriptionId)
                {
                description = row2.GetColumn(""description"").ToString();
                }
            }
        }

        subroutine.Add(manDataId.ToString(), description.ToString());
    }

ArrayData.Add(subroutine);
}
</code></pre>

<p>I haven't tried running the above code, so there is probably a problem or two in there, but it gets the point across of looping through thousands of records using <code>foreach (var row3 in idLookup)</code>. The alternative seems to be making a DB query, but that seems more intensive than just looping through what is in memory, but it seems like there is a better way that I am missing on how to get the <code>id</code> or <code>table2id</code> without using a foreach.</p>

<p>To make this the longest looking question in history :) Here is my SQL that I have so far:</p>

<pre><code>SELECT table1.id, table1.field1, table1.field2,
table2.description, fee.amt as fee FROM table2
INNER JOIN table1
ON table2.id = table1.codeid1
OR table2.id = table1.codeid2
OR table2.id = table1.codeid3
OR table2.id = table1.codeid4
OR table2.id = table1.codeid5
OR table2.id = table1.codeid6
OR table2.id = table1.codeid7
OR table2.id = table1.codeid8
OR table2.id = table1.codeid9
OR table2.id = table1.codeid10
OR table2.id = table1.codeid11
OR table2.id = table1.codeid12
OR table2.id = table1.codeid13
OR table2.id = table1.codeid14
OR table2.id = table1.codeid15
OR table2.id = table1.codeid16
OR table2.id = table1.codeid17
OR table2.id = table1.codeid18
OR table2.id = table1.codeid19
OR table2.id = table1.codeid20
INNER JOIN fee ON table2.id = fee.id
WHERE table1.codetype1 = 0
AND table1.codetype2 = 0
AND table1.codetype3 = 0
AND table1.codetype4 = 0
AND table1.codetype5 = 0
AND table1.codetype6 = 0
AND table1.codetype7 = 0
AND table1.codetype8 = 0
AND table1.codetype9 = 0
AND table1.codetype10 = 0
AND table1.codetype11 = 0
AND table1.codetype12 = 0
AND table1.codetype13 = 0
AND table1.codetype14 = 0
AND table1.codetype15 = 0
AND table1.codetype16 = 0
AND table1.codetype17 = 0
AND table1.codetype18 = 0
AND table1.codetype19 = 0
AND table1.codetype20 = 0
</code></pre>

<p>This works great as long as there isn't any of the <code>codetype</code> that have a 1, otherwise that record will be skipped. There will likely not be a single row where all <code>codeid / codetype</code> are filled out, nor will there ever be a case where <code>codetype</code> will be 1 across the board to match the inverse of this query.</p>
","<p>I suppose you mean you want to connect to this database from within an automated test case?
(as opposed to using it as Tosca's database, which would not be supported)</p>

<ul>
<li>When do you get the error message? </li>
<li>How does your test step look like where you connect to the database?</li>
<li>Does the connection string work, when you create a ODBC data source
via Windows?</li>
<li>Which version of Tosca are you using?</li>
</ul>

<p>Did you check and follow the <a href=""https://support.tricentis.com/community/manuals_detail.do?lang=en&amp;version=11.1.0&amp;url=engines_3.0/database/tbox_database_engine.htm"" rel=""nofollow noreferrer"">manual</a>?</p>

<p><strong>Update: Looks like your connection string has the words ""connection string: "" in there? This does not look right.</strong></p>
"
"<p>I've been playing around lately with SQL Data Services. Although (or perhaps because) I can knock out a well-structured relational database in my sleep, I'm struggling to get my head round how to design a performant database in an environment that has (for example) no enforcement of referential integrity and no indexes on columns other than the primary key.</p>

<p>Does anyone know of any guidelines?</p>

<p>Maybe a place to start would be how to create a many-to-many join that can be traversed from either side in a performant manner, even with vast numbers of <strike>rows</strike> entities?</p>
","<p>It seems the phrases that are being used are:</p>

<ul>
<li><p>Spread your data out amongst many containers for best performance</p></li>
<li><p>Modeling your data via Entities </p></li>
<li><p>Process your queries in parallel for best performance</p></li>
<li><p>Caching data in the service hosted middle tier</p></li>
</ul>

<p>This would imply that we have to start thinking like OO modellers rather than in a relational mind-set. Performance seems to rely on the ability to massively parallelise an object query in a smiliar way to creating a LINQ query that can take advantage of parallelisation.</p>
"
"<p>Folks, I have installed the msodbcsql package at least several dozen times.  Never had this issue come up.  Even spent the entire day yesterday trying to fix this.</p>

<p>Step 1: add the apt key and repo to sourced.list.d
Step 2: apt-get install the msodbcsql17, mssql-tools, php odbc_pdo extensions, etc
Step 3: create a very basic test.php that (on all other existing older servers WORKS) makes a test connection to my mssql db.</p>

<p>Fails with:</p>

<pre><code>SQLSTATE[01000] SQLDriverConnect: 0 [unixODBC][Driver Manager]Can't open lib '/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1' : file not found
</code></pre>

<p>Yes, the file exists.</p>

<p>I have tried chmod 755 to the file, still says it does not exist.</p>

<p>I have done a ldd against the file, here is the output:</p>

<pre><code>    linux-vdso.so.1 (0x00007ffe13bf8000)
libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc1e865a000)
librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc1e8452000)
libodbcinst.so.2 =&gt; /usr/lib/x86_64-linux-gnu/libodbcinst.so.2 (0x00007fc1e823d000)
libcrypto.so.1.0.2 =&gt; not found
libkrb5.so.3 =&gt; /usr/lib/x86_64-linux-gnu/libkrb5.so.3 (0x00007fc1e7f67000)
libgssapi_krb5.so.2 =&gt; /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2 (0x00007fc1e7d1c000)
libssl.so.1.0.2 =&gt; not found
libuuid.so.1 =&gt; /lib/x86_64-linux-gnu/libuuid.so.1 (0x00007fc1e7b15000)
libstdc++.so.6 =&gt; /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fc1e778c000)
libm.so.6 =&gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc1e73ee000)
libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc1e71d6000)
libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc1e6fb7000)
libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc1e6bc6000)
/lib64/ld-linux-x86-64.so.2 (0x00007fc1e8c65000)
libltdl.so.7 =&gt; /usr/lib/x86_64-linux-gnu/libltdl.so.7 (0x00007fc1e69bc000)
libk5crypto.so.3 =&gt; /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 (0x00007fc1e678a000)
libcom_err.so.2 =&gt; /lib/x86_64-linux-gnu/libcom_err.so.2 (0x00007fc1e6586000)
libkrb5support.so.0 =&gt; /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 (0x00007fc1e637b000)
libkeyutils.so.1 =&gt; /lib/x86_64-linux-gnu/libkeyutils.so.1 (0x00007fc1e6177000)
libresolv.so.2 =&gt; /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007fc1e5f5c000)
</code></pre>

<p>Two libraries seem to be missing.  I found I have libssl1.1 and 1.0.0.  I have tried creating symlinks to BOTH the 1.1 and 1.0.0 versions of the libssl shared objects.  Still get the same error.</p>

<p>Again, YES - I have copied/pasted the exact path the error screams about.  The path is correct.</p>

<p>I have tried using isql command line as well - same exact error.  So this is certainly something with the odbc&lt;->msodbcsql library.  This is NOT specific to the PHP/PDO/ODBC stuff.</p>

<p>Interestingly, the mssql-cli command line tool (from the Microsoft repo) DOES work.  Running ldd against it says it is NOT a dynamic executable.</p>

<p>Installed versions of all libraries involved:</p>

<pre><code>unixodbc = 2.3.7
libodbc1 = 2.3.7
odbcinst = 2.3.7
msodbcsql = 17.3.1.1-1
</code></pre>

<p>All versions are the latest available from Microsoft repo.  All under Ubuntu 18.04.  I just re-tried the procedure (I have done countless times) on my laptop - same result.  Complaining that the driver library is not found.</p>

<p>Output of odbcinst -j is as follows:</p>

<pre><code>unixODBC 2.3.7
DRIVERS............: /etc/odbcinst.ini
SYSTEM DATA SOURCES: /etc/odbc.ini
FILE DATA SOURCES..: /etc/ODBCDataSources
USER DATA SOURCES..: /root/.odbc.ini
SQLULEN Size.......: 8
SQLLEN Size........: 8
SQLSETPOSIROW Size.: 8
</code></pre>

<p>Yes, the files exist at /etc/odbcinst.ini and /etc/odb.ini.</p>

<p>Contents of /etc/odbcinst.ini:</p>

<pre><code>[ODBC Driver 17 for SQL Server]
Description=Microsoft ODBC Driver 17 for SQL Server
Driver=/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1
UsageCount=1
</code></pre>

<p>Running ""stat /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1"":</p>

<pre><code>Size: 2046672       Blocks: 4000       IO Block: 4096   regular file
</code></pre>

<p>I have found very little info regarding this issue.  The few posts I have came across either did NOT solve the issue, or have NOT been answered.</p>

<p>So.. am I missing something here?  Or should I file a bug report?</p>

<p>On a side note, I have had nothing but issues with Ubuntu 18.04 and the msodbc stuff since day one.  Originally, there was the libcurl3/4 issue.  I suppose that has been fixed now.  But seems this may be... a bug?</p>
","<p>Found the answer myself, could have come up with this earlier, but I'm going to leave this here in case someone else stumbles across this issue:</p>

<p>I already thought it had something to to with access rights/permissions, as it was basically working from everywhere but Apache, but I couldn't figure out why, until I found out that this is caused by systemd:</p>

<p>Systemd has a feature preventing services from accessing <code>/tmp</code> called SecureTmp. Deactivating this feature for Apache as described <a href=""https://support.plesk.com/hc/en-us/articles/115000063849-Directories-like-tmp-systemd-private-overflow-cause-server-crash-due-to-lack-of-disk-space"" rel=""nofollow noreferrer"">here</a> for testing solved the issue. I will now see how I can do this differently as I do not want to leave this feature disabled. Hope this is helpful for someone :-)</p>
"
"<p>Im using SQL+ dot net and really liking it so far, but Im having trouble with the formatting for phone number. With the following code:</p>

<pre><code>--+SqlPlusRoutine
    --&amp;Author=Vincent Grazapoli
    --&amp;Comment=Inserts a new Customer Billing Record and returns the new CustomerBillingId
    --&amp;SelectType=NonQuery
--+SqlPlusRoutine
CREATE PROCEDURE [dbo].[CustomerBillingInsert]
(
    @CustomerBillingId int output,

--+Required
    @CustomerId int,

--+Required
--+StringLength=8,64
--+Email
    @Email varchar(64),

--+Required
--+StringLength=16,20
--+CreditCard
    @CreditCard varchar(20),

--+Required
--+StringLength=10,16
--+Phone
    @Phone varchar(16),

--...
-- other parameters
</code></pre>

<p>The email and credit card tags work as expected, but the phone number seems to only prevent letters, and allows numbers like 1234 etc.
What am I doing wrong?</p>

<p>The generated code is as follows:</p>

<pre><code>    public class CustomerBillingInsertInput
        {
            [Required(AllowEmptyStrings = false)]
            public int? CustomerId { set; get; }

            [EmailAddress]
            [DataType(DataType.EmailAddress)]
            [Required(AllowEmptyStrings = false)]
            [StringLength(64, MinimumLength = 8)]
            public string Email { set; get; }

            [CreditCard]
            [DataType(DataType.CreditCard)]
            [Required(AllowEmptyStrings = false)]
            [StringLength(20, MinimumLength = 16)]
            public string CreditCard { set; get; }

            [Phone]
            [DataType(DataType.PhoneNumber)]
            [Required(AllowEmptyStrings = false)]
            [StringLength(16, MinimumLength = 10)]
            public string Phone { set; get; }

//Other properties

            /// &lt;summary&gt;
            /// Use this method to validate the instance.
            /// If the method returns false, the ValidationResults list will be populated.
            /// &lt;/summary&gt;
            public bool IsValid()
            {
                ValidationResults = new List&lt;ValidationResult&gt;();
                return Validator.TryValidateObject(this, new ValidationContext(this), ValidationResults, true);
            }

            /// &lt;summary&gt;
            /// ValidationResults populated from the IsValid() call.
            /// &lt;/summary&gt;
            public List&lt;ValidationResult&gt; ValidationResults{ set; get; }
        }
}
}
</code></pre>
","<p>So I received a response from sqlplus.net feedback and the answer they gave me was as follows, might help someone even if you're not using sql+ dot net.</p>

<p>The --+Phone tag is doing what is expected, however, the logic for that annotation in c# is quite liberal since it has to apply globally. For your case you can do one of the following:
Use the --+RexExPattern tag along with an appropriate supplemental tag for the error message like so.</p>

<pre><code>--+Required
--+StringLength=10,16
--+Phone
--+RegExPattern=^[0-9]{10,12}$
    --&amp;ErrorMessage=Phone is not a vaid format.
    @Phone varchar(16),
</code></pre>

<p>That would translate to the following data annotation.</p>

<pre><code>[RegularExpression(@""^[0-9]{10,12}$"",ErrorMessage = ""Phone is not a vaid format."")]
</code></pre>

<p>Or, and this would be my preference, leave your semantic tags the way they are, and use a service like twilio to send a text with a verification code. Have your user confirm that verification code on a subsequent form post, and you are golden.</p>

<p>Confirming the phone number, or email for that matter, is really the only way to be sure, and since it looks like you are persisting customer billing information, it would be worth the extra work.</p>
"
"<p>After installing csvkit with the following command</p>

<pre><code>$ sudo -HE pip install --upgrade -e git+git://github.com/wireservice/csvkit.git@master#egg=csvkit
</code></pre>

<p>and trying to import a <code>.csv</code> as follows:</p>

<pre><code>csvsql --db mysql://root:root@127.0.0.1:3306/jira_test --insert --table bugs_temp --no-constraints --overwrite --create-if-not-exists --no-inference --blanks bugs_temp.csv
</code></pre>

<p>I get the following error(s)</p>

<pre><code>/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_2"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_3"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_4"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_5"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_6"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_7"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_8"".
/usr/local/lib/python2.7/dist-packages/agate/utils.py:292: DuplicateColumnWarning: Column name ""0"" already exists in Table. Column will be renamed to ""0_9"".
(in table 'bugs_temp', column 'ESS-3146'): VARCHAR requires a length on dialect mysql
</code></pre>
","<p><sup>(This answer might be somewhat uncomplete. I don't have experience with <code>csvsql</code>, so I'm making it a wiki.)</sup></p>

<p><code>csvkit</code> <a href=""https://github.com/wireservice/csvkit/blob/d1cc54ffab1eb4f0a3a5afc8b7183db4ee1ec800/requirements-py3.txt#L6"" rel=""nofollow noreferrer"">depends</a> on <a href=""https://agate-sql.readthedocs.io/en/latest/"" rel=""nofollow noreferrer""><code>agate-sql</code></a>, which <a href=""https://github.com/wireservice/agate-sql/blob/1ad8c6548ca9653a5f4bd796311ddbd6ce2e7fcb/requirements-py3.txt#L8"" rel=""nofollow noreferrer"">depends</a> on <a href=""https://www.sqlalchemy.org/"" rel=""nofollow noreferrer""><code>sqlalchemy</code></a>.</p>

<p><code>sqlalchemy</code> contains <a href=""https://github.com/sqlalchemy/sqlalchemy/blob/201c4a60e4b8af56d9c02a3675d1443ba4171c89/lib/sqlalchemy/dialects/mysql/base.py#L1951"" rel=""nofollow noreferrer"">this line of code</a>:</p>

<pre><code>    ""VARCHAR requires a length on dialect %s"" % self.dialect.name
</code></pre>

<p>It seems like <em>you need to specify lengths for all Strings</em>, citing <a href=""https://stackoverflow.com/questions/5569963/varchar-requires-a-length-when-rendered-on-mysql#5586870"">this answer</a>.</p>

<p>That means you cannot use the <code>--no-constraints</code> option:</p>

<pre><code>  --no-constraints      Generate a schema without length limits or null
                        checks. Useful when sampling big tables.
</code></pre>

<p>See <a href=""https://csvkit.readthedocs.io/en/latest/scripts/csvsql.html"" rel=""nofollow noreferrer"">https://csvkit.readthedocs.io/en/latest/scripts/csvsql.html</a></p>
"
"<p>I can't find anywhere in the documentation a way to execute a raw SQL string with <a href=""https://docs.nanosql.io/setup"" rel=""nofollow noreferrer"">nanoSQL</a>. If not possible could anyone recommend another in-memory SQL solution where I can run raw SQLs?</p>
","<p>Library author here, it's not possible to do this since nanoSQL has it's own query language.</p>

<p>However, this is a planned feature for nanoSQL 2.0.  Keep an eye on the readme.md in this repo under ""Query Support""
<a href=""https://github.com/ClickSimply/Nano-SQL"" rel=""nofollow noreferrer"">https://github.com/ClickSimply/Nano-SQL</a></p>
"
"<p>I have a query like this:</p>

<pre><code>SET @a = (SELECT GROUP_CONCAT(Id) FROM MyTable1 WHERE Id &lt; 10);
SELECT * FROM MyTable2  WHERE find_in_set(IdLite, @a); 
SELECT * FROM MyTable3  WHERE find_in_set(IdLite, @a);
SELECT * FROM MyTable4  WHERE find_in_set(IdLite, @a); 
</code></pre>

<p>I've tryed to use this code to get resut:</p>

<pre><code>Using ds As DataSet = MySqlHelper.ExecuteDataset(CnStr, SqlStr)
</code></pre>

<p>but I get error:  </p>

<blockquote>
  <p>Fatal error encountered during command execution.</p>
</blockquote>

<p>Error message is:</p>

<blockquote>
  <p>Parameter '@a' must be defined.</p>
</blockquote>

<p>I've also tryed:</p>

<pre><code>SELECT * FROM MyTable2  WHERE find_in_set(IdLite, 
     @a := (SELECT GROUP_CONCAT(Id) FROM MyTable1 WHERE Id &lt; 10)); 
SELECT * FROM MyTable3  WHERE find_in_set(IdLite, @a);
SELECT * FROM MyTable4  WHERE find_in_set(IdLite, @a); 
</code></pre>

<p>but I get the same error.<br>
What's the correct way to get result into a <code>DataSet</code>?</p>
","<p>This might get you part of the way.  </p>

<p>First get rid of <code>DB_Functions</code>.  MySQLHelper has a method to create the DataSet for you; in general, db Ops are so query-specific that there is very little that is generic and reusable.  The exception to this is building the ConnectionString: MySQL has <a href=""https://dev.mysql.com/doc/connector-net/en/connector-net-connection-options.html"" rel=""nofollow"">gobs of cool options</a> you can enable/disable via the connection string.  But for that you just need the standard <code>MySqlConnectionStringBuilder</code>.</p>



<h2>Build a DataSet:</h2>

<pre class=""lang-vb prettyprint-override""><code>' form/class level vars
Private dsSample As DataSet
Private MySqlConnStr As String = ""...""

...
Dim SQL = ""SELECT Id, FirstName, Middle, LastName FROM Employee""

Using dbcon As New MySqlConnection(MySQLConnStr)
    dsSample = MySqlHelper.ExecuteDataset(dbcon, SQL)
    dsSample.Tables(0).TableName = ""Emps""
End Using
</code></pre>

<p>There does not appear to be a way to specify a tablename when you build it, so that is a separate step.  </p>

<h2>Update a Record</h2>

<p>To update a single row, you want <code>ExecuteNonQuery</code>; this will also allow you to use Parameters:</p>

<pre class=""lang-vb prettyprint-override""><code>Dim uSQL = ""UPDATE Employee SET Middle = @p1 WHERE Id = @p2""
Using dbcon As New MySqlConnection(MySQLConnStr)
    Dim params(1) As MySqlParameter

    params(0) = New MySqlParameter(""@p1"", MySqlDbType.String)
    params(0).Value = ""Q""

    params(1) = New MySqlParameter(""@p2"", MySqlDbType.Int32)
    params(1).Value = 4583

    dbcon.Open()
    Dim rows = MySqlHelper.ExecuteNonQuery(dbcon, uSQL, params)
End Using
</code></pre>

<p>Again, this is not really any simpler than using a fully configured DataAdapter, which would be simply:</p>

<pre class=""lang-vb prettyprint-override""><code>dsSample.Tables(""Emps"").Rows(1).Item(""Middle"") = ""X""
daSample.Update(dsSample.Tables(""Emps""))
</code></pre>

<p>I am not exactly sure what value the <code>UpdateDataSet</code> method adds.  I <em>think</em> it is the ""helper"" counterpart for the above, but since it doesn't provide for Parameters, I don't have much use for it.  The docs for it are sketchy.</p>

<p>The <code>commandtext</code> would appear to be the SQL for a single row. Note that the DataAdapter.Update method above would add any new rows added, delete the deleted ones and update values for any row with changed values - potentially dozens or even hundreds of db Ops with one line of code.</p>
"
"<p>I am playing around with MySQL 8.0.14 and InnoDB cluster.
I am currently stuck at creating the Group replication via mySQL shell.</p>

<p>Since I want to use SSL, I am required to set ipWhitelist on dba.createCluster() which is shown below:</p>

<p><code>var cluster = dba.createCluster('testCluster4', {ipWhitelist:'somedns-1.tosqlnode'})</code></p>

<p>The cluster is successfully created. Now I want to add another instance.</p>

<p><code>cluster.addInstance('ca@somedns-2.tosqlnode', {ipWhitelist:'somedns-1.tosqlnode,somedns-2.tosqlnode'})</code></p>

<p>This fails as the first instance is showing an error that states that a non-whitelisted instance is trying to connect.</p>

<hr>

<p>So create another one:</p>

<p><code>var cluster = dba.createCluster('testCluster5', {ipWhitelist:'somedns-1.tosqlnode,somedns-2.tosqlnode'})</code></p>

<p>The cluster is successfully created. Now I want to add another instance.</p>

<p><code>cluster.addInstance('ca@somedns-2.tosqlnode', {ipWhitelist:'somedns-1.tosqlnode,somedns-2.tosqlnode'})</code></p>

<p>Instance is successfully added.</p>

<hr>

<p>Is it really necessary to know all instance addresses at cluster creation? I cannot find a way via MySQL shell to change the initial ipWhitelist.</p>
","<p>If you want to be able to add nodes on the fly, you need to set <code>group_replication_ip_whitelist</code> to <code>AUTOMATIC</code>. This is done when not specifying any ipWhitelist from the Shell during configuration (default). If not, you have to do what Miguel wrote above.</p>
"
"<p>We have a timestamp column that, when queried, displays this syntax:</p>

<pre><code>12/18/2018 11:27:35 AM
</code></pre>

<p>However, this makes it confusing as the timestamp actually stores the milisecond values, as well. This means that when querying for '12/18/2018 11:27:35 AM', we return no results unless we use the date_trunc() function.</p>

<p>Is there anyway to force the engine to display the literal value?</p>

<p>We are using SQL Manager for PostgreSQL</p>
","<p>Use the To_Char function on your column to set the output display format:</p>

<pre><code>to_char(yourColumn, 'mm/dd/yyyy hh:mi:ss.us am')
</code></pre>
"
"<p>I have a problem where I have to get the column names and their values from all the Tables in my schema and show that the result in a grid. 
I have used the direct approach for this but I have to implement the SqlSiphon structure. For this I have to make getters and setters of each of the column of each Table in the schema which is impossible.</p>

<p>What should I use to get the Column names and their values dynamically from the table.</p>

<pre><code>SELECT * FROM INFORMATION_SCHEMA.COLUMNS 
WHERE 
TABLE_NAME = '"" + @Tablename1 + ""' AND TABLE_SCHEMA='dbo'""
</code></pre>

<p>What will be the best dynamic solution?
And what will be Best to use List , Dictionay or something like 2d Array which will give the column names as well as column values?</p>
","<p>Might be worth taking a step back an comparing against how other people solve similar problems.</p>

<p>Typically, each table on a database represents an entity, and you also have a class per entity, and you may use an ORM system to avoid duplication of work. So, in a typical system, you have a table for customers, and a table for invoices, and a table for invoice lines, etc. and then a class that represents a customer, a class for an invoice, a class for an invoice line, etc. As you later add functionality (and possible columns/properties) you change the classes, rather than just seeing what columns are on the database - you can of course decorate these with XML documentation and get Intelisense goodness.</p>

<p>There are many ORM systems out there, and each have their strengths and weaknesses, but I personally like <a href=""http://msdn.microsoft.com/en-us/library/bb387007%28v=vs.110%29.aspx"" rel=""nofollow"">LINQ to SQL</a> for adding onto an existing data model.</p>
"
"<p>I've been using JoSQL for quite a few months now and today I came across a problem I am not sure how to solve. I probably could solve it by binding variables/placeholders, but I'd like to include the fields in the query.</p>

<p>SELECT * FROM ...MyObject WHERE getType != com.mypackage.myclass.TYPE_A</p>

<p>This is the query that I have. TYPE_A is a public static final int attribute in ""myclass"" class. Accessing methods (such as getType) is easy, because getType is expected to be a method from MyObject - just that I do not write round brackets after it (this is how JoSQL works as far as I know).</p>

<p>Does anyone happen to have an idea how to access a public static final field?</p>

<p>JoSQL uses gentlyweb-utils; it seems to be some sort of Accessor/Getter/Setter framework. I'd love to access that attribute without having to bind variables, but I haven't been able to do so.</p>

<p>Thanks for your help in advance! I really appreciate it.</p>
","<p>You can use: <code>Query.getColumns</code> this will return a list of: <code>SelectItemExpression</code> you can then use: <code>getAlias</code> on each to return the ""column"" name.</p>

<p>So:</p>

<pre><code>Query q = new Query ();
q.parse (""SELECT a FROM java.lang.String"");

List cols = q.getColumns ();

for (int i = 0; i &lt; cols.size (); i++)
{

    SelectItemExpresion sie = (SelectItemExpression) cols.get (i);

    System.out.println (i + "": "" + sie.getAlias ());

}
</code></pre>

<p>Of course if the column doesn't have an alias then you could try using the <code>toString</code> method instead or digging around in the <code>Expression</code> to find more suitable data.</p>
"
"<p>How to get the elements of an XML if we know the nodename.I have a sample XML like below.</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;!DOCTYPE PARTS SYSTEM ""parts.dtd""&gt;
&lt;?xml-stylesheet type=""text/css"" href=""xmlpartsstyle.css""?&gt;
&lt;PARTS&gt;
   &lt;TITLE&gt;Computer Parts&lt;/TITLE&gt;
&lt;PART&gt;
  &lt;ITEM&gt;Motherboard&lt;/ITEM&gt;
  &lt;MANUFACTURER&gt;ASUS&lt;/MANUFACTURER&gt;
  &lt;MODEL&gt;P3B-F&lt;/MODEL&gt;
  &lt;COST&gt; 123.00&lt;/COST&gt;
&lt;/PART&gt;
&lt;PART&gt;
  &lt;ITEM&gt;Video Card&lt;/ITEM&gt;
  &lt;MANUFACTURER&gt;ATI&lt;/MANUFACTURER&gt;
  &lt;MODEL&gt;All-in-Wonder Pro&lt;/MODEL&gt;
  &lt;COST&gt; 160.00&lt;/COST&gt;
&lt;/PART&gt;
&lt;PART&gt;
  &lt;ITEM&gt;Sound Card&lt;/ITEM&gt;
  &lt;MANUFACTURER&gt;Creative Labs&lt;/MANUFACTURER&gt;
  &lt;MODEL&gt;Sound Blaster Live&lt;/MODEL&gt;
  &lt;COST&gt; 80.00&lt;/COST&gt;
&lt;/PART&gt;
&lt;PART&gt;
  &lt;ITEM inch Monitor&lt;/ITEM&gt;
  &lt;MANUFACTURER&gt;LG Electronics&lt;/MANUFACTURER&gt;
  &lt;MODEL&gt; 995E&lt;/MODEL&gt;
  &lt;COST&gt; 290.00&lt;/COST&gt;
&lt;/PART&gt;
</code></pre>

<p></p>

<p>I want to get all the  element in an array using esql. How to do it?</p>
","<p>Assuming you are parsing the xml on entry then you can simply just use the following:</p>

<pre><code>SET myvar[] = InputRoot.XMLNSC.PARTS.PART[]
</code></pre>
"
"<p><a href=""https://github.com/yandex-qatools/postgresql-embedded"" rel=""nofollow noreferrer"">Here</a> is embedpostgresql library for starting a postgres process via java. The <a href=""https://github.com/yandex-qatools/postgresql-embedded#how-to-use-avoid-archive-extraction-on-every-run"" rel=""nofollow noreferrer"">instruction</a> about reusing already extracted postgres from zip archive. But how to change archive directory? By default they store maven to <code>%USER_HOME\.embedpostgresql%</code> I want to change it to maven repository but what is needed to invoke to set such property?</p>
","<p>You just have to use the <strong>artifactStorePath()</strong> method from <em>DownloadConfigBuilder</em> class:</p>

<pre><code>final FixedPath cachedDir = new FixedPath(""/tmp/cacheddir"");
final FixedPath artifactStoreDir = new FixedPath(""/tmp/artstoredir"");
 IRuntimeConfig runtimeConfig = new RuntimeConfigBuilder().defaults(cmd)
 .artifactStore(new CachedArtifactStoreBuilder()
         .defaults(cmd)
         .tempDir(cachedDir)
         .download(new DownloadConfigBuilder()
                 .defaultsForCommand(cmd)
                 .artifactStorePath(artifactStoreDir)
                 .packageResolver(new PackagePaths(cmd, cachedDir))
                 .build()))
 .build();

//starting Postgres
 final PostgresStarter&lt;PostgresExecutable, PostgresProcess&gt; runtime =
 PostgresStarter.getInstance(runtimeConfig);
</code></pre>

<p>I cloned the <a href=""https://github.com/yandex-qatools/postgresql-embedded"" rel=""nofollow noreferrer"">git repository</a>, did a 'grep -R embedpostgresql *' and found the first hint in ru.yandex.qatools.embed.postgresql.config.DownloadConfigBuilder class. Then I tested by myself and it works!</p>
"
"<p>I'm trying PL/SQL on online Oracle SQL Worksheet - <a href=""https://livesql.oracle.com/apex/f?p=590:1:3323436307214::NO:::"" rel=""nofollow noreferrer"">Live Oracle SQL</a>.</p>

<p>I'm unable to display the output of the block, in spite of adding <code>SET SERVEROUTPUT ON;</code></p>

<p>This is my code</p>

<pre><code>SET SERVEROUTPUT ON;

declare
    i number:=2;
    j number:=0;
    counter number:=0;
    flag number;
begin
    loop
        if (i=2) then
            counter:=counter+1;
            dbms_output.put(i ||' ');

        else
            j:=2;
            flag:=0;
            loop
                if(mod(i, j)=0) then
                    flag:=1;
                end if;
                exit when (i=j) or flag=1;
            end loop;
            if(flag=0) then
                counter:=counter+1;
                dbms_output.put(j ||' ');
            end if;
        end if;
    i:=i+1;
    exit when counter=10;
    end loop;
end;
/
</code></pre>

<p>This is the console message</p>

<pre><code>Unsupported Command
Statement processed.
</code></pre>

<p>Any idea how to get it working?</p>
","<p>I actually changed <code>dbms_output.put()</code> to <code>dbms_output.put_line()</code> and it worked. Any idea how to make <code>dbms_output.put()</code> work?</p>

<p>I want the output in a single line.</p>
"
"<p>I want to simulate simple SQL statement(Create &amp; Select) processing in Java.</p>

<p>For example, consider following two relations</p>

<pre><code>CREATE TABLE X( a, b, c);
CREATE TABLE Y( c, d, e);
</code></pre>

<p><strong>Ques 1:</strong> 
Now what data structures can I use to store the relation name X and Y along with their attributes .</p>

<p>Also consider the select statement:</p>

<pre><code>SELECT a,d FROM X,Y WHERE X.c = Y.c ;
</code></pre>

<p><strong>Ques 2:</strong> 
How to confirm whether <strong>a belongs to X or Y</strong> and <strong>d belongs to X or Y</strong> ?
How does this processing is taken care inside SQL query processing engine.</p>

<p>I can find numerous query evaluation plans that pushes the SELECT statement below JOIN operation if the Selection depends on only on a single relation. For this purpose I need to know <strong>How to confirm to which relation the attribute belongs to?</strong>  </p>

<p>Thanks in advance.</p>
","<blockquote>
  <p>Ques 1: Now what data structures can I use to store the relation name X and Y along with their attributes .</p>
</blockquote>

<p>Use any structure you like, in Java you'd most probably use classes that correspond to X and Y and which have fields that correspond to a, b, c for X and c, d, e for Y.</p>

<blockquote>
  <p>SELECT a,d FROM X,Y WHERE X.c = Y.c ;</p>
  
  <p>Ques 2: How to confirm whether a belongs to X or Y and d belongs to X or Y ? How does this processing is taken care inside SQL query processing engine.</p>
</blockquote>

<p>Since <code>a</code> only exists in X and <code>d</code> only exists in Y the database knows what's meant. If they'd exist in the other table as well you'd need to specify which of them you mean (e.g. <code>SELECT x.a, d FROM X x, Y y ...</code>).</p>
"
"<p>So I need to import the ITIS database into pdadmin4 1.5 (postgresql)</p>

<p>The read.me file gives me these information:</p>

<pre><code>1. Open a terminal or command prompt and navigate to the folder where you
   unzipped the ITIS download file.

2. Enter the following:

      psql -U root -f ITIS.sql

   If your PostgreSql user is not root, substitute your user
   name for root. Also, if you are updating a remote 
   server, you will need to  add a -h &lt;servername&gt; to 
   the command

3. When prompted for your PostgreSql user's password, enter
   it and the load process will start.
</code></pre>

<p>The directory of the file is:</p>

<pre><code>/Users/kostas/Desktop/itisPostgreSql063017/ITIS.sql
</code></pre>

<p>How do I open a terminal or command prompt and navigate to the folder where I unzipped the ITIS download file and how do I see if my PostgreSql user is not root?</p>

<p>Thanks!</p>
","<p>Issue was solved by using the following maven plugin:</p>

<pre><code>        &lt;plugin&gt;
           &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
           &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
           &lt;configuration&gt;
              &lt;argLine&gt;-Dfile.encoding=UTF8&lt;/argLine&gt;
           &lt;/configuration&gt;
        &lt;/plugin&gt;
</code></pre>

<p>the above plugin will solve encoding issue for <strong>command line build</strong>.</p>

<p><strong>UPDATE:</strong> to solve the issue when you <strong>run project on server</strong>:</p>

<p>1- Run As</p>

<p>2- Run Configurations</p>

<p>3- Change the encoding for the server to be UTF-8 as in the image below</p>

<p><a href=""https://i.stack.imgur.com/Tqyvz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Tqyvz.png"" alt=""enter image description here""></a></p>
"
"<p>I want to show the result of the Oracles <code>dbms_sqltune.report_sql_monitor</code> in my browser (Firefox), but I get only this page:</p>

<blockquote>
  <p>Alternate HTML content should be placed here. This content requires the Adobe Flash Player. Get Flash</p>
</blockquote>

<p>In detail I first run the query bellow with the SQL_ID to be traced:</p>

<pre><code>select dbms_sqltune.report_sql_monitor(
sql_id =&gt; 'c9uht72r7gga8',  
report_level =&gt;'ALL',
type =&gt; 'ACTIVE') from dual
</code></pre>

<p>The result of this query is a HTML document, that I cut and paste in a file, that I open in my browser (Firefox)</p>

<p>But instead to get the required query overview, I get the message above.</p>

<p><a href=""https://i.stack.imgur.com/rhZkA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rhZkA.png"" alt=""enter image description here""></a></p>

<p>My Flash is installed and in recent version as I see it in <code>Add-on</code>overview</p>

<p><a href=""https://i.stack.imgur.com/4kG0I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4kG0I.png"" alt=""enter image description here""></a></p>

<p>Re-install Flash using admin account (Windows 7 Profesional) doesn't help.</p>
",""
"<p>I've been trying to measure the execution time of some queries executed from an application. I've been trying to use sys.dm_exec_query_stats.last_elapsed_time 
but I'm confused about the values that it is returning. When I execute the query </p>

<pre><code>SELECT TOP 1 qs.plan_handle, qs.last_execution_time, qs.last_elapsed_time, qs.min_elapsed_time FROM sys.dm_exec_query_stats qs ORDER BY qs.last_execution_time DESC
</code></pre>

<p>from the ssms, it clearly executes and returns the results just as soon as the display can refresh, clearly much faster than one second. However, the values being returned for last_elapsed_time are consistently around 1700-1800 milliseconds. </p>

<p>What am I missing?</p>

<p>Here is the <a href=""http://msdn.microsoft.com/en-us/library/ms189741.aspx"" rel=""nofollow"">msdn info page</a> on sys.dm_exec_query_stats. I'm using sql server 2012. </p>
","<p>They are not millisecond values, but MICROsecond values, as is stated clearly on the msdn info page for the table.</p>
"
"<p>I have a very basic question about database programming, here's the problem:</p>

<p>I want to create/read/edit/etc.. data from database without using Entity Framework, and for this job I've chosen SqlFu.</p>

<p>I want to put the stored procedures to create, update, delete on the database and the views to get entities.</p>

<p>My doubt is: If I have an table <em>Employee</em>, that has a one-to-many relationship to <em>Tasks</em> table, when I create a Sql View to retrieve <em>Employee</em> entity, should I retrieve the data in <em>Tasks</em> table that is related to the employee?</p>

<p>If so, how to do that with a single View in SQL Server? If not, I should have different Sql Views that retrieve data from each table and <em>bind</em> the relationship in the application?</p>

<p>I'm a bit lost in this subject :S</p>
","<p>You can't do that, SqlFu is a data mapper, mapping a query result to a poco. The attributes used to decorate a Poco are for table creation only and not for querying. So, there's no mapping in an ORM sense.</p>
"