Source,Question,Reply
Stackoverflow,"Also ` LEFT JOIN ` , ` RIGHT JOIN ` ` FULL JOIN ` fit ?","**Use prepared statements parameterized queries . ** These SQL statements sent parsed database server separately parameters . This way impossible attacker inject malicious SQL . You basically two options achieve : 1 . Using [ PDO ] ( http : //php.net/manual/en/book.pdo.php ) ( supported database driver ) : $ stmt = $ pdo- > prepare ( 'SELECT * FROM employees WHERE name = : name ' ) ; $ stmt- > execute ( array ( 'name ' = > $ name ) ) ; foreach ( $ stmt $ row ) { // something $ row } 2 . Using [ MySQLi ] ( http : //php.net/manual/en/book.mysqli.php ) ( MySQL ) : $ stmt = $ dbConnection- > prepare ( 'SELECT * FROM employees WHERE name = ? ' ) ; $ stmt- > bind_param ( 's ' , $ name ) ; // 's ' specifies variable type = > 'string' $ stmt- > execute ( ) ; $ result = $ stmt- > get_result ( ) ; ( $ row = $ result- > fetch_assoc ( ) ) { // something $ row } If 're connecting database MySQL , driver- specific second option refer ( e.g . ` pg_prepare ( ) ` ` pg_execute ( ) ` PostgreSQL ) . PDO universal option . # # Correctly setting connection Note using ` PDO ` access MySQL database _real_ prepared statements **not used default** . To fix disable emulation prepared statements . An example creating connection using PDO : $ dbConnection = new PDO ( 'mysql : dbname=dbtest ; host=127.0.0.1 ; charset=utf8 ' , 'user ' , 'pass ' ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_EMULATE_PREPARES , false ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_ERRMODE , PDO : :ERRMODE_EXCEPTION ) ; In example error mode n't strictly necessary , **but advised add it** . This way script stop ` Fatal Error ` something goes wrong . And gives developer chance ` catch ` error ( ) ` throw ` n ` PDOException ` . What **mandatory** , however , first ` setAttribute ( ) ` line , tells PDO disable emulated prepared statements use _real_ prepared statements . This makes sure statement values n't parsed PHP sending MySQL server ( giving possible attacker chance inject malicious SQL ) . Although set ` charset ` options constructor , important note 'older ' versions PHP ( < 5.3.6 ) [ silently ignored charset parameter ] ( http : //php.net/manual/en/ref.pdo-mysql.connection.php ) DSN . # # Explanation What happens SQL statement pass ` prepare ` parsed compiled database server . By specifying parameters ( either ` ? ` named parameter like ` : name ` example ) tell database engine want filter . Then call ` execute ` , prepared statement combined parameter values specify . The important thing parameter values combined compiled statement , SQL string . SQL injection works tricking script including malicious strings creates SQL send database . So sending actual SQL separately parameters , limit risk ending something n't intend . Any parameters send using prepared statement treated strings ( although database engine may optimization parameters may end numbers , course ) . In example , ` $ name ` variable contains ` 'Sarah ' ; DELETE FROM employees ` result would simply search string ` `` 'Sarah ' ; DELETE FROM employees '' ` , end [ empty table ] ( http : //xkcd.com/327/ ) . Another benefit using prepared statements execute statement many times session parsed compiled , giving speed gains . Oh , since asked insert , 's example ( using PDO ) : $ preparedStatement = $ db- > prepare ( 'INSERT INTO table ( column ) VALUES ( : column ) ' ) ; $ preparedStatement- > execute ( array ( 'column ' = > $ unsafeValue ) ) ; # # Can prepared statements used dynamic queries ? While still use prepared statements query parameters , structure dynamic query parametrized certain query features parametrized . For specific scenarios , best thing use whitelist filter restricts possible values . // Value whitelist // $ dir 'DESC ' otherwise 'ASC' ( empty ( $ dir ) || $ dir ! == 'DESC ' ) { $ dir = 'ASC ' ; }"
Stackoverflow,"In **SQL Server** , 's possible ` insert ` table using ` SELECT ` statement : INSERT INTO Table ( col1 , col2 , col3 ) SELECT col1 , col2 , col3 FROM other_table WHERE sql = 'cool' Is also possible _update_ via ` SELECT ` ? I temporary table containing values , would like update another table using values . Perhaps something like : UPDATE Table SET col1 , col2 SELECT col1 , col2 FROM other_table WHERE sql = 'cool' WHERE Table.id = other_table.id","UPDATE Table_A SET Table_A.col1 = Table_B.col1 , Table_A.col2 = Table_B.col2 FROM Some_Table AS Table_A INNER JOIN Other_Table AS Table_B ON Table_A.id = Table_B.id WHERE Table_A.col3 = 'cool '"
Stackoverflow,"In **SQL Server** , 's possible ` insert ` table using ` SELECT ` statement : INSERT INTO Table ( col1 , col2 , col3 ) SELECT col1 , col2 , col3 FROM other_table WHERE sql = 'cool' Is also possible _update_ via ` SELECT ` ? I temporary table containing values , would like update another table using values . Perhaps something like : UPDATE Table SET col1 , col2 SELECT col1 , col2 FROM other_table WHERE sql = 'cool' WHERE Table.id = other_table.id","UPDATE Table_A SET Table_A.col1 = Table_B.col1 , Table_A.col2 = Table_B.col2 FROM Some_Table AS Table_A INNER JOIN Other_Table AS Table_B ON Table_A.id = Table_B.id WHERE Table_A.col3 = 'cool '"
Stackoverflow,"If user input inserted without modification SQL query , application becomes vulnerable [ SQL injection ] ( https : //stackoverflow.com/a/332367/ ) , like following example : $ unsafe_variable = $ _POST [ 'user_input ' ] ; mysql_query ( `` INSERT INTO ` table ` ( ` column ` ) VALUES ( ' $ unsafe_variable ' ) '' ) ; That 's user input something like ` value ' ) ; DROP TABLE table ; -- ` , query becomes : INSERT INTO ` table ` ( ` column ` ) VALUES ( 'value ' ) ; DROP TABLE table ; -- ' ) What done prevent happening ?","**Use prepared statements parameterized queries . ** These SQL statements sent parsed database server separately parameters . This way impossible attacker inject malicious SQL . You basically two options achieve : 1 . Using [ PDO ] ( http : //php.net/manual/en/book.pdo.php ) ( supported database driver ) : $ stmt = $ pdo- > prepare ( 'SELECT * FROM employees WHERE name = : name ' ) ; $ stmt- > execute ( array ( 'name ' = > $ name ) ) ; foreach ( $ stmt $ row ) { // something $ row } 2 . Using [ MySQLi ] ( http : //php.net/manual/en/book.mysqli.php ) ( MySQL ) : $ stmt = $ dbConnection- > prepare ( 'SELECT * FROM employees WHERE name = ? ' ) ; $ stmt- > bind_param ( 's ' , $ name ) ; // 's ' specifies variable type = > 'string' $ stmt- > execute ( ) ; $ result = $ stmt- > get_result ( ) ; ( $ row = $ result- > fetch_assoc ( ) ) { // something $ row } If 're connecting database MySQL , driver- specific second option refer ( e.g . ` pg_prepare ( ) ` ` pg_execute ( ) ` PostgreSQL ) . PDO universal option . # # Correctly setting connection Note using ` PDO ` access MySQL database _real_ prepared statements **not used default** . To fix disable emulation prepared statements . An example creating connection using PDO : $ dbConnection = new PDO ( 'mysql : dbname=dbtest ; host=127.0.0.1 ; charset=utf8 ' , 'user ' , 'pass ' ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_EMULATE_PREPARES , false ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_ERRMODE , PDO : :ERRMODE_EXCEPTION ) ; In example error mode n't strictly necessary , **but advised add it** . This way script stop ` Fatal Error ` something goes wrong . And gives developer chance ` catch ` error ( ) ` throw ` n ` PDOException ` . What **mandatory** , however , first ` setAttribute ( ) ` line , tells PDO disable emulated prepared statements use _real_ prepared statements . This makes sure statement values n't parsed PHP sending MySQL server ( giving possible attacker chance inject malicious SQL ) . Although set ` charset ` options constructor , important note 'older ' versions PHP ( < 5.3.6 ) [ silently ignored charset parameter ] ( http : //php.net/manual/en/ref.pdo-mysql.connection.php ) DSN . # # Explanation What happens SQL statement pass ` prepare ` parsed compiled database server . By specifying parameters ( either ` ? ` named parameter like ` : name ` example ) tell database engine want filter . Then call ` execute ` , prepared statement combined parameter values specify . The important thing parameter values combined compiled statement , SQL string . SQL injection works tricking script including malicious strings creates SQL send database . So sending actual SQL separately parameters , limit risk ending something n't intend . Any parameters send using prepared statement treated strings ( although database engine may optimization parameters may end numbers , course ) . In example , ` $ name ` variable contains ` 'Sarah ' ; DELETE FROM employees ` result would simply search string ` `` 'Sarah ' ; DELETE FROM employees '' ` , end [ empty table ] ( http : //xkcd.com/327/ ) . Another benefit using prepared statements execute statement many times session parsed compiled , giving speed gains . Oh , since asked insert , 's example ( using PDO ) : $ preparedStatement = $ db- > prepare ( 'INSERT INTO table ( column ) VALUES ( : column ) ' ) ; $ preparedStatement- > execute ( array ( 'column ' = > $ unsafeValue ) ) ; # # Can prepared statements used dynamic queries ? While still use prepared statements query parameters , structure dynamic query parametrized certain query features parametrized . For specific scenarios , best thing use whitelist filter restricts possible values . // Value whitelist // $ dir 'DESC ' otherwise 'ASC' ( empty ( $ dir ) || $ dir ! == 'DESC ' ) { $ dir = 'ASC ' ; }"
Stackoverflow,"If user input inserted without modification SQL query , application becomes vulnerable [ SQL injection ] ( https : //stackoverflow.com/a/332367/ ) , like following example : $ unsafe_variable = $ _POST [ 'user_input ' ] ; mysql_query ( `` INSERT INTO ` table ` ( ` column ` ) VALUES ( ' $ unsafe_variable ' ) '' ) ; That 's user input something like ` value ' ) ; DROP TABLE table ; -- ` , query becomes : INSERT INTO ` table ` ( ` column ` ) VALUES ( 'value ' ) ; DROP TABLE table ; -- ' ) What done prevent happening ?","**Use prepared statements parameterized queries . ** These SQL statements sent parsed database server separately parameters . This way impossible attacker inject malicious SQL . You basically two options achieve : 1 . Using [ PDO ] ( http : //php.net/manual/en/book.pdo.php ) ( supported database driver ) : $ stmt = $ pdo- > prepare ( 'SELECT * FROM employees WHERE name = : name ' ) ; $ stmt- > execute ( array ( 'name ' = > $ name ) ) ; foreach ( $ stmt $ row ) { // something $ row } 2 . Using [ MySQLi ] ( http : //php.net/manual/en/book.mysqli.php ) ( MySQL ) : $ stmt = $ dbConnection- > prepare ( 'SELECT * FROM employees WHERE name = ? ' ) ; $ stmt- > bind_param ( 's ' , $ name ) ; // 's ' specifies variable type = > 'string' $ stmt- > execute ( ) ; $ result = $ stmt- > get_result ( ) ; ( $ row = $ result- > fetch_assoc ( ) ) { // something $ row } If 're connecting database MySQL , driver- specific second option refer ( e.g . ` pg_prepare ( ) ` ` pg_execute ( ) ` PostgreSQL ) . PDO universal option . # # Correctly setting connection Note using ` PDO ` access MySQL database _real_ prepared statements **not used default** . To fix disable emulation prepared statements . An example creating connection using PDO : $ dbConnection = new PDO ( 'mysql : dbname=dbtest ; host=127.0.0.1 ; charset=utf8 ' , 'user ' , 'pass ' ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_EMULATE_PREPARES , false ) ; $ dbConnection- > setAttribute ( PDO : :ATTR_ERRMODE , PDO : :ERRMODE_EXCEPTION ) ; In example error mode n't strictly necessary , **but advised add it** . This way script stop ` Fatal Error ` something goes wrong . And gives developer chance ` catch ` error ( ) ` throw ` n ` PDOException ` . What **mandatory** , however , first ` setAttribute ( ) ` line , tells PDO disable emulated prepared statements use _real_ prepared statements . This makes sure statement values n't parsed PHP sending MySQL server ( giving possible attacker chance inject malicious SQL ) . Although set ` charset ` options constructor , important note 'older ' versions PHP ( < 5.3.6 ) [ silently ignored charset parameter ] ( http : //php.net/manual/en/ref.pdo-mysql.connection.php ) DSN . # # Explanation What happens SQL statement pass ` prepare ` parsed compiled database server . By specifying parameters ( either ` ? ` named parameter like ` : name ` example ) tell database engine want filter . Then call ` execute ` , prepared statement combined parameter values specify . The important thing parameter values combined compiled statement , SQL string . SQL injection works tricking script including malicious strings creates SQL send database . So sending actual SQL separately parameters , limit risk ending something n't intend . Any parameters send using prepared statement treated strings ( although database engine may optimization parameters may end numbers , course ) . In example , ` $ name ` variable contains ` 'Sarah ' ; DELETE FROM employees ` result would simply search string ` `` 'Sarah ' ; DELETE FROM employees '' ` , end [ empty table ] ( http : //xkcd.com/327/ ) . Another benefit using prepared statements execute statement many times session parsed compiled , giving speed gains . Oh , since asked insert , 's example ( using PDO ) : $ preparedStatement = $ db- > prepare ( 'INSERT INTO table ( column ) VALUES ( : column ) ' ) ; $ preparedStatement- > execute ( array ( 'column ' = > $ unsafeValue ) ) ; # # Can prepared statements used dynamic queries ? While still use prepared statements query parameters , structure dynamic query parametrized certain query features parametrized . For specific scenarios , best thing use whitelist filter restricts possible values . // Value whitelist // $ dir 'DESC ' otherwise 'ASC' ( empty ( $ dir ) || $ dir ! == 'DESC ' ) { $ dir = 'ASC ' ; }"
Stackoverflow,"Optimizing SQLite tricky . Bulk-insert performance C application vary 85 inserts per second 96,000 inserts per second ! **Background : ** We using SQLite part desktop application . We large amounts configuration data stored XML files parsed loaded SQLite database processing application initialized . SQLite ideal situation 's fast , requires specialized configuration , database stored disk single file . **Rationale : ** _Initially I disappointed performance I seeing._ It turns-out performance SQLite vary significantly ( bulk-inserts selects ) depending database configured 're using API . It trivial matter figure options techniques , I thought prudent create community wiki entry share results Stack Overflow readers order save others trouble investigations . **The Experiment : ** Rather simply talking performance tips general sense ( i.e . _ '' Use transaction ! `` _ ) , I thought best write C code _actually measure_ impact various options . We 're going start simple data : * A 28 MB TAB-delimited text file ( approximately 865,000 records ) [ complete transit schedule city Toronto ] ( http : //www.toronto.ca/open/datasets/ttc-routes ) * My test machine 3.60 GHz P4 running Windows XP . * The code compiled [ Visual C++ ] ( http : //en.wikipedia.org/wiki/Visual_C % 2B % 2B # 32-bit_versions ) 2005 `` Release '' `` Full Optimization '' ( /Ox ) Favor Fast Code ( /Ot ) . * I 'm using SQLite `` Amalgamation '' , compiled directly test application . The SQLite version I happen bit older ( 3.6.7 ) , I suspect results comparable latest release ( please leave comment think otherwise ) . _Let 's write code ! _ **The Code : ** A simple C program reads text file line-by-line , splits string values inserts data SQLite database . In `` baseline '' version code , database created , actually insert data : /************************************************************* Baseline code experiment SQLite performance . Input data 28 MB TAB-delimited text file complete Toronto Transit System schedule/route info http : //www.toronto.ca/open/datasets/ttc-routes/ **************************************************************/ # include < stdio.h > # include < stdlib.h > # include < time.h > # include < string.h > # include `` sqlite3.h '' # define INPUTDATA `` C : \\TTC_schedule_scheduleitem_10-27-2009.txt '' # define DATABASE `` c : \\TTC_schedule_scheduleitem_10-27-2009.sqlite '' # define TABLE `` CREATE TABLE IF NOT EXISTS TTC ( id INTEGER PRIMARY KEY , Route_ID TEXT , Branch_Code TEXT , Version INTEGER , Stop INTEGER , Vehicle_Index INTEGER , Day Integer , Time TEXT ) '' # define BUFFER_SIZE 256 int main ( int argc , char **argv ) { sqlite3 * db ; sqlite3_stmt * stmt ; char * sErrMsg = 0 ; char * tail = 0 ; int nRetCode ; int n = 0 ; clock_t cStartClock ; FILE * pFile ; char sInputBuf [ BUFFER_SIZE ] = `` \0 '' ; char * sRT = 0 ; /* Route */ char * sBR = 0 ; /* Branch */ char * sVR = 0 ; /* Version */ char * sST = 0 ; /* Stop Number */ char * sVI = 0 ; /* Vehicle */ char * sDT = 0 ; /* Date */ char * sTM = 0 ; /* Time */ char sSQL [ BUFFER_SIZE ] = `` \0 '' ; /*********************************************/ /* Open Database create Schema */ sqlite3_open ( DATABASE , & db ) ; sqlite3_exec ( db , TABLE , NULL , NULL , & sErrMsg ) ; /*********************************************/ /* Open input file import Database*/ cStartClock = clock ( ) ; pFile = fopen ( INPUTDATA , '' r '' ) ; ( ! feof ( pFile ) ) { fgets ( sInputBuf , BUFFER_SIZE , pFile ) ; sRT = strtok ( sInputBuf , `` \t '' ) ; /* Get Route */ sBR = strtok ( NULL , `` \t '' ) ; /* Get Branch */ sVR = strtok ( NULL , `` \t '' ) ; /* Get Version */ sST = strtok ( NULL , `` \t '' ) ; /* Get Stop Number */ sVI = strtok ( NULL , `` \t '' ) ; /* Get Vehicle */ sDT = strtok ( NULL , `` \t '' ) ; /* Get Date */ sTM = strtok ( NULL , `` \t '' ) ; /* Get Time */ /* ACTUAL INSERT WILL GO HERE */ n++ ; } fclose ( pFile ) ; printf ( `` Imported % records % 4.2f seconds\n '' , n , ( clock ( ) - cStartClock ) / ( double ) CLOCKS_PER_SEC ) ; sqlite3_close ( db ) ; return 0 ; } * * * # # The `` Control '' Running code as-is n't actually perform database operations , give us idea fast raw C file I/O string processing operations . > Imported 864913 records 0.94 seconds Great ! We 920,000 inserts per second , provided n't actually inserts : - ) * * * # # The `` Worst-Case-Scenario '' We 're going generate SQL string using values read file invoke SQL operation using sqlite3_exec : sprintf ( sSQL , `` INSERT INTO TTC VALUES ( NULL , ' % ' , ' % ' , ' % ' , ' % ' , ' % ' , ' % ' , ' % ' ) '' , sRT , sBR , sVR , sST , sVI , sDT , sTM ) ; sqlite3_exec ( db , sSQL , NULL , NULL , & sErrMsg ) ; This going slow SQL compiled VDBE code every insert every insert happen transaction . _How slow ? _ > Imported 864913 records 9933.61 seconds Yikes ! 2 hours 45 minutes ! That 's **85 inserts per second . ** # # Using Transaction By default , SQLite evaluate every INSERT / UPDATE statement within unique transaction . If performing large number inserts , 's advisable wrap operation transaction : sqlite3_exec ( db , `` BEGIN TRANSACTION '' , NULL , NULL , & sErrMsg ) ; pFile = fopen ( INPUTDATA , '' r '' ) ; ( ! feof ( pFile ) ) { ... } fclose ( pFile ) ; sqlite3_exec ( db , `` END TRANSACTION '' , NULL , NULL , & sErrMsg ) ; > Imported 864913 records 38.03 seconds That 's better . Simply wrapping inserts single transaction improved performance **23,000 inserts per second . ** # # Using Prepared Statement Using transaction huge improvement , recompiling SQL statement every insert n't make sense using SQL over-and-over . Let 's use ` sqlite3_prepare_v2 ` compile SQL statement bind parameters statement using ` sqlite3_bind_text ` : /* Open input file import database */ cStartClock = clock ( ) ; sprintf ( sSQL , `` INSERT INTO TTC VALUES ( NULL , @ RT , @ BR , @ VR , @ ST , @ VI , @ DT , @ TM ) '' ) ; sqlite3_prepare_v2 ( db , sSQL , BUFFER_SIZE , & stmt , & tail ) ; sqlite3_exec ( db , `` BEGIN TRANSACTION '' , NULL , NULL , & sErrMsg ) ; pFile = fopen ( INPUTDATA , '' r '' ) ; ( ! feof ( pFile ) ) { fgets ( sInputBuf , BUFFER_SIZE , pFile ) ; sRT = strtok ( sInputBuf , `` \t '' ) ; /* Get Route */ sBR = strtok ( NULL , `` \t '' ) ; /* Get Branch */ sVR = strtok ( NULL , `` \t '' ) ; /* Get Version */ sST = strtok ( NULL , `` \t '' ) ; /* Get Stop Number */ sVI = strtok ( NULL , `` \t '' ) ; /* Get Vehicle */ sDT = strtok ( NULL , `` \t '' ) ; /* Get Date */ sTM = strtok ( NULL , `` \t '' ) ; /* Get Time */ sqlite3_bind_text ( stmt , 1 , sRT , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 2 , sBR , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 3 , sVR , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 4 , sST , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 5 , sVI , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 6 , sDT , -1 , SQLITE_TRANSIENT ) ; sqlite3_bind_text ( stmt , 7 , sTM , -1 , SQLITE_TRANSIENT ) ; sqlite3_step ( stmt ) ; sqlite3_clear_bindings ( stmt ) ; sqlite3_reset ( stmt ) ; n++ ; } fclose ( pFile ) ; sqlite3_exec ( db , `` END TRANSACTION '' , NULL , NULL , & sErrMsg ) ; printf ( `` Imported % records % 4.2f seconds\n '' , n , ( clock ( ) - cStartClock ) / ( double ) CLOCKS_PER_SEC ) ; sqlite3_finalize ( stmt ) ; sqlite3_close ( db ) ; return 0 ; > Imported 864913 records 16.27 seconds Nice ! There 's little bit code ( n't forget call ` sqlite3_clear_bindings ` ` sqlite3_reset ` ) , 've doubled performance **53,000 inserts per second . ** # # PRAGMA synchronous = OFF By default , SQLite pause issuing OS-level write command . This guarantees data written disk . By setting ` synchronous = OFF ` , instructing SQLite simply hand-off data OS writing continue . There 's chance database file may become corrupted computer suffers catastrophic crash ( power failure ) data written platter : /* Open database create schema */ sqlite3_open ( DATABASE , & db ) ; sqlite3_exec ( db , TABLE , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` PRAGMA synchronous = OFF '' , NULL , NULL , & sErrMsg ) ; > Imported 864913 records 12.41 seconds The improvements smaller , 're **69,600 inserts per second . ** # # PRAGMA journal_mode = MEMORY Consider storing rollback journal memory evaluating ` PRAGMA journal_mode = MEMORY ` . Your transaction faster , lose power program crashes transaction database could left corrupt state partially-completed transaction : /* Open database create schema */ sqlite3_open ( DATABASE , & db ) ; sqlite3_exec ( db , TABLE , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` PRAGMA journal_mode = MEMORY '' , NULL , NULL , & sErrMsg ) ; > Imported 864913 records 13.50 seconds A little slower previous optimization **64,000 inserts per second . ** # # PRAGMA synchronous = OFF _and_ PRAGMA journal_mode = MEMORY Let 's combine previous two optimizations . It 's little risky ( case crash ) , 're importing data ( running bank ) : /* Open database create schema */ sqlite3_open ( DATABASE , & db ) ; sqlite3_exec ( db , TABLE , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` PRAGMA synchronous = OFF '' , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` PRAGMA journal_mode = MEMORY '' , NULL , NULL , & sErrMsg ) ; > Imported 864913 records 12.00 seconds Fantastic ! We 're able **72,000 inserts per second . ** # # Using In-Memory Database Just kicks , let 's build upon previous optimizations redefine database filename 're working entirely RAM : # define DATABASE `` : memory : '' > Imported 864913 records 10.94 seconds It 's super-practical store database RAM , 's impressive perform **79,000 inserts per second . ** # # Refactoring C Code Although specifically SQLite improvement , I n't like extra ` char* ` assignment operations ` ` loop . Let 's quickly refactor code pass output ` strtok ( ) ` directly ` sqlite3_bind_text ( ) ` , let compiler try speed things us : pFile = fopen ( INPUTDATA , '' r '' ) ; ( ! feof ( pFile ) ) { fgets ( sInputBuf , BUFFER_SIZE , pFile ) ; sqlite3_bind_text ( stmt , 1 , strtok ( sInputBuf , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Route */ sqlite3_bind_text ( stmt , 2 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Branch */ sqlite3_bind_text ( stmt , 3 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Version */ sqlite3_bind_text ( stmt , 4 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Stop Number */ sqlite3_bind_text ( stmt , 5 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Vehicle */ sqlite3_bind_text ( stmt , 6 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Date */ sqlite3_bind_text ( stmt , 7 , strtok ( NULL , `` \t '' ) , -1 , SQLITE_TRANSIENT ) ; /* Get Time */ sqlite3_step ( stmt ) ; /* Execute SQL Statement */ sqlite3_clear_bindings ( stmt ) ; /* Clear bindings */ sqlite3_reset ( stmt ) ; /* Reset VDBE */ n++ ; } fclose ( pFile ) ; **Note : We back using real database file . In-memory databases fast , necessarily practical** > Imported 864913 records 8.94 seconds A slight refactoring string processing code used parameter binding allowed us perform **96,700 inserts per second . ** I think safe say _plenty fast_ . As start tweak variables ( i.e . page size , index creation , etc . ) benchmark . * * * # # Summary ( far ) _I hope 're still ! _ The reason started road bulk-insert performance varies wildly SQLite , 's always obvious changes need made speed-up operation . Using compiler ( compiler options ) , version SQLite data 've optimized code usage SQLite go **from worst-case scenario 85 inserts per second 96,000 inserts per second ! ** * * * # # CREATE INDEX INSERT vs. INSERT CREATE INDEX Before start measuring ` SELECT ` performance , know 'll creating indexes . It 's suggested one answers bulk inserts , faster create index data inserted ( opposed creating index first inserting data ) . Let 's try : **Create Index Insert Data** sqlite3_exec ( db , `` CREATE INDEX 'TTC_Stop_Index ' ON 'TTC ' ( 'Stop ' ) '' , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` BEGIN TRANSACTION '' , NULL , NULL , & sErrMsg ) ; ... > Imported 864913 records 18.13 seconds **Insert Data Create Index** ... sqlite3_exec ( db , `` END TRANSACTION '' , NULL , NULL , & sErrMsg ) ; sqlite3_exec ( db , `` CREATE INDEX 'TTC_Stop_Index ' ON 'TTC ' ( 'Stop ' ) '' , NULL , NULL , & sErrMsg ) ; > Imported 864913 records 13.66 seconds As expected , bulk-inserts slower one column indexed , make difference index created data inserted . Our no- index baseline 96,000 inserts per second . **Creating index first inserting data gives us 47,700 inserts per second , whereas inserting data first creating index gives us 63,300 inserts per second . ** * * * I 'd gladly take suggestions scenarios try ... And compiling similar data SELECT queries soon .",There steps see tables SQLite database : 1 . List tables database : .tables 2 . List table looks : .schema tablename 3 . Print entire table : SELECT * FROM tablename ; 4 . List available SQLite prompt commands : .help
Stackoverflow,How column default value added existing table [ SQL Server 2000 ] ( http : //en.wikipedia.org/wiki/Microsoft_SQL_Server # Genesis ) / [ SQL Server 2005 ] ( http : //en.wikipedia.org/wiki/Microsoft_SQL_Server # SQL_Server_2005 ) ?,"# # Syntax : ALTER TABLE { TABLENAME } ADD { COLUMNNAME } { TYPE } { NULL|NOT NULL } CONSTRAINT { CONSTRAINT_NAME } DEFAULT { DEFAULT_VALUE } WITH VALUES # # Example : ALTER TABLE SomeTable ADD SomeCol Bit NULL -- Or NOT NULL . CONSTRAINT D_SomeTable_SomeCol -- When Omitted Default-Constraint Name autogenerated . DEFAULT ( 0 ) -- Optional Default-Constraint . WITH VALUES -- Add Column Nullable want Default Value Existing Records . # # Notes : **Optional Constraint Name : ** If leave ` CONSTRAINT D_SomeTable_SomeCol ` SQL Server autogenerate Default-Contraint funny Name like : ` DF__SomeTa__SomeC__4FB7FEF6 ` **Optional With-Values Statement : ** The ` WITH VALUES ` needed Column Nullable want Default Value used Existing Records . If Column ` NOT NULL ` , automatically use Default Value Existing Records , whether specify ` WITH VALUES ` . **How Inserts work Default-Constraint : ** If insert Record ` SomeTable ` **_not_** Specify ` SomeCol ` 's value , Default ` 0 ` . If insert Record **_and_** Specify ` SomeCol ` 's value ` NULL ` ( column allows nulls ) , Default-Constraint **_not_** used ` NULL ` inserted Value . Notes based everyone 's great feedback . Special Thanks : @ Yatrix , @ WalterStabosz , @ YahooSerious , @ StackMan Comments ."
Stackoverflow,How column default value added existing table [ SQL Server 2000 ] ( http : //en.wikipedia.org/wiki/Microsoft_SQL_Server # Genesis ) / [ SQL Server 2005 ] ( http : //en.wikipedia.org/wiki/Microsoft_SQL_Server # SQL_Server_2005 ) ?,"# # Syntax : ALTER TABLE { TABLENAME } ADD { COLUMNNAME } { TYPE } { NULL|NOT NULL } CONSTRAINT { CONSTRAINT_NAME } DEFAULT { DEFAULT_VALUE } WITH VALUES # # Example : ALTER TABLE SomeTable ADD SomeCol Bit NULL -- Or NOT NULL . CONSTRAINT D_SomeTable_SomeCol -- When Omitted Default-Constraint Name autogenerated . DEFAULT ( 0 ) -- Optional Default-Constraint . WITH VALUES -- Add Column Nullable want Default Value Existing Records . # # Notes : **Optional Constraint Name : ** If leave ` CONSTRAINT D_SomeTable_SomeCol ` SQL Server autogenerate Default-Contraint funny Name like : ` DF__SomeTa__SomeC__4FB7FEF6 ` **Optional With-Values Statement : ** The ` WITH VALUES ` needed Column Nullable want Default Value used Existing Records . If Column ` NOT NULL ` , automatically use Default Value Existing Records , whether specify ` WITH VALUES ` . **How Inserts work Default-Constraint : ** If insert Record ` SomeTable ` **_not_** Specify ` SomeCol ` 's value , Default ` 0 ` . If insert Record **_and_** Specify ` SomeCol ` 's value ` NULL ` ( column allows nulls ) , Default-Constraint **_not_** used ` NULL ` inserted Value . Notes based everyone 's great feedback . Special Thanks : @ Yatrix , @ WalterStabosz , @ YahooSerious , @ StackMan Comments ."
Stackoverflow,"Would recommend using [ datetime ] ( https : //dev.mysql.com/doc/refman/5.0/en/datetime.html ) [ timestamp ] ( https : //dev.mysql.com/doc/refman/5.0/en/datetime.html ) field , ( using MySQL ) ? I 'm working PHP server side .","Timestamps MySQL generally used track changes records , often updated every time record changed . If want store specific value use datetime field . If meant want decide using UNIX timestamp native MySQL datetime field , go native format . You calculations within MySQL way ` ( `` SELECT DATE_ADD ( my_datetime , INTERVAL 1 DAY ) '' ) ` simple change format value UNIX timestamp ` ( `` SELECT UNIX_TIMESTAMP ( my_datetime ) '' ) ` query record want operate PHP ."
Stackoverflow,"I need add specific column exist . I something like following , always returns false : IF EXISTS ( SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'myTableName' AND COLUMN_NAME = 'myColumnName ' ) How I check column exists table SQL Server database ?","SQL Server 2005 onwards : IF EXISTS ( SELECT 1 FROM sys.columns WHERE Name = N'columnName' AND Object_ID = Object_ID ( N'schemaName.tableName ' ) ) BEGIN -- Column Exists END Martin Smith 's version shorter : IF COL_LENGTH ( 'schemaName.tableName ' , 'columnName ' ) IS NOT NULL BEGIN -- Column Exists END"
Stackoverflow,"I need add specific column exist . I something like following , always returns false : IF EXISTS ( SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'myTableName' AND COLUMN_NAME = 'myColumnName ' ) How I check column exists table SQL Server database ?","SQL Server 2005 onwards : IF EXISTS ( SELECT 1 FROM sys.columns WHERE Name = N'columnName' AND Object_ID = Object_ID ( N'schemaName.tableName ' ) ) BEGIN -- Column Exists END Martin Smith 's version shorter : IF COL_LENGTH ( 'schemaName.tableName ' , 'columnName ' ) IS NOT NULL BEGIN -- Column Exists END"
Stackoverflow,"I need add specific column exist . I something like following , always returns false : IF EXISTS ( SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = 'myTableName' AND COLUMN_NAME = 'myColumnName ' ) How I check column exists table SQL Server database ?","SQL Server 2005 onwards : IF EXISTS ( SELECT 1 FROM sys.columns WHERE Name = N'columnName' AND Object_ID = Object_ID ( N'schemaName.tableName ' ) ) BEGIN -- Column Exists END Martin Smith 's version shorter : IF COL_LENGTH ( 'schemaName.tableName ' , 'columnName ' ) IS NOT NULL BEGIN -- Column Exists END"
Stackoverflow,What command short key I use exit PostgreSQL command line utility ` psql ` ?,Try ( ` psql ` command-line tool ) : \d+ tablename See [ manual ] ( http : //www.postgresql.org/docs/current/interactive/app- psql.html # APP-PSQL-META-COMMANDS ) info .
Stackoverflow,What command short key I use exit PostgreSQL command line utility ` psql ` ?,"Type ` \q ` press ` ENTER ` quit ` psql ` . **UPDATE : 19-OCT-2018** As _PostgreSQL 11_ , keywords `` ` quit ` `` `` ` exit ` `` PostgreSQL command-line interface included help make easier leave command-line tool ."
Stackoverflow,What command short key I use exit PostgreSQL command line utility ` psql ` ?,Try ( ` psql ` command-line tool ) : \d+ tablename See [ manual ] ( http : //www.postgresql.org/docs/current/interactive/app- psql.html # APP-PSQL-META-COMMANDS ) info .
Stackoverflow,"I two tables , ` movies ` ` categories ` , I get ordered list **categoryID** first **Name** . The movie table three columns , **ID , Name , CategoryID** . The category table two columns , **ID , Name** . I tried something like following , n't work . var movies = _db.Movies.OrderBy ( = > { m.CategoryID , m.Name } )",This work : var movies = _db.Movies.OrderBy ( c = > c.Category ) .ThenBy ( n = > n.Name )
Stackoverflow,I 'm using MySQL command line utility navigate database . Now I need see list user accounts . How I ? I 'm using MySQL version ` 5.4.1 ` .,"Use query : SELECT User FROM mysql.user ; Which output table like : + -- -- -- -+ | User | + -- -- -- -+ | root | + -- -- -- -+ | user2 | + -- -- -- -+ As Matthew Scharley [ points comments answer ] ( https : //stackoverflow.com/questions/1135245/how-to-get-a-list-of- mysql-user-accounts/1135251 # comment953239_1135251 ) , group ` User ` column 'd like see unique usernames ."
Stackoverflow,"I trying ` INSERT INTO ` table using input another table . Although entirely feasible many database engines , I always seem struggle remember correct syntax ` SQL ` engine day ( [ MySQL ] ( http : //en.wikipedia.org/wiki/MySQL ) , [ Oracle ] ( http : //en.wikipedia.org/wiki/Oracle_Database ) , [ SQL Server ] ( http : //en.wikipedia.org/wiki/Microsoft_SQL_Server ) , [ Informix ] ( http : //en.wikipedia.org/wiki/IBM_Informix ) , [ DB2 ] ( http : //en.wikipedia.org/wiki/IBM_DB2 ) ) . Is silver-bullet syntax coming SQL standard ( example , [ SQL-92 ] ( http : //en.wikipedia.org/wiki/SQL-92 ) ) would allow insert values without worrying underlying database ?",Try : INSERT INTO table1 ( column1 ) SELECT col1 FROM table2 This standard ANSI SQL work DBMS It definitely works : * Oracle * MS SQL Server * MySQL * Postgres * SQLite v3 * Teradata * DB2 * Sybase * Vertica * HSQLDB * H2 * AWS RedShift * SAP HANA
Stackoverflow,I want query something SQL 's ` like ` query : SELECT * FROM users WHERE name LIKE ' % % ' How I achieve MongoDB ? I ca n't find operator ` like ` [ documentation ] ( http : //www.mongodb.org/display/DOCS/Advanced+Queries ) .,"That would : db.users.find ( { `` name '' : /.*m . */ } ) , similar : db.users.find ( { `` name '' : /m/ } ) You 're looking something contains `` '' somewhere ( SQL 's ' ` % ` ' operator equivalent Regexp 's ' ` . * ` ' ) , something `` '' anchored beginning string ."
Stackoverflow,"I need update table **SQL Server 2005** data 'parent' table , see : **sale** id ( int ) udid ( int ) assid ( int ) **ud** id ( int ) assid ( int ) ` sale.assid ` contains correct value update ` ud.assid ` . What query ? I 'm thinking ` join ` I 'm sure possible .","Syntax strictly depends SQL DBMS 're using . Here ways ANSI/ISO ( aka work SQL DBMS ) , MySQL , SQL Server , Oracle . Be advised suggested ANSI/ISO method typically much slower two methods , 're using SQL DBMS MySQL , SQL Server , Oracle , may way go ( e.g . SQL DBMS n't support ` MERGE ` ) : ANSI/ISO : update ud set assid = ( select sale.assid sale sale.udid = ud.id ) exists ( select * sale sale.udid = ud.id ) ; MySQL : update ud u inner join sale u.id = s.udid set u.assid = s.assid SQL Server : update u set u.assid = s.assid ud u inner join sale u.id = s.udid PostgreSQL : update ud set ud.assid = s.assid sale ud.id = s.udid ; Note target table must repeated ` FROM ` clause Postgres . Oracle : update ( select u.assid new_assid , s.assid old_assid ud u inner join sale u.id = s.udid ) set up.new_assid = up.old_assid SQLite : update ud set assid = ( select sale.assid sale sale.udid = ud.id ) RowID ( select RowID ud sale.udid = ud.id ) ;"
Stackoverflow,"What SQL used list tables , rows within tables SQLite database file - I attached ` ATTACH ` command SQLite 3 command line tool ?",There steps see tables SQLite database : 1 . List tables database : .tables 2 . List table looks : .schema tablename 3 . Print entire table : SELECT * FROM tablename ; 4 . List available SQLite prompt commands : .help
Stackoverflow,"I want delete using ` INNER JOIN ` **SQL Server 2008** . But I get error : > Msg 156 , Level 15 , State 1 , Line 15 > Incorrect syntax near keyword 'INNER ' . My code : DELETE FROM WorkRecord2 INNER JOIN Employee ON EmployeeRun=EmployeeNo WHERE Company = ' 1 ' AND Date = '2013-05-06 '","You need specify table deleting , version alias : DELETE w FROM WorkRecord2 w INNER JOIN Employee e ON EmployeeRun=EmployeeNo WHERE Company = ' 1 ' AND Date = '2013-05-06 '"
Stackoverflow,"I 'm trying install version 1.2.2 MySQL_python adaptor , using fresh virtualenv created ` -- no-site-packages ` option . The current version shown PyPi [ 1.2.3 ] ( http : //pypi.python.org/pypi/MySQL-python/1.2.3 ) . Is way install older version ? I found article stating : pip install MySQL_python==1.2.2 When installed , however , still shows ` MySQL_python-1.2.3-py2.6.egg-info ` site packages . Is problem specific package , I something wrong ?","It seems mysql_config missing system installer could find . Be sure mysql_config really installed . For example Debian/Ubuntu must install package : sudo apt-get install libmysqlclient-dev Maybe mysql_config path , case compile mysql suite . **Update : ** For recent versions debian/ubuntu ( 2018 ) sudo apt install default-libmysqlclient-dev"
Stackoverflow,What difference returning ` IQueryable < T > ` vs. ` IEnumerable < T > ` ? IQueryable < Customer > custs = c db.Customers c.City == `` < City > '' select c ; IEnumerable < Customer > custs = c db.Customers c.City == `` < City > '' select c ; Will deferred execution one preferred ?,"Yes , give [ deferred execution ] ( https : //msdn.microsoft.com/en- us/library/bb738633\ ( v=vs.110\ ) .aspx # Anchor_0 ) . The difference [ ` IQueryable < T > ` ] ( https : //msdn.microsoft.com/en- us/library/bb351562.aspx ) interface allows LINQ-to-SQL ( LINQ.-to- anything really ) work . So refine query [ ` IQueryable < T > ` ] ( https : //msdn.microsoft.com/en-us/library/bb351562.aspx ) , query executed database , possible . For [ ` IEnumerable < T > ` ] ( https : //msdn.microsoft.com/en- us/library/9eekhta0.aspx ) case , LINQ-to-object , meaning objects matching original query loaded memory database . In code : IQueryable < Customer > custs = ... ; // Later ... var goldCustomers = custs.Where ( c = > c.IsGold ) ; That code execute SQL select gold customers . The following code , hand , execute original query database , filtering non-gold customers memory : IEnumerable < Customer > custs = ... ; // Later ... var goldCustomers = custs.Where ( c = > c.IsGold ) ; This quite important difference , working [ ` IQueryable < T > ` ] ( https : //msdn.microsoft.com/en-us/library/bb351562.aspx ) many cases save returning many rows database . Another prime example paging : If use [ ` Take ` ] ( https : //msdn.microsoft.com/en-us/library/bb300906.aspx ) [ ` Skip ` ] ( https : //msdn.microsoft.com/en-us/library/bb357513.aspx ) [ ` IQueryable ` ] ( https : //msdn.microsoft.com/en- us/library/system.linq.iqueryable.aspx ) , get number rows requested ; [ ` IEnumerable < T > ` ] ( https : //msdn.microsoft.com/en- us/library/9eekhta0.aspx ) cause rows loaded memory ."
Stackoverflow,"If application requires database comes built data , best way ship application ? Should I : 1 . Precreate SQLite database include ` .apk ` ? 2 . Include SQL commands application create database insert data first use ? The drawbacks I see : 1 . Possible SQLite version mismatches might cause problems I currently n't know database go access . 2 . It may take really long time create populate database device . Any suggestions ? Pointers documentation regarding issues would greatly appreciated .","I found way _ ` ReignDesign blog ` _ article titled [ Using SQLite database Android applications ] ( http : //www.reigndesign.com/blog/using-your-own-sqlite-database- in-android-applications/ ) . Basically precreate database , put assets directory apk , first use copy ` /data/data/YOUR_PACKAGE/databases/ ` directory ."
Stackoverflow,What easiest way save PL/pgSQL output PostgreSQL database CSV file ? I 'm using PostgreSQL 8.4 pgAdmin III PSQL plugin I run queries .,"Do want resulting file server , client ? # Server side If want something easy re-use automate , use Postgresql's built [ COPY ] ( http : //www.postgresql.org/docs/current/interactive/sql- copy.html ) command . e.g . Copy ( Select * From foo ) To '/tmp/test.csv ' With CSV DELIMITER ' , ' ; **This approach runs entirely remote server** \- ca n't write local PC . It also needs run Postgres `` superuser '' ( normally called '' root '' ) Postgres ca n't stop nasty things machine's local filesystem . That n't actually mean connected superuser ( automating would security risk different kind ) , use [ ` SECURITY DEFINER ` option ` CREATE FUNCTION ` ] ( http : //www.postgresql.org/docs/current/interactive/sql- createfunction.html ) make function _runs though superuser_ . The crucial part function perform additional checks , by-pass security - could write function exports exact data need , could write something accept various options long meet strict whitelist . You need check two things : 1 . Which **files** user allowed read/write disk ? This might particular directory , instance , filename might suitable prefix extension . 2 . Which **tables** user able read/write database ? This would normally defined ` GRANT ` database , function running superuser , tables would normally `` bounds '' fully accessible . You probably ’ want let someone invoke function add rows end “ users ” table… I 've written [ blog post expanding approach ] ( http : //rwec.co.uk/q/pg- copy ) , including examples functions export ( import ) files tables meeting strict conditions . * * * # Client side The approach **do file handling client side** , i.e . application script . The Postgres server n't need know file 're copying , spits data client puts somewhere . The underlying syntax ` COPY TO STDOUT ` command , graphical tools like pgAdmin wrap nice dialog . The ** ` psql ` command-line client** special `` meta-command '' called ** ` \copy ` ** , takes options `` real '' ` COPY ` , run inside client : \copy ( Select * From foo ) To '/tmp/test.csv ' With CSV Note terminating ` ; ` , meta-commands terminated newline , unlike SQL commands . From [ docs ] ( http : //www.postgresql.org/docs/current/interactive/app- psql.html # APP-PSQL-META-COMMANDS-COPY ) : > Do confuse COPY psql instruction \copy . \copy invokes COPY FROM STDIN COPY TO STDOUT , fetches/stores data file accessible psql client . Thus , file accessibility access rights depend client rather server \copy used . Your application programming language _may_ also support pushing fetching data , generally use ` COPY FROM STDIN ` / ` TO STDOUT ` within standard SQL statement , way connecting input/output stream . PHP 's PostgreSQL handler ( _not_ PDO ) includes basic [ ` pg_copy_from ` ] ( http : //www.php.net/manual/en/function.pg-copy-from.php ) [ ` pg_copy_to ` ] ( http : //www.php.net/manual/en/function.pg-copy-to.php ) functions copy to/from PHP array , may efficient large data sets ."
Stackoverflow,"I 've learning Functions Stored Procedure quite I n't know I use function stored procedure . They look , maybe I kinda newbie . Can one tell ?","Functions computed values perform permanent environmental changes SQL Server ( i.e . INSERT UPDATE statements allowed ) . A function used inline SQL statements returns scalar value , joined upon returns result set . **A point worth noting comments , summarize answer . Thanks @ Sean K Anderson : ** > Functions follow computer-sciency definition MUST return value alter data receive parameters ( arguments ) . Functions allowed change anything , must least one parameter , must return value . Stored procs parameter , change database objects , return value ."
Stackoverflow,"In MongoDB shell , I list collections current database I 'm using ?","You ... JS ( shell ) : db.getCollectionNames ( ) node.js : db.listCollections ( ) non-JS ( shell ) : show collections The reason I call non-JS : $ mongo prodmongo/app -- eval `` show collections '' MongoDB shell version : 3.2.10 connecting : prodmongo/app 2016-10-26T19:34:34.886-0400 E QUERY [ thread1 ] SyntaxError : missing ; statement @ ( shell eval ) :1:5 $ mongo prodmongo/app -- eval `` db.getCollectionNames ( ) '' MongoDB shell version : 3.2.10 connecting : prodmongo/app [ `` Profiles '' , `` Unit_Info '' ] If really want sweet , sweet ` show collections ` output , : $ mongo prodmongo/app -- eval `` db.getCollectionNames ( ) .join ( '\n ' ) '' MongoDB shell version : 3.2.10 connecting : prodmongo/app Profiles Unit_Info"
Stackoverflow,"I using Ruby Rails 3.1 pre version . I like use PostgreSQL , problem installing ` pg ` gem . It gives following error : $ gem install pg Building native extensions . This could take ... ERROR : Error installing pg : ERROR : Failed build gem native extension . /home/u/.rvm/rubies/ruby-1.9.2-p0/bin/ruby extconf.rb checking pg_config ... No pg_config ... trying anyway . If building fails , please try -- with-pg-config=/path/to/pg_config checking libpq-fe.h ... Ca n't find 'libpq-fe.h header *** extconf.rb failed *** Could create Makefile due reason , probably lack necessary libraries and/or headers . Check mkmf.log file details . You may need configuration options . Provided configuration options : -- with-opt-dir -- without-opt-dir -- with-opt-include -- without-opt-include= $ { opt-dir } /include -- with-opt-lib -- without-opt-lib= $ { opt-dir } /lib -- with-make-prog -- without-make-prog -- srcdir= . -- curdir -- ruby=/home/u/.rvm/rubies/ruby-1.9.2-p0/bin/ruby -- with-pg -- without-pg -- with-pg-dir -- without-pg-dir -- with-pg-include -- without-pg-include= $ { pg-dir } /include -- with-pg-lib -- without-pg-lib= $ { pg-dir } /lib -- with-pg-config -- without-pg-config -- with-pg_config -- without-pg_config Gem files remain installed /home/u/.rvm/gems/ruby-1.9.2-p0/gems/pg-0.11.0 inspection . Results logged /home/u/.rvm/gems/ruby-1.9.2-p0/gems/pg-0.11.0/ext/gem_make.out How I solve problem ?","It looks like Ubuntu header part [ ` libpq-dev ` package ] ( http : //packages.ubuntu.com/natty/i386/libpq-dev/filelist ) ( least following Ubuntu versions : [ 11.04 ] ( http : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_11.04_.28Natty_Narwhal.29 ) ( Natty Narwhal ) , [ 10.04 ] ( https : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_10.04_LTS_.28Lucid_Lynx.29 ) ( Lucid Lynx ) , [ 11.10 ] ( http : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_11.10_.28Oneiric_Ocelot.29 ) ( Oneiric Ocelot ) , [ 12.04 ] ( http : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_12.04_LTS_.28Precise_Pangolin.29 ) ( Precise Pangolin ) , [ 14.04 ] ( https : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_14.04_LTS_.28Trusty_Tahr.29 ) ( Trusty Tahr ) [ 18.04 ] ( https : //en.wikipedia.org/wiki/Ubuntu_version_history # Ubuntu_18.04_LTS_\ ( Bionic_Beaver\ ) ) ( Bionic Beaver ) ) : ... /usr/include/postgresql/libpq-fe.h ... So try installing ` libpq-dev ` equivalent OS : * For Ubuntu/Debian systems : ` sudo apt-get install libpq-dev ` * On [ Red Hat Linux ] ( http : //en.wikipedia.org/wiki/Red_Hat_Linux ) ( RHEL ) systems : ` yum install postgresql-devel ` * For Mac [ Homebrew ] ( https : //en.wikipedia.org/wiki/Homebrew_ % 28package_management_software % 29 ) : ` brew install postgresql ` * For Mac [ MacPorts ] ( http : //en.wikipedia.org/wiki/MacPorts ) PostgreSQL : ` gem install pg -- -- with-pg-config=/opt/local/lib/postgresql [ version number ] /bin/pg_config ` * For [ OpenSuse ] ( https : //en.wikipedia.org/wiki/OpenSUSE ) : ` zypper postgresql-devel ` * For [ ArchLinux ] ( https : //en.wikipedia.org/wiki/Arch_Linux ) : ` pacman -S postgresql-libs `"
Stackoverflow,"I 'm trying update column ` visited ` give value 1 . I use MySQL workbench , I 'm writing statement SQL editor inside workbench . I 'm writing following command : UPDATE tablename SET columnname=1 ; It gives following error : > You using safe update mode tried update table without WHERE uses KEY column To disable safe mode , toggle option ... . I followed instructions , I unchecked ` safe update ` option ` Edit ` menu ` Preferences ` ` SQL Editor ` . The error still appear & I 'm able update value . Please , tell wrong ?",It looks like MySql session [ safe-updates option ] ( http : //dev.mysql.com/doc/refman/5.5/en/mysql-command- options.html # option_mysql_safe-updates ) set . This means ca n't update delete records without specifying key ( ex . ` primary key ` ) clause . Try : SET SQL_SAFE_UPDATES = 0 ; Or modify query follow rule ( use ` primary key ` ` clause ` ) .
Stackoverflow,"I inserted records SQL Server database table . The table primary key defined auto increment identity seed set “ Yes ” . This done primarily SQL Azure , table primary key identity defined . But since I delete records table , identity seed tables disturbed index column ( auto-generated increment 1 ) get disturbed . **How I reset identity column I deleted records column sequence ascending numerical order ? ** The identity column used foreign key anywhere database .","The [ ` DBCC CHECKIDENT ` ] ( http : //technet.microsoft.com/en- us/library/ms176057.aspx ) management command used reset identity counter . The command syntax : DBCC CHECKIDENT ( table_name [ , { NORESEED | { RESEED [ , new_reseed_value ] } } ] ) [ WITH NO_INFOMSGS ] Example : DBCC CHECKIDENT ( ' [ TestTable ] ' , RESEED , 0 ) ; GO It supported previous versions Azure SQL Database , supported . * * * Please note ` new_reseed_value ` argument varied across SQL Server versions [ according documentation ] ( https : //docs.microsoft.com/en- us/sql/t-sql/database-console-commands/dbcc-checkident-transact-sql ) : > If rows present table , next row inserted _new_reseed_value_ value . In version SQL Server 2008 R2 earlier , next row inserted uses _new_reseed_value_ \+ current increment value . However , **I find information misleading** ( plain wrong actually ) observed behaviour indicates least SQL Server 2012 still uses _new_reseed_value_ \+ current increment value logic . Microsoft even contradicts ` Example C ` found page : > C. Forcing current identity value new value > > The following example forces current identity value AddressTypeID column AddressType table value 10 . Because table existing rows , next row inserted use 11 value , , new current increment value defined column value plus 1 . USE AdventureWorks2012 ; GO DBCC CHECKIDENT ( 'Person.AddressType ' , RESEED , 10 ) ; GO Still , leaves option different behaviour newer SQL Server versions . I guess way sure , Microsoft clear things documentation , actual tests usage ."
Stackoverflow,"Several months ago I learned answer Stack Overflow perform multiple updates MySQL using following syntax : INSERT INTO table ( id , field , field2 ) VALUES ( 1 , A , X ) , ( 2 , B , Y ) , ( 3 , C , Z ) ON DUPLICATE KEY UPDATE field=VALUES ( Col1 ) , field2=VALUES ( Col2 ) ; I 've switched PostgreSQL apparently correct . It's referring correct tables I assume 's matter different keywords used I 'm sure PostgreSQL documentation covered . To clarify , I want insert several things already exist update .","**Warning : safe executed multiple sessions time** ( see caveats ) . * * * Another clever way `` UPSERT '' postgresql two sequential UPDATE/INSERT statements designed succeed effect . UPDATE table SET field= ' C ' , field2= ' Z ' WHERE id=3 ; INSERT INTO table ( id , field , field2 ) SELECT 3 , ' C ' , ' Z' WHERE NOT EXISTS ( SELECT 1 FROM table WHERE id=3 ) ; The UPDATE succeed row `` id=3 '' already exists , otherwise effect . The INSERT succeed row `` id=3 '' already exist . You combine two single string run single SQL statement execute application . Running together single transaction highly recommended . This works well run isolation locked table , subject race conditions mean might still fail duplicate key error row inserted concurrently , might terminate row inserted row deleted concurrently . A ` SERIALIZABLE ` transaction PostgreSQL 9.1 higher handle reliably cost high serialization failure rate , meaning 'll retry lot . See [ upsert complicated ] ( http : //www.depesz.com/2012/06/10/why-is-upsert-so- complicated/ ) , discusses case detail . This approach also [ subject lost updates ` read committed ` isolation unless application checks affected row counts verifies either ` insert ` ` update ` affected row ] ( https : //dba.stackexchange.com/q/78510/7788 ) ."
Stackoverflow,"I 've created database , example 'mydb ' . CREATE DATABASE mydb CHARACTER SET utf8 COLLATE utf8_bin ; CREATE USER 'myuser ' @ ' % ' IDENTIFIED BY PASSWORD '*HASH ' ; GRANT ALL ON mydb . * TO 'myuser ' @ ' % ' ; GRANT ALL ON mydb TO 'myuser ' @ ' % ' ; GRANT CREATE ON mydb TO 'myuser ' @ ' % ' ; FLUSH PRIVILEGES ; Now login database everywhere , ca n't create tables . How grant privileges database ( future ) tables . I ca n't create tables 'mydb ' database . I always get : CREATE TABLE ( c CHAR ( 20 ) CHARACTER SET utf8 COLLATE utf8_bin ) ; ERROR 1142 ( 42000 ) : CREATE command denied user 'myuser ' @ ' ... ' table 't '","GRANT ALL PRIVILEGES ON mydb . * TO 'myuser ' @ ' % ' WITH GRANT OPTION ; This I create `` Super User '' privileges ( although I would normally specify host ) . # IMPORTANT NOTE While answer solve problem access , ` WITH GRANT OPTION ` creates MySQL user [ edit permissions users ] ( https : //dev.mysql.com/doc/refman/5.6/en/privileges- provided.html # priv_grant-option ) . > The GRANT OPTION privilege enables give users remove users privileges possess . For security reasons , use type user account process public access ( i.e . website ) . It recommended [ create user database privileges ] ( https : //stackoverflow.com/a/15707789/2370483 ) kind use ."
Stackoverflow,"This dead simple , I _cannot_ get work life . I 'm trying connect remotely MySQL server . connecting mysql -u root -h localhost -p works fine , trying mysql -u root -h 'any ip address ' -p fails error ERROR 1130 ( 00000 ) : Host `` xxx.xx.xxx.xxx '' allowed connect MySQL server In ` mysql.user ` table , exactly entry user 'root' host 'localhost ' another host ' % ' . I 'm wits ' end , idea proceed . Any ideas welcome .","Possibly security precaution . You could try adding new administrator account : mysql > CREATE USER 'monty ' @ 'localhost ' IDENTIFIED BY 'some_pass ' ; mysql > GRANT ALL PRIVILEGES ON * . * TO 'monty ' @ 'localhost' - > WITH GRANT OPTION ; mysql > CREATE USER 'monty ' @ ' % ' IDENTIFIED BY 'some_pass ' ; mysql > GRANT ALL PRIVILEGES ON * . * TO 'monty ' @ ' % ' - > WITH GRANT OPTION ; Although Pascal others noted 's great idea user kind access open IP . If need administrative user , use root , leave localhost . For action specify exactly privileges need limit accessibility user Pascal suggest . Edit : From MySQL FAQ : > If figure get Access denied , remove user table entries Host values containing wildcards ( entries contain ' % ' ' _ ' characters ) . A common error insert new entry Host= ' % ' User='some_user ' , thinking allows specify localhost connect machine . The reason work default privileges include entry Host='localhost ' User= '' . Because entry Host value 'localhost ' specific ' % ' , used preference new entry connecting localhost ! The correct procedure insert second entry Host='localhost ' User='some_user ' , delete entry Host='localhost ' User= '' . After deleting entry , remember issue FLUSH PRIVILEGES statement reload grant tables . See also Section 5.4.4 , “ Access Control , Stage 1 : Connection Verification ” ."
Stackoverflow,I project I 'd like use .NET 4.0 features core requirement I use System.Data.SQLite framework compiled 2.X . I see mention possible [ accepted answer ] ( https : //stackoverflow.com/questions/1609894/how-to- reference-net-4-0-assembly-within-net-3-5-projects ) I n't see actually achieve . When I try run 4.0 project referencing 2.X assembly I get : > _Mixed mode assembly built version 'v2.0.50727 ' runtime loaded 4.0 runtime without additional configuration information._ What `` additional configuration '' necessary ?,"In order use CLR 2.0 _mixed mode assembly_ , need modify App.Config file include : < ? xml version= '' 1.0 '' ? > < configuration > < startup **useLegacyV2RuntimeActivationPolicy= `` true '' ** > < supportedRuntime version= '' v4.0 '' sku= '' .NETFramework , Version=v4.0 '' / > < /startup > < /configuration > The key ` useLegacyV2RuntimeActivationPolicy ` flag . This causes CLR use latest version ( 4.0 ) load mixed mode assembly . Without , work . Note matters mixed mode ( C++/CLI ) assemblies . You load managed CLR 2 assemblies without specifying ` app.config ` ."
Stackoverflow,I table ` story_category ` database corrupt entries . The next query returns corrupt entries : SELECT * FROM story_category WHERE category_id NOT IN ( SELECT DISTINCT category.id FROM category INNER JOIN story_category ON category_id=category.id ) ; I tried delete executing : DELETE FROM story_category WHERE category_id NOT IN ( SELECT DISTINCT category.id FROM category INNER JOIN story_category ON category_id=category.id ) ; But I get next error : > # 1093 - You ca n't specify target table 'story_category ' update FROM clause How I overcome ?,"The problem MySQL , whatever inane reason , n't allow write queries like : UPDATE myTable SET myTable.A = ( SELECT B FROM myTable INNER JOIN ... ) That , 're ` UPDATE ` / ` INSERT ` / ` DELETE ` table , can't reference table inner query _ ( **can** however reference field outer table ... ) _ * * * The solution replace instance ` myTable ` sub-query ` ( SELECT * FROM myTable ) ` , like UPDATE myTable SET myTable.A = ( SELECT B FROM ( SELECT * FROM myTable ) AS something INNER JOIN ... ) This apparently causes necessary fields implicitly copied temporary table , 's allowed . I found solution [ ] ( http : //www.xaprb.com/blog/2006/06/23/how-to- select-from-an-update-target-in-mysql/ ) . A note article : > You ’ want ` SELECT * FROM table ` subquery real life ; I wanted keep examples simple . In reality , selecting columns need innermost query , adding good ` WHERE ` clause limit results , ."
Stackoverflow,"Is way restrict certain tables mysqldump command ? For example , I 'd use following syntax dump _only_ table1 table2 : mysqldump -u username -p database table1 table2 > database.sql But similar way dump tables _except_ table1 table2 ? I n't found anything mysqldump documentation , brute-force ( specifying table names ) way go ?",You use [ \ -- ignore- table ] ( http : //dev.mysql.com/doc/refman/5.1/en/mysqldump.html # option_mysqldump_ignore- table ) option . So could mysqldump -u USERNAME -pPASSWORD DATABASE -- ignore-table=DATABASE.table1 > database.sql There whitespace ` -p ` ( typo ) . If want ignore multiple tables use simple script like # ! /bin/bash PASSWORD=XXXXXX HOST=XXXXXX USER=XXXXXX DATABASE=databasename DB_FILE=dump.sql EXCLUDED_TABLES= ( table1 table2 table3 table4 tableN ) IGNORED_TABLES_STRING= '' TABLE `` $ { EXCLUDED_TABLES [ @ ] } '' : IGNORED_TABLES_STRING+= '' -- ignore-table= $ { DATABASE } . $ { TABLE } '' done echo `` Dump structure '' mysqldump -- host= $ { HOST } -- user= $ { USER } -- password= $ { PASSWORD } -- single-transaction -- no-data -- routines $ { DATABASE } > $ { DB_FILE } echo `` Dump content '' mysqldump -- host= $ { HOST } -- user= $ { USER } -- password= $ { PASSWORD } $ { DATABASE } -- no-create-info -- skip-triggers $ { IGNORED_TABLES_STRING } > > $ { DB_FILE }
Stackoverflow,"I table ~500k rows ; varchar ( 255 ) UTF8 column ` filename ` contains file name ; I 'm trying strip various strange characters filename - thought I 'd use character class : ` [ ^a-zA-Z0-9 ( ) _ .\- ] ` Now , **is function MySQL lets replace regular expression** ? I 'm looking similar functionality REPLACE ( ) function - simplified example follows : SELECT REPLACE ( 'stackowerflow ' , 'ower ' , 'over ' ) ; Output : `` stackoverflow '' /* something like exist ? */ SELECT X_REG_REPLACE ( 'Stackoverflow ' , '/ [ A-Zf ] / ' , '- ' ) ; Output : `` -tackover-low '' I know [ REGEXP/RLIKE ] ( https : //stackoverflow.com/a/6943142/19746 ) , check _if_ match , _what_ match . ( I _could_ `` ` SELECT pkey_id , filename FROM foo WHERE filename RLIKE ' [ ^a-zA-Z0-9 ( ) _ .\- ] ' ` `` PHP script , ` preg_replace ` '' ` UPDATE foo ... WHERE pkey_id= ... ` `` , looks like last-resort slow & ugly hack )","No . But access server , could use user defined function ( UDF ) like [ mysql-udf-regexp ] ( https : //github.com/hholzgra/mysql-udf-regexp ) . **EDIT : ** MySQL 8.0+ could use natively REGEXP_REPLACE . More answer"
Stackoverflow,I problems trying install ` mysql2 ` gem Rails . When I try install running ` bundle install ` ` gem install mysql2 ` gives following error : > Error installing mysql2 : ERROR : Failed build gem native extension . How I fix successfully install ` mysql2 ` ?,"On Ubuntu/Debian distributions using aptitude : sudo apt-get install libmysql-ruby libmysqlclient-dev Package ` libmysql-ruby ` phased replaced ` ruby-mysql ` . [ This ] ( https : //askubuntu.com/a/641953 ) I found solution . If command n't work ` libmysql-ruby ` found , following sufficient : sudo apt-get install libmysqlclient-dev On Red Hat/CentOS distributions using yum : sudo yum install mysql-devel On Mac OS X [ Homebrew ] ( http : //mxcl.github.com/homebrew/ ) : brew install mysql"
Stackoverflow,"When I executed following command : ALTER TABLE ` mytable ` ADD UNIQUE ( ` column1 ` , ` column2 ` ) ; I got error message : # 1071 - Specified key long ; max key length 767 bytes Information column1 column2 : column1 varchar ( 20 ) utf8_general_ci column2 varchar ( 500 ) utf8_general_ci I think ` varchar ( 20 ) ` requires 21 bytes ` varchar ( 500 ) ` requires 501 bytes . So total bytes 522 , less 767 . So I get error message ? # 1071 - Specified key long ; max key length 767 bytes","767 bytes [ stated prefix limitation ] ( http : //dev.mysql.com/doc/refman/5.1/en/create-index.html ) InnoDB tables MySQL version 5.6 ( prior versions ) . It 's 1,000 bytes long MyISAM tables . In MySQL version 5.7 upwards limit increased 3072 bytes . You also aware set index big char varchar field utf8mb4 encoded , divide max index prefix length 767 bytes ( 3072 bytes ) 4 resulting 191 . This maximum length utf8mb4 character four bytes . For utf8 character would three bytes resulting max index prefix length 254 . One option place lower limit VARCHAR fields . Another option ( according [ response issue ] ( http : //bugs.mysql.com/bug.php ? id=6604 ) ) get subset column rather entire amount , i.e . : ALTER TABLE ` mytable ` ADD UNIQUE ( column1 ( 15 ) , column2 ( 200 ) ) ; Tweak need get key apply , I wonder would worth review data model regarding entity see there's improvements would allow implement intended business rules without hitting MySQL limitation ."
Stackoverflow,"How I rename column table ` xyz ` ? The columns : Manufacurerid , name , status , AI , PK , int I want rename ` manufacturerid ` I tried using PHPMyAdmin panel , I get error : MySQL said : Documentation # 1025 - Error rename '.\shopping\ # sql-c98_26 ' '.\shopping\tblmanufacturer ' ( errno : 150 )","Lone Ranger close ... fact , also need specify datatype renamed column . For example : ALTER TABLE ` xyz ` CHANGE ` manufacurerid ` ` manufacturerid ` INT ; Remember : * Replace INT whatever column data type ( REQUIRED ) * Tilde/ Backtick ( ` ) optional"
Stackoverflow,"I installed SQL Server Express 2012 home server . I 'm trying connect Visual Studio 2012 desktop PC , repeatedly getting well-known error : > A network-related instance-specific error occurred establishing connection SQL Server . The server found accessible . Verify instance name correct SQL Server configured allow remote connections . ( provider : Named Pipes Provider , error : 40 - Could open connection SQL Server ) What I 've done try fix : * Run SQL Server Configuration Manager server enable SQL Server Browser * Add Windows Firewall exception server TCP , ports 1433 1434 local subnet . * Verify I login SQL Server instance user I 'm logged desktop . * Verify I 'm using Windows Authentication SQL Server instance . * Repeatedly restart SQL Server whole dang server . * Pull hair . **How I get SQL Server 2012 Express allow remote connections ! ? **","Well , [ glad I asked ] ( https : //stackoverflow.blog/2012/05/22/encyclopedia-stack- exchange/ ) . The solution I finally discovered : [ How I configure SQL Server Express allow remote tcp/ip connections port 1433 ? ] ( http : //support.webecs.com/KB/a868/how-do-i-configure-sql-server- express-to-allow-remote.aspx ) 1 . Run SQL Server Configuration Manager . 2 . Go SQL Server Network Configuration > Protocols SQLEXPRESS . 3 . Make sure TCP/IP enabled . So far , good , entirely expected . But : 1 . Right-click TCP/IP select **Properties** . 2 . Verify , IP2 , IP Address set computer 's IP address local subnet . 3 . Scroll IPAll . 4 . Make sure **TCP Dynamic Ports** **blank** . ( Mine set 5-digit port number . ) 5 . Make sure **TCP Port** set **1433** . ( Mine blank . ) ( Also , follow steps , 's _not_ necessary enable SQL Server Browser , need allow port 1433 , 1434 . ) These extra five steps something I ca n't remember ever previous version SQL Server , Express otherwise . They appear necessary I 'm using named instance ( myservername\SQLEXPRESS ) server instead default instance . See : [ Configure Server Listen Specific TCP Port ( SQL Server Configuration Manager ) ] ( https : //msdn.microsoft.com/en-us/library/ms177440.aspx )"
Stackoverflow,Is database query faster I insert multiple rows : like INSERT ... . UNION INSERT ... . UNION ( I need insert like 2-3000 rows ),"> ` INSERT ` statements use ` VALUES ` syntax insert multiple rows . To , include multiple lists column values , enclosed within parentheses separated commas . **Example : ** INSERT INTO tbl_name ( , b , c ) VALUES ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) ; [ Source ] ( http : //dev.mysql.com/doc/refman/5.5/en/insert.html )"
Stackoverflow,How I limit SQL Server Profiler trace specific database ? I ca n't see filter trace see events databases instance I connect .,"Under Trace properties > Events Selection tab > select show columns . Now column filters , see database name . Enter database name Like section see traces database ."
Stackoverflow,"I installed [ LAMP ] ( http : //en.wikipedia.org/wiki/LAMP_ % 28software_bundle % 29 ) [ Ubuntu 12.04 LTS ] ( http : //en.wikipedia.org/wiki/List_of_Ubuntu_releases # Ubuntu_12.04_LTS_.28Precise_Pangolin.29 ) ( Precise Pangolin ) set root password [ phpMyAdmin ] ( http : //en.wikipedia.org/wiki/PhpMyAdmin ) . I forgot password I unable login . When I try change password terminal I get : > ERROR 2002 ( HY000 ) : Ca n't connect local MySQL server socket '/var/run/mysqld/mysqld.sock ' ( 2 ) How I fix ? I unable open LAMP , uninstall reinstall .","I problem solved installing ` mysql-server ` , make sure installed ` mysql-server ` , ` mysql-client ` something else . That error means file ` /var/run/mysqld/mysqld.sock ` n't exists , n't install ` mysql-server ` , file would exist . But ` mysql-server ` already installed running , need check config files . The config files : /etc/my.cnf /etc/mysql/my.cnf /var/lib/mysql/my.cnf In ` /etc/my.cnf ` , socket file config may ` /tmp/mysql.sock ` ` /etc/mysql/my.cnf ` socket file config may ` /var/run/mysqld/mysqld.sock ` . So , remove rename ` /etc/mysql/my.cnf ` , let mysql use ` /etc/my.cnf ` , problem may solved ."
Stackoverflow,"In place 're split using mysqli PDO stuff like prepared statements transaction support . Some projects use one , . There little realistic likelihood us ever moving another RDBMS . I prefer PDO single reason allows named parameters prepared statements , far I aware mysqli . Are pros cons choosing one standard consolidate projects use one approach ?","Well , could argue object oriented aspect , prepared statements , fact becomes standard , etc . But I know time , convincing somebody works better killer feature . So : A really nice thing PDO fetch data , injecting automatically object . If n't want use [ ORM ] ( http : //en.wikipedia.org/wiki/Object-relational_mapping ) ( cause 's quick script ) like object mapping , 's REALLY cool : class Student { public $ id ; public $ first_name ; public $ last_name public function getFullName ( ) { return $ this- > first_name . ' '. $ this- > last_name } } try { $ dbh = new PDO ( `` mysql : host= $ hostname ; dbname=school '' , $ username , $ password ) $ stmt = $ dbh- > query ( `` SELECT * FROM students '' ) ; /* MAGIC HAPPENS HERE */ $ stmt- > setFetchMode ( PDO : :FETCH_INTO , new Student ) ; foreach ( $ stmt $ student ) { echo $ student- > getFullName ( ) . ' < br / > ' ; } $ dbh = null ; } catch ( PDOException $ e ) { echo $ e- > getMessage ( ) ; }"
Stackoverflow,"What difference ` flush ( ) ` ` commit ( ) ` SQLAlchemy ? I 've read docs , none wiser - seem assume pre- understanding I n't . I 'm particularly interested impact memory usage . I 'm loading data database series files ( around 5 million rows total ) session occasionally falling - 's large database machine much memory . I 'm wondering I 'm using many ` commit ( ) ` enough ` flush ( ) ` calls - without really understanding difference , 's hard tell !","Just FYI , also specify things column attributes . For instance , I might done : .order_by ( model.Entry.amount.desc ( ) ) This handy since use places relation definition , etc . For information , [ refer ] ( http : //docs.sqlalchemy.org/en/latest/orm/tutorial.html )"
Stackoverflow,I want use Oracle SQL Developer generate ER diagram DB tables I new Oracle tool . What process creating ER diagram SQL Developer ?,"Create diagram existing database schema subset follows : 1 . Click **File → Data Modeler → Import → Data Dictionary** . 2 . Select DB connection ( add one none ) . 3 . Click **Next** . 4 . Check one schema names . 5 . Click **Next** . 6 . Check one objects import . 7 . Click **Next** . 8 . Click **Finish** . The ERD displayed . Export diagram follows : 1 . Click **File → Data Modeler → Print Diagram → To Image File** . 2 . Browse select export file location . 3 . Click **Save** . The diagram exported . To export vector format , use **To PDF File** , instead . This allows simplified editing using [ Inkscape ] ( https : //inkscape.org/ ) ( vector image editor ) . These instructions may work SQL Developer 3.2.09.23 4.1.3.20 ."
Stackoverflow,"Is nice easy way drop tables MySQL database , ignoring foreign key constraints may ?","The best efficient way catch `` table found '' exception : avoids overhead checking table exists twice ; suffer problem DROP fails reason ( might important ) exception still raised caller : BEGIN EXECUTE IMMEDIATE 'DROP TABLE ' || table_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -942 THEN RAISE ; END IF ; END ; **ADDENDUM** For reference , equivalent blocks object types : Sequence BEGIN EXECUTE IMMEDIATE 'DROP SEQUENCE ' || sequence_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -2289 THEN RAISE ; END IF ; END ; View BEGIN EXECUTE IMMEDIATE 'DROP VIEW ' || view_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -942 THEN RAISE ; END IF ; END ; Trigger BEGIN EXECUTE IMMEDIATE 'DROP TRIGGER ' || trigger_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -4080 THEN RAISE ; END IF ; END ; Index BEGIN EXECUTE IMMEDIATE 'DROP INDEX ' || index_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -1418 THEN RAISE ; END IF ; END ; Column BEGIN EXECUTE IMMEDIATE 'ALTER TABLE ' || table_name || ' DROP COLUMN ' || column_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -904 THEN RAISE ; END IF ; END ; Database Link BEGIN EXECUTE IMMEDIATE 'DROP DATABASE LINK ' || dblink_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -2024 THEN RAISE ; END IF ; END ; Materialized View BEGIN EXECUTE IMMEDIATE 'DROP MATERIALIZED VIEW ' || mview_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -12003 THEN RAISE ; END IF ; END ; Type BEGIN EXECUTE IMMEDIATE 'DROP TYPE ' || type_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -4043 THEN RAISE ; END IF ; END ; Constraint BEGIN EXECUTE IMMEDIATE 'ALTER TABLE ' || table_name || ' DROP CONSTRAINT ' || constraint_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -2443 THEN RAISE ; END IF ; END ; Scheduler Job BEGIN DBMS_SCHEDULER.drop_job ( job_name ) ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -27475 THEN RAISE ; END IF ; END ; User / Schema BEGIN EXECUTE IMMEDIATE 'DROP USER ' || user_name ; /* may may want add CASCADE */ EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -1918 THEN RAISE ; END IF ; END ; Package BEGIN EXECUTE IMMEDIATE 'DROP PACKAGE ' || package_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -4043 THEN RAISE ; END IF ; END ; Procedure BEGIN EXECUTE IMMEDIATE 'DROP PROCEDURE ' || procedure_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -4043 THEN RAISE ; END IF ; END ; Function BEGIN EXECUTE IMMEDIATE 'DROP FUNCTION ' || function_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -4043 THEN RAISE ; END IF ; END ; Tablespace BEGIN EXECUTE IMMEDIATE 'DROP TABLESPACE ' || tablespace_name ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -959 THEN RAISE ; END IF ; END ;"
Stackoverflow,"I upgraded system installed MySql 5.7.9 php web application I working . I query dynamically created , run older versions MySql works fine . Since upgrading 5.7 I get error : > Expression # 1 SELECT list GROUP BY clause contains nonaggregated column 'support_desk.mod_users_groups.group_id ' functionally dependent columns GROUP BY clause ; incompatible sql_mode=only_full_group_by Note Manual page Mysql 5.7 topic [ Server SQL Modes ] ( http : //dev.mysql.com/doc/refman/5.7/en/sql-mode.html ) . This query giving trouble : SELECT mod_users_groups.group_id AS 'value ' , group_name AS 'text ' FROM mod_users_groups LEFT JOIN mod_users_data ON mod_users_groups.group_id = mod_users_data.group_id WHERE mod_users_groups.active = 1 AND mod_users_groups.department_id = 1 AND mod_users_groups.manage_work_orders = 1 AND group_name ! = 'root ' AND group_name ! = 'superuser ' GROUP BY group_name HAVING COUNT ( ` user_id ` ) > 0 ORDER BY group_name I googling issue , I n't understand ` only_full_group_by ` enough figure I need fix query . Can I turn ` only_full_group_by ` option , something else I need ? Let know need information .","You try disable ` only_full_group_by ` setting executing following : mysql > set global sql_mode='STRICT_TRANS_TABLES , NO_ZERO_IN_DATE , NO_ZERO_DATE , ERROR_FOR_DIVISION_BY_ZERO , NO_AUTO_CREATE_USER , NO_ENGINE_SUBSTITUTION ' ; mysql > set session sql_mode='STRICT_TRANS_TABLES , NO_ZERO_IN_DATE , NO_ZERO_DATE , ERROR_FOR_DIVISION_BY_ZERO , NO_AUTO_CREATE_USER , NO_ENGINE_SUBSTITUTION ' ; MySQL 8 accept ` NO_AUTO_CREATE_USER ` needs removed ."
Stackoverflow,"How I get : id Name Value 1 A 4 1 B 8 2 C 9 id Column 1 A:4 , B:8 2 C:9","**No CURSOR , WHILE loop , User-Defined Function needed** . Just need creative FOR XML PATH . [ Note : This solution works SQL 2005 later . Original question n't specify version use . ] CREATE TABLE # YourTable ( [ ID ] INT , [ Name ] CHAR ( 1 ) , [ Value ] INT ) INSERT INTO # YourTable ( [ ID ] , [ Name ] , [ Value ] ) VALUES ( 1 , ' A',4 ) INSERT INTO # YourTable ( [ ID ] , [ Name ] , [ Value ] ) VALUES ( 1 , ' B',8 ) INSERT INTO # YourTable ( [ ID ] , [ Name ] , [ Value ] ) VALUES ( 2 , ' C',9 ) SELECT [ ID ] , STUFF ( ( SELECT ' , ' + [ Name ] + ' : ' + CAST ( [ Value ] AS VARCHAR ( MAX ) ) FROM # YourTable WHERE ( ID = Results.ID ) FOR XML PATH ( `` ) , TYPE ) .value ( ' ( ./text ( ) ) [ 1 ] ' , 'VARCHAR ( MAX ) ' ) ,1,2 , '' ) AS NameValues FROM # YourTable Results GROUP BY ID DROP TABLE # YourTable"
Stackoverflow,"I running query MySQL SELECT ID FROM ( SELECT ID , msisdn FROM ( SELECT * FROM TT2 ) ) ; giving error : > Every derived table must alias . What 's causing error ?","Every derived table ( AKA sub-query ) must indeed alias . I.e . query brackets must given alias ( ` AS whatever ` ) , used refer rest outer query . SELECT ID FROM ( SELECT ID , msisdn FROM ( SELECT * FROM TT2 ) AS T ) AS T In case , course , entire query could replaced : SELECT ID FROM TT2"
Stackoverflow,"What difference SQL , PL-SQL T-SQL ? Can anyone explain differences three , provide scenarios would relevantly used ?","* ` SQL ` query language operate sets . It less standardized , used almost relational database management systems : SQL Server , Oracle , MySQL , PostgreSQL , DB2 , Informix , etc . * ` PL/SQL ` proprietary procedural language used Oracle * ` PL/pgSQL ` procedural language used PostgreSQL * ` TSQL ` proprietary procedural language used Microsoft SQL Server . Procedural languages designed extend SQL 's abilities able integrate well SQL . Several features local variables string/data processing added . These features make language Turing- complete . They also used write stored procedures : pieces code residing server manage complex business rules hard impossible manage pure set-based operations ."
Stackoverflow,In Microsoft SQL Server I get query execution plan query / stored procedure ?,"There number methods obtaining execution plan , one use depend circumstances . Usually use SQL Server Management Studio get plan , however reason ca n't run query SQL Server Management Studio might find helpful able obtain plan via SQL Server Profiler inspecting plan cache . # # Method 1 - Using SQL Server Management Studio SQL Server comes couple neat features make easy capture execution plan , simply make sure `` Include Actual Execution Plan '' menu item ( found `` Query '' menu ) ticked run query normal . ! [ Include Action Execution Plan menu item ] ( https : //i.stack.imgur.com/0iCgU.png ) If trying obtain execution plan statements stored procedure execute stored procedure , like : exec p_Example 42 When query completes see extra tab entitled `` Execution plan '' appear results pane . If ran many statements may see many plans displayed tab . ! [ Screenshot Execution Plan ] ( https : //i.stack.imgur.com/iHKOE.png ) From inspect execution plan SQL Server Management Studio , right click plan select `` Save Execution Plan As ... '' save plan file XML format . # # Method 2 - Using SHOWPLAN options This method similar method 1 ( fact SQL Server Management Studio internally ) , however I included completeness n't SQL Server Management Studio available . Before run query , run **one** following statements . The statement must statement batch , i.e . execute another statement time : SET SHOWPLAN_TEXT ON SET SHOWPLAN_ALL ON SET SHOWPLAN_XML ON SET STATISTICS PROFILE ON SET STATISTICS XML ON -- The recommended option use These connection options need run per connection . From point statements run acompanied **additional resultset** containing execution plan desired format - simply run query normally would see plan . Once done turn option following statement : SET < < option > > OFF # # # Comparison execution plan formats Unless strong preference recommendation use ` STATISTICS XML ` option . This option equivalent `` Include Actual Execution Plan '' option SQL Server Management Studio supplies information convenient format . * ` SHOWPLAN_TEXT ` \- Displays basic text based estimated execution plan , without executing query * ` SHOWPLAN_ALL ` \- Displays text based estimated execution plan cost estimations , without executing query * ` SHOWPLAN_XML ` \- Displays XML based estimated execution plan cost estimations , without executing query . This equivalent `` Display Estimated Execution Plan ... '' option SQL Server Management Studio . * ` STATISTICS PROFILE ` \- Executes query displays text based actual execution plan . * ` STATISTICS XML ` \- Executes query displays XML based actual execution plan . This equivalent `` Include Actual Execution Plan '' option SQL Server Management Studio . # # Method 3 - Using SQL Server Profiler If ca n't run query directly ( query n't run slowly execute directly - remember want plan query performing badly ) , capture plan using SQL Server Profiler trace . The idea run query trace capturing one '' Showplan '' events running . Note depending load **can** use method production environment , however obviously use caution . The SQL Server profiling mechanisms designed minimize impact database n't mean wo n't _any_ performance impact . You may also problems filtering identifying correct plan trace database heavy use . You obviously check DBA see happy precious database ! 1 . Open SQL Server Profiler create new trace connecting desired database wish record trace . 2 . Under `` Events Selection '' tab check `` Show events '' , check `` Performance '' - > `` Showplan XML '' row run trace . 3 . While trace running , whatever need get slow running query run . 4 . Wait query complete stop trace . 5 . To save trace right click plan xml SQL Server Profiler select `` Extract event data ... '' save plan file XML format . The plan get equivalent `` Include Actual Execution Plan '' option SQL Server Management Studio . # # Method 4 - Inspecting query cache If ca n't run query directly also ca n't capture profiler trace still obtain estimated plan inspecting SQL query plan cache . We inspect plan cache querying SQL Server [ DMVs ] ( http : //msdn.microsoft.com/en-us/library/ms188754.aspx ) . The following basic query list cached query plans ( xml ) along SQL text . On database also need add additional filtering clauses filter results plans interested . SELECT UseCounts , Cacheobjtype , Objtype , TEXT , query_plan FROM sys.dm_exec_cached_plans CROSS APPLY sys.dm_exec_sql_text ( plan_handle ) CROSS APPLY sys.dm_exec_query_plan ( plan_handle ) Execute query click plan XML open plan new window - right click select `` Save execution plan ... '' save plan file XML format . # Notes : Because many factors involved ( ranging table index schema data stored table statistics ) **always** try obtain execution plan database interested ( normally one experiencing performance problem ) . You ca n't capture execution plan encrypted stored procedures . # # `` actual '' vs `` estimated '' execution plans An _actual_ execution plan one SQL Server actually runs query , whereas _estimated_ execution plan SQL Server works _would_ without executing query . Although logically equivalent , actual execution plan much useful contains additional details statistics actually happened executing query . This essential diagnosing problems SQL Servers estimations ( statistics date ) . * [ Estimated Actual execution plan revisited ] ( http : //sqlserverpedia.com/blog/sql-server-bloggers/estimated-and-actual-execution-plan-revisited/ ) # # How I interpret query execution plan ? This topic worthy enough ( free ) [ book ] ( https : //www.simple- talk.com/books/sql-books/sql-server-execution-plans-second-edition-by-grant- fritchey/ ) right . # # See also : * [ Execution Plan Basics ] ( http : //www.simple-talk.com/sql/performance/execution-plan-basics/ ) * [ SHOWPLAN Permission Transact-SQL Batches ] ( http : //msdn.microsoft.com/en-us/library/ms178086.aspx ) * [ SQL Server 2008 – Using Query Hashes Query Plan Hashes ] ( http : //www.dbsnaps.com/sql-server/sql-server-query-hashes/ ) * [ Analyzing SQL Server Plan Cache ] ( http : //www.mssqltips.com/sqlservertip/1661/analyzing-the-sql-server-plan-cache/ )"
Stackoverflow,I want keep backup MySQL databases . I 100 MySQL databases . I want export time import MySQL server one time . How I ?,# # Export : mysqldump -u root -p -- all-databases > alldb.sql Look [ documentation mysqldump ] ( http : //dev.mysql.com/doc/refman/5.5/en/mysqldump.html ) . You may want use options mentioned comments : mysqldump -u root -p -- opt -- all-databases > alldb.sql mysqldump -u root -p -- all-databases -- skip-lock-tables > alldb.sql # # Import : mysql -u root -p < alldb.sql
Stackoverflow,"I 'm trying setup MySQL mac os 10.6 using Homebrew ` brew install mysql 5.1.52 ` . Everything goes well I also successful ` mysql_install_db ` . However I try connect server using : /usr/local/Cellar/mysql/5.1.52/bin/mysqladmin -u root password 'mypass' I get : /usr/local/Cellar/mysql/5.1.52/bin/mysqladmin : connect server 'localhost ' failed error : 'Access denied user 'root ' @ 'localhost ' ( using password : NO ) ' I 've tried access ` mysqladmin mysql using -u root -proot ` well , n't work without password . This brand new installation brand new machine far I know new installation must accessible without root password . I also tried : /usr/local/Cellar/mysql/5.1.52/bin/mysql_secure_installation I also get ERROR 1045 ( 28000 ) : Access denied user 'root ' @ 'localhost ' ( using password : NO )","I think one end position older versions mysql already installed . I problem none solutions worked . I fixed thus : Used brew 's ` remove ` & ` cleanup ` commands , unloaded ` launchctl ` script , deleted mysql directory ` /usr/local/var ` , deleted existing ` /etc/my.cnf ` ( leave one , apply ) launchctl plist Updated string plist . Note also alternate security script directory based version MySQL installing . Step-by-step : brew remove mysql brew cleanup launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist rm ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist sudo rm -rf /usr/local/var/mysql I started scratch : 1. installed mysql ` brew install mysql ` 2. ran commands brew suggested : ( see note : ) unset TMPDIR mysql_install_db -- verbose -- user= ` whoami ` -- basedir= '' $ ( brew -- prefix mysql ) '' -- datadir=/usr/local/var/mysql -- tmpdir=/tmp 3 . Start mysql ` mysql.server start ` command , able log 4 . Used alternate security script : /usr/local/Cellar/mysql/5.5.10/bin/mysql_secure_installation 5 . Followed ` launchctl ` section brew package script output , # start launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist # stop launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.mysql.plist 6 . Boom . Hope helps someone ! **Note : ** ` -- force ` bit ` brew cleanup ` also cleanup outdated kegs , think 's new-ish homebrew feature . **Note second : ** commenter says step 2 required . I n't want test , YMMV !"
Stackoverflow,I want create varchar column SQL contain ` N'guid ' ` ` guid ` generated GUID .NET ( [ Guid.NewGuid ] ( https : //docs.microsoft.com/en- us/dotnet/api/system.guid.newguid ) ) - class System.Guid . What length ` varchar ` I expect GUID ? Is static length ? Should I use ` nvarchar ` ( GUID ever use Unicode characters ) ? varchar ( Guid.Length ) PS . I n't want use SQL row guid data-type . I asking ` Guid.MaxLength ` .,"It depends format Guid : * ` Guid.NewGuid ( ) .ToString ( ) ` = > **36** characters ( Hyphenated ) outputs : ` 12345678-1234-1234-1234-123456789abc ` * ` Guid.NewGuid ( ) .ToString ( `` D '' ) ` = > **36** characters ( Hyphenated , ` ToString ( ) ` ) outputs : ` 12345678-1234-1234-1234-123456789abc ` * ` Guid.NewGuid ( ) .ToString ( `` N '' ) ` = > **32** characters ( Digits ) outputs : ` 12345678123412341234123456789abc ` * ` Guid.NewGuid ( ) .ToString ( `` B '' ) ` = > **38** characters ( Braces ) outputs : ` { 12345678-1234-1234-1234-123456789abc } ` * ` Guid.NewGuid ( ) .ToString ( `` P '' ) ` = > **38** characters ( Parentheses ) outputs : ` ( 12345678-1234-1234-1234-123456789abc ) ` * ` Guid.NewGuid ( ) .ToString ( `` X '' ) ` = > **68** characters ( Hexadecimal ) outputs : ` { 0x12345678,0x1234,0x1234 , { 0x12,0x34,0x12,0x34,0x56,0x78,0x9a,0xbc } } `"
Stackoverflow,"I table primary key varchar ( 255 ) . Some cases arisen 255 characters n't enough . I tried changing field text , I get following error : BLOB/TEXT column 'message_id ' used key specification without key length I fix ? edit : I also point table composite primary key multiple columns .","The error happens MySQL index first N chars BLOB ` TEXT ` column . So The error mainly happens field/column type ` TEXT ` BLOB belong ` TEXT ` ` BLOB ` types ` TINYBLOB ` , ` MEDIUMBLOB ` , ` LONGBLOB ` , ` TINYTEXT ` , ` MEDIUMTEXT ` , ` LONGTEXT ` try make primary key index . With full ` BLOB ` ` TEXT ` without length value , MySQL unable guarantee uniqueness column ’ variable dynamic size . So , using ` BLOB ` ` TEXT ` types index , value N must supplied MySQL determine key length . However , MySQL ’ support key length limit ` TEXT ` ` BLOB ` . ` TEXT ( 88 ) ` simply ’ work . The error also pop try convert table column ` non- TEXT ` ` non-BLOB ` type ` VARCHAR ` ` ENUM ` ` TEXT ` ` BLOB ` type , column already defined unique constraints index . The Alter Table SQL command fail . The solution problem remove ` TEXT ` ` BLOB ` column index unique constraint set another field primary key . If can't , wanting place limit ` TEXT ` ` BLOB ` column , try use ` VARCHAR ` type place limit length . By default , ` VARCHAR ` limited maximum 255 characters limit must specified implicitly within bracket right declaration , i.e ` VARCHAR ( 200 ) ` limit 200 characters long . Sometimes , even though ’ use ` TEXT ` ` BLOB ` related type table , Error 1170 may also appear . It happens situation specify ` VARCHAR ` column primary key , wrongly set length characters size . ` VARCHAR ` accepts 256 characters , anything ` VARCHAR ( 512 ) ` force MySQL auto-convert ` VARCHAR ( 512 ) ` ` SMALLTEXT ` datatype , subsequently fails error 1170 key length column used primary key unique non-unique index . To solve problem , specify figure less 256 size ` VARCHAR ` field . Reference : [ MySQL Error 1170 ( 42000 ) : BLOB/TEXT Column Used Key Specification Without Key Length ] ( http : //www.mydigitallife.info/2007/07/09/mysql- error-1170-42000-blobtext-column-used-in-key-specification-without-a-key- length/ )"
Stackoverflow,"I want export import .sql file MySQL database command line . Is command export .sql file MySQL ? Then I import ? When import , may constraints like enable/disable **foreign key check** **export table structure** . Can set **options** ` mysqldump ` ?","Looks like query taking longer . From stack trace code able determine exactly query . This type timeout three causes ; 1 . There 's deadlock somewhere 2 . The database 's statistics and/or query plan cache incorrect 3 . The query complex needs tuned A deadlock difficult fix , 's easy determine whether case . Connect database Sql Server Management Studio . In left pane right-click server node select _Activity Monitor_ . Take look running processes . Normally idle running . When problem occurs identify blocked process process state . If right-click process select _details_ 'll show last query executed process . The second issue cause database use sub-optimal query plan . It resolved clearing statistics : exec sp_updatestats If n't work could also try dbcc freeproccache You server heavy load temporarily incur big performace hit stored procs queries recompiled first executed . However , since state issue occurs _sometimes_ , stack trace indicates application starting , I think 're running query run occasionally . You may better forcing SQL Server reuse previous query plan . See [ answer ] ( https : //stackoverflow.com/a/8590152/64096 ) details . I 've already touched third issue , easily determine whether query needs tuning executing query manually , example using Sql Server Management Studio . If query takes long complete , even resetting statistics 'll probably need tune . For help , post exact query new question ."
Stackoverflow,I working query Sql Server 2005 I need convert value ` DateTime ` variable ` varchar ` variable ` yyyy-mm-dd ` format ( without time part ) . How I ?,"Here 's test sql styles . DECLARE @ datetime SET @ = GETDATE ( ) select convert ( nvarchar ( MAX ) , @ , 0 ) output , 0 style union select convert ( nvarchar ( MAX ) , @ , 1 ) , 1 union select convert ( nvarchar ( MAX ) , @ , 2 ) , 2 union select convert ( nvarchar ( MAX ) , @ , 3 ) , 3 union select convert ( nvarchar ( MAX ) , @ , 4 ) , 4 union select convert ( nvarchar ( MAX ) , @ , 5 ) , 5 union select convert ( nvarchar ( MAX ) , @ , 6 ) , 6 union select convert ( nvarchar ( MAX ) , @ , 7 ) , 7 union select convert ( nvarchar ( MAX ) , @ , 8 ) , 8 union select convert ( nvarchar ( MAX ) , @ , 9 ) , 9 union select convert ( nvarchar ( MAX ) , @ , 10 ) , 10 union select convert ( nvarchar ( MAX ) , @ , 11 ) , 11 union select convert ( nvarchar ( MAX ) , @ , 12 ) , 12 union select convert ( nvarchar ( MAX ) , @ , 13 ) , 13 union select convert ( nvarchar ( MAX ) , @ , 14 ) , 14 -- 15 19 valid union select convert ( nvarchar ( MAX ) , @ , 20 ) , 20 union select convert ( nvarchar ( MAX ) , @ , 21 ) , 21 union select convert ( nvarchar ( MAX ) , @ , 22 ) , 22 union select convert ( nvarchar ( MAX ) , @ , 23 ) , 23 union select convert ( nvarchar ( MAX ) , @ , 24 ) , 24 union select convert ( nvarchar ( MAX ) , @ , 25 ) , 25 -- 26 99 valid union select convert ( nvarchar ( MAX ) , @ , 100 ) , 100 union select convert ( nvarchar ( MAX ) , @ , 101 ) , 101 union select convert ( nvarchar ( MAX ) , @ , 102 ) , 102 union select convert ( nvarchar ( MAX ) , @ , 103 ) , 103 union select convert ( nvarchar ( MAX ) , @ , 104 ) , 104 union select convert ( nvarchar ( MAX ) , @ , 105 ) , 105 union select convert ( nvarchar ( MAX ) , @ , 106 ) , 106 union select convert ( nvarchar ( MAX ) , @ , 107 ) , 107 union select convert ( nvarchar ( MAX ) , @ , 108 ) , 108 union select convert ( nvarchar ( MAX ) , @ , 109 ) , 109 union select convert ( nvarchar ( MAX ) , @ , 110 ) , 110 union select convert ( nvarchar ( MAX ) , @ , 111 ) , 111 union select convert ( nvarchar ( MAX ) , @ , 112 ) , 112 union select convert ( nvarchar ( MAX ) , @ , 113 ) , 113 union select convert ( nvarchar ( MAX ) , @ , 114 ) , 114 union select convert ( nvarchar ( MAX ) , @ , 120 ) , 120 union select convert ( nvarchar ( MAX ) , @ , 121 ) , 121 -- 122 125 valid union select convert ( nvarchar ( MAX ) , @ , 126 ) , 126 union select convert ( nvarchar ( MAX ) , @ , 127 ) , 127 -- 128 , 129 valid union select convert ( nvarchar ( MAX ) , @ , 130 ) , 130 union select convert ( nvarchar ( MAX ) , @ , 131 ) , 131 -- 132 valid order BY style Here 's result output style Apr 28 2014 9:31AM 0 04/28/14 1 14.04.28 2 28/04/14 3 28.04.14 4 28-04-14 5 28 Apr 14 6 Apr 28 , 14 7 09:31:28 8 Apr 28 2014 9:31:28:580AM 9 04-28-14 10 14/04/28 11 140428 12 28 Apr 2014 09:31:28:580 13 09:31:28:580 14 2014-04-28 09:31:28 20 2014-04-28 09:31:28.580 21 04/28/14 9:31:28 AM 22 2014-04-28 23 09:31:28 24 2014-04-28 09:31:28.580 25 Apr 28 2014 9:31AM 100 04/28/2014 101 2014.04.28 102 28/04/2014 103 28.04.2014 104 28-04-2014 105 28 Apr 2014 106 Apr 28 , 2014 107 09:31:28 108 Apr 28 2014 9:31:28:580AM 109 04-28-2014 110 2014/04/28 111 20140428 112 28 Apr 2014 09:31:28:580 113 09:31:28:580 114 2014-04-28 09:31:28 120 2014-04-28 09:31:28.580 121 2014-04-28T09:31:28.580 126 2014-04-28T09:31:28.580 127 28 جمادى الثانية 1435 9:31:28:580AM 130 28/06/1435 9:31:28:580AM 131 Make ` nvarchar ( max ) ` shorter trim time . For example : select convert ( nvarchar ( 11 ) , GETDATE ( ) , 0 ) union select convert ( nvarchar ( max ) , GETDATE ( ) , 0 ) outputs : May 18 2018 May 18 2018 9:57AM"
Stackoverflow,"With two classes , I 've tried connect MySQL database . However , I always get error : > Wed Dec 09 22:46:52 CET 2015 WARN : Establishing SSL connection without server 's identity verification recommended . According MySQL 5.5.45+ , 5.6.26+ 5.7.6+ requirements SSL connection must established default explicit option n't set . For compliance existing applications using SSL verifyServerCertificate property set 'false ' . You need either explicitly disable SSL setting useSSL=false , set useSSL=true provide truststore server certificate verification . This test class ` main ` method : public class TestDatabase { public static void main ( String [ ] args ) { Database db = new Database ( ) ; try { db.connect ( ) ; } catch ( Exception e ) { e.printStackTrace ( ) ; } db.close ( ) ; } } This ` Database ` class : import java.sql.Connection ; import java.sql.DriverManager ; import java.sql.SQLException ; public class Database { private Connection con ; public void connect ( ) throws Exception { ( con ! = null ) return ; try { Class.forName ( `` com.mysql.jdbc.Driver '' ) ; } catch ( ClassNotFoundException e ) { throw new Exception ( `` No database '' ) ; } String connectionURL = `` jdbc : mysql : //localhost:3306/Peoples '' ; con = DriverManager.getConnection ( connectionURL , `` root '' , `` milos23 '' ) ; } public void close ( ) { ( con ! = null ) { try { con.close ( ) ; } catch ( SQLException e ) { e.printStackTrace ( ) ; } } } }","Your connection URL look like , jdbc : mysql : //localhost:3306/Peoples ? autoReconnect=true & useSSL=false This disable SSL also suppress SSL errors ."
Stackoverflow,I created tables ` SQLiteOpenHelper ` ` onCreate ( ) ` receive SQLiteException : table SQLiteException : column errors . Why ? > **NOTE : ** > > ( This amalgamated summary tens similar questions every week . Attempting provide `` canonical '' community wiki question/answer questions directed good reference . ),"[ ` SQLiteOpenHelper ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html ) [ ` onCreate ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # onCreate % 28android.database.sqlite.SQLiteDatabase % 29 ) [ ` onUpgrade ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # onUpgrade % 28android.database.sqlite.SQLiteDatabase , % 20int , % 20int % 29 ) callbacks invoked database actually opened , example call [ ` getWritableDatabase ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # getWritableDatabase % 28 % 29 ) . The database opened database helper object created . ` SQLiteOpenHelper ` versions database files . The version number ` int ` argument passed [ constructor ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # SQLiteOpenHelper % 28android.content.Context , % 20java.lang.String , % 20android.database.sqlite.SQLiteDatabase.CursorFactory , % 20int % 29 ) . In database file , version number stored [ ` PRAGMA user_version ` ] ( http : //www.sqlite.org/pragma.html # pragma_schema_version ) . ` onCreate ( ) ` run database file exist created . If ` onCreate ( ) ` returns successfully ( n't throw exception ) , database assumed created requested version number . As implication , catch ` SQLException ` ` onCreate ( ) ` . ` onUpgrade ( ) ` called database file exists stored version number lower requested constructor . The ` onUpgrade ( ) ` update table schema requested version . When changing table schema code ( ` onCreate ( ) ` ) , make sure database updated . Two main approaches : 1 . Delete old database file ` onCreate ( ) ` run . This often preferred development time control installed versions data loss issue . Some ways delete database file : * Uninstall application . Use application manager ` adb uninstall your.package.name ` shell . * Clear application data . Use application manager . 2 . Increment database version ` onUpgrade ( ) ` invoked . This slightly complicated code needed . * For development time schema upgrades data loss issue , use ` execSQL ( `` DROP TABLE IF EXISTS < tablename > '' ) ` remove existing tables call ` onCreate ( ) ` recreate database . * For released versions , implement data migration ` onUpgrade ( ) ` users n't lose data ."
Stackoverflow,I created tables ` SQLiteOpenHelper ` ` onCreate ( ) ` receive SQLiteException : table SQLiteException : column errors . Why ? > **NOTE : ** > > ( This amalgamated summary tens similar questions every week . Attempting provide `` canonical '' community wiki question/answer questions directed good reference . ),"[ ` SQLiteOpenHelper ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html ) [ ` onCreate ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # onCreate % 28android.database.sqlite.SQLiteDatabase % 29 ) [ ` onUpgrade ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # onUpgrade % 28android.database.sqlite.SQLiteDatabase , % 20int , % 20int % 29 ) callbacks invoked database actually opened , example call [ ` getWritableDatabase ( ) ` ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # getWritableDatabase % 28 % 29 ) . The database opened database helper object created . ` SQLiteOpenHelper ` versions database files . The version number ` int ` argument passed [ constructor ] ( http : //developer.android.com/reference/android/database/sqlite/SQLiteOpenHelper.html # SQLiteOpenHelper % 28android.content.Context , % 20java.lang.String , % 20android.database.sqlite.SQLiteDatabase.CursorFactory , % 20int % 29 ) . In database file , version number stored [ ` PRAGMA user_version ` ] ( http : //www.sqlite.org/pragma.html # pragma_schema_version ) . ` onCreate ( ) ` run database file exist created . If ` onCreate ( ) ` returns successfully ( n't throw exception ) , database assumed created requested version number . As implication , catch ` SQLException ` ` onCreate ( ) ` . ` onUpgrade ( ) ` called database file exists stored version number lower requested constructor . The ` onUpgrade ( ) ` update table schema requested version . When changing table schema code ( ` onCreate ( ) ` ) , make sure database updated . Two main approaches : 1 . Delete old database file ` onCreate ( ) ` run . This often preferred development time control installed versions data loss issue . Some ways delete database file : * Uninstall application . Use application manager ` adb uninstall your.package.name ` shell . * Clear application data . Use application manager . 2 . Increment database version ` onUpgrade ( ) ` invoked . This slightly complicated code needed . * For development time schema upgrades data loss issue , use ` execSQL ( `` DROP TABLE IF EXISTS < tablename > '' ) ` remove existing tables call ` onCreate ( ) ` recreate database . * For released versions , implement data migration ` onUpgrade ( ) ` users n't lose data ."
Stackoverflow,PostgreSQL introduced [ JSONB ] ( http : //www.depesz.com/2014/03/25/waiting- for-9-4-introduce-jsonb-a-structured-format-for-storing-json/ ) already trending [ hacker news ] ( https : //news.ycombinator.com/item ? id=7457197 ) . It would great someone could explain 's different Hstore JSON previously present PostgreSQL . What 's advantages limitations someone consider using ?,"First , [ ` hstore ` ] ( http : //www.postgresql.org/docs/9.3/static/hstore.html ) contrib module , allows store key = > value pairs , keys values ` text ` ( however values sql ` NULL ` ) . Both ` json ` & ` jsonb ` allows store valid JSON _value_ ( defined [ spec ] ( http : //json.org ) ) . F.ex . valid JSON representations : ` null ` , ` true ` , ` [ 1 , false , '' string '' , { `` foo '' : '' bar '' } ] ` , ` { `` foo '' : '' bar '' , '' baz '' : [ null ] } ` \- ` hstore ` little subset compared JSON capable ( need subset , 's fine ) . The difference ` json ` & ` jsonb ` storage : * ` json ` stored plain text format , * ` jsonb ` stored binary representation There 3 major consequences : * ` jsonb ` usually takes disk space store ` json ` ( sometimes ) * ` jsonb ` takes time build input representation ` json ` * ` json ` operations take _significantly_ time ` jsonb ` ( & parsing also needs done time operation ` json ` typed value ) When ` jsonb ` available stable release , two major use cases , easily select : 1 . If work JSON representation application , PostgreSQL used store & retrieve representation , use ` json ` . 2 . If lot operations JSON value PostgreSQL , use indexing JSON field , use ` jsonb ` ."
Stackoverflow,"I connecting MySQL - 8.0 MySQL Workbench getting error : > Authentication plugin 'caching_sha2_password ' loaded : dlopen ( /usr/local/mysql/lib/plugin/caching_sha2_password.so , 2 ) : image found I tried client tool well . Any solution ?",Note : For MAC OS 1 . Open MySQL System Preferences > Initialize Database > 2 . Type new password . 3 . Choose 'Use legacy password' 4 . Start Server . 5 . Now connect MySQL Workbench ! [ Image description ] ( https : //i.stack.imgur.com/qGdmc.jpg )
Stackoverflow,Is command find databases size Postgres ? I able find size specific database using following command : select pg_database_size ( 'databaseName ' ) ;,GRANT database need . Grant tables directly . Granting privileges database mostly used grant revoke connect privileges . This allows specify may stuff database sufficient permissions . You want instead : GRANT ALL PRIVILEGES ON TABLE side_adzone TO jerry ; This take care issue .
Stackoverflow,"My question rather simple . I 'm aware concept UUID I want generate one refer 'item ' 'store ' DB . Seems reasonable right ? The problem following line returns error : honeydb= # insert items values ( uuid_generate_v4 ( ) , 54.321 , 31 , 'desc 1 ' , 31.94 ) ; ERROR : function uuid_generate_v4 ( ) exist LINE 2 : uuid_generate_v4 ( ) , 54.321 , 31 , 'desc 1 ' , 31.94 ) ; ^ HINT : No function matches given name argument types . You might need add explicit type casts . I 've read page : < http : //www.postgresql.org/docs/current/static/uuid- ossp.html > ! [ enter image description ] ( https : //i.stack.imgur.com/BDeZ3.png ) I 'm running Postgres 8.4 Ubuntu 10.04 x64 .","` uuid-ossp ` contrib module , n't loaded server default . You must load database use . For modern PostgreSQL versions ( 9.1 newer ) 's easy : CREATE EXTENSION IF NOT EXISTS `` uuid-ossp '' ; 9.0 must instead run SQL script load extension . See [ documentation contrib modules 8.4 ] ( http : //www.postgresql.org/docs/8.4/static/contrib.html ) . For Pg 9.1 newer instead read [ current contrib docs ] ( http : //www.postgresql.org/docs/current/static/contrib.html ) [ ` CREATE EXTENSION ` ] ( http : //www.postgresql.org/docs/current/static/sql- createextension.html ) . These features exist 9.0 older versions , like 8.4 . If 're using packaged version PostgreSQL might need install separate package containing contrib modules extensions . Search package manager database 'postgres ' 'contrib ' ."
Stackoverflow,"I many users web site ( 20000-60000 per day ) , download site mobile files . I remote access server ( windows server 2008-R2 ) . I 've received _ '' Server unavailable '' _ errors , seeing connection timeout error . I 'm familiar - occur I fix ? The full error : > Server Error '/ ' Application . Timeout expired . The timeout period elapsed prior completion operation server responding . The statement terminated . Description : An unhandled exception occurred execution current web request . Please review stack trace information error originated code . > > Exception Details : System.Data.SqlClient.SqlException : Timeout expired . The timeout period elapsed prior completion operation server responding . The statement terminated . > > Source Error : > > An unhandled exception generated execution current web request . Information regarding origin location exception identified using exception stack trace . > > Stack Trace : > > [ SqlException ( 0x80131904 ) : Timeout expired . The timeout period elapsed prior completion operation server responding . The statement terminated . ] > System.Data.SqlClient.SqlConnection.OnError ( SqlException exception , Boolean breakConnection ) +404 > System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning ( ) +412 > System.Data.SqlClient.TdsParser.Run ( RunBehavior runBehavior , SqlCommand cmdHandler , SqlDataReader dataStream , BulkCopySimpleResultSet bulkCopyHandler , TdsParserStateObject stateObj ) +1363 > System.Data.SqlClient.SqlCommand.FinishExecuteReader ( SqlDataReader ds , RunBehavior runBehavior , String resetOptionsString ) +6387741 > System.Data.SqlClient.SqlCommand.RunExecuteReaderTds ( CommandBehavior cmdBehavior , RunBehavior runBehavior , Boolean returnStream , Boolean async ) +6389442 > System.Data.SqlClient.SqlCommand.RunExecuteReader ( CommandBehavior cmdBehavior , RunBehavior runBehavior , Boolean returnStream , String method , DbAsyncResult result ) +538 > System.Data.SqlClient.SqlCommand.InternalExecuteNonQuery ( DbAsyncResult result , String methodName , Boolean sendToPipe ) +689 > System.Data.SqlClient.SqlCommand.ExecuteNonQuery ( ) +327 > NovinMedia.Data.DbObject.RunProcedure ( String storedProcName , IDataParameter [ ] parameters , Int32 & rowsAffected ) +209 > DataLayer.OnlineUsers.Update_SessionEnd_And_Online ( Object Session_End , Boolean Online ) +440 > NiceFileExplorer.Global.Application_Start ( Object sender , EventArgs e ) +163 > > [ HttpException ( 0x80004005 ) : Timeout expired . The timeout period elapsed prior completion operation server responding . The statement terminated . ] > System.Web.HttpApplicationFactory.EnsureAppStartCalledForIntegratedMode ( HttpContext context , HttpApplication app ) +4052053 > System.Web.HttpApplication.RegisterEventSubscriptionsWithIIS ( IntPtr appContext , HttpContext context , MethodInfo [ ] handlers ) +191 > System.Web.HttpApplication.InitSpecial ( HttpApplicationState state , MethodInfo [ ] handlers , IntPtr appContext , HttpContext context ) +352 > System.Web.HttpApplicationFactory.GetSpecialApplicationInstance ( IntPtr appContext , HttpContext context ) +407 > System.Web.Hosting.PipelineRuntime.InitializeApplication ( IntPtr appContext ) +375 > > [ HttpException ( 0x80004005 ) : Timeout expired . The timeout period elapsed prior completion operation server responding . The statement terminated . ] > System.Web.HttpRuntime.FirstRequestInit ( HttpContext context ) +11686928 System.Web.HttpRuntime.EnsureFirstRequestInit ( HttpContext context ) +141 System.Web.HttpRuntime.ProcessRequestNotificationPrivate ( IIS7WorkerRequest wr , HttpContext context ) +4863749 * * * **EDIT AFTER ANSWERS : ** ` Application_Start ` ` Global.asax ` like : protected void Application_Start ( object sender , EventArgs e ) { Application [ `` OnlineUsers '' ] = 0 ; OnlineUsers.Update_SessionEnd_And_Online ( DateTime.Now , false ) ; AddTask ( `` DoStuff '' , 10 ) ; } The stored procedure called : ALTER Procedure [ dbo ] . [ sp_OnlineUsers_Update_SessionEnd_And_Online ] @ Session_End datetime , @ Online bit As Begin Update OnlineUsers SET [ Session_End ] = @ Session_End , [ Online ] = @ Online End I two methods getting online users : 1. using ` Application [ `` OnlineUsers '' ] = 0 ; ` 2. one using database So , method # 2 I reset OnlineUsers ` Application_Start ` . There 482,751 records table .","**Updated** For MS SQL Server 2012 USE [ master ] ; DECLARE @ kill varchar ( 8000 ) = `` ; SELECT @ kill = @ kill + 'kill ' + CONVERT ( varchar ( 5 ) , session_id ) + ' ; ' FROM sys.dm_exec_sessions WHERE database_id = db_id ( 'MyDB ' ) EXEC ( @ kill ) ; For MS SQL Server 2000 , 2005 , 2008 USE master ; DECLARE @ kill varchar ( 8000 ) ; SET @ kill = `` ; SELECT @ kill = @ kill + 'kill ' + CONVERT ( varchar ( 5 ) , spid ) + ' ; ' FROM master..sysprocesses WHERE dbid = db_id ( 'MyDB ' ) EXEC ( @ kill ) ;"
Stackoverflow,"I 'm using SQLdatareader build POCOs database . The code works except encounters null value database . For example , FirstName column database contains null value , exception thrown . employee.FirstName = sqlreader.GetString ( indexFirstName ) ; What best way handle null values situation ?","You need check ` IsDBNull ` : ( ! SqlReader.IsDBNull ( indexFirstName ) ) { employee.FirstName = sqlreader.GetString ( indexFirstName ) ; } That 's reliable way detect handle situation . I wrapped things extension methods tend return default value column indeed ` null ` : public static string SafeGetString ( SqlDataReader reader , int colIndex ) { ( ! reader.IsDBNull ( colIndex ) ) return reader.GetString ( colIndex ) ; return string.Empty ; } Now call like : employee.FirstName = SqlReader.SafeGetString ( indexFirstName ) ; 'll never worry exception ` null ` value ."
Stackoverflow,"I following error sqlite3-ruby install : Building native extensions . This could take ... ERROR : Error installing sqlite3-ruby : ERROR : Failed build gem native extension . /usr/bin/ruby1.8 extconf.rb checking sqlite3.h ... sqlite3.h missing . Try 'port install sqlite3 +universal ' 'yum install sqlite3-devel' *** extconf.rb failed *** Could create Makefile due reason , probably lack necessary libraries and/or headers . Check mkmf.log file details . You may need configuration options . Provided configuration options : -- with-opt-dir -- without-opt-dir -- with-opt-include -- without-opt-include= $ { opt-dir } /include -- with-opt-lib -- without-opt-lib= $ { opt-dir } /lib -- with-make-prog -- without-make-prog -- srcdir= . -- curdir -- ruby=/usr/bin/ruby1.8 -- with-sqlite3-dir -- without-sqlite3-dir -- with-sqlite3-include -- without-sqlite3-include= $ { sqlite3-dir } /include -- with-sqlite3-lib -- without-sqlite3-lib= $ { sqlite3-dir } /lib Gem files remain installed /usr/lib/ruby/gems/1.8/gems/sqlite3-ruby-1.3.1 inspection . Results logged /usr/lib/ruby/gems/1.8/gems/sqlite3-ruby-1.3.1/ext/sqlite3/gem_make.out sqlite3.h located /usr/include/ sudo gem install sqlite3-ruby -- without-sqlite3-include=/usr/include n't work ERROR : While executing gem ... ( OptionParser : :InvalidOption ) invalid option : -- without-sqlite3-include=/usr/include Ubuntu 10.04",You need SQLite3 development headers gem ’ native extension compile . You install running ( possibly ` sudo ` ) : apt-get install libsqlite3-dev
Stackoverflow,"I seem unable re-create simple user I 've deleted , even root MySQL . My case : user 'jack ' existed , I deleted mysql.user order recreate . I see vestiges table . If I execute command , random username , say 'jimmy ' , works fine ( originally 'jack ' ) . What I done corrupt user 'jack ' I undo corruption order re-create 'jack ' valid user installation MySQL ? See example . ( Of course , originally , much time creation 'jack ' removal . ) mysql > CREATE USER 'jack ' @ 'localhost ' IDENTIFIED BY 'test123 ' ; Query OK , 0 rows affected ( 0.00 sec ) mysql > select user , host user ; + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | user | host | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | root | 127.0.0.1 | | debian-sys-maint | localhost | | jack | localhost | | root | localhost | | root | russ-elite-book | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ 5 rows set ( 0.00 sec ) mysql > delete user user = 'jack ' ; Query OK , 1 row affected ( 0.00 sec ) mysql > select user , host user ; + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | user | host | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | root | 127.0.0.1 | | debian-sys-maint | localhost | | root | localhost | | root | russ-elite-book | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ 4 rows set ( 0.00 sec ) mysql > CREATE USER 'jack ' @ 'localhost ' IDENTIFIED BY 'test123 ' ; ERROR 1396 ( HY000 ) : Operation CREATE USER failed 'jack ' @ 'localhost' mysql > CREATE USER 'jimmy ' @ 'localhost ' IDENTIFIED BY 'test123 ' ; Query OK , 0 rows affected ( 0.00 sec ) mysql > select user , host user ; + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | user | host | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ | root | 127.0.0.1 | | debian-sys-maint | localhost | | jimmy | localhost | | root | localhost | | root | russ-elite-book | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -+ 5 rows set ( 0.00 sec )","yes bug . However , I found small workaround . * Assume user , drop user * After deleting user , need flush mysql privileges * Now create user . That solve . Assuming want create user admin @ localhost , would commands : drop user admin @ localhost ; flush privileges ; create user admin @ localhost identified ' _admins_password_ ' Cheers"
Stackoverflow,"Error SQL query : -- -- Database : ` work ` -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Table structure table ` administrators ` -- CREATE TABLE IF NOT EXISTS ` administrators ` ( ` user_id ` varchar ( 30 ) NOT NULL , ` password ` varchar ( 30 ) NOT NULL ) ENGINE = InnoDB DEFAULT CHARSET = latin1 ; MySQL said : # 1046 - No database selected need help .","You need tell MySQL database use : USE database_name ; create table . In case database exist , need create : CREATE DATABASE database_name ; followed : USE database_name ;"
Stackoverflow,"How I call **psql** **does n't prompt password** ? This I : psql -Umyuser < myscript.sql However , I could n't find argument passes password , psql always prompts .","There several ways authenticate PostgreSQL . You may wish investigate alternatives password authentication < https : //www.postgresql.org/docs/current/static/client-authentication.html > . To answer question , ways provide password password- based authentication . The obvious way via password prompt . Instead , provide password pgpass file ` PGPASSWORD ` environment variable . See : * < https : //www.postgresql.org/docs/9.0/static/libpq-pgpass.html > * < https : //www.postgresql.org/docs/9.0/interactive/libpq-envars.html > There option provide password command line argument information often available users , therefore insecure . However , Linux/Unix environments provide environment variable single command like : PGPASSWORD=yourpass psql ..."
Stackoverflow,"I MS SQL Server 2008 Express system contains database I would like 'copy rename ' ( testing purposes ) I unaware simple way achieve . I notice R2 version SQL Server copy database wizard , sadly I ca n't upgrade . The database question around gig . I attempted restore backup database I want copy new database , luck .","1 . Install Microsoft SQL Management Studio , download free Microsoft 's website : **Version 2008** Microsoft SQL Management Studio 2008 part [ SQL Server 2008 Express Advanced Services ] ( http : //www.microsoft.com/en- us/download/details.aspx ? id=1842 ) **Version 2012** Click [ download button ] ( http : //www.microsoft.com/en- us/download/details.aspx ? id=29062 ) check ` ENU\x64\SQLManagementStudio_x64_ENU.exe ` **Version 2014** Click [ download button ] ( http : //www.microsoft.com/en- us/download/details.aspx ? id=42299 ) check MgmtStudio ` 64BIT\SQLManagementStudio_x64_ENU.exe ` 2 . Open **Microsoft SQL Management Studio** . 3 . Backup original database .BAK file ( db - > Task - > Backup ) . 4 . Create empty database new name ( clone ) . Note comments optional . 5 . Click clone database open restore dialog ( see image ) ! [ restore dialog ] ( https : //i.stack.imgur.com/4reCD.png ) 6 . Select Device add backup file step 3 . ! [ add backup file ] ( https : //i.stack.imgur.com/szMBC.png ) 7 . Change destination test database ! [ change destination ] ( https : //i.stack.imgur.com/qxFWk.png ) 8 . Change location database files , must different original . You type directly text box , add postfix . ( NOTE : Order important . Select checkbox , change filenames . ) ! [ change location ] ( https : //i.stack.imgur.com/pBzT4.png ) 9 . Check WITH REPLACE WITH KEEP_REPLICATION ! [ replace ] ( https : //i.stack.imgur.com/QvGoB.png )"
Stackoverflow,"I 'm bit strange problem . I 'm trying add foreign key one table references another , failing reason . With limited knowledge MySQL , thing could possibly suspect foreign key different table referencing one I trying reference . Here picture table relationships , generated via phpMyAdmin : [ Relationships ] ( http : //img14.imageshack.us/img14/5415/phpmyadminrelation.png ) I 've done ` SHOW CREATE TABLE ` query tables , ` sourcecodes_tags ` table foreign key , ` sourcecodes ` referenced table . CREATE TABLE ` sourcecodes ` ( ` id ` int ( 11 ) unsigned NOT NULL AUTO_INCREMENT , ` user_id ` int ( 11 ) unsigned NOT NULL , ` language_id ` int ( 11 ) unsigned NOT NULL , ` category_id ` int ( 11 ) unsigned NOT NULL , ` title ` varchar ( 40 ) CHARACTER SET utf8 NOT NULL , ` description ` text CHARACTER SET utf8 NOT NULL , ` views ` int ( 11 ) unsigned NOT NULL , ` downloads ` int ( 11 ) unsigned NOT NULL , ` time_posted ` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP , PRIMARY KEY ( ` id ` ) , KEY ` user_id ` ( ` user_id ` ) , KEY ` language_id ` ( ` language_id ` ) , KEY ` category_id ` ( ` category_id ` ) , CONSTRAINT ` sourcecodes_ibfk_3 ` FOREIGN KEY ( ` language_id ` ) REFERENCES ` languages ` ( ` id ` ) ON DELETE CASCADE ON UPDATE CASCADE , CONSTRAINT ` sourcecodes_ibfk_1 ` FOREIGN KEY ( ` user_id ` ) REFERENCES ` users ` ( ` id ` ) ON DELETE CASCADE ON UPDATE CASCADE , CONSTRAINT ` sourcecodes_ibfk_2 ` FOREIGN KEY ( ` category_id ` ) REFERENCES ` categories ` ( ` id ` ) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=latin1 CREATE TABLE ` sourcecodes_tags ` ( ` sourcecode_id ` int ( 11 ) unsigned NOT NULL , ` tag_id ` int ( 11 ) unsigned NOT NULL , KEY ` sourcecode_id ` ( ` sourcecode_id ` ) , KEY ` tag_id ` ( ` tag_id ` ) , CONSTRAINT ` sourcecodes_tags_ibfk_1 ` FOREIGN KEY ( ` tag_id ` ) REFERENCES ` tags ` ( ` id ` ) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB DEFAULT CHARSET=latin1 It would great anyone could tell going , I 've formal training anything MySQL : ) Thanks . **Edit : ** This code generates error : ALTER TABLE sourcecodes_tags ADD FOREIGN KEY ( sourcecode_id ) REFERENCES sourcecodes ( id ) ON DELETE CASCADE ON UPDATE CASCADE",Quite likely ` sourcecodes_tags ` table contains ` sourcecode_id ` values longer exists ` sourcecodes ` table . You get rid first . Here 's query find IDs : SELECT DISTINCT sourcecode_id FROM sourcecodes_tags tags LEFT JOIN sourcecodes sc ON tags.sourcecode_id=sc.id WHERE sc.id IS NULL ;
Stackoverflow,"Why would someone use ` WHERE 1=1 AND < conditions > ` SQL clause ( Either SQL obtained concatenated strings , either view definition ) I 've seen somewhere would used protect SQL Injection , seems weird . If injection ` WHERE 1 = 1 AND injected OR 1=1 ` would result ` injected OR 1=1 ` . Later edit : What usage view definition ? * * * Thank answers . Still , I n't understand would someone use construction defining view , use inside stored procedure . Take example : CREATE VIEW vTest AS SELECT FROM Table WHERE 1=1 AND table.Field=Value","If list conditions known compile time instead built run time , n't worry whether one one condition . You generate like : < condition > concatenate together . With ` 1=1 ` start , initial ` ` something associate . I 've never seen used kind injection protection , say n't seem like would help much . I _have_ seen used implementation convenience . The SQL query engine end ignoring ` 1=1 ` performance impact ."
Stackoverflow,"When I use ` KEY ` , ` PRIMARY KEY ` , ` UNIQUE KEY ` ` INDEX ` ?","` KEY ` ` INDEX ` synonyms MySQL . They mean thing . In databases would use [ indexes ] ( http : //en.wikipedia.org/wiki/Index_\ ( database\ ) ) improve speed data retrieval . An index typically created columns used ` JOIN ` , ` WHERE ` , ` ORDER BY ` clauses . Imagine table called ` users ` want search users last name 'Smith ' . Without index , database would go records table : slow , records database , work find result . On hand , index help database skip quickly relevant pages 'Smith ' records held . This similar , humans , go phone book directory find someone last name : We n't start searching directory cover cover , long inserted information order use skip quickly 'S ' pages . Primary keys unique keys similar . A primary key column , combination columns , uniquely identify row . It special case [ unique key ] ( http : //en.wikipedia.org/wiki/Primary_key ) . A table one primary key , one unique key . When specify unique key column , two distinct rows table value . Also note columns defined primary keys unique keys automatically indexed MySQL ."
Stackoverflow,"Assuming database project called ` MyDatabase ` file called ` MyDatabase.jfm ` appears root project directory . * It exclusively locked project open Visual Studio * It binary file * It started appearing recently ( past couple days ) I done Google search , offered insight . There references old software , results spam/trojanware . I also looked SO , produced results either . **Does anyone know 's ? ** The plan add [ gitignore file Visual Studio ] ( https : //github.com/github/gitignore ) , I need know first submit PR ... **UPDATE** This breaking Team Explorer Changes view . There changes shown following Output window : [ ! [ Screenshot error output tab ] ( https : //i.stack.imgur.com/wUWHC.png ) ] ( https : //i.stack.imgur.com/wUWHC.png ) As I adding file .gitignore file [ submitting PR ] ( https : //github.com/github/gitignore/pull/2028 ) . It would good eventually learn file came ...","This issue caused [ ESENT engine ] ( https : //en.wikipedia.org/wiki/Extensible_Storage_Engine ) relied SQL Projects adding new file . This new feature Windows 10 Anniversary Edition avoid data loss , fact SQL Projects store .dbmdl cache file project root means locked file added Git source control . Notes : * A [ pull request ] ( https : //github.com/github/gitignore/pull/2104 ) fix GitIgnore accepted Visual Studio team working include future updates . This ensure new projects .gitignore file includes .jfm problem occur * The core SSDT team working provide solution future update , best solution manually add discussed comments . **Disclosure** : I work SSDT team Microsoft ."
Stackoverflow,"I ca n't believe I could n't find working solution hour searching . I 'm following [ article ] ( http : //www.dotnetcurry.com/showarticle.aspx ? ID=941 ) Entity Framework 6.0 gives simple walk-through Code First . I created project installed latest [ EF Nuget package ] ( https : //www.nuget.org/packages/EntityFramework/6.0.2 ) project compile . I also verified I Microsoft SQL Server 2012 Express LocalDB installed came Visual Studio 2013 . I n't instances SQL installed local computer . The program runs entries added database outputted console . But article says `` check localdb '' n't say ! I n't see '.mdf ' '.ldf' files created project folder . I tried every way connect Visual Studio 's Server Explorer LocalDB . The wizard locate ` ( localdb ) ` find provider Server Explorer accept connection string like ` ( localdb ) \v11.0 ; Integrated Security=true ; ` I 've seen asked several places StackOverflow answer works marked answer . Please help , n't frustrating ! What steps connect Visual Studio Server Explorer LocalDB ?","In **Visual Studio 2012** I enter : ( localdb ) \v11.0 **Visual Studio 2015** **Visual Studio 2017** changed : ( localdb ) \MSSQLLocalDB server name adding ` Microsoft SQL Server Data ` source : View/Server Explorer/ ( Right click ) Data Connections/Add Connection database names populated . I n't need steps accepted answer , although would nice server name available automatically server name combo box . You also browse LocalDB database names available machine using : View/SQL Server Object Explorer ."
Stackoverflow,I 'm trying determine instances sql server/sql express I installed ( either manually programmatically ) examples telling run SQL query determine assumes I 'm already connected particular instance .,Maybe execute following **_Visual Studio Tools_** command prompt : aspnet_regiis -i You read **ASP.NET IIS Registration Tool ( Aspnet_regiis.exe ) ** [ ] ( http : //msdn.microsoft.com/en- us/library/k6h9cz8h % 28v=vs.100 % 29.aspx ) .
Stackoverflow,"After installing SQL Server 2008 , I find ` SQL Server Configuration Manager ` ` Start / SQL Server 2008 / Configuration Tools ` menu . What I install tool ?","If happen using Windows 8 , 's get : * The newer Microsoft SQL Server Configuration Manager snap-in Microsoft Management Console program . * It stand-alone program used previous versions Microsoft Windows operating systems , * SQL Server Configuration Manager ’ appear application running Windows 8 . * To open SQL Server Configuration Manager , Search charm , Apps , type : ` SQLServerManager14.msc ` [ SQL Server 2017 ] ` SQLServerManager13.msc ` [ SQL Server 2016 ] ` SQLServerManager12.msc ` [ SQL Server 2014 ] ` SQLServerManager11.msc ` [ SQL Server 2012 ] ` SQLServerManager10.msc ` [ SQL Server 2008 ] , press ` Enter ` . Text kindly reproduced [ SQL Server Configuration Manager changes Windows 8 ] ( http : //babyraj.com/sql-server-configuration-manager-changes- windows-8/ ) * * * Detailed info MSDN : [ SQL Server Configuration Manager ] ( http : //technet.microsoft.com/en-us/library/ms174212.aspx )"
Stackoverflow,"I looking docs and/or examples new JSON functions PostgreSQL 9.2 . Specifically , given series JSON records : [ { name : `` Toby '' , occupation : `` Software Engineer '' } , { name : `` Zaphod '' , occupation : `` Galactic President '' } ] How would I write SQL find record name ? In vanilla SQL : SELECT * json_data WHERE `` name '' = `` Toby '' The official dev manual quite sparse : * < http : //www.postgresql.org/docs/devel/static/datatype-json.html > * < http : //www.postgresql.org/docs/devel/static/functions-json.html > # # # Update I I 've put together [ gist detailing currently possible PostgreSQL 9.2 ] ( https : //gist.github.com/2715918 ) . Using custom functions , possible things like : SELECT id , json_string ( data , 'name ' ) FROM things WHERE json_string ( data , 'name ' ) LIKE ' G % ' ; # # # Update II I 've moved JSON functions project : [ **PostSQL** ] ( https : //github.com/tobyhede/postsql ) \- set functions transforming PostgreSQL PL/v8 totally awesome JSON document store","**Update** : [ With PostgreSQL 9.5 ] ( http : //www.postgresql.org/docs/9.5/static/functions-json.html ) , ` jsonb ` manipulation functionality within PostgreSQL ( none ` json ` ; casts required manipulate ` json ` values ) . Merging 2 ( ) JSON objects ( concatenating arrays ) : SELECT jsonb ' { `` '' :1 } ' || jsonb ' { `` b '' :2 } ' , -- yield jsonb ' { `` '' :1 , '' b '' :2 } ' jsonb ' [ `` '' ,1 ] ' || jsonb ' [ `` b '' ,2 ] ' -- yield jsonb ' [ `` '' ,1 , '' b '' ,2 ] ' So , **setting simple key** done using : SELECT jsonb ' { `` '' :1 } ' || jsonb_build_object ( ' < key > ' , ' < value > ' ) Where ` < key > ` string , ` < value > ` whatever type ` to_jsonb ( ) ` accepts . For **setting value deep JSON hierarchy** , ` jsonb_set ( ) ` function used : SELECT jsonb_set ( ' { `` '' : [ null , { `` b '' : [ ] } ] } ' , ' { a,1 , b,0 } ' , jsonb ' { `` c '' :3 } ' ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ { `` c '' :3 } ] } ] } ' Full parameter list ` jsonb_set ( ) ` : jsonb_set ( target jsonb , path text [ ] , new_value jsonb , create_missing boolean default true ) ` path ` contain JSON array indexes & negative integers appear count end JSON arrays . However , non-existing , positive JSON array index append element end array : SELECT jsonb_set ( ' { `` '' : [ null , { `` b '' : [ 1,2 ] } ] } ' , ' { a,1 , b,1000 } ' , jsonb ' 3 ' , true ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ 1,2,3 ] } ] } ' For **inserting JSON array ( preserving original values ) ** , ` jsonb_insert ( ) ` function used ( _in 9.6+ ; function , section_ ) : SELECT jsonb_insert ( ' { `` '' : [ null , { `` b '' : [ 1 ] } ] } ' , ' { a,1 , b,0 } ' , jsonb ' 2 ' ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ 2,1 ] } ] } ' , SELECT jsonb_insert ( ' { `` '' : [ null , { `` b '' : [ 1 ] } ] } ' , ' { a,1 , b,0 } ' , jsonb ' 2 ' , true ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ 1,2 ] } ] } ' Full parameter list ` jsonb_insert ( ) ` : jsonb_insert ( target jsonb , path text [ ] , new_value jsonb , insert_after boolean default false ) Again , negative integers appear ` path ` count end JSON arrays . So , f.ex . appending end JSON array done : SELECT jsonb_insert ( ' { `` '' : [ null , { `` b '' : [ 1,2 ] } ] } ' , ' { a,1 , b , -1 } ' , jsonb ' 3 ' , true ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ 1,2,3 ] } ] } ' , However , function working slightly differently ( ` jsonb_set ( ) ` ) ` path ` ` target ` JSON object 's key . In case , add new key-value pair JSON object key used . If used , raise error : SELECT jsonb_insert ( ' { `` '' : [ null , { `` b '' : [ 1 ] } ] } ' , ' { a,1 , c } ' , jsonb ' [ 2 ] ' ) -- yield jsonb ' { `` '' : [ null , { `` b '' : [ 1 ] , '' c '' : [ 2 ] } ] } ' , SELECT jsonb_insert ( ' { `` '' : [ null , { `` b '' : [ 1 ] } ] } ' , ' { a,1 , b } ' , jsonb ' [ 2 ] ' ) -- raise SQLSTATE 22023 ( invalid_parameter_value ) : replace existing key **Deleting key ( index ) ** JSON object ( , array ) done ` - ` operator : SELECT jsonb ' { `` '' :1 , '' b '' :2 } ' - ' ' , -- yield jsonb ' { `` b '' :2 } ' jsonb ' [ `` '' ,1 , '' b '' ,2 ] ' - 1 -- yield jsonb ' [ `` '' , '' b '' ,2 ] ' **Deleting , deep JSON hierarchy** done ` # - ` operator : SELECT ' { `` '' : [ null , { `` b '' : [ 3.14 ] } ] } ' # - ' { a,1 , b,0 } ' -- yield jsonb ' { `` '' : [ null , { `` b '' : [ ] } ] } ' **For 9.4** , use modified version original answer ( ) , instead aggregating JSON string , aggregate json object directly ` json_object_agg ( ) ` . **Original answer** : It possible ( without plpython plv8 ) pure SQL ( needs 9.3+ , work 9.2 ) CREATE OR REPLACE FUNCTION `` json_object_set_key '' ( `` json '' json , `` key_to_set '' TEXT , `` value_to_set '' anyelement ) RETURNS json LANGUAGE sql IMMUTABLE STRICT AS $ function $ SELECT concat ( ' { ' , string_agg ( to_json ( `` key '' ) || ' : ' || `` value '' , ' , ' ) , ' } ' ) : :json FROM ( SELECT * FROM json_each ( `` json '' ) WHERE `` key '' < > `` key_to_set '' UNION ALL SELECT `` key_to_set '' , to_json ( `` value_to_set '' ) ) AS `` fields '' $ function $ ; [ SQLFiddle ] ( http : //sqlfiddle.com/ # ! 15/d41d8/2902 ) **Edit** : A version , sets multiple keys & values : CREATE OR REPLACE FUNCTION `` json_object_set_keys '' ( `` json '' json , `` keys_to_set '' TEXT [ ] , `` values_to_set '' anyarray ) RETURNS json LANGUAGE sql IMMUTABLE STRICT AS $ function $ SELECT concat ( ' { ' , string_agg ( to_json ( `` key '' ) || ' : ' || `` value '' , ' , ' ) , ' } ' ) : :json FROM ( SELECT * FROM json_each ( `` json '' ) WHERE `` key '' < > ALL ( `` keys_to_set '' ) UNION ALL SELECT DISTINCT ON ( `` keys_to_set '' [ `` index '' ] ) `` keys_to_set '' [ `` index '' ] , CASE WHEN `` values_to_set '' [ `` index '' ] IS NULL THEN 'null ' : :json ELSE to_json ( `` values_to_set '' [ `` index '' ] ) END FROM generate_subscripts ( `` keys_to_set '' , 1 ) AS `` keys '' ( `` index '' ) JOIN generate_subscripts ( `` values_to_set '' , 1 ) AS `` values '' ( `` index '' ) USING ( `` index '' ) ) AS `` fields '' $ function $ ; **Edit 2** : @ ErwinBrandstetter [ noted ] ( https : //stackoverflow.com/questions/18209625/how-do-i-modify-fields- inside-the-new-postgresql-json- datatype/23500670 ? noredirect=1 # comment39388159_23500670 ) functions works like so-called ` UPSERT ` ( updates field exists , inserts exist ) . Here variant , ` UPDATE ` : CREATE OR REPLACE FUNCTION `` json_object_update_key '' ( `` json '' json , `` key_to_set '' TEXT , `` value_to_set '' anyelement ) RETURNS json LANGUAGE sql IMMUTABLE STRICT AS $ function $ SELECT CASE WHEN ( `` json '' - > `` key_to_set '' ) IS NULL THEN `` json '' ELSE ( SELECT concat ( ' { ' , string_agg ( to_json ( `` key '' ) || ' : ' || `` value '' , ' , ' ) , ' } ' ) FROM ( SELECT * FROM json_each ( `` json '' ) WHERE `` key '' < > `` key_to_set '' UNION ALL SELECT `` key_to_set '' , to_json ( `` value_to_set '' ) ) AS `` fields '' ) : :json END $ function $ ; **Edit 3** : Here recursive variant , set ( ` UPSERT ` ) leaf value ( uses first function answer ) , located key-path ( keys refer inner objects , inner arrays supported ) : CREATE OR REPLACE FUNCTION `` json_object_set_path '' ( `` json '' json , `` key_path '' TEXT [ ] , `` value_to_set '' anyelement ) RETURNS json LANGUAGE sql IMMUTABLE STRICT AS $ function $ SELECT CASE COALESCE ( array_length ( `` key_path '' , 1 ) , 0 ) WHEN 0 THEN to_json ( `` value_to_set '' ) WHEN 1 THEN `` json_object_set_key '' ( `` json '' , `` key_path '' [ l ] , `` value_to_set '' ) ELSE `` json_object_set_key '' ( `` json '' , `` key_path '' [ l ] , `` json_object_set_path '' ( COALESCE ( NULLIF ( ( `` json '' - > `` key_path '' [ l ] ) : :text , 'null ' ) , ' { } ' ) : :json , `` key_path '' [ l+1 : u ] , `` value_to_set '' ) ) END FROM array_lower ( `` key_path '' , 1 ) l , array_upper ( `` key_path '' , 1 ) u $ function $ ; **Update** : functions compacted ."
Stackoverflow,"I 'm running server office process files report results remote MySQL server . The files processing takes time process dies halfway following error : 2006 , MySQL server gone away I 've heard MySQL setting , **wait_timeout** , I need change server office remote MySQL server ?","I 've encountered number times I 've normally found answer low default setting [ ` max_allowed_packet ` ] ( https : //dev.mysql.com/doc/refman/5.7/en/server-system- variables.html # sysvar_max_allowed_packet ) . Raising ` /etc/my.cnf ` ( ` [ mysqld ] ` ) 8 16M usually fixes . ( The default MySql 5.7 ` 4194304 ` , 4MB . ) [ mysqld ] max_allowed_packet=16M * * * Note : Just create line exist Note : This set server 's running . Use ` set global max_allowed_packet=104857600 ` . This sets 100MB ."
Stackoverflow,I 'm wondering difference ` RDD ` ` DataFrame ` _ ( Spark 2.0.0 DataFrame mere type alias ` Dataset [ Row ] ` ) _ Apache Spark ? Can convert one ?,"If structure flat : val df = Seq ( ( 1L , `` '' , `` foo '' , 3.0 ) ) .toDF df.printSchema // root // | -- _1 : long ( nullable = false ) // | -- _2 : string ( nullable = true ) // | -- _3 : string ( nullable = true ) // | -- _4 : double ( nullable = false ) simplest thing use ` toDF ` method : val newNames = Seq ( `` id '' , `` x1 '' , `` x2 '' , `` x3 '' ) val dfRenamed = df.toDF ( newNames : _* ) dfRenamed.printSchema // root // | -- id : long ( nullable = false ) // | -- x1 : string ( nullable = true ) // | -- x2 : string ( nullable = true ) // | -- x3 : double ( nullable = false ) If want rename individual columns use either ` select ` ` alias ` : df.select ( $ '' _1 '' .alias ( `` x1 '' ) ) easily generalized multiple columns : val lookup = Map ( `` _1 '' - > `` foo '' , `` _3 '' - > `` bar '' ) df.select ( df.columns.map ( c = > col ( c ) .as ( lookup.getOrElse ( c , c ) ) ) : _* ) ` withColumnRenamed ` : df.withColumnRenamed ( `` _1 '' , `` x1 '' ) use ` foldLeft ` rename multiple columns : lookup.foldLeft ( df ) ( ( acc , ca ) = > acc.withColumnRenamed ( ca._1 , ca._2 ) ) With nested structures ( ` structs ` ) one possible option renaming selecting whole structure : val nested = spark.read.json ( sc.parallelize ( Seq ( `` '' '' { `` foobar '' : { `` foo '' : { `` bar '' : { `` first '' : 1.0 , `` second '' : 2.0 } } } , `` id '' : 1 } '' '' '' ) ) ) nested.printSchema // root // | -- foobar : struct ( nullable = true ) // | | -- foo : struct ( nullable = true ) // | | | -- bar : struct ( nullable = true ) // | | | | -- first : double ( nullable = true ) // | | | | -- second : double ( nullable = true ) // | -- id : long ( nullable = true ) @ transient val foobarRenamed = struct ( struct ( struct ( $ '' foobar.foo.bar.first '' .as ( `` x '' ) , $ '' foobar.foo.bar.first '' .as ( `` '' ) ) .alias ( `` point '' ) ) .alias ( `` location '' ) ) .alias ( `` record '' ) nested.select ( foobarRenamed , $ '' id '' ) .printSchema // root // | -- record : struct ( nullable = false ) // | | -- location : struct ( nullable = false ) // | | | -- point : struct ( nullable = false ) // | | | | -- x : double ( nullable = true ) // | | | | -- : double ( nullable = true ) // | -- id : long ( nullable = true ) Note may affect ` nullability ` metadata . Another possibility rename casting : nested.select ( $ '' foobar '' .cast ( `` struct < location : struct < point : struct < x : double , : double > > > '' ) .alias ( `` record '' ) ) .printSchema // root // | -- record : struct ( nullable = true ) // | | -- location : struct ( nullable = true ) // | | | -- point : struct ( nullable = true ) // | | | | -- x : double ( nullable = true ) // | | | | -- : double ( nullable = true ) : import org.apache.spark.sql.types._ nested.select ( $ '' foobar '' .cast ( StructType ( Seq ( StructField ( `` location '' , StructType ( Seq ( StructField ( `` point '' , StructType ( Seq ( StructField ( `` x '' , DoubleType ) , StructField ( `` '' , DoubleType ) ) ) ) ) ) ) ) ) ) .alias ( `` record '' ) ) .printSchema // root // | -- record : struct ( nullable = true ) // | | -- location : struct ( nullable = true ) // | | | -- point : struct ( nullable = true ) // | | | | -- x : double ( nullable = true ) // | | | | -- : double ( nullable = true )"
Stackoverflow,"What mean SqlConnection `` enlisted '' transaction ? Does simply mean commands I execute connection participate transaction ? If , circumstances SqlConnection _automatically_ enlisted ambient TransactionScope Transaction ? See questions code comments . My guess question 's answer follows question parenthesis . # # Scenario 1 : Opening connections INSIDE transaction scope using ( TransactionScope scope = new TransactionScope ( ) ) using ( SqlConnection conn = ConnectToDB ( ) ) { // Q1 : Is connection automatically enlisted transaction ? ( Yes ? ) // // Q2 : If I open ( run commands ) second connection , // identical connection string , // , , relationship second connection first ? // // Q3 : Will second connection 's automatic enlistment // current transaction scope cause transaction // escalated distributed transaction ? ( Yes ? ) } # # Scenario 2 : Using connections INSIDE transaction scope opened OUTSIDE //Assume ambient transaction active SqlConnection new_or_existing_connection = ConnectToDB ( ) ; //or passed method parameter using ( TransactionScope scope = new TransactionScope ( ) ) { // Connection opened transaction scope created // Q4 : If I start executing commands connection , // automatically become enlisted current transaction scope ? ( No ? ) // // Q5 : If enlisted , commands I execute connection // participate ambient transaction ? ( No ? ) // // Q6 : If commands connection // participating current transaction , committed // even rollback current transaction scope ? ( Yes ? ) // // If thoughts correct , disturbing , // would look like I 'm executing commands // transaction scope , fact I 'm , // I following ... // // Now enlisting existing connection current transaction conn.EnlistTransaction ( Transaction.Current ) ; // // Q7 : Does method explicitly enlist pre-existing connection // current ambient transaction , commands I // execute connection participate // ambient transaction ? ( Yes ? ) // // Q8 : If existing connection already enlisted transaction // I called method , would happen ? Might error thrown ? ( Probably ? ) // // Q9 : If existing connection already enlisted transaction // I NOT call method enlist , would commands // I execute participate 's existing transaction rather // current transaction scope . ( Yes ? ) }","I 've done tests since asking question found answers , since one else replied . Please let know I've missed anything . **Q1 . ** Yes , unless `` enlist=false '' specified connection string . The connection pool finds usable connection . A usable connection one that's enlisted transaction one 's enlisted transaction . **Q2 . ** The second connection independent connection , participates transaction . I 'm sure interaction commands two connections , since 're running database , I think errors occur commands issued time : errors like [ `` Transaction context use another session '' ] ( https : //stackoverflow.com/questions/2858750/what-is-the-reason-of- transaction-context-in-use-by-another-session/2885059 # 2885059 ) **Q3 . ** Yes , gets escalated distributed transaction , enlisting one connection , even connection string , causes become distributed transaction , confirmed checking non-null GUID Transaction.Current.TransactionInformation.DistributedIdentifier . *Update : I read somewhere fixed SQL Server 2008 , MSDTC used connection string used connections ( long connections open time ) . That allows open connection close multiple times within transaction , could make better use connection pool opening connections late possible closing soon possible . **Q4 . ** No . A connection opened transaction scope active , automatically enlisted newly created transaction scope . **Q5 . ** No . Unless open connection transaction scope , enlist existing connection scope , basically NO TRANSACTION . Your connection must automatically manually enlisted transaction scope order commands participate transaction . **Q6 . ** Yes , commands connection participating transaction committed issued , even though code happens executed transaction scope block got rolled back . If connection enlisted current transaction scope , 's participating transaction , committing rolling back transaction effect commands issued connection enlisted transaction scope ... [ guy found ] ( https : //stackoverflow.com/questions/1707566/data- committed-even-though-system-transactions-transactionscope-commit-not-call ) . That 's hard one spot unless understand automatic enlistment process : occurs connection opened _inside_ active transaction scope . **Q7 . ** Yes . An existing connection explicitly enlisted current transaction scope calling EnlistTransaction ( Transaction.Current ) . You also enlist connection separate thread transaction using DependentTransaction , like , I 'm sure two connections involved transaction database may interact ... errors may occur , course second enlisted connection causes transaction escalate distributed transaction . **Q8 . ** An error may thrown . If TransactionScopeOption.Required used , connection already enlisted transaction scope transaction , error ; fact , 's new transaction created scope , transaction count ( @ @ trancount ) increase . If , however , use TransactionScopeOption.RequiresNew , get helpful error message upon attempting enlist connection new transaction scope transaction : `` Connection currently transaction enlisted . Finish current transaction retry . '' And yes , complete transaction connection enlisted , safely enlist connection new transaction . _Update : If previously called BeginTransaction connection , slightly different error thrown try enlist new transaction scope transaction : `` Can enlist transaction local transaction progress connection . Finish local transaction retry . '' On hand , safely call BeginTransaction SqlConnection enlisted transaction scope transaction , actually increase @ @ trancount one , unlike using Required option nested transaction scope , cause increase . Interestingly , go create another nested transaction scope Required option , get error , nothing changes result already active transaction scope transaction ( remember @ @ trancount increased transaction scope transaction already active Required option used ) ._ **Q9 . ** Yes . Commands participate whatever transaction connection enlisted , regardless active transaction scope C # code ."
Stackoverflow,"We application running locally 're experiencing following error : > ORA-12514 : TNS : listener currently know service requested connect descriptor I 've tested connection using ` TNSPing ` resolved correctly I tried ` SQLPlus ` try connecting , failed error . I used syntax ` SQLPlus ` : sqlplus username/password @ addressname [ host name ] We verified : * TNS Listener server running . * Oracle server running . We n't know changes made environment . Anything else test ?","I issue fix make sure ` tnsnames.ora ` ` SERVICE_NAME ` valid service name database . To find valid service names , use following query oracle : select value v $ parameter name='service_names' Once I updated ` tnsnames.ora ` : TEST = ( DESCRIPTION = ( ADDRESS_LIST = ( ADDRESS = ( PROTOCOL = TCP ) ( HOST = * < validhost > * ) ( PORT = * < validport > * ) ) ) ( CONNECT_DATA = ( SERVER = DEDICATED ) ( SERVICE_NAME = * < servicenamefromDB > * ) ) ) I ran : sqlplus user @ TEST Success ! The listener basically telling whatever service_name using n't valid service according DB . ( *I running sqlplus Win7 client workstation remote DB blame DBAs ; ) * )"
Stackoverflow,"I two columns table users namely ` registerDate lastVisitDate ` consist datetime data type . I would like following . 1 . Set registerDate defaults value MySQL NOW ( ) 2 . Set lastVisitDate default value ` 0000-00-00 00:00:00 ` Instead null uses default . Because table already exists existing records , I would like use Modify table . I 've tried using two piece code , neither works . ALTER TABLE users MODIFY registerDate datetime DEFAULT NOW ( ) ALTER TABLE users MODIFY registerDate datetime DEFAULT CURRENT_TIMESTAMP ; It gives Error : ` ERROR 1067 ( 42000 ) : Invalid default value 'registerDate ' ` Is possible set default datetime value NOW ( ) MySQL ?","As MySQL 5.6.5 , use ` DATETIME ` type dynamic default value : CREATE TABLE foo ( creation_time DATETIME DEFAULT CURRENT_TIMESTAMP , modification_time DATETIME ON UPDATE CURRENT_TIMESTAMP ) Or even combine rules : modification_time DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP Reference : < http : //dev.mysql.com/doc/refman/5.7/en/timestamp-initialization.html > < http : //optimize-this.blogspot.com/2012/04/datetime-default-now-finally- available.html > Prior 5.6.5 , need use ` TIMESTAMP ` data type , automatically updates whenever record modified . Unfortunately , however , one auto-updated ` TIMESTAMP ` field exist per table . CREATE TABLE mytable ( mydate TIMESTAMP ) See : < http : //dev.mysql.com/doc/refman/5.1/en/create-table.html > If want prevent MySQL updating timestamp value ` UPDATE ` ( triggers ` INSERT ` ) change definition : CREATE TABLE mytable ( mydate TIMESTAMP DEFAULT CURRENT_TIMESTAMP )"
Stackoverflow,"Why one TIMESTAMP column CURRENT_TIMESTAMP DEFAULT ON UPDATE clause ? CREATE TABLE ` foo ` ( ` ProductID ` INT ( 10 ) UNSIGNED NOT NULL , ` AddedDate ` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP , ` UpdatedDate ` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ) ENGINE=INNODB ; The error results : > Error Code : 1293 > > Incorrect table definition ; one TIMESTAMP column CURRENT_TIMESTAMP DEFAULT ON UPDATE clause","This limitation , due historical , code legacy reasons , lifted recent versions MySQL : > Changes MySQL 5.6.5 ( 2012-04-10 , Milestone 8 ) > > Previously , one TIMESTAMP column per table could automatically initialized updated current date time . This restriction lifted . Any TIMESTAMP column definition combination DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP clauses . In addition , clauses used DATETIME column definitions . For information , see Automatic Initialization Updating TIMESTAMP DATETIME . < http : //dev.mysql.com/doc/relnotes/mysql/5.6/en/news-5-6-5.html >"
Stackoverflow,How I list extensions already installed database schema psql ? See also * [ Finding list available extensions PostgreSQL ships ] ( https : //dba.stackexchange.com/questions/166283/where-can-i-find-a-list-of-the-extensions-for-postgresql ),In psql would \dx See manual details : < http : //www.postgresql.org/docs/current/static/app-psql.html > Doing plain SQL would select ` pg_extension ` : SELECT * FROM pg_extension < http : //www.postgresql.org/docs/current/static/catalog-pg-extension.html >
Stackoverflow,"The query I 'm running follows , however I 'm getting error : > # 1054 - Unknown column 'guaranteed_postcode ' 'IN/ALL/ANY subquery' SELECT ` users ` . ` first_name ` , ` users ` . ` last_name ` , ` users ` . ` email ` , SUBSTRING ( ` locations ` . ` raw ` , -6,4 ) AS ` guaranteed_postcode ` FROM ` users ` LEFT OUTER JOIN ` locations ` ON ` users ` . ` id ` = ` locations ` . ` user_id ` WHERE ` guaranteed_postcode ` NOT IN # fake col used ( SELECT ` postcode ` FROM ` postcodes ` WHERE ` region ` IN ( 'australia' ) ) My question : I unable use fake column clause DB query ?","You use column aliases GROUP BY , ORDER BY , HAVING clauses . > Standard SQL n't allow refer column alias WHERE clause . This restriction imposed WHERE code executed , column value may yet determined . Copied [ MySQL documentation ] ( https : //dev.mysql.com/doc/refman/8.0/en/problems-with- alias.html ) As pointed comments , using HAVING instead may work . Make sure give read [ WHERE vs HAVING ] ( https : //stackoverflow.com/questions/2905292/where-vs- having/18710763 # 18710763 ) though ."
Stackoverflow,How execute raw SQL SQLAlchemy ? I python web app runs flask interfaces database SQLAlchemy . I need way run raw SQL . The query involves multiple table joins along Inline views . I 've tried : connection = db.session.connection ( ) connection.execute ( < sql > ) But I keep getting gateway errors .,"There several ways ` UPDATE ` using ` sqlalchemy ` 1 ) user.no_of_logins += 1 session.commit ( ) 2 ) session.query ( ) .\ filter ( User.username == form.username.data ) .\ update ( { `` no_of_logins '' : ( User.no_of_logins +1 ) } ) session.commit ( ) 3 ) conn = engine.connect ( ) stmt = User.update ( ) .\ values ( no_of_logins= ( User.no_of_logins + 1 ) ) .\ ( User.username == form.username.data ) conn.execute ( stmt ) 4 ) setattr ( user , 'no_of_logins ' , user.no_of_logins+1 ) session.commit ( )"
Stackoverflow,I following table schema maps user_customers permissions live MySQL database : mysql > describe user_customer_permission ; + -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | Field | Type | Null | Key | Default | Extra | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | id | int ( 11 ) | NO | PRI | NULL | auto_increment | | user_customer_id | int ( 11 ) | NO | PRI | NULL | | | permission_id | int ( 11 ) | NO | PRI | NULL | | + -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + 3 rows set ( 0.00 sec ) I would like remove primary keys user_customer_id permission_id retain primary key id . When I run command : alter table user_customer_permission drop primary key ; I get following error : ERROR 1075 ( 42000 ) : Incorrect table definition ; one auto column must defined key How I drop column 's primary key ?,"Without index , maintaining autoincrement column becomes expensive , 's ` MySQL ` requires autoincrement column leftmost part index . You remove autoincrement property dropping key : ALTER TABLE user_customer_permission MODIFY id INT NOT NULL ; ALTER TABLE user_customer_permission DROP PRIMARY KEY ; Note composite ` PRIMARY KEY ` covers three columns ` id ` guaranteed unique . If happens unique , make ` PRIMARY KEY ` ` AUTO_INCREMENT ` : ALTER TABLE user_customer_permission MODIFY id INT NOT NULL PRIMARY KEY AUTO_INCREMENT ;"
Stackoverflow,"How I declare variable use PostgreSQL 8.3 query ? In MS SQL Server I : DECLARE @ myvar INT SET @ myvar = 5 SELECT * FROM somewhere WHERE something = @ myvar How I PostgreSQL ? According documentation variables declared simply `` name type ; '' , gives syntax error : myvar INTEGER ; Could someone give example correct syntax ?","I accomplished goal using [ ` WITH ` clause ] ( https : //www.postgresql.org/docs/current/static/queries-with.html ) , 's nowhere near elegant thing . Though example 's really overkill . I also n't particularly recommend . WITH myconstants ( var1 , var2 ) ( values ( 5 , 'foo ' ) ) SELECT * FROM somewhere , myconstants WHERE something = var1 OR something_else = var2 ;"
Stackoverflow,"I MS SQL 2005 database table ` Test ` column ` ID ` . ` ID ` identity column . I rows table corresponding ID auto incremented value . Now I would like change every ID table like : ` ID = ID + 1 ` But I I get error : > Can update identity column 'ID ' . I 've tried : ALTER TABLE Test NOCHECK CONSTRAINT ALL set identity_insert ID ON But solve problem . I need identity set column , I need change values well time time . So question accomplish task .",You need set identity_insert YourTable ON Then delete row reinsert different identity . Once done insert n't forget turn identity_insert set identity_insert YourTable OFF
Stackoverflow,"The following code gives error - `` No implicit conversion DBnull int . '' SqlParameter [ ] parameters = new SqlParameter [ 1 ] ; SqlParameter planIndexParameter = new SqlParameter ( `` @ AgeIndex '' , SqlDbType.Int ) ; planIndexParameter.Value = ( AgeItem.AgeIndex== null ) ? DBNull.Value : AgeItem.AgeIndex ; parameters [ 0 ] = planIndexParameter ;","The problem ` ? : ` operator determine return type either returning ` int ` value DBNull type value , compatible . You course cast instance AgeIndex type ` object ` would satisfy ` ? : ` requirement . You use ` ? ? ` null-coalescing operator follows SqlParameter [ ] parameters = new SqlParameter [ 1 ] ; SqlParameter planIndexParameter = new SqlParameter ( `` @ AgeIndex '' , SqlDbType.Int ) ; planIndexParameter.Value = ( object ) AgeItem.AgeIndex ? ? DBNull.Value ; parameters [ 0 ] = planIndexParameter ; Here quote [ MSDN documentation ] ( http : //msdn.microsoft.com/en- us/library/ty67wk28.aspx ) ` ? : ` operator explains problem > Either type first_expression second_expression must , implicit conversion must exist one type ."
Stackoverflow,"I trying import .sql file failing creating tables . Here 's query fails : CREATE TABLE ` data ` ( ` id ` int ( 10 ) unsigned NOT NULL , ` name ` varchar ( 100 ) NOT NULL , ` value ` varchar ( 15 ) NOT NULL , UNIQUE KEY ` id ` ( ` id ` , ` name ` ) , CONSTRAINT ` data_ibfk_1 ` FOREIGN KEY ( ` id ` ) REFERENCES ` keywords ` ( ` id ` ) ON DELETE CASCADE ON UPDATE CASCADE ) ENGINE=InnoDB DEFAULT CHARSET=latin1 ; I exported .sql database , I dropped tables im trying import , failing ? > MySQL : Ca n't create table './dbname/data.frm ' ( errno : 150 )","I problem ` ALTER TABLE ADD FOREIGN KEY ` . After hour , I found conditions must satisfied get error 150 : 1 . The Parent table must exist define foreign key reference . You must define tables right order : Parent table first , Child table . If tables references , must create one table without FK constraints , create second table , add FK constraint first table ` ALTER TABLE ` . 2 . The two tables must support foreign key constraints , i.e . ` ENGINE=InnoDB ` . Other storage engines silently ignore foreign key definitions , return error warning , FK constraint saved . 3 . The referenced columns Parent table must left-most columns key . Best key Parent ` PRIMARY KEY ` ` UNIQUE KEY ` . 4 . The FK definition must reference PK column ( ) order PK definition . For example , FK ` REFERENCES Parent ( , b , c ) ` Parent 's PK must defined columns order ` ( , c , b ) ` . 5 . The PK column ( ) Parent table must data type FK column ( ) Child table . For example , PK column Parent table ` UNSIGNED ` , sure define ` UNSIGNED ` corresponding column Child table field . Exception : length strings may different . For example , ` VARCHAR ( 10 ) ` reference ` VARCHAR ( 20 ) ` vice versa . 6 . Any string-type FK column ( ) must character set collation corresponding PK column ( ) . 7 . If data already Child table , every value FK column ( ) must match value Parent table PK column ( ) . Check query like : SELECT COUNT ( * ) FROM Child LEFT OUTER JOIN Parent ON Child.FK = Parent.PK WHERE Parent.PK IS NULL ; This must return zero ( 0 ) unmatched values . Obviously , query generic example ; must substitute table names column names . 8 . Neither Parent table Child table ` TEMPORARY ` table . 9 . Neither Parent table Child table ` PARTITIONED ` table . 10 . If declare FK ` ON DELETE SET NULL ` option , FK column ( ) must nullable . 11 . If declare constraint name foreign key , constraint name must unique whole schema , table constraint defined . Two tables may constraint name . Hope helps ."
Stackoverflow,I would like give user permissions database without making admin . The reason I want moment DEV PROD different DBs cluster I n't want user able change production objects must able change objects DEV . I tried : grant ALL database MY_DB group MY_GROUP ; n't seem give permission . Then I tried : grant privileges schema MY_SCHEMA group MY_GROUP ; seems give permission create objects query\delete objects schema belong users I could go giving USAGE permission user MY_SCHEMA would complain permissions table ... So I guess question : easy way giving permissions user DB ? I 'm working PostgreSQL 8.1.23 .,"The user needs access _database_ , obviously : GRANT CONNECT ON DATABASE my_db TO my_user ; And ( least ) ` USAGE ` privilege _schema_ : GRANT USAGE ON SCHEMA public TO my_user ; Then , permissions _tables_ ( requires Postgres **9.0** later ) : GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO my_user ; And n't forget _sequences_ ( ) : GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO my_user ; For **older versions** could use `` Grant Wizard '' pgAdmin III ( default GUI ) . More : * [ **How manage DEFAULT PRIVILEGES USERs DATABASE vs SCHEMA ? ** ] ( https : //dba.stackexchange.com/questions/117109/how-to-manage-default-privileges-for-users-on-a-database-vs-schema/117661 # 117661 ) * [ Grant privileges particular database PostgreSQL ] ( https : //stackoverflow.com/questions/24918367/grant-privileges-for-a-particular-database-in-postgresql/24923877 # 24923877 ) * [ How grant privileges views arbitrary user ] ( https : //stackoverflow.com/questions/10491208/how-to-grant-all-privileges-on-views-to-arbitrary-user/10491275 # 10491275 ) But really , [ upgrade current version ] ( https : //www.postgresql.org/support/versioning/ ) ."
Stackoverflow,"I database schema named : ` nyummy ` table named ` cimory ` : create table nyummy.cimory ( id numeric ( 10,0 ) null , name character varying ( 60 ) null , city character varying ( 50 ) null , CONSTRAINT cimory_pkey PRIMARY KEY ( id ) ) ; I want export ` cimory ` table 's data insert SQL script file . However , I want export records/data city equal 'tokyo ' ( assume city data lowercase ) . How ? It n't matter whether solution freeware GUI tools command line ( although GUI tools solution better ) . I tried pgAdmin III , I ca n't find option .","Create table set want export use command line utility pg_dump export file : create table export_table select id , name , city nyummy.cimory city = 'tokyo' $ pg_dump -- table=export_table -- data-only -- column-inserts my_database > data.sql ` -- column-inserts ` dump insert commands column names . ` -- data-only ` dump schema . As commented , creating view instead table obviate table creation whenever new export necessary ."
Stackoverflow,I installed mysql server linux box IP = 192.168.1.100 try connect IP alway error ( 111 ) . use localhost 127.0.0.1 OK. beer @ beer-laptop # ifconfig | grep `` inet addr '' inet addr:127.0.0.1 Mask:255.0.0.0 inet addr:192.168.1.100 Bcast:192.168.1.255 Mask:255.255.255.0 beer @ beer-laptop # mysql -ubeer -pbeer -h192.168.1.100 ERROR 2003 ( HY000 ) : Ca n't connect MySQL server '192.168.1.100 ' ( 111 ) beer @ beer-laptop # mysql -ubeer -pbeer -hlocalhost Welcome MySQL monitor . Commands end ; \g . Your MySQL connection id 160 Server version : 5.1.31-1ubuntu2 ( Ubuntu ) Type 'help ; ' '\h ' help . Type '\c ' clear buffer . mysql > beer @ beer-laptop # mysql -ubeer -pbeer -h127.0.0.1 Welcome MySQL monitor . Commands end ; \g . Your MySQL connection id 161 Server version : 5.1.31-1ubuntu2 ( Ubuntu ) Type 'help ; ' '\h ' help . Type '\c ' clear buffer . mysql > Connect another machine also error 111. another @ another-laptop # mysql -ubeer -pbeer -h192.168.1.100 ERROR 2003 ( HY000 ) : Ca n't connect MySQL server '192.168.1.100 ' ( 111 ) How difference use localhost/127.0.0.1 192.168.1.100 case . I n't know connect database another machine . Help please . Thank .,"It probably means MySQL server listening localhost interface . If lines like : skip-networking bind-address = 127.0.0.1 In [ ` my.cnf ` configuration file ] ( https : //stackoverflow.com/a/2485758/1243247 ) , comment _ ( add # beginning lines ) _ , restart MySQL . sudo service mysql restart Of course , , must administrator server ."
Stackoverflow,"In PHP , accessing MySQL database PDO parametrized query , check final query ( replaced tokens ) ? Is way check gets really executed database ?","Your EXEC example would NOT parameterized . You need parameterized queries ( prepared statements circles ) prevent input like causing damage : > ' ; DROP TABLE bar ; -- Try putting fuz variable ( n't , value bar table ) . More subtle damaging queries possible well . Here 's example parameters Sql Server : Public Function GetBarFooByBaz ( ByVal Baz As String ) As String Dim sql As String = `` SELECT foo FROM bar WHERE baz= @ Baz '' Using cn As New SqlConnection ( `` Your connection string '' ) , _ cmd As New SqlCommand ( sql , cn ) cmd.Parameters.Add ( `` @ Baz '' , SqlDbType.VarChar , 50 ) .Value = Baz Return cmd.ExecuteScalar ( ) .ToString ( ) End Using End Function Stored procedures sometimes credited preventing SQL injection . However , time still call using query parameters n't help . If use stored procedures _exclusively_ , turn permissions SELECT , UPDATE , ALTER , CREATE , DELETE , etc ( everything EXEC ) application user account get protection way ."
Stackoverflow,"For several reasons I n't liberty talk , defining view Sql Server 2005 database like : CREATE VIEW [ dbo ] . [ MeterProvingStatisticsPoint ] AS SELECT CAST ( 0 AS BIGINT ) AS 'RowNumber ' , CAST ( 0 AS BIGINT ) AS 'ProverTicketId ' , CAST ( 0 AS INT ) AS 'ReportNumber ' , GETDATE ( ) AS 'CompletedDateTime ' , CAST ( 1.1 AS float ) AS 'MeterFactor ' , CAST ( 1.1 AS float ) AS 'Density ' , CAST ( 1.1 AS float ) AS 'FlowRate ' , CAST ( 1.1 AS float ) AS 'Average ' , CAST ( 1.1 AS float ) AS 'StandardDeviation ' , CAST ( 1.1 AS float ) AS 'MeanPlus2XStandardDeviation ' , CAST ( 1.1 AS float ) AS 'MeanMinus2XStandardDeviation' WHERE 0 = 1 The idea Entity Framework create entity based query , , generates error states following : > Warning 6002 : The table/view 'Keystone_Local.dbo.MeterProvingStatisticsPoint ' primary key defined . The key inferred definition created read- table/view . And decides CompletedDateTime field entity primary key . We using EdmGen generate model . Is way entity framework include field view primary key ?","We problem solution : To force entity framework use column primary key , use ISNULL . To force entity framework use column primary key , use NULLIF . An easy way apply wrap select statement view another select . Example : SELECT ISNULL ( MyPrimaryID , -999 ) MyPrimaryID , NULLIF ( AnotherProperty , '' ) AnotherProperty FROM ( ... ) AS temp"
Stackoverflow,I SDF file I would like retrieve schema query UI . How I ? I Visual Studio installed machine I would like install little software possible .,"To alias table 'd say : DELETE f FROM dbo.foods AS f WHERE f.name IN ( ... ) ; I fail see point aliasing specific ` DELETE ` statement , especially since ( least IIRC ) longer conforms strict ANSI . But yes , comments suggest , may necessary query forms ( eg correlation ) ."
Stackoverflow,I regularly need delete data PostgreSQL database rebuild . How would I directly SQL ? At moment I 've managed come SQL statement returns commands I need execute : SELECT 'TRUNCATE TABLE ' || tablename || ' ; ' FROM pg_tables WHERE tableowner='MYUSER ' ; But I ca n't see way execute programmatically I .,"FrustratedWithFormsDesigner correct , PL/pgSQL . Here 's script : CREATE OR REPLACE FUNCTION truncate_tables ( username IN VARCHAR ) RETURNS void AS $ $ DECLARE statements CURSOR FOR SELECT tablename FROM pg_tables WHERE tableowner = username AND schemaname = 'public ' ; BEGIN FOR stmt IN statements LOOP EXECUTE 'TRUNCATE TABLE ' || quote_ident ( stmt.tablename ) || ' CASCADE ; ' ; END LOOP ; END ; $ $ LANGUAGE plpgsql ; This creates stored function ( need ) afterwards use like : SELECT truncate_tables ( 'MYUSER ' ) ;"
Stackoverflow,"When : DELETE FROM ` jobs ` WHERE ` job_id ` =1 LIMIT 1 It errors : # 1451 - Can delete update parent row : foreign key constraint fails ( paymesomething.advertisers , CONSTRAINT advertisers_ibfk_1 FOREIGN KEY ( advertiser_id ) REFERENCES jobs ( advertiser_id ) ) Here tables : CREATE TABLE IF NOT EXISTS ` advertisers ` ( ` advertiser_id ` int ( 11 ) unsigned NOT NULL AUTO_INCREMENT , ` name ` varchar ( 255 ) NOT NULL , ` password ` char ( 32 ) NOT NULL , ` email ` varchar ( 128 ) NOT NULL , ` address ` varchar ( 255 ) NOT NULL , ` phone ` varchar ( 255 ) NOT NULL , ` fax ` varchar ( 255 ) NOT NULL , ` session_token ` char ( 30 ) NOT NULL , PRIMARY KEY ( ` advertiser_id ` ) , UNIQUE KEY ` email ` ( ` email ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=2 ; INSERT INTO ` advertisers ` ( ` advertiser_id ` , ` name ` , ` password ` , ` email ` , ` address ` , ` phone ` , ` fax ` , ` session_token ` ) VALUES ( 1 , 'TEST COMPANY ' , `` , `` , `` , `` , `` , `` ) ; CREATE TABLE IF NOT EXISTS ` jobs ` ( ` job_id ` int ( 11 ) unsigned NOT NULL AUTO_INCREMENT , ` advertiser_id ` int ( 11 ) unsigned NOT NULL , ` name ` varchar ( 255 ) NOT NULL , ` shortdesc ` varchar ( 255 ) NOT NULL , ` longdesc ` text NOT NULL , ` address ` varchar ( 255 ) NOT NULL , ` time_added ` int ( 11 ) NOT NULL , ` active ` tinyint ( 1 ) NOT NULL , ` moderated ` tinyint ( 1 ) NOT NULL , PRIMARY KEY ( ` job_id ` ) , KEY ` advertiser_id ` ( ` advertiser_id ` , ` active ` , ` moderated ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 AUTO_INCREMENT=2 ; INSERT INTO ` jobs ` ( ` job_id ` , ` advertiser_id ` , ` name ` , ` shortdesc ` , ` longdesc ` , ` address ` , ` active ` , ` moderated ` ) VALUES ( 1 , 1 , 'TEST ' , 'TESTTEST ' , 'TESTTESTES ' , `` , 0 , 0 ) ; ALTER TABLE ` advertisers ` ADD CONSTRAINT ` advertisers_ibfk_1 ` FOREIGN KEY ( ` advertiser_id ` ) REFERENCES ` jobs ` ( ` advertiser_id ` ) ;",The simple way would disable foreign key check ; make changes re-enable foreign key check . SET FOREIGN_KEY_CHECKS=0 ; -- disable SET FOREIGN_KEY_CHECKS=1 ; -- re-enable
Stackoverflow,"I simple SQL query PostgreSQL 8.3 grabs bunch comments . I provide _sorted_ list values ` IN ` construct ` WHERE ` clause : SELECT * FROM comments WHERE ( comments.id IN ( 1,3,2,4 ) ) ; This returns comments arbitrary order happens ids like ` 1,2,3,4 ` . I want resulting rows sorted like list ` IN ` construct : ` ( 1,3,2,4 ) ` . How achieve ?","` EXISTS ` faster engine found hit , quit looking condition proved true . With ` IN ` collect results sub-query processing ."
Stackoverflow,"The following simplest possible example , though solution able scale however many n top results needed : Given table like , person , group , age columns , would **get 2 oldest people group ? ** ( Ties within groups yield results , give first 2 alphabetical order ) + -- -- -- -- + -- -- -- -+ -- -- -+ | Person | Group | Age | + -- -- -- -- + -- -- -- -+ -- -- -+ | Bob | 1 | 32 | | Jill | 1 | 34 | | Shawn | 1 | 42 | | Jake | 2 | 29 | | Paul | 2 | 36 | | Laura | 2 | 39 | + -- -- -- -- + -- -- -- -+ -- -- -+ Desired result set : + -- -- -- -- + -- -- -- -+ -- -- -+ | Shawn | 1 | 42 | | Jill | 1 | 34 | | Laura | 2 | 39 | | Paul | 2 | 36 | + -- -- -- -- + -- -- -- -+ -- -- -+ * * * **NOTE : ** This question builds previous one- [ Get records max value group grouped SQL results ] ( https : //stackoverflow.com/q/12102200/165673 ) \- getting single top row group , received great MySQL-specific answer @ Bohemian : select * ( select * mytable order ` Group ` , Age desc , Person ) x group ` Group ` Would love able build , though I n't see .","Here one way , using ` UNION ALL ` ( See [ SQL Fiddle Demo ] ( http : //sqlfiddle.com/ # ! 2/4c0a5/18 ) ) . This works two groups , two groups , would need specify ` group ` number add queries ` group ` : ( select * mytable ` group ` = 1 order age desc LIMIT 2 ) UNION ALL ( select * mytable ` group ` = 2 order age desc LIMIT 2 ) There variety ways , see article determine best route situation : < http : //www.xaprb.com/blog/2006/12/07/how-to-select-the-firstleastmax-row-per- group-in-sql/ > Edit : This might work , generates row number record . Using example link return records row number less equal 2 : select person , ` group ` , age ( select person , ` group ` , age , ( @ num : =if ( @ group = ` group ` , @ num +1 , ( @ group : = ` group ` , 1 , 1 ) ) ) row_number test CROSS JOIN ( select @ num : =0 , @ group : =null ) c order ` Group ` , Age desc , person ) x x.row_number < = 2 ; See [ Demo ] ( http : //rextester.com/UPG27258 )"
Stackoverflow,"Is way `` limit '' result ELOQUENT ORM Laravel ? SELECT * FROM ` games ` LIMIT 30 , 30 And Eloquent ?",Create Game model extends Eloquent use : Game : :take ( 30 ) - > skip ( 30 ) - > get ( ) ; ` take ( ) ` get 30 records ` skip ( ) ` offset 30 records . * * * In recent Laravel versions also use : Game : :limit ( 30 ) - > offset ( 30 ) - > get ( ) ;
Stackoverflow,"I two tables , ` table1 ` parent table column ` ID ` ` table2 ` column ` IDFromTable1 ` ( actual name ) I put FK ` IDFromTable1 ` ` ID ` ` table1 ` I get error ` Foreign key constraint incorrectly formed error ` . I would like delete table 2 record ` table1 ` record gets deleted . Thanks help ALTER TABLE ` table2 ` ADD CONSTRAINT ` FK1 ` FOREIGN KEY ( ` IDFromTable1 ` ) REFERENCES ` table1 ` ( ` ID ` ) ON UPDATE CASCADE ON DELETE CASCADE ; Let know information needed . I new mysql","I ran problem HeidiSQL . The error receive cryptic . My problem ended foreign key column referencing column type length . The foreign key column ` SMALLINT ( 5 ) UNSIGNED ` referenced column ` INT ( 10 ) UNSIGNED ` . Once I made exact type , foreign key creation worked perfectly ."
Stackoverflow,"I need dump **.sql** **.csv** file SQLite ( I 'm using SQLite3 API ) . I 've found documentation importing/loading tables , entire databases . Right , I type : sqlite3prompt > .import FILENAME TABLE I get syntax error , since 's expecting table entire DB .",To import SQL file use following : sqlite > .read < filename > To import CSV file need specify file type destination table : sqlite > .mode csv < table > sqlite > .import < filename > < table >
Stackoverflow,"I quite easily dump data text file : sqlcmd -S myServer -d myDB -E -Q `` select col1 , col2 , col3 SomeTable '' -o `` MyData.txt '' However , I looked help files ` SQLCMD ` seen option specifically CSV . Is way dump data table CSV text file using ` SQLCMD ` ?","You run something like : sqlcmd -S MyServer -d myDB -E -Q `` select col1 , col2 , col3 SomeTable '' -o `` MyData.csv '' -h-1 -s '' , '' -w 700 * ` -h-1 ` removes column name headers result * ` -s '' , '' ` sets column seperator , * ` -w 700 ` sets row width 700 chars ( need wide longest row wrap next line )"
Stackoverflow,"I got following error MySQL query . ` # 126 - Incorrect key file table ` I even declared key table , I indices . Does anyone know could problem ?","Every Time happened , 's full disk experience . **EDIT** It also worth noting caused full ramdisk things like altering large table ramdisk configured . You temporarily comment ramdisk line allow operations can't increase size ."
Stackoverflow,"I table tags want get highest count tags list . Sample data looks like id ( 1 ) tag ( 'night ' ) id ( 2 ) tag ( 'awesome ' ) id ( 3 ) tag ( 'night ' ) using SELECT COUNT ( * ) , ` Tag ` ` images-tags ` GROUP BY ` Tag ` gets back data I 'm looking perfectly . However , I would like organize , highest tag counts first , limit send first 20 . I tried ... SELECT COUNT ( id ) , ` Tag ` ` images-tags ` GROUP BY ` Tag ` ORDER BY COUNT ( id ) DESC LIMIT 20 I keep getting `` Invalid use group function - ErrNr 1111 '' What I wrong ? I 'm using MySQL 4.1.25-Debian","In versions MySQL , simply alias aggregate SELECT list , order alias : SELECT COUNT ( id ) AS theCount , ` Tag ` ` images-tags ` GROUP BY ` Tag ` ORDER BY theCount DESC LIMIT 20"
Stackoverflow,"I 'm getting strange error processing large number data ... Error Number : 1267 Illegal mix collations ( latin1_swedish_ci , IMPLICIT ) ( utf8_general_ci , COERCIBLE ) operation '=' SELECT COUNT ( * ) num keywords WHERE campaignId='12 ' AND LCASE ( keyword ) ='hello æ˜ ” ã‹ã‚‰ ã‚ã‚‹ å ´æ‰€' What I resolve ? Can I escape string somehow error would n't occur , I need change table encoding somehow , , I change ?",SET collation_connection = 'utf8_general_ci ' ; databases ALTER DATABASE your_database_name CHARACTER SET utf8 COLLATE utf8_general_ci ; ALTER TABLE your_table_name CONVERT TO CHARACTER SET utf8 COLLATE utf8_general_ci ; MySQL sneaks swedish sometimes sensible reason .
Stackoverflow,"I come pandas background used reading data CSV files dataframe simply changing column names something useful using simple command : df.columns = new_column_name_list However , n't work pyspark dataframes created using sqlContext . The solution I could figure easily following : df = sqlContext.read.format ( `` com.databricks.spark.csv '' ) .options ( header='false ' , inferschema='true ' , delimiter='\t ' ) .load ( `` data.txt '' ) oldSchema = df.schema , k enumerate ( oldSchema.fields ) : k.name = new_column_name_list [ ] df = sqlContext.read.format ( `` com.databricks.spark.csv '' ) .options ( header='false ' , delimiter='\t ' ) .load ( `` data.txt '' , schema=oldSchema ) This basically defining variable twice inferring schema first renaming column names loading dataframe updated schema . Is better efficient way like pandas ? My spark version 1.5.0","There many ways : * Option 1 . Using [ selectExpr ] ( https : //spark.apache.org/docs/latest/api/python/pyspark.sql.html ? highlight=selectexpr # pyspark.sql.DataFrame.selectExpr ) . data = sqlContext.createDataFrame ( [ ( `` Alberto '' , 2 ) , ( `` Dakota '' , 2 ) ] , [ `` Name '' , `` askdaosdka '' ] ) data.show ( ) data.printSchema ( ) # Output # + -- -- -- -+ -- -- -- -- -- + # | Name|askdaosdka| # + -- -- -- -+ -- -- -- -- -- + # |Alberto| 2| # | Dakota| 2| # + -- -- -- -+ -- -- -- -- -- + # root # | -- Name : string ( nullable = true ) # | -- askdaosdka : long ( nullable = true ) df = data.selectExpr ( `` Name name '' , `` askdaosdka age '' ) df.show ( ) df.printSchema ( ) # Output # + -- -- -- -+ -- -+ # | name|age| # + -- -- -- -+ -- -+ # |Alberto| 2| # | Dakota| 2| # + -- -- -- -+ -- -+ # root # | -- name : string ( nullable = true ) # | -- age : long ( nullable = true ) * Option 2 . Using [ withColumnRenamed ] ( https : //spark.apache.org/docs/latest/api/python/pyspark.sql.html ? highlight=selectexpr # pyspark.sql.DataFrame.withColumnRenamed ) , notice method allows `` overwrite '' column . oldColumns = data.schema.names newColumns = [ `` name '' , `` age '' ] df = reduce ( lambda data , idx : data.withColumnRenamed ( oldColumns [ idx ] , newColumns [ idx ] ) , xrange ( len ( oldColumns ) ) , data ) df.printSchema ( ) df.show ( ) * Option 3. using [ alias ] ( https : //spark.apache.org/docs/latest/api/python/pyspark.sql.html ? highlight=column # pyspark.sql.Column.alias ) , Scala also use [ ] ( http : //spark.apache.org/docs/latest/api/scala/index.html # org.apache.spark.sql.Column ) . pyspark.sql.functions import * data = data.select ( col ( `` Name '' ) .alias ( `` name '' ) , col ( `` askdaosdka '' ) .alias ( `` age '' ) ) data.show ( ) # Output # + -- -- -- -+ -- -+ # | name|age| # + -- -- -- -+ -- -+ # |Alberto| 2| # | Dakota| 2| # + -- -- -- -+ -- -+ * Option 4 . Using [ sqlContext.sql ] ( http : //spark.apache.org/docs/latest/api/python/pyspark.sql.html # pyspark.sql.SQLContext.sql ) , lets use SQL queries ` DataFrames ` registered tables . sqlContext.registerDataFrameAsTable ( data , `` myTable '' ) df2 = sqlContext.sql ( `` SELECT Name AS name , askdaosdka age myTable '' ) df2.show ( ) # Output # + -- -- -- -+ -- -+ # | name|age| # + -- -- -- -+ -- -+ # |Alberto| 2| # | Dakota| 2| # + -- -- -- -+ -- -+"
Stackoverflow,"I following UPSERT PostgreSQL 9.5 : INSERT INTO chats ( `` user '' , `` contact '' , `` name '' ) VALUES ( $ 1 , $ 2 , $ 3 ) , ( $ 2 , $ 1 , NULL ) ON CONFLICT ( `` user '' , `` contact '' ) DO NOTHING RETURNING id ; If conflicts returns something like : -- -- -- -- -- | id | -- -- -- -- -- 1 | 50 | -- -- -- -- -- 2 | 51 | -- -- -- -- -- But conflicts n't return rows : -- -- -- -- -- | id | -- -- -- -- -- I want return new ` id ` columns conflicts return existing ` id ` columns conflicting columns . **Can done ? ** If , **how ? **","The [ currently accepted answer ] ( https : //stackoverflow.com/a/37543015/939860 ) seems ok _few_ conflicts , small tuples triggers . And avoids **_concurrency issue 1_** brute force ( see ) . The simple solution appeal , side effects may less important . For cases , though , **_not_** update identical rows without need . Even see difference surface , **various side effects** : * It might fire triggers fired . * It write-locks `` innocent '' rows , possibly incurring costs concurrent transactions . * It might make row seem new , though 's old ( transaction timestamp ) . * **Most importantly** , [ PostgreSQL 's MVCC model ] ( https : //www.postgresql.org/docs/current/static/mvcc-intro.html ) new row version written either way , matter whether row data . This incurs performance penalty UPSERT , table bloat , index bloat , performance penalty subsequent operations table , ` VACUUM ` cost . A minor effect duplicates , _massive_ mostly dupes . You achieve ( almost ) without empty updates side effects . # # Without concurrent write load WITH input_rows ( usr , contact , name ) AS ( VALUES ( text 'foo1 ' , text 'bar1 ' , text 'bob1 ' ) -- type casts first row , ( 'foo2 ' , 'bar2 ' , 'bob2 ' ) -- ? ) , ins AS ( INSERT INTO chats ( usr , contact , name ) SELECT * FROM input_rows ON CONFLICT ( usr , contact ) DO NOTHING RETURNING id -- , usr , contact -- return columns ? ) SELECT ' ' AS source -- ' ' 'inserted' , id -- , usr , contact -- return columns ? FROM ins UNION ALL SELECT 's ' AS source -- 's ' 'selected' , c.id -- , usr , contact -- return columns ? FROM input_rows JOIN chats c USING ( usr , contact ) ; -- columns unique index The ` source ` column optional addition demonstrate works . You may actually need tell difference cases ( another advantage empty writes ) . The final ` JOIN chats ` works newly inserted rows attached [ data-modifying CTE ] ( https : //www.postgresql.org/docs/current/static/queries- with.html ) yet visible underlying table . ( All parts SQL statement see snapshots underlying tables . ) Since ` VALUES ` expression free-standing ( directly attached ` INSERT ` ) Postgres derive data types target columns may add explicit type casts . [ The manual : ] ( https : //www.postgresql.org/docs/current/static/sql-values.html ) > When ` VALUES ` used ` INSERT ` , values automatically coerced data type corresponding destination column . When 's used contexts , might necessary specify correct data type . If entries quoted literal constants , coercing first sufficient determine assumed type . The query may bit expensive _few_ dupes , due overhead CTE additional ` SELECT ` ( cheap since perfect index definition - unique constraint implemented index ) . May ( much ) faster _many_ duplicates . The effective cost additional writes depends many factors . But **fewer side effects hidden costs** case . It 's probably cheaper overall . ( Attached sequences still advanced , since default values filled _before_ testing conflicts . ) About CTEs : * [ Are SELECT type queries type nested ? ] ( https : //stackoverflow.com/questions/22749253/are-select-type-queries-the-only-type-that-can-be-nested/22750328 # 22750328 ) * [ Deduplicate SELECT statements relational division ] ( https : //dba.stackexchange.com/questions/111345/deduplicate-select-statements-in-relational-division/111362 # 111362 ) # # With concurrent write load Assuming default ` READ COMMITTED ` transaction isolation . Related answer dba.SE detailed explanation : * [ Concurrent transactions result race condition unique constraint insert ] ( https : //dba.stackexchange.com/questions/212580/postgres-concurrent-transactions-resulting-in-race-condition-unique-foreign-key ) The best strategy defend race conditions depends exact requirements , number size rows table UPSERTs , number concurrent transactions , likelihood conflicts , available resources factors ... # # # Concurrency issue 1 If concurrent transaction written row transaction tries UPSERT , transaction wait one finish . If transaction ends ` ROLLBACK ` ( error , i.e . automatic ` ROLLBACK ` ) , transaction proceed normally . Minor side effect : gaps sequential numbers . But missing rows . If transaction ends normally ( implicit explicit ` COMMIT ` ) , ` INSERT ` detect conflict ( ` UNIQUE ` index / constraint absolute ) ` DO NOTHING ` , hence also return row . ( Also lock row demonstrated _concurrency issue 2_ , since 's _not visible_ . ) The ` SELECT ` sees snapshot start query also return yet invisible row . **_Any rows missing result set ( even though exist underlying table ) ! _** This **may ok is** . Especially returning rows like example satisfied knowing row . If 's good enough , various ways around . You could check row count output repeat statement match row count input . May good enough rare case . The point start new query ( transaction ) , see newly committed rows . **Or** check missing result rows _within_ query _overwrite_ brute force trick demonstrated [ Alextoni's answer ] ( https : //stackoverflow.com/a/37543015/939860 ) . WITH input_rows ( usr , contact , name ) AS ( ... ) -- see , ins AS ( INSERT INTO chats AS c ( usr , contact , name ) SELECT * FROM input_rows ON CONFLICT ( usr , contact ) DO NOTHING RETURNING id , usr , contact -- need unique columns later join ) , sel AS ( SELECT ' ' : : '' char '' AS source -- ' ' 'inserted' , id , usr , contact FROM ins UNION ALL SELECT 's ' : : '' char '' AS source -- 's ' 'selected' , c.id , usr , contact FROM input_rows JOIN chats c USING ( usr , contact ) ) , ups AS ( -- RARE corner case INSERT INTO chats AS c ( usr , contact , name ) -- another UPSERT , UPDATE SELECT . * FROM input_rows LEFT JOIN sel USING ( usr , contact ) -- columns unique index WHERE s.usr IS NULL -- missing ! ON CONFLICT ( usr , contact ) DO UPDATE -- 've asked nicely 1st time ... SET name = c.name -- ... time overwrite old value -- SET name = EXCLUDED.name -- alternatively overwrite *new* value RETURNING ' u ' : : '' char '' AS source -- ' u ' updated , id -- , usr , contact -- return columns ? ) SELECT source , id FROM sel UNION ALL TABLE ups ; It 's like query , add one step CTE ` ups ` , return **_complete_** result set . That last CTE nothing time . Only rows go missing returned result , use brute force . More overhead , yet . The conflicts pre-existing rows , likely outperform simple approach . One side effect : 2nd UPSERT writes rows order , re-introduces possibility deadlocks ( see ) _three more_ transactions writing rows overlap . If 's problem , need different solution . # # # Concurrency issue 2 If concurrent transactions write involved columns affected rows , make sure rows found still later stage transaction , **lock rows** cheaply : ... ON CONFLICT ( usr , contact ) DO UPDATE SET name = name WHERE FALSE -- never executed , still locks row ... And add [ locking clause ` SELECT ` well , like ` FOR UPDATE ` ] ( https : //www.postgresql.org/docs/current/static/sql-select.html # SQL- FOR-UPDATE-SHARE ) . This makes competing write operations wait till end transaction , locks released . So brief . More details explanation : * [ How include excluded rows RETURNING INSERT ... ON CONFLICT ] ( https : //stackoverflow.com/questions/35949877/how-to-include-excluded-rows-in-returning-from-insert-on-conflict/35953488 # 35953488 ) * [ Is SELECT INSERT function prone race conditions ? ] ( https : //stackoverflow.com/questions/15939902/is-select-or-insert-in-a-function-prone-to-race-conditions/15950324 # 15950324 ) # # # Deadlocks ? Defend **deadlocks** inserting rows **consistent order** . See : * [ Deadlock multi-row INSERTs despite ON CONFLICT DO NOTHING ] ( https : //dba.stackexchange.com/questions/194756/deadlock-with-multi-row-inserts-despite-on-conflict-do-nothing/195220 # 195220 ) # # Data types casts # # # Existing table template data types ... Explicit type casts first row data free-standing ` VALUES ` expression may inconvenient . There ways around . You use existing relation ( table , view , ... ) row template . The target table obvious choice use case . Input data coerced appropriate types automatically , like ` VALUES ` clause ` INSERT ` : WITH input_rows AS ( ( SELECT usr , contact , name FROM chats LIMIT 0 ) -- copies column names types UNION ALL VALUES ( 'foo1 ' , 'bar1 ' , 'bob1 ' ) -- type casts needed , ( 'foo2 ' , 'bar2 ' , 'bob2 ' ) ) ... This work data types ( explanation linked answer bottom ) . The next trick works _all_ data types : # # # ... names If insert whole rows ( columns table - least set _leading_ columns ) , omit column names , . Assuming table ` chats ` example 3 columns used : WITH input_rows AS ( SELECT * FROM ( VALUES ( ( NULL : :chats ) . * ) -- copies whole row definition ( 'foo1 ' , 'bar1 ' , 'bob1 ' ) -- type casts needed , ( 'foo2 ' , 'bar2 ' , 'bob2 ' ) ) sub OFFSET 1 ) ... Detailed explanation alternatives : * [ Casting NULL type updating multiple rows ] ( https : //stackoverflow.com/questions/12426363/casting-null-type-when-updating-multiple-rows/12427434 # 12427434 ) * * * Aside : n't use reserved words like ` `` user '' ` identifier . That 's loaded footgun . Use legal , lower-case , unquoted identifiers . I replaced ` usr ` ."
Stackoverflow,"When I right click indexes folder table `` New Index '' menu item grayed . I n't understand . I 've deleted data table case , refreshed restarted SSMS , luck . I 'm using SQL Server 2012 Business Intelligence SP1 CTP .","**Solution : ** Close table designers database diagrams try . If n't help , close windows Management Studio . **Cause : ** The `` New Index '' option gets disabled table schema- locked ."
Stackoverflow,"At every company I worked , I found people still writing SQL queries ANSI-89 standard : select a.id , b.id , b.address_1 person , address b a.id = b.id rather ANSI-92 standard : select a.id , b.id , b.address_1 person inner join address b a.id = b.id For extremely simple query like , 's big difference readability , large queries I find join criteria grouped listing table makes much easier see I might issues join , let 's keep filtering WHERE clause . Not mention I feel outer joins much intuitive ( + ) syntax Oracle . As I try evangelize ANSI-92 people , concrete performance benefits using ANSI-92 ANSI-89 ? I would try , Oracle setups n't allow us use EXPLAIN PLAN - would n't want people try optimize code , would ya ?","According `` SQL Performance Tuning '' Peter Gulutzan Trudy Pelzer , six eight RDBMS brands tested , difference optimization performance SQL-89 versus SQL-92 style joins . One assume RDBMS engines transform syntax internal representation optimizing executing query , human-readable syntax makes difference . I also try evangelize SQL-92 syntax . Sixteen years approved , 's time people start using ! And brands SQL database support , 's reason continue use abhorrent ` ( + ) ` Oracle syntax ` *= ` Microsoft/Sybase syntax . As 's hard break developer community SQL-89 habit , I assume 's large `` base pyramid '' programmers code copy & paste , using ancient examples books , magazine articles , another code base , people n't learn new syntax abstractly . Some people pattern-match , people learn rote . I gradually seeing people using SQL-92 syntax frequently I used , though . I 've answering SQL questions online since 1994 ."
Stackoverflow,I searched solution problem internet checked SO questions solution worked case . I want create foreign key table sira_no metal_kod . ALTER TABLE sira_no ADD CONSTRAINT METAL_KODU FOREIGN KEY ( METAL_KODU ) REFERENCES metal_kod ( METAL_KODU ) ON DELETE SET NULL ON UPDATE SET NULL ; This script returns : Error Code : 1005 . Ca n't create table 'ebs. # sql-f48_1a3 ' ( errno : 150 ) I tried adding index referenced table : CREATE INDEX METAL_KODU_INDEX ON metal_kod ( METAL_KODU ) ; I checked METAL_KODU tables ( charset collation ) . But could n't find solution problem . Does anyone idea ? Thanks advance . EDIT : Here metal_kod table : METAL_KODU varchar ( 4 ) NO PRI DURUM bit ( 1 ) NO METAL_ISMI varchar ( 30 ) NO AYAR_YOGUNLUK smallint ( 6 ) YES 100,"Error Code : 1005 -- wrong primary key reference code usually 's due reference FK field exist . might typo mistake , check case , 's field-type mismatch . FK- linked fields must match definitions exactly . Some Known causes may : 1 . The two key fields type and/or size ’ match exactly . For example , one ` INT ( 10 ) ` key field needs ` INT ( 10 ) ` well ` INT ( 11 ) ` ` TINYINT ` . You may want confirm field size using ` SHOW ` ` CREATE ` ` TABLE ` Query Browser sometimes visually show ` INTEGER ` ` INT ( 10 ) ` ` INT ( 11 ) ` . You also check one ` SIGNED ` ` UNSIGNED ` . They need exactly . 2 . One key field trying reference index and/or primary key . If one fields relationship primary key , must create index field . 3 . The foreign key name duplicate already existing key . Check name foreign key unique within database . Just add random characters end key name test . 4 . One tables ` MyISAM ` table . In order use foreign keys , tables must ` InnoDB ` . ( Actually , tables ` MyISAM ` ’ get error message - ’ create key . ) In Query Browser , specify table type . 5 . You specified cascade ` ON ` ` DELETE ` ` SET ` ` NULL ` , relevant key field set ` NOT ` ` NULL ` . You fix either changing cascade setting field allow ` NULL ` values . 6 . Make sure Charset Collate options table level well individual field level key columns . 7 . You default value ( ie default=0 ) foreign key column 8 . One fields relationship part combination ( composite ) key ’ individual index . Even though field index part composite key , must create separate index key field order use constraint . 9 . You syntax error ` ALTER ` statement mistyped one field names relationship 10 . The name foreign key exceeds max length 64 chars . details refer : [ MySQL Error Number 1005 Can ’ create table ] ( http : //verysimple.com/2006/10/22/mysql-error-number-1005-cant-create- table-mydbsql-328_45frm-errno-150/ )"
Stackoverflow,"Anyone knows good SQL builder library Java like [ Squiggle ] ( http : //code.google.com/p/squiggle-sql/ ) ( maintained anymore seems ) . Preferably , project active development . Preferably syntax like [ Zend_Db_Select ] ( http : //framework.zend.com/manual/en/zend.db.select.html ) , something allow make query like String query = db.select ( ) .from ( 'products ' ) .order ( 'product_id ' ) ;",[ Querydsl ] ( http : //www.querydsl.com/ ) [ jOOQ ] ( http : //www.jooq.org ) two popular choices .
Stackoverflow,"I trouble loading Django fixtures MySQL database contenttypes conflicts . First I tried dumping data app like : ./manage.py dumpdata escola > fixture.json I kept getting missing foreign key problems , app `` escola '' uses tables applications . I kept adding additional apps I got : ./manage.py dumpdata contenttypes auth escola > fixture.json Now problem following constraint violation I try load data test fixture : IntegrityError : ( 1062 , `` Duplicate entry 'escola-t23aluno ' key 2 '' ) It seems problem Django trying dynamically recreate contenttypes different primary key values conflict primary key values fixture . This appears bug documented : < http : //code.djangoproject.com/ticket/7052 > The problem recommended workaround dump contenttypes app I 'm already ! ? What gives ? If makes difference I custom model permissions documented : < http : //docs.djangoproject.com/en/dev/ref/models/options/ # permissions >",` manage.py dumpdata -- natural ` use durable representation foreign keys . In django called `` natural keys '' . For example : * ` Permission.codename ` used favour ` Permission.id ` * ` User.username ` used favour ` User.id ` Read : [ natural keys section `` serializing django objects '' ] ( http : //docs.djangoproject.com/en/dev/topics/serialization/ # natural- keys ) Some useful arguments ` dumpdata ` : * ` -- indent=4 ` make human readable . * ` -e sessions ` exclude session data * ` -e admin ` exclude history admin actions admin site * ` -e contenttypes -e auth.Permission ` exclude objects recreated automatically schema every time ` syncdb ` . Only use together ` -- natural ` else might end badly aligned id numbers .
Stackoverflow,"Whenever I try drop database I get : ERROR : database `` pilot '' accessed users DETAIL : There 1 session using database . When I use : SELECT pg_terminate_backend ( pg_stat_activity.pid ) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'TARGET_DB ' ; I terminated connection DB , I try drop database somehow someone automatically connects database gives error . What could ? No one uses database , except .","You prevent future connections : REVOKE CONNECT ON DATABASE thedb FROM public ; ( possibly users/roles ; see ` \l+ ` ` psql ` ) You terminate connections db except : SELECT pid , pg_terminate_backend ( pid ) FROM pg_stat_activity WHERE datname = current_database ( ) AND pid < > pg_backend_pid ( ) ; On older versions ` pid ` called ` procpid ` 'll deal . Since 've revoked ` CONNECT ` rights , whatever trying auto-connect longer able . You 'll able drop DB . This wo n't work 're using superuser connections normal operations , 're need fix problem first ."
Stackoverflow,"I new Docker , trying go tutorial setting MemSQL Docker image - < http : //docs.memsql.com/4.0/setup/docker/ > . I Mac , tutorial uses ` boot2docker ` seems deprecated . The VM needs 4GB memory run . The tutorial specifies ` boot2docker ` I find way docker- machine/docker toolbox . Here command I using error I getting trying go tutorial without altering boot2docker config . docker run -- rm -- net=host memsql/quickstart check-system Error : MemSQL requires least 4 GB memory run .","You via command line . For example , change machine default 1cpu/2048MB RAM run : docker-machine stop VBoxManage modifyvm default -- cpus 2 VBoxManage modifyvm default -- memory 4096 docker-machine start"
Stackoverflow,"I need know make SQL query run daily using SQL Server Agent job , minimum required configuration settings .",1 . Expand SQL Server Agent node right click Jobs node SQL Server Agent select ` 'New Job ' ` 2 . In ` 'New Job ' ` window enter name job description ` 'General ' ` tab . 3 . Select ` 'Steps ' ` left hand side window click ` 'New ' ` bottom . 4 . In ` 'Steps ' ` window enter step name select database want query run . 5 . Paste T-SQL command want run Command window click ` 'OK ' ` . 6 . Click ` 'Schedule ' ` menu left New Job window enter schedule information ( e.g . daily time ) . 7 . Click ` 'OK ' ` \- . ( There course options add - I would say bare minimum need get job set scheduled )
Stackoverflow,I use msyql.data 8.08 .net core connect mysql5.7.18 following exception thrown : ` MySql.Data.MySqlClient.MySqlException : “ The host localhost support SSL connections. ” ` How deal ?,I problem today moving MySql.Data 7.0.7 8.0.8 . I able move forward adding `` SslMode=none '' connection string . You endup something like : server= { 0 } ; user id= { 1 } ; password= { 2 } ; persistsecurityinfo=True ; port= { 3 } ; database= { 4 } ; SslMode=none ( replacing values database details )
Stackoverflow,"I large data table . There 10 million records table . What best way query Delete LargeTable readTime < dateadd ( MONTH , -7 , GETDATE ( ) )","1 . If Deleting All rows table simplest option Truncate table , something like TRUNCATE TABLE LargeTable GO Truncate table simply empty table , use WHERE clause limit rows deleted triggers fired . 2 . On hand deleting 80-90 Percent data , say total 11 Million rows want delete 10 million another way would Insert 1 million rows ( records want keep ) another staging table . Truncate Large table Insert back 1 Million rows . 3 . Or permissions/views objects large table underlying table doesnt get affected dropping table get relatively small amount rows another table drop table create another table schema import rows back ex-Large table . 4 . One last option I think change database 's ` Recovery Mode SIMPLE ` delete rows smaller batches using loop something like this.. DECLARE @ Deleted_Rows INT ; SET @ Deleted_Rows = 1 ; WHILE ( @ Deleted_Rows > 0 ) BEGIN -- Delete small number rows time DELETE TOP ( 10000 ) LargeTable WHERE readTime < dateadd ( MONTH , -7 , GETDATE ( ) ) SET @ Deleted_Rows = @ @ ROWCOUNT ; END dont forget change Recovery mode back full I think take backup make fully affective ( change recovery modes ) ."
Stackoverflow,"Assuming I tables ` student ` , ` club ` , ` student_club ` : student { id name } club { id name } student_club { student_id club_id } I want know find students soccer ( 30 ) baseball ( 50 ) club . While query n't work , 's closest thing I far : SELECT student . * FROM student INNER JOIN student_club sc ON student.id = sc.student_id LEFT JOIN club c ON c.id = sc.club_id WHERE c.id = 30 AND c.id = 50","I curious . And know , curiosity reputation killing cats . # # So , fastest way skin cat ? The precise cat-skinning environment test : * **PostgreSQL 9.0** Debian Squeeze decent RAM settings . * 6.000 students , 24.000 club memberships ( data copied similar database real life data . ) * Slight diversion naming schema question : ` student.id ` ` student.stud_id ` ` club.id ` ` club.club_id ` . * I named queries author thread , index two . * I ran queries couple times populate cache , I picked best 5 EXPLAIN ANALYZE . * Relevant indexes ( optimum - long lack fore-knowledge clubs queried ) : ALTER TABLE student ADD CONSTRAINT student_pkey PRIMARY KEY ( stud_id ) ; ALTER TABLE student_club ADD CONSTRAINT sc_pkey PRIMARY KEY ( stud_id , club_id ) ; ALTER TABLE club ADD CONSTRAINT club_pkey PRIMARY KEY ( club_id ) ; CREATE INDEX sc_club_id_idx ON student_club ( club_id ) ; ` club_pkey ` required queries . Primary keys implement unique indexes automatically In PostgreSQL . The last index make known shortcoming [ multi-column indexes ] ( http : //www.postgresql.org/docs/current/interactive/indexes- multicolumn.html ) PostgreSQL : > A multicolumn B-tree index used query conditions involve subset index 's columns , index efficient constraints leading ( leftmost ) columns . # Results : Total runtimes EXPLAIN ANALYZE . # # # 1 ) Martin 2 : 44.594 ms SELECT s.stud_id , s.name FROM student JOIN student_club sc USING ( stud_id ) WHERE sc.club_id IN ( 30 , 50 ) GROUP BY 1,2 HAVING COUNT ( * ) > 1 ; * * * # # # 2 ) Erwin 1 : 33.217 ms SELECT s.stud_id , s.name FROM student JOIN ( SELECT stud_id FROM student_club WHERE club_id IN ( 30 , 50 ) GROUP BY 1 HAVING COUNT ( * ) > 1 ) sc USING ( stud_id ) ; * * * # # # 3 ) Martin 1 : 31.735 ms SELECT s.stud_id , s.name FROM student WHERE student_id IN ( SELECT student_id FROM student_club WHERE club_id = 30 INTERSECT SELECT stud_id FROM student_club WHERE club_id = 50 ) ; * * * # # # 4 ) Derek : 2.287 ms SELECT s.stud_id , s.name FROM student WHERE s.stud_id IN ( SELECT stud_id FROM student_club WHERE club_id = 30 ) AND s.stud_id IN ( SELECT stud_id FROM student_club WHERE club_id = 50 ) ; * * * # # # 5 ) Erwin 2 : 2.181 ms SELECT s.stud_id , s.name FROM student WHERE EXISTS ( SELECT * FROM student_club WHERE stud_id = s.stud_id AND club_id = 30 ) AND EXISTS ( SELECT * FROM student_club WHERE stud_id = s.stud_id AND club_id = 50 ) ; * * * # # # 6 ) Sean : 2.043 ms SELECT s.stud_id , s.name FROM student JOIN student_club x ON s.stud_id = x.stud_id JOIN student_club ON s.stud_id = y.stud_id WHERE x.club_id = 30 AND y.club_id = 50 ; The last three perform pretty much . 4 ) 5 ) result query plan . # # Late Additions : Fancy SQL , performance ca n't keep . # # # 7 ) ypercube 1 : 148.649 ms SELECT s.stud_id , s.name FROM student AS WHERE NOT EXISTS ( SELECT * FROM club AS c WHERE c.club_id IN ( 30 , 50 ) AND NOT EXISTS ( SELECT * FROM student_club AS sc WHERE sc.stud_id = s.stud_id AND sc.club_id = c.club_id ) ) ; * * * # # # 8 ) ypercube 2 : 147.497 ms SELECT s.stud_id , s.name FROM student AS WHERE NOT EXISTS ( SELECT * FROM ( SELECT 30 AS club_id UNION ALL SELECT 50 ) AS c WHERE NOT EXISTS ( SELECT * FROM student_club AS sc WHERE sc.stud_id = s.stud_id AND sc.club_id = c.club_id ) ) ; As expected , two perform almost . Query plan results table scans , planner n't find way use indexes . * * * # # # 9 ) wildplasser 1 : 49.849 ms WITH RECURSIVE two AS ( SELECT 1 : :int AS level , stud_id FROM student_club sc1 WHERE sc1.club_id = 30 UNION SELECT two.level + 1 AS level , sc2.stud_id FROM student_club sc2 JOIN two USING ( stud_id ) WHERE sc2.club_id = 50 AND two.level = 1 ) SELECT s.stud_id , s.student FROM student JOIN two USING ( studid ) WHERE two.level > 1 ; Fancy SQL , decent performance CTE . Very exotic query plan . Again , would interesting 9.1 handles . I going upgrade db cluster used 9.1 soon . Maybe I 'll rerun whole shebang ... * * * # # # 10 ) wildplasser 2 : 36.986 ms WITH sc AS ( SELECT stud_id FROM student_club WHERE club_id IN ( 30,50 ) GROUP BY stud_id HAVING COUNT ( * ) > 1 ) SELECT s.* FROM student JOIN sc USING ( stud_id ) ; CTE variant query 2 ) . Surprisingly , result slightly different query plan exact data . I found sequential scan ` student ` , subquery-variant used index . * * * # # # 11 ) ypercube 3 : 101.482 ms Another late addition @ ypercube . It positively amazing , many ways . SELECT s.stud_id , s.student FROM student JOIN student_club sc USING ( stud_id ) WHERE sc.club_id = 10 -- member 1st club ... AND NOT EXISTS ( SELECT * FROM ( SELECT 14 AS club_id ) AS c -- ca n't excluded missing 2nd WHERE NOT EXISTS ( SELECT * FROM student_club AS WHERE d.stud_id = sc.stud_id AND d.club_id = c.club_id ) ) * * * # # # 12 ) erwin 3 : 2.377 ms @ ypercube 's 11 ) actually mind-twisting reverse approach simpler variant , also still missing . Performs almost fast top cats . SELECT s.* FROM student JOIN student_club x USING ( stud_id ) WHERE sc.club_id = 10 -- member 1st club ... AND EXISTS ( -- ... membership 2nd exists SELECT * FROM student_club AS WHERE y.stud_id = s.stud_id AND y.club_id = 14 ) # # # 13 ) erwin 4 : 2.375 ms Hard believe , 's another , genuinely new variant . I see potential two memberships , also ranks among top cats two . SELECT s.* FROM student AS WHERE EXISTS ( SELECT * FROM student_club AS x JOIN student_club AS USING ( stud_id ) WHERE x.stud_id = s.stud_id AND x.club_id = 14 AND y.club_id = 10 ) # # Dynamic number club memberships In words : varying number filters . This question asked exactly **_two_** club memberships . But many use cases prepare varying number . Detailed discussion related later answer : * [ Using column multiple times WHERE clause ] ( https : //stackoverflow.com/questions/47351766/using-same-column-multiple-times-in-where-clause/47512013 # 47512013 )"
Stackoverflow,"I 'm new SqlServer , right I ` SqlLocalDb ` installed work locally . Good , I see two connection strings typically works : Data Source= ( localdb ) \v11.0 ; Integrated Security=true ; Server= ( localdb ) \v11.0 ; Integrated Security=true ; What exact difference two ?","For full list connection string keywords , including entirely synonymous , please refer [ ` SqlConnection.ConnectionString ` documentation ] ( http : //msdn.microsoft.com/en- gb/library/system.data.sqlclient.sqlconnection.connectionstring.aspx ) : These entirely equivalent : > * Data Source > * Server > * Address > * Addr > * Network Address >"
Stackoverflow,"I 'm running data import ( using C # /Linq ) , naturally I 'm trying optimize queries much possible . To end I 'm running trace DB using SQL Server Profiler , trace filtered SQL login name ( 's name uniquely attributed data import process ) . Strangely enough , SQL statements really quick : ) - queries even break 1ms mark . But spaced queries several rows EventClass `` Audit Login '' `` Audit Logout '' - duration `` Audit Logout '' minute ! Has got something fact I 'm using transactions import ? If , way find big-hitting queries I clean ?","If I remember correct , duration Audit Logout amount time connection open . E.g . nothing speed command - amount time login 'logged ' ."
Stackoverflow,"Looking documentation Postgres 9.4 datatype JSONB , immediately obvious updates JSONB columns . Documentation JSONB types functions : < http : //www.postgresql.org/docs/9.4/static/functions-json.html > < http : //www.postgresql.org/docs/9.4/static/datatype-json.html > As examples , I basic table structure : CREATE TABLE test ( id serial , data jsonb ) ; Inserting easy , : INSERT INTO test ( data ) values ( ' { `` name '' : `` my-name '' , `` tags '' : [ `` tag1 '' , `` tag2 '' ] } ' ) ; Now , would I update 'data ' column ? This invalid syntax : UPDATE test SET data- > 'name ' = 'my-other-name ' WHERE id = 1 ; Is documented somewhere obvious I missed ? Thanks .","If 're able upgrade Postgresql 9.5 , ` jsonb_set ` command available , others mentioned . In following SQL statements , I 've omitted ` ` clause brevity ; obviously , 'd want add back . Update name : UPDATE test SET data = jsonb_set ( data , ' { name } ' , ' '' my-other-name '' ' ) ; Replace tags ( oppose adding removing tags ) : UPDATE test SET data = jsonb_set ( data , ' { tags } ' , ' [ `` tag3 '' , `` tag4 '' ] ' ) ; Replacing second tag ( 0-indexed ) : UPDATE test SET data = jsonb_set ( data , ' { tags,1 } ' , ' '' tag5 '' ' ) ; Append tag ( ~~this work long fewer 999 tags ; changing argument 999 1000 generates error~~ . This longer appears case Postgres 9.5.3 ; much larger index used ) : UPDATE test SET data = jsonb_set ( data , ' { tags,999999999 } ' , ' '' tag6 '' ' , true ) ; Remove last tag : UPDATE test SET data = data # - ' { tags , -1 } ' Complex update ( delete last tag , insert new tag , change name ) : UPDATE test SET data = jsonb_set ( jsonb_set ( data # - ' { tags , -1 } ' , ' { tags,999999999 } ' , ' '' tag3 '' ' , true ) , ' { name } ' , ' '' my-other-name '' ' ) ; It 's important note examples , 're actually updating single field JSON data . Instead , 're creating temporary , modified version data , assigning modified version back column . In practice , result , keeping mind make complex updates , like last example , understandable . In complex example , three transformations three temporary versions : First , last tag removed . Then , version transformed adding new tag . Next , second version transformed changing ` name ` field . The value ` data ` column replaced final version ."
Stackoverflow,"I want execute one update raw sql like : update table set f1= ? f2= ? f3= ? This SQL executed ` ActiveRecord : :Base.connection.execute ` , I n't know pass dynamic parameter values method . Could someone give help ?","It n't look like Rails API exposes methods generically . You could try accessing underlying connection using 's methods , e.g . MySQL : st = ActiveRecord : :Base.connection.raw_connection.prepare ( `` update table set f1= ? f2= ? f3= ? '' ) st.execute ( f1 , f2 , f3 ) st.close I 'm sure ramifications ( connections left open , etc ) . I would trace Rails code normal update see aside actual query . Using prepared queries save small amount time database , unless 're million times row , 'd probably better building update normal Ruby substitution , e.g . ActiveRecord : :Base.connection.execute ( `` update table set f1= # { ActiveRecord : :Base.sanitize ( f1 ) } '' ) using ActiveRecord like commenters said ."
Stackoverflow,"I 'm confused import SQL dump file . I ca n't seem import database without creating database first MySQL . This error displayed ` database_name ` yet created : ` username ` = username someone access database original server . ` database_name ` = name database original server $ mysql -u username -p -h localhost database_name < dumpfile.sql Enter password : ERROR 1049 ( 42000 ) : Unknown database 'database_name ' If I log MySQL root create database , ` database_name ` mysql -u root create database database_name ; create user username ; # username user database I got dump . grant privileges database_name . * username @ '' localhost '' identified 'password ' ; exit mysql attempt import sql dump : $ mysql -u username -p database_name < dumpfile.sql Enter password : ERROR 1007 ( HY000 ) line 21 : Ca n't create database 'database_name ' ; database exists How I supposed import SQL dumpfile ?",This **known bug** MySQL . [ bug 42996 ] ( http : //bugs.mysql.com/bug.php ? id=42996 ) [ bug 40477 ] ( http : //bugs.mysql.com/bug.php ? id=40477 ) As see known issue since 2008 fixed yet ! ! ! **WORK AROUND** You first need create database import . It n't need tables . Then import database . 1. first start MySQL command line ( apply username password need ) ` C : \ > mysql -u user -p ` 2 . Create database exit mysql > DROP DATABASE database ; mysql > CREATE DATABASE database ; mysql > Exit 3 . Import selected database dump file ` C : \ > mysql -u user -p -h localhost -D database -o < dumpfile.sql ` You replace localhost IP domain MySQL server want import . The reason DROP command mysql prompt sure start empty clean database .
Stackoverflow,I 'd like function behaving mysql_real_escape_string without connecting database times I need dry testing without DB connection . mysql_escape_string deprecated therefore undesirable . Some findings : < http : //www.gamedev.net/community/forums/topic.asp ? topic_id=448909 > < http : //w3schools.invisionzone.com/index.php ? showtopic=20064 >,"The difference ` mysql_escape_string ` treats string raw bytes , adds escaping believes 's appropriate . ` mysql_real_escape_string ` , hand , uses information character set used MySQL connection . This means string escaped treating multi-byte characters properly ; i.e. , wo n't insert escaping characters middle character . This need connection ` mysql_real_escape_string ` ; 's necessary order know string treated . However , instead escaping , 's better idea use parameterized queries MySQLi library ; previously bugs escaping routine , 's possible could appear . Parameterizing query much , much harder mess , 's less likely get compromised MySQL bug ."
Stackoverflow,"I 'm confused import SQL dump file . I ca n't seem import database without creating database first MySQL . This error displayed ` database_name ` yet created : ` username ` = username someone access database original server . ` database_name ` = name database original server $ mysql -u username -p -h localhost database_name < dumpfile.sql Enter password : ERROR 1049 ( 42000 ) : Unknown database 'database_name ' If I log MySQL root create database , ` database_name ` mysql -u root create database database_name ; create user username ; # username user database I got dump . grant privileges database_name . * username @ '' localhost '' identified 'password ' ; exit mysql attempt import sql dump : $ mysql -u username -p database_name < dumpfile.sql Enter password : ERROR 1007 ( HY000 ) line 21 : Ca n't create database 'database_name ' ; database exists How I supposed import SQL dumpfile ?",This **known bug** MySQL . [ bug 42996 ] ( http : //bugs.mysql.com/bug.php ? id=42996 ) [ bug 40477 ] ( http : //bugs.mysql.com/bug.php ? id=40477 ) As see known issue since 2008 fixed yet ! ! ! **WORK AROUND** You first need create database import . It n't need tables . Then import database . 1. first start MySQL command line ( apply username password need ) ` C : \ > mysql -u user -p ` 2 . Create database exit mysql > DROP DATABASE database ; mysql > CREATE DATABASE database ; mysql > Exit 3 . Import selected database dump file ` C : \ > mysql -u user -p -h localhost -D database -o < dumpfile.sql ` You replace localhost IP domain MySQL server want import . The reason DROP command mysql prompt sure start empty clean database .
Stackoverflow,I downloaded latest version SQL Express 2012 I connect localhost . I tried localhost\SQLExpress Windows authentication gives error message saying connect . Am I missing something ? I 've used SQL Server 2008 I 've never issues connecting localhost . It seems ca n't even find . Also Services I see SQL Server VSS Writer . Is way ? Or I missing something ? Thanks,"[ According Aaron Bertand ] ( https : //stackoverflow.com/questions/10624287/cant-connect-to-sql- server-2005-localhost/10625298 # 10625298 ) : 1 . You need verify SQL Server service running . You going ` Start > Control Panel > Administrative Tools > Services ` , checking service SQL Server ( ` SQLEXPRESS ` ) running . If , start . 2 . While 're services applet , also make sure service [ SQL Browser ] ( https : //technet.microsoft.com/en-us/library/ms181087\ ( v=sql.105\ ) .aspx ) started . If , [ start ] ( https : //www.tenforums.com/tutorials/4499-start-stop-disable-services-windows-10-a.html ) . 3 . You need make sure SQL Server allowed use TCP/IP named pipes . You turn opening SQL Server Configuration Manager ` Start > Programs > Microsoft SQL Server 2012 > Configuration Tools ` ( ` SQL Server Configuration Manager ` ) , [ make sure TCP/IP Named Pipes enabled ] ( https : //www.blackbaud.com/files/support/infinityinstaller/content/installermaster/tkenablenamedpipesandtcpipconnections.htm ) . [ ! [ SQL Server Configuration Manager ] ( https : //i.stack.imgur.com/dPM5j.png ) ] ( https : //i.stack.imgur.com/dPM5j.png ) 4 . Verify SQL Server connection authentication mode matches connection string : * If 're connecting using username password , need configure SQL Server accept `` SQL Server Authentication Mode '' : -- YOU MUST RESTART YOUR SQL SERVER AFTER RUNNING THIS ! USE [ master ] GO DECLARE @ SqlServerAndWindowsAuthenticationMode INT = 2 ; EXEC xp_instance_regwrite N'HKEY_LOCAL_MACHINE ' , N'Software\Microsoft\MSSQLServer\MSSQLServer ' , N'LoginMode ' , REG_DWORD , @ SqlServerAndWindowsAuthenticationMode ; GO * If 're connecting using `` Integrated Security=true '' ( Windows Mode ) , error comes debugging web applications , need [ add ApplicationPoolIdentity SQL Server login ] ( https : //blogs.msdn.microsoft.com/ericparvin/2015/04/14/how-to-add-the-applicationpoolidentity-to-a-sql-server-login/ ) : 5. otherwise , run ` Start - > Run - > Services.msc ` If , running ? If 's running It sounds like n't get everything installed . Launch install file chose option `` New installation add features existing installation '' . From able make sure database engine service gets installed ."
Stackoverflow,"HSQLDB 2.0 soon released . I wonder outperform H2 since , far I know , users prefer H2 HSQLDB . I interested MVCC support HSQLDB 2.0 . I learned MVCC H2 still experimental . With regards support/documentation , concurrency , performance , better two ?","1 . You clear data dropping schema . The default schema called PUBLIC . If execute SQL satement , clear data drop tables . DROP SCHEMA PUBLIC CASCADE 2 . Alternatively , need table schema object definitions , create file : database containing objects data , add property .properties file . Using type database tests , changes data persisted files_read_only=true 3 . The latest alternative , available HSQLDB 2.2.6 later allows clear data schema keeping tables . In example , PUBLIC schema cleared . TRUNCATE SCHEMA public AND COMMIT This statement enhanced latest versions HSQLDB . See < http : //hsqldb.org/doc/2.0/guide/dataaccess-chapt.html # dac_truncate_statement > **Truncate Statement**"
Stackoverflow,"Ok , dilemma I database set 5 tables exact data structure . The data separated manner localization purposes split total 4.5 million records . A majority time one table needed well . However , sometimes data needed 2 tables needs sorted user defined column . This I problems . data columns : id , band_name , song_name , album_name , genre MySQL statment : SELECT * us_music , de_music ` genre ` = 'punk' MySQL spits error : # 1052 - Column 'genre ' clause ambiguous Obviously , I wrong . Anyone care shed light ?","I think 're looking [ UNION ] ( http : //dev.mysql.com/doc/refman/5.1/en/union.html ) clause , la ( SELECT * us_music ` genre ` = 'punk ' ) UNION ( SELECT * de_music ` genre ` = 'punk ' )"
Stackoverflow,"I 'm learning Laravel , working migration file creating users table . I trying populate user record part migration : public function ( ) { Schema : :create ( 'users ' , function ( $ table ) { $ table- > increments ( 'id ' ) ; $ table- > string ( 'email ' , 255 ) ; $ table- > string ( 'password ' , 64 ) ; $ table- > boolean ( 'verified ' ) ; $ table- > string ( 'token ' , 255 ) ; $ table- > timestamps ( ) ; DB : :table ( 'users ' ) - > insert ( array ( 'email ' = > 'name @ domain.com ' , 'verified ' = > true ) ) ; } ) ; } But I 'm getting following error running ` php artisan migrate ` : SQLSTATE [ 42S02 ] : Base table view found : 1146 Table 'vantage.users ' n't exist This obviously Artisan n't yet created table , documentation seems say way using Fluent Query populate data part migration . Anyone know ? Thanks !","Do n't put DB : :insert ( ) inside Schema : :create ( ) , create method finish making table insert stuff . Try instead : public function ( ) { // Create table Schema : :create ( 'users ' , function ( $ table ) { $ table- > increments ( 'id ' ) ; $ table- > string ( 'email ' , 255 ) ; $ table- > string ( 'password ' , 64 ) ; $ table- > boolean ( 'verified ' ) ; $ table- > string ( 'token ' , 255 ) ; $ table- > timestamps ( ) ; } ) ; // Insert stuff DB : :table ( 'users ' ) - > insert ( array ( 'email ' = > 'name @ domain.com ' , 'verified ' = > true ) ) ; }"
Stackoverflow,"Is possible prevent SQL injections Node.js ( preferably module ) way PHP Prepared Statements protected . If , ? If , **what examples** might bypass code I've provided ( see ) . * * * Some Context : I 'm making web application back-end stack consisting Node.js + MySql using [ node-mysql ] ( https : //github.com/felixge/node-mysql ) module . From usability perspective , module great , yet implemented something akin PHP 's [ Prepared Statements ] ( http : //php.net/manual/en/pdo.prepared-statements.php ) ( though I'm aware [ todo ] ( https : //github.com/felixge/node-mysql # todo ) ) . From understanding , PHP 's implementation prepared statements , among things , [ helped greatly ] ( https : //stackoverflow.com/questions/5741187/sql-injection-that-gets- around-mysql-real-escape-string/12118602 # 12118602 ) prevention SQL injections . I 'm worried , though , node.js app may open similar attacks , [ even string escaping provided default ] ( https : //github.com/felixge/node-mysql # escaping-query-values ) ( code snippet ) . node-mysql seems popular mysql connector node.js , I wondering people might ( anything ) account issue - even issue node.js begin ( sure would n't , since user/client-side input involved ) . **Should I switch [ node-mysql-native ] ( https : //github.com/sidorares/nodejs- mysql-native ) time , since provide prepared statements ? ** I 'm hesitant , seem active node- mysql ( though may mean complete ) . Here snippet user registration code , uses [ sanitizer ] ( https : //github.com/theSmaw/Caja-HTML-Sanitizer ) module , along node-mysql 's prepared statement-like syntax ( , I mentioned , character escaping ) , prevent cross site scripting sql injections , respectively : // Prevent xss var clean_user = sanitizer.sanitize ( username ) ; // assume password hashed already var post = { Username : clean_user , Password : hash } ; // This uses connection.escape ( ) underneath var query = connection.query ( 'INSERT INTO users SET ? ' , post , function ( err , results ) { // Can Sql injection happen ? } ) ;","Pay attention [ documentation ` node- mysql ` ] ( https : //github.com/felixge/node-mysql # escaping-query-values ) : > If paid attention , may noticed escaping allows neat things like : > > > var post = { id : 1 , title : 'Hello MySQL ' } ; > var query = connection.query ( 'INSERT INTO posts SET ? ' , post , function ( err , result ) { > // Neat ! > } ) ; > console.log ( query.sql ) ; // INSERT INTO posts SET ` id ` = 1 , ` title ` = 'Hello MySQL' > Notice use ` SET ` instead ` VALUES ` . ` INSERT INTO ... SET x = ` valid MySQL query , ` INSERT INTO ... VALUES x = ` ."
Stackoverflow,What wrong code lots error debugging . I writing code singleton class connect database mysql . Here code package com.glomindz.mercuri.util ; import java.sql.Connection ; import java.sql.Driver ; import java.sql.DriverManager ; import java.sql.SQLException ; public class MySingleTon { String url = `` jdbc : mysql : //localhost:3306/ '' ; String dbName = `` test '' ; String driver = `` com.mysql.jdbc.Driver '' ; String userName = `` root '' ; String password = `` '' ; private static MySingleTon myObj ; private Connection Con ; private MySingleTon ( ) { System.out.println ( `` Hello '' ) ; Con= createConnection ( ) ; } @ SuppressWarnings ( `` rawtypes '' ) public Connection createConnection ( ) { Connection connection = null ; try { // Load JDBC driver Class driver_class = Class.forName ( driver ) ; Driver driver = ( Driver ) driver_class.newInstance ( ) ; DriverManager.registerDriver ( driver ) ; connection = DriverManager.getConnection ( url + dbName ) ; } catch ( ClassNotFoundException e ) { e.printStackTrace ( ) ; } catch ( SQLException e ) { e.printStackTrace ( ) ; } catch ( IllegalAccessException e ) { e.printStackTrace ( ) ; } catch ( InstantiationException e ) { e.printStackTrace ( ) ; } return connection ; } /** * Create static method get instance . */ public static MySingleTon getInstance ( ) { ( myObj == null ) { myObj = new MySingleTon ( ) ; } return myObj ; } public static void main ( String [ ] ) { MySingleTon st = MySingleTon.getInstance ( ) ; } } I new java . Please help .,"It seems **mysql connectivity library included** project . Solve problem following one proposed solutions : * **MAVEN PROJECTS SOLUTION** Add mysql-connector dependency pom.xml project file : < dependency > < groupId > mysql < /groupId > < artifactId > mysql-connector-java < /artifactId > < version > 5.1.39 < /version > < /dependency > Here versions : < https : //mvnrepository.com/artifact/mysql/mysql-connector-java > * **ALL PROJECTS SOLUTION** Add jar library manually project . **Right Click project -- > build path -- > configure build path** In ` Libraries Tab ` **press** ` Add External Jar ` ` Select ` jar . You find zip mysql-connector [ ] ( http : //dev.mysql.com/downloads/connector/j/5.0.html ) * **_Explanation : _** When building project , java throws exception file ( com.mysql.jdbc.Driver class ) mysql connectivity library found . The solution adding library project , java find com.mysql.jdbc.Driver"
Stackoverflow,"**The problem : ** I need device agnostic ( e.g . HTML5 ) solution storing querying 250,000+ rows data offline phone tablet type device ( e.g . iOS/Android ) . The idea I people working remote areas without cellular data connection need run queries data edit offline . Partly geo-location based assets area ( uses GPS ) show assets let edited . When return office sync data back office server . The reason I 'm approaching web standard point view basically save money time writing HTML5 works across multiple platforms rather writing twice Objective C Java . Also write something 's platform agnostic 're locked n't go ship everyone moves newer one . We similar app written Windows Mobile 5 , 's useless platform dead . The offline database device needs : * fast ( responses 2 seconds ) * potentially perform joins relationships tables able query database * select data within certain range criteria e.g . x & co-ordinate based GPS reading . **Options : ** _**HTML5 local storage : _** Fine small amounts data < 5,000 key/values , even store arrays/objects convert JSON . _Cons : _ * For 10,000 rows even high end machine browser slow crawl . * Ca n't complex queries data pull data want iterate whole storage manually search . * Limitations amount storage stored _**Web SQL Database : _** * Meets requirements . * Fast run query 250,000 rows ( 1-2secs ) * Can create complex queries , joins etc * Supported Safari , Android Opera work iOS Android devices _Cons : _ * Deprecated November 2010 * Security flaw cross-directory attacks . Not really issue wo n't shared hosting _**IndexedDB : _** Key/value object store similar local storage except indexes . _Cons : _ * Slow run query 200,000 rows ( 15-18secs ) * Ca n't run complex queries * Ca n't joins tables * Not supported main phone tablet devices e.g . iPad/Android * Standard complete This leaves option implementing deprecated Web SQL method may work another year . IndexedDB local storage unusable present . I 'm sure Mozilla Microsoft got Web SQL Database standard deprecated W3C let happen . Supposedly 77 % desktop browser market . On advanced mobile devices Mozilla Microsoft nearly zero influence [ Safari , Opera Android 90 % market share ] ( http : //www.netmarketshare.com/ ) . How Mozilla & Microsoft dictate standard used mobile market offline storage likely used n't make sense . In [ comments Mozilla ] ( http : //hacks.mozilla.org/2010/06/beyond- html5-database-apis-and-the-road-to-indexeddb/ ) wanted go IndexedDB instead mainly 'developer aesthetics ' like idea running SQL JavaScript . I 'm buying . 1 . Currently proposed standard inferior extremely basic NoSQL implementation slow n't even support advanced features people need database . There lot boilerplate code establish database get data claim people write nice abstraction libraries top provide advanced features . As Oct 2011 're nowhere seen . 2 . They 've deprecated existing Web SQL standard actually works implemented main mobile/tablet browsers . Whereas 'new ' 'better ' standard available major mobile browsers . 3 . What developers supposed use next 3-5 years IndexedDB specification might get around standardised , features , implemented main mobile/tablet browsers 's nice libraries make things easier ? The W3C keep Web SQL Database standard running parallel fix issues . It already support major mobile platforms works pretty well . The fact Mozilla Microsoft two players desktop browser share able get standard scrapped pretty dubious could seen attempt hinder progress mobile web platforms able catch offer competing solutions iOS/Safari Android . In conclusion anyone solution problem work iOS/Android phone/tablet devices . Maybe nice wrapper API use multiple database implementations background querying capability lets choose database priority . I 've seen things like [ lawnchair ] ( http : //westcoastlogic.com/lawnchair/ ) I 'm pretty sure lets use local storage default falls back others . I think I 'd rather used Web SQL ( default ) slower options . Any help solution much appreciated , thanks !","Let answer step step > app stores data localStorage web SQL , user switches different standard browser Android , app opened new browser mean stored data unavailable ? The data saved 'cache ' ( exactly cache ) browser , change browser , set settings default browser removed changed , data go . > If user n't use app year ( case realistic necessarily bad scenario ) , data expired like cookie , maybe pushed browser 's storage deluge data apps ? No , data stay matter long used . So even clear browser cache , still . > Or data destroyed even earlier , : - user visits another site browser - browser manually closed - browser process killed dies - etc No , data stays right . : - ) > Or localStorage web SQL kind storage delete ( Android ) go Settings > Apps actively remove user data associated app ? Yes , data goes either manually delete app uninstall app . It stay cases . EDIT : In case iOS , OS remove data local storage shortage memory device ."
Stackoverflow,"I want run small PostgreSQL database runs memory , unit test I write . For instance : @ Before void setUp ( ) { String port = runPostgresOnRandomPort ( ) ; connectTo ( `` postgres : //localhost : '' +port+ '' /in_memory_db '' ) ; // ... } Ideally I 'll single postgres executable checked version control , unit test use . Something like ` HSQL ` , postgres . How I ? Were I get Postgres version ? How I instruct use disk ?","For best read performance need [ multicolumn index ] ( https : //www.postgresql.org/docs/current/static/indexes- multicolumn.html ) : CREATE INDEX user_msg_log_combo_idx ON user_msg_log ( user_id , aggr_date DESC NULLS LAST ) To make ** [ index scans ] ( https : //wiki.postgresql.org/wiki/Index- only_scans ) ** possible , add otherwise needed column ` running_total ` : CREATE INDEX user_msg_log_combo_covering_idx ON user_msg_log ( user_id , aggr_date DESC NULLS LAST , running_total ) Why ` DESC NULLS LAST ` ? * [ Unused index range dates query ] ( https : //dba.stackexchange.com/a/90183/3684 ) For **_few_** rows per ` user_id ` small tables simple ` DISTINCT ON ` among fastest simplest solutions : * [ Select first row GROUP BY group ? ] ( https : //stackoverflow.com/questions/3800551/select-first-row-in-each-group-by-group/7630564 # 7630564 ) For **_many_** rows per ` user_id ` [ **loose index scan** ] ( https : //wiki.postgresql.org/wiki/Loose_indexscan ) would ( much ) efficient . That 's implemented Postgres ( least Postgres 10 ) , ways emulate : # # 1\ . No separate table unique users The following solutions go beyond 's covered [ **Postgres Wiki** ] ( https : //wiki.postgresql.org/wiki/Loose_indexscan ) . With separate ` users ` table , solutions **_2._** typically simpler faster . # # # 1a . Recursive CTE ` LATERAL ` join [ Common Table Expressions ] ( https : //www.postgresql.org/docs/current/static/queries-with.html ) require Postgres **8.4+** . [ ` LATERAL ` ] ( https : //www.postgresql.org/docs/current/static/queries-table- expressions.html # QUERIES-LATERAL ) requires Postgres **9.3+** . WITH RECURSIVE cte AS ( ( -- parentheses required SELECT user_id , aggr_date , running_total FROM user_msg_log WHERE aggr_date < = : mydate ORDER BY user_id , aggr_date DESC NULLS LAST LIMIT 1 ) UNION ALL SELECT u.user_id , u.aggr_date , u.running_total FROM cte c , LATERAL ( SELECT user_id , aggr_date , running_total FROM user_msg_log WHERE user_id > c.user_id -- lateral reference AND aggr_date < = : mydate -- repeat condition ORDER BY user_id , aggr_date DESC NULLS LAST LIMIT 1 ) u ) SELECT user_id , aggr_date , running_total FROM cte ORDER BY user_id ; This preferable current versions Postgres 's simple retrieve arbitrary columns . More explanation chapter _2a._ . # # # 1b . Recursive CTE correlated subqueries Convenient retrieve either _single column_ _whole row_ . The example uses whole row type table . Other variants possible . WITH RECURSIVE cte AS ( ( SELECT u -- whole row FROM user_msg_log u WHERE aggr_date < = : mydate ORDER BY user_id , aggr_date DESC NULLS LAST LIMIT 1 ) UNION ALL SELECT ( SELECT u1 -- , whole row FROM user_msg_log u1 WHERE user_id > ( c.u ) .user_id -- parentheses access row type AND aggr_date < = : mydate -- repeat predicate ORDER BY user_id , aggr_date DESC NULLS LAST LIMIT 1 ) FROM cte c WHERE ( c.u ) .user_id IS NOT NULL -- NOT NULL column row ) SELECT ( u ) . * -- finally decompose row FROM cte WHERE ( u ) .user_id IS NOT NULL -- column defined NOT NULL ORDER BY ( u ) .user_id ; It could misleading test row value ` c.u IS NOT NULL ` . This returns ` true ` every single column tested row ` NOT NULL ` would fail single ` NULL ` value contained . ( I bug answer time . ) Instead , assert row found previous iteration , test single column row defined ` NOT NULL ` ( like primary key ) . More : * [ NOT NULL constraint set columns ] ( https : //stackoverflow.com/questions/21021102/not-null-constraint-over-a-set-of-columns/21026085 # 21026085 ) * [ IS NOT NULL test record return TRUE variable set ] ( https : //stackoverflow.com/questions/21765198/is-not-null-test-for-a-record-does-not-return-true-when-variable-is-set/21770324 # 21770324 ) More explanation query chapter _2b._ . Related answers : * [ Query last N related rows per row ] ( https : //stackoverflow.com/questions/25957558/querying-last-n-related-records-in-postgres/25965393 # 25965393 ) * [ GROUP BY one column , sorting another PostgreSQL ] ( https : //dba.stackexchange.com/a/74811/3684 ) # # 2\ . With separate ` users ` table Table layout hardly matters long exactly one row per relevant ` user_id ` . Example : CREATE TABLE users ( user_id serial PRIMARY KEY , username text NOT NULL ) ; Ideally , table physically sorted . See : * [ Optimize Postgres timestamp query range ] ( https : //stackoverflow.com/questions/13998139/optimize-postgres-timestamp-query-range/14007963 # 14007963 ) Or 's small enough ( low cardinality ) hardly matters . Else , sorting rows query help optimize performance . [ See Gang Liang 's addition . ] ( https : //stackoverflow.com/a/36223670/939860 ) # # # 2a . ` LATERAL ` join SELECT u.user_id , l.aggr_date , l.running_total FROM users u CROSS JOIN LATERAL ( SELECT aggr_date , running_total FROM user_msg_log WHERE user_id = u.user_id -- lateral reference AND aggr_date < = : mydate ORDER BY aggr_date DESC NULLS LAST LIMIT 1 ) l ; [ ` JOIN LATERAL ` ] ( https : //www.postgresql.org/docs/current/static/sql- select.html ) allows reference preceding ` FROM ` items query level . You get one index ( -only ) look-up per user . * [ What difference LATERAL subquery PostgreSQL ? ] ( https : //stackoverflow.com/questions/28550679/what-is-the-difference-between-lateral-and-a-subquery-in-postgresql/28557803 # 28557803 ) Consider possible improvement sorting ` users ` table [ suggested Gang Liang another answer ] ( https : //stackoverflow.com/a/36223670/939860 ) . If physical sort order ` users ` table happens match index ` user_msg_log ` , n't need . You n't get results users missing ` users ` table , even entries ` user_msg_log ` . Typically , would **foreign key** constraint enforcing referential integrity rule . You also n't get row user matching entry ` user_msg_log ` . That conforms original question . If need include rows result use ** ` LEFT JOIN LATERAL ... ON true ` ** instead ` CROSS JOIN LATERAL ` : * [ Call set-returning function array argument multiple times ] ( https : //stackoverflow.com/questions/26107915/call-a-set-returning-function-with-an-array-argument-multiple-times/26514968 # 26514968 ) This form also best retrieve **more one rows** ( ) per user . Just use ** ` LIMIT n ` ** instead ` LIMIT 1 ` . Effectively , would : JOIN LATERAL ... ON true CROSS JOIN LATERAL ... , LATERAL ... The latter lower priority , though . Explicit ` JOIN ` binds comma . # # # 2b . Correlated subquery Good choice retrieve **single column** **single row** . Code example : * [ Optimize groupwise maximum query ] ( https : //stackoverflow.com/questions/24244026/optimize-groupwise-maximum-query/24377356 # 24377356 ) The possible **multiple columns** , need smarts : CREATE TEMP TABLE combo ( aggr_date date , running_total int ) ; SELECT user_id , ( my_combo ) . * -- note parentheses FROM ( SELECT u.user_id , ( SELECT ( aggr_date , running_total ) : :combo FROM user_msg_log WHERE user_id = u.user_id AND aggr_date < = : mydate ORDER BY aggr_date DESC NULLS LAST LIMIT 1 ) AS my_combo FROM users u ) sub ; * Like ` LEFT JOIN LATERAL ` , variant includes _all_ users , even without entries ` user_msg_log ` . You get ` NULL ` ` my_combo ` , easily filter ` WHERE ` clause outer query need . Nitpick : outer query ca n't distinguish whether subquery find row values returned happen NULL - result . You would include ` NOT NULL ` column subquery sure . * A correlated subquery return **single value** . You wrap multiple columns composite type . But decompose later , Postgres demands well-known composite type . Anonymous records decomposed providing column definition list . * Use registered type like row type existing table , create type . Register composite type explicitly ( permanently ) ` CREATE TYPE ` , create temporary table ( dropped automatically end session ) provide row type temporarily . Cast type : ` ( aggr_date , running_total ) : :combo ` * Finally , want decompose ` combo ` query level . Due weakness query planner would evaluate subquery column ( Postgres 9.6 - improvements planned Postgres 10 ) . Instead , make subquery decompose outer query . Related : * [ Get values first last row per group ] ( https : //stackoverflow.com/questions/25170215/get-values-from-first-and-last-row-per-group/25173081 # 25173081 ) Demonstrating 4 queries 100k log entries 1k users : [ **SQL Fiddle** ] ( http : //sqlfiddle.com/ # ! 15/56cd3/30 ) \- pg 9.6 _db < > fiddle [ ] ( https : //dbfiddle.uk/ ? rdbms=postgres_10 & fiddle=05b6e1d34b05ff5b29996e4444488d12 ) _ \- pg 10"
Stackoverflow,I already seen < http : //dev.mysql.com/doc/refman/4.1/en/mysql-config- wizard-file-location.html > [ know mysql my.cnf location ] ( https : //stackoverflow.com/questions/2482234/how-to-know-mysql-my- cnf-location ) < http : //dev.mysql.com/doc/refman/5.1/en/option-files.html > But I still stuck ages old question ! `` Where my.ini '' I using windows server 2008 mysql 5.5.28 . I installed service using mysqld -- install I able use mysql server using sqlyog . But unfortunately I able find my.ini ` installation directory ` ` c : \ ` neither ` c : \windows ` ` data_dir ` query ` show variables like '' mysql_home '' ` returned nothing well . Any suggestions ?,"**my.ini LOCATION ON WINDOWS MYSQL 5.6 MSI ( USING THE INSTALL WIZARD ) ** Open Windows command shell type : ` echo % PROGRAMDATA % ` . On Windows Vista results : ` C : \ProgramData ` . According < http : //dev.mysql.com/doc/refman/5.6/en/option-files.html > , first location MySQL look ` % PROGRAMDATA % \MySQL\MySQL Server 5.6\my.ini ` . In Windows shell ` ls `` % PROGRAMDATA % \MySQL\MySQL Server 5.6\my.ini '' ` , see file . **Unlike suggestions find Stackoverflow around web , putting file ` C : \Program Files\MySQL\MySQL Server 5.6\my.ini ` WILL NOT WORK . Neither ` C : \Program Files ( x86 ) \MySQL\MySQL Server 5.1 ` . ** The reason quoted MySQL link posted : > On Windows , MySQL programs read startup options following files , specified order ( top items used first ) . The 5.6 MSI installer **does** create my.ini **highest** priority location , meaning file ever found/used , except one created installer . The solution accepted work 5.6 MSI-based installs ."
Stackoverflow,"There MySQL ` table ` definition taken ` SQLYog Enterprise ` : Table Create Table -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - etape_prospection CREATE TABLE ` etape_prospection ` ( ` etape_prosp_id ` int ( 10 ) NOT NULL AUTO_INCREMENT , ` type_prosp_id ` int ( 10 ) NOT NULL DEFAULT ' 0 ' , ` prosp_id ` int ( 10 ) NOT NULL DEFAULT ' 0 ' , ` etape_prosp_date ` datetime DEFAULT NULL , ` etape_prosp_comment ` text , PRIMARY KEY ( ` etape_prosp_id ` ) , KEY ` concerne_fk ` ( ` prosp_id ` ) , KEY ` de_type_fk ` ( ` type_prosp_id ` ) ) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=latin1 I want change ` default charset ` table ` latin1 ` ` utf8 ` . How ?","If want change table ` default character set ` character columns new character set , use statement like : ALTER TABLE tbl_name CONVERT TO CHARACTER SET charset_name ; So query : ALTER TABLE etape_prospection CONVERT TO CHARACTER SET utf8 ;"
Stackoverflow,When I try install tSQLt onto existing database get following error : > The database owner SID recorded master database differs database owner SID recorded database `` . You correct situation resetting owner database `` using ALTER AUTHORIZATION statement .,"This problem arise database restored backup SID database owner match owners SID listed master database . Here solution uses `` ALTER AUTHORIZATION '' statement recommended error message : DECLARE @ Command VARCHAR ( MAX ) = 'ALTER AUTHORIZATION ON DATABASE : : [ < < DatabaseName > > ] TO [ < < LoginName > > ] ' SELECT @ Command = REPLACE ( REPLACE ( @ Command , ' < < DatabaseName > > ' , SD.Name ) , ' < < LoginName > > ' , SL.Name ) FROM master..sysdatabases SD JOIN master..syslogins SL ON SD.SID = SL.SID WHERE SD.Name = DB_NAME ( ) PRINT @ Command EXEC ( @ Command )"
Stackoverflow,"When I try install mysql2 gem , fails apparent errors . Does anyone know work around mysql2 installs ? $ sudo gem install mysql2 Building native extensions . This could take ... ERROR : Error installing mysql2 : ERROR : Failed build gem native extension . /System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/bin/ruby extconf.rb checking rb_thread_blocking_region ( ) ... checking mysql_query ( ) -lmysqlclient ... checking main ( ) -lm ... yes checking mysql_query ( ) -lmysqlclient ... checking main ( ) -lz ... yes checking mysql_query ( ) -lmysqlclient ... checking main ( ) -lsocket ... checking mysql_query ( ) -lmysqlclient ... checking main ( ) -lnsl ... checking mysql_query ( ) -lmysqlclient ... checking main ( ) -lmygcc ... checking mysql_query ( ) -lmysqlclient ... *** extconf.rb failed *** Could create Makefile due reason , probably lack necessary libraries and/or headers . Check mkmf.log file details . You may need configuration options . Provided configuration options : -- with-opt-dir -- without-opt-dir -- with-opt-include -- without-opt-include= $ { opt-dir } /include -- with-opt-lib -- without-opt-lib= $ { opt-dir } /lib -- with-make-prog -- without-make-prog -- srcdir= . -- curdir -- ruby=/System/Library/Frameworks/Ruby.framework/Versions/1.8/usr/bin/ruby -- with-mysql-config -- without-mysql-config -- with-mysql-dir -- without-mysql-dir -- with-mysql-include -- without-mysql-include= $ { mysql-dir } /include -- with-mysql-lib -- without-mysql-lib= $ { mysql-dir } /lib -- with-mysqlclientlib -- without-mysqlclientlib -- with-mlib -- without-mlib -- with-mysqlclientlib -- without-mysqlclientlib -- with-zlib -- without-zlib -- with-mysqlclientlib -- without-mysqlclientlib -- with-socketlib -- without-socketlib -- with-mysqlclientlib -- without-mysqlclientlib -- with-nsllib -- without-nsllib -- with-mysqlclientlib -- without-mysqlclientlib -- with-mygcclib -- without-mygcclib -- with-mysqlclientlib -- without-mysqlclientlib Gem files remain installed /Library/Ruby/Gems/1.8/gems/mysql2-0.2.6 inspection . Results logged /Library/Ruby/Gems/1.8/gems/mysql2-0.2.6/ext/mysql2/gem_make.out",Ubuntu : sudo apt-get install libmysqlclient-dev # ( mysql development headers ) sudo gem install mysql2 -- -- with-mysql-dir=/etc/mysql/ That 's ! Result : Building native extensions . This could take ... Successfully installed mysql2-0.2.6 1 gem installed Installing ri documentation mysql2-0.2.6 ... Enclosing class/module 'mMysql2 ' class Result known Enclosing class/module 'mMysql2 ' class Client known Installing RDoc documentation mysql2-0.2.6 ... Enclosing class/module 'mMysql2 ' class Result known Enclosing class/module 'mMysql2 ' class Client known
Stackoverflow,Is way I get Node.js app communicate Microsoft SQL ? I n't seen MS SQL drivers wild ? I 'm putting simple app together need able communicate existing MS SQL database ( otherwise I would gone mongoDB Redis ),The solution enable TCP connections disabled default . ! [ enter image description ] ( https : //i.stack.imgur.com/7ElnG.png )
Stackoverflow,"How I get SQL Server server instance name current connection , using T-SQL script ?","Just found answer , [ SO question ] ( https : //stackoverflow.com/questions/129861/how-can-i-query-the-name- of-the-current-sql-server-database-instance ) ( literally , inside question , answer ) : SELECT @ @ servername returns servername\instance far default instance SELECT @ @ servicename returns instance name , even default ( MSSQLSERVER )"
Stackoverflow,"How I convert LocalDate ` java.sql.Date ` ? Attempt : Record r = new Record ( ) ; LocalDate date = new Date ( 1967 , 06 , 22 ) ; r.setDateOfBirth ( new Date ( date ) ) ; This fails _ ( wo n't compile ) _ I find Joda time stuff . I 'm using Java 8","The answer really simple ; import java.sql.Date ; ... LocalDate locald = LocalDate.of ( 1967 , 06 , 22 ) ; **Date date = Date.valueOf ( locald ) ; // Magic happens ! ** r.setDateOfBirth ( date ) ; If would want convert way around , like : Date date = r.getDate ( ) ; LocalDate localD = date.toLocalDate ( ) ; ` r ` record 're using JOOQ ` .getDate ( ) ` method getting date record ; let 's say date column called date_of_birth , get method called ` getDateOfBirth ( ) ` ."
Stackoverflow,What advantages disadvantages storing JSON data MySQL database vs. serialized array ?,"1 . JSON [ encode ] ( http : //www.php.net/manual/en/function.json-encode.php `` json_encode '' ) ( ) & [ decode ] ( http : //www.php.net/manual/en/function.json-decode.php `` json_decode '' ) ( ) * PHP Version > = 5.0.0 * Nesting Limit 20 . * PHP Version > = 5.2.3 * Nesting Limit 128 . * PHP Version > = 5.3.0 * Nesting Limit 512 . * Small footprint vs PHP 's serialize 'd string . 2 . [ serialize ] ( http : //www.php.net/manual/en/function.serialize.php `` serialize '' ) ( ) & [ unserialize ] ( http : //www.php.net/manual/en/function.unserialize.php `` unserialize '' ) ( ) * PHP Version > = 4.0.0 * Methods lost PHP Datatype Object . * __wakeup ( ) magic method called object unserialize . ( VERY POWERFUL ) * It noted times best [ base64 encode ] ( http : //php.net/manual/en/function.base64-encode.php `` base64_encode '' ) strings put database , [ base64 decode ] ( http : //php.net/manual/en/function.base64-decode.php `` base64_decode '' ) strings taken database function , issues handling white space characters . The choice ."
Stackoverflow,I currently use following ALWAYS prompts manually type password . Is way pass command line launching executable ? mysqladmin processlist -u root -p,Just found answer ... . mysqladmin processlist -u root -pYOURPASSWORDHERE No space password -p
Stackoverflow,"While working system I 'm creating , I attempted use following query project : SELECT topics.id , topics.name , topics.post_count , topics.view_count , COUNT ( posts.solved_post ) AS solved_post , ( SELECT users.username AS posted_by , users.id AS posted_by_id FROM users WHERE users.id = posts.posted_by ) FROM topics LEFT OUTER JOIN posts ON posts.topic_id = topics.id WHERE topics.cat_id = : cat GROUP BY topics.id '' : cat '' bound PHP code I 'm using PDO . 2 valid value '' : cat '' . That query though gives error : `` # 1241 - Operand contain 1 column ( ) '' What stumps I would think query would work problem . Selecting columns , selecting two another table , continuing . I ca n't figure problem . Is simple fix , another way write query ?","Syntax error , remove ` ( ) ` ` select ` . insert table2 ( name , subject , student_id , result ) select name , subject , student_id , result table1 ;"
Stackoverflow,"How ALTER used drop column MySQL table column exists ? I know I use ` ALTER my_table DROP COLUMN my_column ` , throw error ` my_column ` exist . Is alternative syntax dropping column conditionally ? I 'm using MySQL version 4.0.18 .","**For MySQL , none : ** [ MySQL Feature Request ] ( http : //bugs.mysql.com/bug.php ? id=10789 ) . Allowing arguably really bad idea , anyway : ` IF EXISTS ` indicates 're running destructive operations database ( ) unknown structure . There may situations acceptable quick-and- dirty local work , 're tempted run statement production data ( migration etc . ) , 're playing fire . But insist , 's difficult simply check existence first client , catch error . MariaDB also supports following starting 10.0.2 : > DROP [ COLUMN ] [ IF EXISTS ] col_name i. e. > ALTER TABLE my_table DROP IF EXISTS my_column ; But 's arguably bad idea rely non-standard feature supported one several forks MySQL ."
Stackoverflow,"I table contains ` Xml ` column : SELECT * FROM Sqm ! [ enter image description ] ( https : //i.stack.imgur.com/nVgUG.png ) A sample ` xml ` data row would : < Sqm version= '' 1.2 '' > < Metrics > < Metric id= '' TransactionCleanupThread.RecordUsedTransactionShift '' type= '' timer '' unit= '' µs '' count= '' 1 '' sum= '' 21490 '' average= '' 21490 '' minValue= '' 73701 '' maxValue= '' 73701 '' > 73701 < /Metric > < Metric id= '' TransactionCleanupThread.RefundOldTrans '' type= '' timer '' unit= '' µs '' count= '' 1 '' sum= '' 184487 '' average= '' 184487 '' minValue= '' 632704 '' maxValue= '' 632704 '' > 632704 < /Metric > < Metric id= '' Database.CreateConnection_SaveContextUserGUID '' type= '' timer '' unit= '' µs '' count= '' 2 '' sum= '' 7562 '' average= '' 3781 '' minValue= '' 12928 '' maxValue= '' 13006 '' standardDeviation= '' 16 '' > 12967 < /Metric > < Metric id= '' Global.CurrentUser '' type= '' timer '' unit= '' µs '' count= '' 6 '' sum= '' 4022464 '' average= '' 670411 '' minValue= '' 15 '' maxValue= '' 13794345 '' standardDeviation= '' 1642047 '' > 2299194 < /Metric > < Metric id= '' Global.CurrentUser_FetchIdentityFromDatabase '' type= '' timer '' unit= '' µs '' count= '' 1 '' sum= '' 4010057 '' average= '' 4010057 '' minValue= '' 13752614 '' maxValue= '' 13752614 '' > 13752614 < /Metric > < /Metrics > < /Sqm > In case data , I would want : SqmId id type unit count sum minValue maxValue standardDeviation Value ===== =================================================== ===== ==== ===== ====== ======== ======== ================= ====== 1 TransactionCleanupThread.RecordUsedTransactionShift timer µs 1 21490 73701 73701 NULL 73701 1 TransactionCleanupThread.RefundOldTrans timer µs 1 184487 632704 632704 NULL 632704 1 Database.CreateConnection_SaveContextUserGUID timer µs 2 7562 12928 13006 16 12967 1 Global.CurrentUser timer µs 6 4022464 15 13794345 1642047 2299194 1 Global.CurrentUser_FetchIdentityFromDatabase timer µs 1 4010057 13752614 13752614 NULL 13752614 2 ... In end I 'll actually performing ` SUM ( ) ` , ` MIN ( ) ` , ` MAX ( ) ` aggregation . But I 'm trying _query_ xml column . In pseudo-code , I would try something like : SELECT SqmId , Data.query ( '/Sqm/Metrics/Metric/ @ id ' ) AS id , Data.query ( '/Sqm/Metrics/Metric/ @ type ' ) AS type , Data.query ( '/Sqm/Metrics/Metric/ @ unit ' ) AS unit , Data.query ( '/Sqm/Metrics/Metric/ @ sum ' ) AS sum , Data.query ( '/Sqm/Metrics/Metric/ @ count ' ) AS count , Data.query ( '/Sqm/Metrics/Metric/ @ minValue ' ) AS minValue , Data.query ( '/Sqm/Metrics/Metric/ @ maxValue ' ) AS maxValue , Data.query ( '/Sqm/Metrics/Metric/ @ standardDeviation ' ) AS standardDeviation , Data.query ( '/Sqm/Metrics/Metric ' ) AS value FROM Sqm But SQL query n't work : > Msg 2396 , Level 16 , State 1 , Line 2 > XQuery [ Sqm.data.query ( ) ] : Attribute may appear outside element I 've hunted , 's amazing poorly documented , exampled , Xml querying . Most resources rather querying **table** , query **variable** ; I 'm . Most resources use xml querying filtering selection , rather reading values . Most resources read hard-coded child nodes ( index ) , rather actual values . # # Related resources I read * < https : //stackoverflow.com/questions/966441/xml-query-in-sql-server-2008 > * [ SQL Server query xml attribute element value ] ( https : //stackoverflow.com/questions/12913724/sql-server-query-xml-attribute-for-an-element-value ) * [ SQL querying XML attributes ] ( https : //stackoverflow.com/questions/14343263/sql-querying-xml-attributes ) * [ SQL Server 2005 XQuery XML-DML - Part 1 ] ( http : //www.codeguru.com/csharp/.net/net_data/article.php/c19491/SQL-Server-2005-XQuery-and-XMLDML -- Part-1.htm ) * [ BOL : XML Support Microsoft SQL Server 2005 ] ( http : //msdn.microsoft.com/en-us/library/ms345117.aspx ) * [ Querying XML SQL Server ] ( http : //blog.jontav.com/post/6811942997/querying-xml-in-sql-server ) * [ Basic SQL Server XML Querying ] ( http : //www.mssqltips.com/sqlservertip/2889/basic-sql-server-xml-querying/ ) * [ BOL : query ( ) Method ( xml Data Type ) ] ( http : //technet.microsoft.com/en-us/library/ms191474.aspx ) * [ XML Workshop V - Reading Values XML Columns ] ( http : //jacobsebastian.blogspot.ca/2007/11/xml-workshop-v-reading-values-from-xml.html ) * [ SQL SERVER – Introduction Discovering XML Data Type Methods – A Primer ] ( http : //blog.sqlauthority.com/2012/04/27/sql-server-introduction-to-discovering-xml-data-type-methods-a-primer/ ) # Update : .value rather .query I tried randomly using ` .value ` , place ` .query ` : SELECT Sqm.SqmId , Data.value ( '/Sqm/Metrics/Metric/ @ id ' , 'varchar ( max ) ' ) AS id , Data.value ( '/Sqm/Metrics/Metric/ @ type ' , 'varchar ( max ) ' ) AS type , Data.value ( '/Sqm/Metrics/Metric/ @ unit ' , 'varchar ( max ) ' ) AS unit , Data.value ( '/Sqm/Metrics/Metric/ @ sum ' , 'varchar ( max ) ' ) AS sum , Data.value ( '/Sqm/Metrics/Metric/ @ count ' , 'varchar ( max ) ' ) AS count , Data.value ( '/Sqm/Metrics/Metric/ @ minValue ' , 'varchar ( max ) ' ) AS minValue , Data.value ( '/Sqm/Metrics/Metric/ @ maxValue ' , 'varchar ( max ) ' ) AS maxValue , Data.value ( '/Sqm/Metrics/Metric/ @ standardDeviation ' , 'varchar ( max ) ' ) AS standardDeviation , Data.value ( '/Sqm/Metrics/Metric ' , 'varchar ( max ) ' ) AS value FROM Sqm But also n't work : > Msg 2389 , Level 16 , State 1 , Line 3 XQuery [ Sqm.data.value ( ) ] : > 'value ( ) ' requires singleton ( empty sequence ) , found operand type 'xdt : untypedAtomic * '","Actually 're close goal , need use [ nodes ( ) ] ( http : //technet.microsoft.com/en-us/library/ms188282.aspx ) method split rows get values : select s.SqmId , m.c.value ( ' @ id ' , 'varchar ( max ) ' ) id , m.c.value ( ' @ type ' , 'varchar ( max ) ' ) type , m.c.value ( ' @ unit ' , 'varchar ( max ) ' ) unit , m.c.value ( ' @ sum ' , 'varchar ( max ) ' ) [ sum ] , m.c.value ( ' @ count ' , 'varchar ( max ) ' ) [ count ] , m.c.value ( ' @ minValue ' , 'varchar ( max ) ' ) minValue , m.c.value ( ' @ maxValue ' , 'varchar ( max ) ' ) maxValue , m.c.value ( ' . ' , 'nvarchar ( max ) ' ) Value , m.c.value ( ' ( text ( ) ) [ 1 ] ' , 'nvarchar ( max ) ' ) Value2 sqm outer apply s.data.nodes ( 'Sqm/Metrics/Metric ' ) ( c ) ** ` [ sql fiddle demo ] ( http : //sqlfiddle.com/ # ! 3/263bc/5 ) ` **"
Stackoverflow,"After reading [ article ] ( https : //www.gamasutra.com/view/news/170502/Indepth_SQL_Server__High_performance_inserts.php ) I decided take closer look way I using Dapper . I ran code empty database var members = new List < Member > ( ) ; ( int = 0 ; < 50000 ; i++ ) { members.Add ( new Member ( ) { Username = i.toString ( ) , IsActive = true } ) ; } using ( var scope = new TransactionScope ( ) ) { connection.Execute ( @ '' insert Member ( Username , IsActive ) values ( @ Username , @ IsActive ) '' , members ) ; scope.Complete ( ) ; } took 20 seconds . That 's 2500 inserts/second . Not bad , great either considering blog achieving 45k inserts/second . Is efficient way Dapper ? Also , side note , running code Visual Studio debugger took **over 3 minutes ! ** I figured debugger would slow little , I really surprised see much . **UPDATE** So using ( var scope = new TransactionScope ( ) ) { connection.Execute ( @ '' insert Member ( Username , IsActive ) values ( @ Username , @ IsActive ) '' , members ) ; scope.Complete ( ) ; } connection.Execute ( @ '' insert Member ( Username , IsActive ) values ( @ Username , @ IsActive ) '' , members ) ; took 20 seconds . But took 4 seconds ! SqlTransaction trans = connection.BeginTransaction ( ) ; connection.Execute ( @ '' insert Member ( Username , IsActive ) values ( @ Username , @ IsActive ) '' , members , transaction : trans ) ; trans.Commit ( ) ;","The best I able achieve 50k records 4 seconds using approach SqlTransaction trans = connection.BeginTransaction ( ) ; connection.Execute ( @ '' insert Member ( Username , IsActive ) values ( @ Username , @ IsActive ) '' , members , transaction : trans ) ; trans.Commit ( ) ;"
Stackoverflow,"From dataframe like test < - data.frame ( 'id'= rep ( 1:5,2 ) , 'string'= LETTERS [ 1:10 ] ) test < - test [ order ( test $ id ) , ] rownames ( test ) < - 1:10 > test id string 1 1 A 2 1 F 3 2 B 4 2 G 5 3 C 6 3 H 7 4 D 8 4 I 9 5 E 10 5 J I want create new one first row id / string pair . If sqldf accepted R code within , query could look like : res < - sqldf ( `` select id , min ( rownames ( test ) ) , string test group id , string '' ) > res id string 1 1 A 3 2 B 5 3 C 7 4 D 9 5 E Is solution short creating new column like test $ row < - rownames ( test ) running sqldf query min ( row ) ?","You use ` duplicated ` quickly . test [ ! duplicated ( test $ id ) , ] Benchmarks , speed freaks : ju < - function ( ) test [ ! duplicated ( test $ id ) , ] gs1 < - function ( ) do.call ( rbind , lapply ( split ( test , test $ id ) , head , 1 ) ) gs2 < - function ( ) do.call ( rbind , lapply ( split ( test , test $ id ) , ` [ ` , 1 , ) ) jply < - function ( ) ddply ( test , . ( id ) , function ( x ) head ( x,1 ) ) jdt < - function ( ) { testd < - as.data.table ( test ) setkey ( testd , id ) # Initial solution ( slow ) # testd [ , lapply ( .SD , function ( x ) head ( x,1 ) ) , = key ( testd ) ] # Faster options : testd [ ! duplicated ( id ) ] # ( 1 ) # testd [ , .SD [ 1L ] , by=key ( testd ) ] # ( 2 ) # testd [ J ( unique ( id ) ) , mult= '' first '' ] # ( 3 ) # testd [ testd [ , .I [ 1L ] , by=id ] ] # ( 4 ) needs v1.8.3 . Allows 2nd , 3rd etc } library ( plyr ) library ( data.table ) library ( rbenchmark ) # sample data set.seed ( 21 ) test < - data.frame ( id=sample ( 1e3 , 1e5 , TRUE ) , string=sample ( LETTERS , 1e5 , TRUE ) ) test < - test [ order ( test $ id ) , ] benchmark ( ju ( ) , gs1 ( ) , gs2 ( ) , jply ( ) , jdt ( ) , replications=5 , order= '' relative '' ) [ ,1:6 ] # test replications elapsed relative user.self sys.self # 1 ju ( ) 5 0.03 1.000 0.03 0.00 # 5 jdt ( ) 5 0.03 1.000 0.03 0.00 # 3 gs2 ( ) 5 3.49 116.333 2.87 0.58 # 2 gs1 ( ) 5 3.58 119.333 3.00 0.58 # 4 jply ( ) 5 3.69 123.000 3.11 0.51 Let 's try , contenders first heat data replications . set.seed ( 21 ) test < - data.frame ( id=sample ( 1e4 , 1e6 , TRUE ) , string=sample ( LETTERS , 1e6 , TRUE ) ) test < - test [ order ( test $ id ) , ] benchmark ( ju ( ) , jdt ( ) , order= '' relative '' ) [ ,1:6 ] # test replications elapsed relative user.self sys.self # 1 ju ( ) 100 5.48 1.000 4.44 1.00 # 2 jdt ( ) 100 6.92 1.263 5.70 1.15"
Stackoverflow,"Here 's I 've done far x64 OS : * Installed Python ( v2.7 -- specifically 2.7.6 ) added system path ( C : \Python27 ) * Installed MS VS C++ 2010 Express Version ( I already VS 2012 without C++ component ) * Installed compiler update Windows SDK 7.1 * Successfully executed node-gyp configure ( add-on directory nodejs\node_modules binding.gyp located ) * ran node-gyp build ( administrator ) ** This crashed , leaving : error : C : \Program Files\nodejs\node_modules\msnodesql > node-gyp build gyp info worked ends ok gyp info using node-gyp @ 0.12.2 gyp info using node @ 0.10.25 | win32 | x64 gyp info spawn C : \Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe gyp info spawn args [ 'build/binding.sln ' , gyp info spawn args '/clp : Verbosity=minimal ' , gyp info spawn args '/nologo ' , gyp info spawn args '/p : Configuration=Release ; Platform=x64 ' ] Building projects solution one time . To enable parallel build , please add `` /m '' switch . **LINK : fatal error LNK1181 : open input file 'kernel32.lib' [ C : \Program Files\nodejs\node_modules\msnodesql\build\sqlserver.vcxproj ] ** gyp ERR ! build error gyp ERR ! stack Error : ` C : \Windows\Microsoft.NET\Framework\v4.0.30319\msbuild.exe ` failed exit code : 1 gyp ERR ! stack ChildProcess.onExit ( C : \Users\RNelson\AppData\Roaming\npm\node_modules\node-gyp\lib\build.js:267:23 ) gyp ERR ! stack ChildProcess.EventEmitter.emit ( events.js:98:17 ) gyp ERR ! stack Process.ChildProcess._handle.onexit ( child_process.js:797:12 ) gyp ERR ! System Windows_NT 6.1.7601 gyp ERR ! command `` node '' `` C : \\Users\\RNelson\\AppData\\Roaming\\npm\\node_modules\\node- gyp\\bin\\node-gyp.js '' `` build '' gyp ERR ! cwd C : \Program Files\nodejs\node_modules\msnodesql gyp ERR ! node -v v0.10.25 gyp ERR ! node-gyp -v v0.12.2 gyp ERR ! ok Any ideas going ? Thanks advance ! Just trying use node-sqlserver MS driver Node.js",I similar problem . I found switch helped -- msvs_version=2012 example npm install -- msvs_version=2012 < package >
Stackoverflow,"I 'm adding table : CREATE TABLE contenttype ( contenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT , class VARBINARY ( 50 ) NOT NULL , packageid INT UNSIGNED NOT NULL , canplace ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , cansearch ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , cantag ENUM ( ' 0 ' , ' 1 ' ) DEFAULT ' 0 ' , canattach ENUM ( ' 0 ' , ' 1 ' ) DEFAULT ' 0 ' , isaggregator ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , PRIMARY KEY ( contenttypeid ) , UNIQUE KEY packageclass ( packageid , class ) ) ; And I get 1050 `` table already exists '' But table NOT exist . Any ideas ? EDIT : details everyone seems believe : ) DESCRIBE contenttype yields : > 1146 - Table 'gunzfact_vbforumdb.contenttype ' n't exist CREATE TABLE gunzfact_vbforumdb.contenttype ( contenttypeid INT UNSIGNED NOT NULL AUTO_INCREMENT , class VARBINARY ( 50 ) NOT NULL , packageid INT UNSIGNED NOT NULL , canplace ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , cansearch ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , cantag ENUM ( ' 0 ' , ' 1 ' ) DEFAULT ' 0 ' , canattach ENUM ( ' 0 ' , ' 1 ' ) DEFAULT ' 0 ' , isaggregator ENUM ( ' 0 ' , ' 1 ' ) NOT NULL DEFAULT ' 0 ' , PRIMARY KEY ( contenttypeid ) , Yields : > 1050 - Table 'contenttype ' already exists","Sounds like [ Schroedinger's table ] ( http : //en.wikipedia.org/wiki/Schr % C3 % B6dinger's_cat ) ... Seriously , probably broken table . Try : * ` DROP TABLE IF EXISTS contenttype ` * ` REPAIR TABLE contenttype ` * If sufficient permissions , delete data files ( /mysql/data/db_name )"
Stackoverflow,"In seems odd turn , I trying create Sql Database project Visual Studio 2012 , getting nutty error : ! [ enter image description ] ( https : //i.stack.imgur.com/CRDtb.png ) And clicking either link leads Page Not Found Microsoft.com , odd . From bare text error message , ( search purposes ) : > **Unable open Database Project** > > This version SQL Server Data Tools compatible database runtime components installed computer . Considering I Sql Server 2012 Developer Edition installed workstation , seems incredible . I open create database project VS2008 Sql Server 2012 DE installed , VS 2012 ?","I Installed SQL Server 2012 Service pack 1 yesterday I started getting problem describe Visual Studio 2012 . Not database projects ; I could use SQL Server Object Explorer , open sql-scripts lots weird database related errors . Always message : '' This version SQL Server Data Tools compatible ... bla , bla , bla ... '' This solution helped : < https : //web.archive.org/web/20150315060703/http : //blog.wharton.com.au/2012/11/16/sql- server-2012-sp-1-breaks-sql-server-database-projects/ > Hope help **Update : ** With _March 2013 Release_ seems also update SQL Server Data Tools available [ ] ( http : //msdn.microsoft.com/en- us/jj650015 ) See comments [ post ] ( http : //blogs.msdn.com/b/ssdt/archive/2013/03/06/sql-server-data-tier- application-framework-march-2013-available.aspx ) details ."
Stackoverflow,"If schedule SQL Server job run every X number minutes , finish previous call # minutes , skip run since already running , run two instances job steps ?",Try something like : DECLARE @ jobId binary ( 16 ) SELECT @ jobId = job_id FROM msdb.dbo.sysjobs WHERE ( name = N'Name Your Job ' ) IF ( @ jobId IS NOT NULL ) BEGIN EXEC msdb.dbo.sp_delete_job @ jobId END DECLARE @ ReturnCode int EXEC @ ReturnCode = msdb.dbo.sp_add_job @ job_name=N'Name Your Job' Best read docs parameters required [ 'sp_add_job ' ] ( http : //msdn.microsoft.com/en-us/library/ms182079.aspx ) [ 'sp_delete_job ' ] ( http : //msdn.microsoft.com/en-us/library/ms188376.aspx )
Stackoverflow,"In MySQL 5.0 following error occur trying create view subquery FROM clause ? > ERROR 1349 ( HY000 ) : View 's SELECT contains subquery FROM clause If limitation MySQL engine , n't implemented feature yet ? Also , good workarounds limitation ? Are workarounds work subquery FROM clause queries expressed without using subquery FROM clause ? * * * An example query ( buried comment ) : SELECT temp.UserName FROM ( SELECT u1.name UserName , COUNT ( m1.UserFromId ) SentCount FROM Message m1 , User u1 WHERE u1.uid = m1.UserFromId Group BY u1.name HAVING SentCount > 3 ) temp","I problem . I wanted create view show information recent year , table records 2009 2011 . Here 's original query : SELECT . * FROM JOIN ( SELECT a.alias , MAX ( a.year ) max_year FROM GROUP BY a.alias ) b ON a.alias=b.alias a.year=b.max_year Outline solution : 1. create view subquery 2. replace subqueries views Here 's solution query : CREATE VIEW v_max_year AS SELECT alias , MAX ( year ) max_year FROM GROUP BY a.alias ; CREATE VIEW v_latest_info AS SELECT . * FROM JOIN v_max_year b ON a.alias=b.alias a.year=b.max_year ; It works fine mysql 5.0.45 , without much speed penalty ( compared executing original sub-query select without views ) ."
Stackoverflow,"I trying install pymssql ubuntu 12.04 using pip . This error I getting . Any help would greatly appreciated I completely lost ! Tried googling unfortunately avail ... Downloading pymssql-2.0.0b1-dev-20130403.tar.gz ( 2.8Mb ) : 2.8Mb downloaded Running setup.py egg_info package pymssql warning : files found matching '*.pyx ' directory 'Cython/Debugger/Tests' warning : files found matching '*.pxd ' directory 'Cython/Debugger/Tests' warning : files found matching '*.h ' directory 'Cython/Debugger/Tests' warning : files found matching '*.pxd ' directory 'Cython/Utility' Compiling module Cython.Plex.Scanners ... Compiling module Cython.Plex.Actions ... Compiling module Cython.Compiler.Lexicon ... Compiling module Cython.Compiler.Scanning ... Compiling module Cython.Compiler.Parsing ... Compiling module Cython.Compiler.Visitor ... Compiling module Cython.Compiler.FlowControl ... Compiling module Cython.Compiler.Code ... Compiling module Cython.Runtime.refnanny ... Installed /home/radek/build/pymssql/Cython-0.19.1-py2.7-linux-x86_64.egg cc -c /tmp/clock_gettimeh7sDgX.c -o tmp/clock_gettimeh7sDgX.o cc tmp/clock_gettimeh7sDgX.o -lrt -o a.out warning : files found matching 'win32/freetds.zip' Installing collected packages : pymssql Running setup.py install pymssql skipping '_mssql.c ' Cython extension ( up-to-date ) building '_mssql ' extension gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/radek/build/pymssql/freetds/nix_64/include -I/usr/include/python2.7 -c _mssql.c -o build/temp.linux-x86_64-2.7/_mssql.o -Wno-parentheses-equality -DMSDBLIB gcc -pthread -shared -Wl , -O1 -Wl , -Bsymbolic-functions -Wl , -Bsymbolic-functions -Wl , -z , relro build/temp.linux-x86_64-2.7/_mssql.o -L/home/radek/build/pymssql/freetds/nix_64/lib -lsybdb -lct -lrt -o build/lib.linux-x86_64-2.7/_mssql.so /usr/bin/ld : find -lct collect2 : ld returned 1 exit status error : command 'gcc ' failed exit status 1 Complete output command /usr/bin/python -c `` import setuptools ; __file__='/home/radek/build/pymssql/setup.py ' ; exec ( compile ( open ( __file__ ) .read ( ) .replace ( '\r\n ' , '\n ' ) , __file__ , 'exec ' ) ) '' install -- single-version-externally-managed -- record /tmp/pip-Et_P1_-record/install-record.txt : running install running build running build_ext skipping '_mssql.c ' Cython extension ( up-to-date ) building '_mssql ' extension creating build creating build/temp.linux-x86_64-2.7 gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/radek/build/pymssql/freetds/nix_64/include -I/usr/include/python2.7 -c _mssql.c -o build/temp.linux-x86_64-2.7/_mssql.o -Wno-parentheses-equality -DMSDBLIB creating build/lib.linux-x86_64-2.7 gcc -pthread -shared -Wl , -O1 -Wl , -Bsymbolic-functions -Wl , -Bsymbolic-functions -Wl , -z , relro build/temp.linux-x86_64-2.7/_mssql.o -L/home/radek/build/pymssql/freetds/nix_64/lib -lsybdb -lct -lrt -o build/lib.linux-x86_64-2.7/_mssql.so /usr/bin/ld : find -lct collect2 : ld returned 1 exit status error : command 'gcc ' failed exit status 1 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Command /usr/bin/python -c `` import setuptools ; __file__='/home/radek/build/pymssql/setup.py ' ; exec ( compile ( open ( __file__ ) .read ( ) .replace ( '\r\n ' , '\n ' ) , __file__ , 'exec ' ) ) '' install -- single-version-externally-managed -- record /tmp/pip-Et_P1_-record/install-record.txt failed error code 1","Make sure ` python-dev ` package installed ( needed compile packages native bindings ( thanks @ ravihemnani ) . Then 'll need install FreeTDS development package ( ` freetds-dev ` ) trying install ` pymssql ` pip : $ sudo apt-get install freetds-dev , _virtualenv_ wherever wish install : $ pip install pymssql"
Stackoverflow,"So something know , played mind I first seen it.. I know ` mysql_escape_string ` deprecated 5.3 actual difference ` mysql_real_escape_string ` . What I thought ` mysql_real_escape_string ` exact ` mysql_escape_string ` apart ` mysql_real_escape_string ` takes second argument mysql resource . I thought well surly must difference strings handled would need 2 functions . So I thought difference purely locale character encodings . ? anyone clear ?","The difference ` mysql_escape_string ` treats string raw bytes , adds escaping believes 's appropriate . ` mysql_real_escape_string ` , hand , uses information character set used MySQL connection . This means string escaped treating multi-byte characters properly ; i.e. , wo n't insert escaping characters middle character . This need connection ` mysql_real_escape_string ` ; 's necessary order know string treated . However , instead escaping , 's better idea use parameterized queries MySQLi library ; previously bugs escaping routine , 's possible could appear . Parameterizing query much , much harder mess , 's less likely get compromised MySQL bug ."
Stackoverflow,"In website , I using MySQL database . I using webservice I database related manipulations . Now In one methods webservice , I get following Error . > select command denied user `` @ '' table `` What could wrong ? Below code I get error . I tried debugging found fails line ` MySqlDataReader result1 = command1.ExecuteReader ( ) ; ` Here code : String addSQL = `` Select Max ( ` TradeID ` ) ` jsontest ` . ` tbl_Positions '' ; MySqlConnection objMyCon = new MySqlConnection ( strProvider ) ; objMyCon.Open ( ) ; MySqlCommand command = objMyCon.CreateCommand ( ) ; command.CommandText = addSQL ; MySqlDataReader result = command.ExecuteReader ( ) ; //int j = command.ExecuteNonQuery ( ) ; ( result.Read ( ) ) { MaxTradeID = Convert.ToInt32 ( result [ 0 ] ) ; } objMyCon.Close ( ) ; ( = 1 ; < = MaxTradeID ; i++ ) { String newSQL = `` Select ` Strike ` , ` LongShort ` , ` Current ` , ` TPLevel ` , ` SLLevel ` ` json ` . ` tbl_Position ` ` TradeID ` = ' '' + + `` ' '' ; MySqlConnection objMyCon1 = new MySqlConnection ( strProvider ) ; objMyCon1.Open ( ) ; MySqlCommand command1 = objMyCon1.CreateCommand ( ) ; command1.CommandText = newSQL ; MySqlDataReader result1 = command1.ExecuteReader ( ) ; objMyCon2.Close ( ) ;","I 'm sure original poster 's issue long since resolved . However , I issue , I thought I 'd explain causing problem . I union query two tables -- 'foo ' 'foo_bar ' . However , SQL statement , I typo : 'foo.bar' So , instead telling 'foo.bar ' table n't exist , error message indicates command denied -- though I n't permissions . Hope helps someone ."
Stackoverflow,"Maybe 's obvious question , I want sure . How know MySQLnd active driver ? I 'm runing PHP 5.3 MySQL 5.1.37 . In phpinfo ( ) mysqlnd listed I ca n't sure I 'm using MySQLnd old driver ... Extract phpinfo ( ) output mysql MySQL Support enabled Active Persistent Links 0 Active Links 0 Client API version mysqlnd 5.0.5-dev - 081106 - $ Revision : 1.3.2.27 $ mysqli MysqlI Support enabled Client API library version mysqlnd 5.0.5-dev - 081106 - $ Revision : 1.3.2.27 $ Active Persistent Links 0 Inactive Persistent Links 0 Active Links 26 mysqlnd mysqlnd enabled Version mysqlnd 5.0.5-dev - 081106 - $ Revision : 1.3.2.27 $ PDO PDO support enabled PDO drivers mysql pdo_mysql PDO Driver MySQL enabled Client API version mysqlnd 5.0.5-dev - 081106 - $ Revision : 1.3.2.27 $ I 'm using PDO , PDO driver says mysql ...","This trick : < ? php $ mysqlnd = function_exists ( 'mysqli_fetch_all ' ) ; ( $ mysqlnd ) { echo 'mysqlnd enabled ! ' ; } To detect active PDO driver , create MySQL PDO object : ( strpos ( $ pdo- > getAttribute ( PDO : :ATTR_CLIENT_VERSION ) , 'mysqlnd ' ) ! == false ) { echo 'PDO MySQLnd enabled ! ' ; }"
Stackoverflow,"I ran ` aptitude install php5-mysql ` ( restarted MySQL/Apache 2 ) , I still getting error : > Fatal error : Call undefined function mysql_connect ( ) /home/validate.php line 21 ` phpinfo ( ) ` says /etc/php5/apache2/conf.d/pdo_mysql.ini file parsed .","I see tagged Ubuntu . Most likely MySQL driver ( possibly MySQL ) installed . Assuming [ SSH ] ( http : //en.wikipedia.org/wiki/Secure_Shell ) terminal access sudo permissions , log server run : sudo apt-get install mysql-server mysql-client php5-mysql If MySQL packages php5-mysql package already installed , update . * * * **UPDATE** Since answer still gets occasional click I going update include **PHP 7** . PHP 7 requires different package MySQL want use different argument apt-get command . sudo apt-get install mysql-server mysql-common php7.0 php7.0-mysql And importantly , ` mysql_connect ( ) ` deprecated since PHP v5.5.0 . Refer official documentation : [ PHP : mysql_connect ( ) ] ( http : //php.net/manual/en/function.mysql-connect.php )"
Stackoverflow,"I 'm using ResultSet Java , sure properly close . I'm considering using ResultSet construct HashMap closing ResultSet . Is HashMap technique efficient , efficient ways handling situation ? I need keys values , using HashMap seemed like logical choice . If using HashMap efficient method , I construct use HashMap code ? Here 's I 've tried : public HashMap resultSetToHashMap ( ResultSet rs ) throws SQLException { ResultSetMetaData md = rs.getMetaData ( ) ; int columns = md.getColumnCount ( ) ; HashMap row = new HashMap ( ) ; ( rs.next ( ) ) { ( int = 1 ; < = columns ; i++ ) { row.put ( md.getColumnName ( ) , rs.getObject ( ) ) ; } } return row ; }","1 . Iterate ResultSet 2 . Create new Object row , store fields need 3 . Add new object ArrayList Hashmap whatever fancy 4 . Close ResultSet , Statement DB connection Done EDIT : posted code , I made changes . public List resultSetToArrayList ( ResultSet rs ) throws SQLException { ResultSetMetaData md = rs.getMetaData ( ) ; int columns = md.getColumnCount ( ) ; ArrayList list = new ArrayList ( 50 ) ; ( rs.next ( ) ) { HashMap row = new HashMap ( columns ) ; ( int i=1 ; < =columns ; ++i ) { row.put ( md.getColumnName ( ) , rs.getObject ( ) ) ; } list.add ( row ) ; } return list ; }"
Stackoverflow,"What differences query , native query , named query typed query ? Does 'alone-standing ' query even exist , abbreviation ? In mind , native Query query written simple sql , whereas named query relates entities ( hibernate-mapping ) . Can someone explain briefly ?","You could maybe use result transformer . Quoting [ Hibernate 3.2 : Transformers HQL SQL ] ( http : //in.relation.to/2006/03/17/hibernate-32-transformers- for-hql-and-sql/ ) : > # # # SQL Transformers > > With native sql returning non-entity beans Map 's often useful instead basic ` Object [ ] ` . With result transformers possible . > > > List resultWithAliasedBean = s.createSQLQuery ( > `` SELECT st.name studentName , co.description courseDescription `` + > `` FROM Enrolment e `` + > `` INNER JOIN Student st e.studentId=st.studentId `` + > `` INNER JOIN Course co e.courseCode=co.courseCode '' ) > .addScalar ( `` studentName '' ) > .addScalar ( `` courseDescription '' ) > .setResultTransformer ( Transformers.aliasToBean ( StudentDTO.class ) ) > .list ( ) ; > > StudentDTO dto = ( StudentDTO ) resultWithAliasedBean.get ( 0 ) ; > > > Tip : ` addScalar ( ) ` calls required HSQLDB make match property name since returns column names uppercase ( e.g . `` STUDENTNAME '' ) . This could also solved custom transformer search property names instead using exact match - maybe provide fuzzyAliasToBean ( ) method ; ) # # # References * Hibernate Reference Guide * [ 16.1.5 . Returning non-managed entities ] ( http : //docs.jboss.org/hibernate/core/3.3/reference/en/html/querysql.html # d0e13904 ) * Hibernate 's Blog * [ Hibernate 3.2 : Transformers HQL SQL ] ( http : //in.relation.to/2006/03/17/hibernate-32-transformers-for-hql-and-sql/ )"
Stackoverflow,"I trying use SQLClient library ASP.net Core cant seem get working . I found article online advising setup working : < http : //blog.developers.ba/using-classic-ado-net-in-asp-net- vnext/ > I simple console application package . My project.json looks like : { `` version '' : `` 1.0.0-* '' , `` description '' : `` DBTest Console Application '' , `` authors '' : [ `` '' ] , `` tags '' : [ `` '' ] , `` projectUrl '' : `` '' , `` licenseUrl '' : `` '' , `` compilationOptions '' : { `` emitEntryPoint '' : true } , `` dependencies '' : { `` System.Data.Common '' : `` 4.0.1-beta-23516 '' , `` System.Data.SqlClient '' : `` 4.0.0-beta-23516 '' } , `` commands '' : { `` DBTest '' : `` DBTest '' } , `` frameworks '' : { `` dnx451 '' : { } , `` dnxcore50 '' : { `` dependencies '' : { `` Microsoft.CSharp '' : `` 4.0.1-beta-23516 '' , `` System.Collections '' : `` 4.0.11-beta-23516 '' , `` System.Console '' : `` 4.0.0-beta-23516 '' , `` System.Linq '' : `` 4.0.1-beta-23516 '' , `` System.Threading '' : `` 4.0.11-beta-23516 '' } } } } And I try following code : using System ; using System.Data.SqlClient ; namespace DBTest { public class Program { public static void Main ( string [ ] args ) { using ( SqlConnection con = new SqlConnection ( ConnStr ) ) { con.Open ( ) ; try { using ( SqlCommand command = new SqlCommand ( `` SELECT * FROM SAMPLETABLE '' , con ) ) { command.ExecuteNonQuery ( ) ; } } catch { Console.WriteLine ( `` Something went wrong '' ) ; } } Console.Read ( ) ; } } } But get following errors : [ ! [ enter image description ] ( https : //i.stack.imgur.com/3psE9.png ) ] ( https : //i.stack.imgur.com/3psE9.png ) Anyone else got working ?","I think may missed part tutorial : > Instead referencing System.Data System.Data.SqlClient need grab Nuget : > > System.Data.Common System.Data.SqlClient . > > Currently creates dependency project.json – > aspnetcore50 section two libraries . > > > `` aspnetcore50 '' : { > `` dependencies '' : { > `` System.Runtime '' : `` 4.0.20-beta-22523 '' , > `` System.Data.Common '' : `` 4.0.0.0-beta-22605 '' , > `` System.Data.SqlClient '' : `` 4.0.0.0-beta-22605 '' > } > } > Try getting System.Data.Common System.Data.SqlClient **via Nuget** see adds dependencies , nutshell missing **System.Runtime . **"
Stackoverflow,"I 'm running PyDev Eclipse ... import pymysql conn = pymysql.connect ( host='localhost ' , port=3306 , user='userid ' , passwd='password ' , db='fan ' ) cur = conn.cursor ( ) print `` writing db '' cur.execute ( `` INSERT INTO cbs_transactions ( leagueID ) VALUES ( 'test val ' ) '' ) print `` wrote db '' The result , top Console says C : ... test.py , Console : writing db wrote db So 's terminating execute command . But I look table MySQL 's empty . A record get inserted . First , n't writing record . Second , I see log error see happened . Usually kind error red code fails .",Did commit ? ` conn.commit ( ) `
Stackoverflow,"I integrating SqlCacheDependency use LinqToSQL datacontext . I using extension class Linq querys found - < http : //code.msdn.microsoft.com/linqtosqlcache > I wired code I open page I get exception - '' The SQL Server Service Broker current database enabled , result query notifications supported . Please enable Service Broker database wish use notifications . '' coming event global.asax protected void Application_Start ( ) { RegisterRoutes ( RouteTable.Routes ) ; //In Application Start Event System.Data.SqlClient.SqlDependency.Start ( new dataContextDataContext ( ) .Connection.ConnectionString ) ; } question ... 1. enable Service Broker SQL server 2008 database ? I tried run query.. ALTER DATABASE tablename SET ENABLE_BROKER never ends runs ever , I manually stop . 2. I set SQL server 2008 , filter DataContext , I need configure something ? thanks help Truegilly","In case anyone else looking solution problem , following command worked great . It releases connections database instead waiting . ALTER DATABASE [ DBNAME ] SET ENABLE_BROKER WITH ROLLBACK IMMEDIATE"
Stackoverflow,"After installing release version VS2012 , I unable find SQL CLR proect template . How I go creating project type . Thanks !","Try CREATE FUNCTION NormalizeString ( @ nvarchar ( max ) , @ normalizationForm nvarchar ( 50 ) ) RETURNS nvarchar ( max ) AS EXTERNAL NAME CLRFunctions . [ CLRFunctions.T ] .NormalizeString"
Stackoverflow,Can login MySQL database fresh install root ID empty/no password like older MySQL versions,"After installed MySQL-community-server 5.7 fresh linux , need find temporary password /var/log/mysqld.log login root . 1 . ` grep 'temporary password ' /var/log/mysqld.log ` 2 . Run ` mysql_secure_installation ` change new password ref : < http : //dev.mysql.com/doc/refman/5.7/en/linux-installation-yum-repo.html >"
Stackoverflow,"I backup script MySQL database , using ` mysqldump ` ` -- tab ` option produces ` .sql ` file structure ` .txt ` file ( pipe-separated ) content . Some tables foreign keys , I import I 'm getting error : > ERROR 1217 ( 23000 ) line 8 : Can delete update parent row : foreign key constraint fails I know using ` SET FOREIGN_KEY_CHECKS=0 ` ( ` SET FOREIGN_KEY_CHECKS=1 ` afterward ) . If I add ` .sql ` file import works . But obviously next ` mysqldump ` get overwritten . I also tried running separate command , like error comes back : echo `` SET FOREIGN_KEY_CHECKS=0 '' | mysql [ user/pass/database ] [ imports ] echo `` SET FOREIGN_KEY_CHECKS=1 '' | mysql [ user/pass/database ] Is way disable FK checks command line ?","The accepted answer RandomSeed could take long time ! Importing table ( drop later ) could wasteful depending size . For file created using mysqldump -u user -ppasswd -- opt -- routines DBname > DBdump.sql I currently get file 7GB , 6GB data log table I n't 'need ' ; reloading file takes couple hours . If I need reload ( development purposes , ever required live recovery ) I skim file thus : sed '/INSERT INTO ` TABLE_TO_SKIP ` /d ' DBdump.sql > reduced.sql And reload : mysql -u user -ppasswd DBname < reduced.sql This gives complete database , `` unwanted '' table created empty . If really n't want tables , simply drop empty tables load finishes . For multiple tables could something like : sed '/INSERT INTO ` TABLE1_TO_SKIP ` /d ' DBdump.sql | \ sed '/INSERT INTO ` TABLE2_TO_SKIP ` /d ' | \ sed '/INSERT INTO ` TABLE3_TO_SKIP ` /d ' > reduced.sql There IS 'gotcha ' - watch procedures dump might contain '' INSERT INTO TABLE_TO_SKIP '' ."
Stackoverflow,"I web application using sqlalchemy ( within Pylons ) . I need effiently change schema able change production version least daily basis , maybe , without losing data . I played little bit sqlalchemy-migrate week-end I would say gave bad impression . First **I think help migration two databases engines** ; 's something could probably done sqlalchemy alone . Second docs seem date . I change command-line options , like giving repository path command , could bug migrate . But worst thing `` manage.py **test** `` command . Not actually **modifies database** ( point clearly indicated documentation I ca n't blame migrate ) first migration script made plain stupid schema migration , leaving upgraded-downgraded db **different schema original** . But `` manage.py test '' answered something like success ! That , even check schema left coherent state . So **is worth using migrate ? ** Is advantage compared Do It Yourself method associated good practices [ proposed S.Lott ] ( https : //stackoverflow.com/questions/4165452/how-to-efficiently-manage- frequent-schema-changes-using-sqlalchemy/4165496 # 4165496 ) ? Are alternatives sqlalchemy-migrate actually simplifying migration process I trying use migrate bad _priori_ ( please show is't clearly superior creating CSV columns proposed link ) ? Many Thanks !","Use Alembic instead : < http : //pypi.python.org/pypi/alembic > Thanks comments , edited add reasoning -- It 's developed author SQLAlchemy , 's brand new well supported . I n't know enough sqlalchemy-migrate give good comparison . But I took quick read clear concise Alembic docs , got autogenerated migration working short time . Autogeneration : Not mode operation , choose , Alembic read application 's sqlalchemy configuration ( instance , declarative model classes set tables , constraints , mappings ) compare actual current state database , output Python script represents delta two . You pass script Alembic 's upgrade command go , differences resolved . A small amount editing migration script hand usually needed , 's ( ) nature migrations , ( b ) something want anyway make sure fully aware exact steps migration going perform run . Alembic brings DVCS-like ability way migrations tracked , . It makes really easy return past state db schema ."
Stackoverflow,"My current situation I application needs notified new data arrives database table . The data coming external source ( I control -- integration option ) . When new data arrives , application needs take certain actions -- basically query new data , handle , insert result local table , etc . I want avoid polling possible , data expected handled real time . That said , making sure data ever gets missed # 1 priority . My questions : 1 . Is SqlDependency generally considered reliable ? 2 . Do I need concerned race conditions , e.g . I handling one change another arrives ? 3 . What happens database gets rebooted ? Will app recover start receiving changes , I need fail-safe timer sort resubscribe notifications periodically ? 4 . Most articles I read topic address SQL Server 2005 . I using SQL Server 2008 R2 . Is newer technique preferred SqlDependency ? 5 . ( Edit ) Also , What application goes ? I guess I would query missed data start ?","The complete list I find ( [ ] ( http : //msdn.microsoft.com/en- us/library/ms181122.aspx ) ) follows : * The projected columns SELECT statement must explicitly stated , table names must qualified two-part names . Notice means tables referenced statement must database . * The statement may use asterisk ( _ ) table_name._ syntax specify columns . * The statement may use unnamed columns duplicate column names . * The statement must reference base table . * The statement must reference tables computed columns . * The projected columns SELECT statement may contain aggregate expressions unless statement uses GROUP BY expression . When GROUP BY expression provided , select list may contain aggregate functions COUNT_BIG ( ) SUM ( ) . However , SUM ( ) may specified nullable column . The statement may specify HAVING , CUBE , ROLLUP . * A projected column SELECT statement used simple expression must appear . * The statement must include PIVOT UNPIVOT operators . * The statement must include UNION , INTERSECT , EXCEPT operators . * The statement must reference view . * The statement must contain following : DISTINCT , COMPUTE COMPUTE BY , INTO . * The statement must reference server global variables ( @ @ variable_name ) . * The statement must reference derived tables , temporary tables , table variables . * The statement must reference tables views databases servers . * The statement must contain subqueries , outer joins , self-joins . * The statement must reference large object types : text , ntext , image . * The statement must use CONTAINS FREETEXT full-text predicates . * The statement must use rowset functions , including OPENROWSET OPENQUERY . * The statement must use following aggregate functions : AVG , COUNT ( * ) , MAX , MIN , STDEV , STDEVP , VAR , VARP . * The statement must use nondeterministic functions , including ranking windowing functions . * The statement must contain user-defined aggregates . * The statement must reference system tables views , including catalog views dynamic management views . * The statement must include FOR BROWSE information . * The statement must reference queue . * The statement must contain conditional statements change return results ( example , WHERE 1=0 ) . * The statement specify READPAST locking hint . * The statement must reference Service Broker QUEUE . * The statement must reference synonyms . * The statement must comparison expression based double/real data types . * The statement must use TOP expression . Additional reference ( ) : * [ Working Query Notifications ] ( http : //msdn.microsoft.com/en-us/library/ms130764.aspx )"
Stackoverflow,"This asked times I find resolution problem . Basically using mysqldump , built tool MySQL Workbench administration tool , I dump database using extended inserts , I get massive long lines data . I understand , speeds inserts inserting data one command ( especially InnoDB ) , formatting makes REALLY difficult actually look data dump file , compare two files diff tool storing version control etc . In case I storing version control use dump files keep track integration test database . Now I know I turn extended inserts , I get one insert per line , works , time restore dump file slower . My core problem OLD tool used use ( MySQL Administrator ) I dump file , basically thing FORMATS INSERT statement put one insert per line , still bulk inserts . So instead : INSERT INTO ` coupon_gv_customer ` ( ` customer_id ` , ` amount ` ) VALUES ( 887 , ' 0.0000 ' ) ,191607 , ' 1.0300 ' ) ; get : INSERT INTO ` coupon_gv_customer ` ( ` customer_id ` , ` amount ` ) VALUES ( 887 , ' 0.0000 ' ) , ( 191607 , ' 1.0300 ' ) ; No matter options I try , seem way able get dump like , really best worlds . Yes , take little space , situations need human read files , makes MUCH useful . Am I missing something way MySQLDump , gone backwards feature old ( deprecated ) MySQL Administrator tool longer available ?",Try use following option : **\ -- skip-extended-insert** It worked .
Stackoverflow,"I stored procedure . How I run example intervals 5 seconds ? Like routine eliminate data time-stamp older one day ? DROP PROCEDURE IF EXISTS ` delete_rows_links ` GO CREATE PROCEDURE delete_rows_links BEGIN DELETE activation_link FROM activation_link_password_reset WHERE TIMESTAMPDIFF ( DAY , ` time ` , NOW ( ) ) < 1 ; END GO","Events run scheduler , started default . Using ` SHOW PROCESSLIST ` possible check whether started . If , run command SET GLOBAL event_scheduler = ON ; run ."
Stackoverflow,"I 'm trying create simple table [ SQLFiddle ] ( http : //www.sqlfiddle.com ) Oracle database keep getting cryptic error : > Create script error . Here simple script failed : create table t1 ( f1 number , f2 number , f3 number ) ; Could anyone help ?","**UPDATE : ** All seems working . See [ answer Jake Feasel ] ( https : //stackoverflow.com/questions/35735405/is-sqlfiddle-broken- errors-for-oracle-sql-server/37646270 ? noredirect=1 # 44610587 ) ( creator SQL Fiddle ) . ~~To summarise info comments answer bring date writing : 1 . SQLFiddle broken year 2017-07-07 working [ Oracle 11g R2 ] ( http : //sqlfiddle.com/ # ! 4 ) . 2 . It also seems broken [ MS SQL Server 2008 ] ( http : //sqlfiddle.com/ # ! 3/d227f ) , [ MS SQL Server 2014 ] ( http : //sqlfiddle.com/ # ! 6/d227f ) [ MySQL 5.5 ] ( http : //sqlfiddle.com/ # ! 2 ) [ MySQL 5.6 ] ( http : //sqlfiddle.com/ # ! 9/dafdd ) working . _ ( Please comment changes I 'll update answer accordingly ) ._~~ **SQLFiddle Alternatives** 1 . < http : //rextester.com > options MySQL , Oracle , PostgreSQL SQL Server `` Language '' dropdown ( well ability save work online similar way SQLFiddle ) . 2 . < http : //db-fiddle.com > options MySQL 5.5 / 5.6 / 5.7 / 8.0 , PostgreSQL 9.4 / 9.5 / 9.6 / 10.0 SQLite 3.16 / 3.17 / 3.18 . 3 . < https : //dbfiddle.uk > options MariaDB 10.2 / 10.3 , MySQL 8.0 , Oracle 11g Release 2 , Postgres 8.4 / 9.4 / 9.5 / 9.6 / 10 / 11 , SQL Server 2012 / 2014 / 2016 / 2017 / 2017 ( Linux ) SQLite 3.8 / 3.16 ( well ability save work online similar way SQLFiddle ) . See discussion [ ] ( https : //dba.meta.stackexchange.com/questions/2686 ) . 4 . Suggested alternatives Oracle [ Oracle Application Express ] ( https : //apex.oracle.com/en/ ) [ Oracle Live SQL ] ( https : //livesql.oracle.com/apex/livesql/file/index.html ) . 5 . A MySQL alternative limited functionality [ sqlize.com ] ( http : //sqlize.com/ ) . 6 . A clunky MySQL alternative [ sqltest.net ] ( https : //sqltest.net/ ) ( find way around adverts ) . 7 . As last resort , I 've occasionally seen answers use [ Stack Exchange Data Explorer ] ( https : //data.stackexchange.com/stackoverflow/query/new ) create custom queries ( SQL Server 2016 ) n't based built-in tables . [ Disclaimer : I 'm sure 's really supposed used kind purpose . ]"
Stackoverflow,"I sure error ! # 1292 - Truncated incorrect DOUBLE value : I n't double value field data ! I wasted whole hour trying figure ! query INSERT INTO call_managment_system.contact_numbers ( account_id , contact_number , contact_extension , main_number , created_by ) SELECT ac.account_id , REPLACE ( REPLACE ( REPLACE ( REPLACE ( ta.phone_number , '- ' , `` ) , ' ' , `` ) , ' ) ' , `` ) , ' ( ' , '' ) AS Phone , IFNULL ( ta.ext , `` ) AS extention , ' 1 ' AS MainNumber , ' 2 ' AS created_by FROM cvsnumbers AS ta INNER JOIN accounts AS ac ON ac.company_code = ta.company_code WHERE LENGTH ( REPLACE ( REPLACE ( REPLACE ( REPLACE ( ta.phone_number , '- ' , `` ) , ' ' , `` ) , ' ) ' , `` ) , ' ( ' , '' ) ) = 10 show create table table results going CREATE TABLE ` contact_numbers ` ( ` number_id ` int ( 10 ) unsigned NOT NULL AUTO_INCREMENT , ` account_id ` int ( 10 ) unsigned NOT NULL DEFAULT ' 0 ' , ` person_id ` int ( 11 ) NOT NULL DEFAULT ' 0 ' , ` contact_number ` char ( 15 ) NOT NULL , ` contact_extension ` char ( 10 ) NOT NULL DEFAULT `` , ` contact_type ` enum ( 'Primary ' , 'Direct ' , 'Cell ' , 'Fax ' , 'Home ' , 'Reception ' , 'Office ' , 'TollFree ' ) NOT NULL DEFAULT 'Primary ' , ` contact_link ` enum ( 'Account ' , 'PDM ' , 'Other ' ) NOT NULL DEFAULT 'Account ' , ` status ` tinyint ( 1 ) NOT NULL DEFAULT ' 1 ' COMMENT ' 0 = inactive , 1=active ' , ` main_number ` tinyint ( 1 ) NOT NULL DEFAULT ' 0 ' COMMENT ' 1 = main phone number ' , ` created_on ` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP , ` created_by ` int ( 11 ) NOT NULL , ` modified_on ` datetime DEFAULT NULL , ` modified_by ` int ( 11 ) NOT NULL DEFAULT ' 0 ' , PRIMARY KEY ( ` number_id ` ) , KEY ` account_id ` ( ` account_id ` ) , KEY ` person_id ` ( ` person_id ` ) ) ENGINE=InnoDB AUTO_INCREMENT=534 DEFAULT CHARSET=utf8","This message means 're trying compare number string ` WHERE ` ` ON ` clause . In query , potential place could occurring ` ON ac.company_code = ta.company_code ` ; either make sure similar declarations , use explicit ` CAST ` convert number string . If turn ` strict ` mode , error turn warning ."
Stackoverflow,"I 've got short piece code originally created **SqlDataAdapter** object . Trying streamline calls little bit , I replaced **SqlDataAdapter** **SqlCommand** moved **SqlConnection** outside loop . Now , whenever I try edit rows data returned **DataTable** , I get **ReadOnlyException** thrown thrown . NOTE : I custom function retrieves employee 's full name based ID . For simplicity , I used `` John Doe '' example code demonstrate point . _ExampleQueryOld_ works **SqlDataAdapter** ; _ExampleQueryNew_ fails **ReadOnlyException** whenever I try write element **DataRow** : * _ExampleQueryOld_ This works issues : public static DataTable ExampleQueryOld ( string targetItem , string [ ] sqlQueryStrings ) { DataTable bigTable = new DataTable ( ) ; ( int = 0 ; < sqlQueryStrings.Length ; i++ ) { string sqlText = sqlQueryStrings [ ] ; DataTable data = new DataTable ( targetItem ) ; using ( SqlDataAdapter da = new SqlDataAdapter ( sqlText , Global.Data.Connection ) ) { try { da.Fill ( data ) ; } catch ( Exception err ) { Global.LogError ( _CODEFILE , err ) ; } } int rowCount = data.Rows.Count ; ( 0 < rowCount ) { int index = data.Columns.IndexOf ( GSTR.Employee ) ; ( int j = 0 ; j < rowCount ; j++ ) { DataRow row = data.Rows [ j ] ; row [ index ] = `` John Doe '' ; // This Version Works } bigTable.Merge ( data ) ; } } return bigTable ; } * _ExampleQueryNew_ This example throws ReadOnlyException : public static DataTable ExampleQueryNew ( string targetItem , string [ ] sqlQueryStrings ) { DataTable bigTable = new DataTable ( ) ; using ( SqlConnection conn = Global.Data.Connection ) { ( int = 0 ; < sqlQueryStrings.Length ; i++ ) { string sqlText = sqlQueryStrings [ ] ; using ( SqlCommand cmd = new SqlCommand ( sqlText , conn ) ) { DataTable data = new DataTable ( targetItem ) ; try { ( cmd.Connection.State == ConnectionState.Closed ) { cmd.Connection.Open ( ) ; } using ( SqlDataReader reader = cmd.ExecuteReader ( ) ) { data.Load ( reader ) ; } } catch ( Exception err ) { Global.LogError ( _CODEFILE , err ) ; } finally { ( ( cmd.Connection.State & ConnectionState.Open ) ! = 0 ) { cmd.Connection.Close ( ) ; } } int rowCount = data.Rows.Count ; ( 0 < rowCount ) { int index = data.Columns.IndexOf ( GSTR.Employee ) ; ( int j = 0 ; j < rowCount ; j++ ) { DataRow row = data.Rows [ j ] ; try { // ReadOnlyException thrown : `` Column 'index ' read . '' row [ index ] = `` John Doe '' ; } catch ( ReadOnlyException roErr ) { Console.WriteLine ( roErr.Message ) ; } } bigTable.Merge ( data ) ; } } } } return bigTable ; } Why I write **DataRow** element one case , ? Is **SqlConnection** still open **SqlDataAdapter** something behind scene ?","using ` DataAdapter.Fill ` load database schema , includes whether column primary key , whether column read-only . To load database schema , use ` DataAdapter.FillSchema ` , 's questions . using ` DataReader ` fill table loads schema . So , ` index ` column read-only ( probably 's primary key ) information loaded ` DataTable ` . Thereby preventing modifying data table . I think @ k3b got right ; setting ` ReadOnly = false ` , able write data table . foreach ( System.Data.DataColumn col tab.Columns ) col.ReadOnly = false ;"
Stackoverflow,"I need look rows falling particular time frame . select * TableA startdate > = '12-01-2012 21:24:00 ' startdate < = '12-01-2012 21:25:33' i.e . look rows timestamp precision seconds , I achieve ?","You need use to_timestamp ( ) convert string proper timestamp value : to_timestamp ( '12-01-2012 21:24:00 ' , 'dd-mm-yyyy hh24 : mi : ss ' ) If column type ` DATE ` ( also supports seconds ) , need use to_date ( ) to_date ( '12-01-2012 21:24:00 ' , 'dd-mm-yyyy hh24 : mi : ss ' ) To get condition use following : select * TableA startdate > = to_timestamp ( '12-01-2012 21:24:00 ' , 'dd-mm-yyyy hh24 : mi : ss ' ) startdate < = to_timestamp ( '12-01-2012 21:25:33 ' , 'dd-mm-yyyy hh24 : mi : ss ' ) You never need use ` to_timestamp ( ) ` column type ` timestamp' _Edit corrected typo_"
Stackoverflow,"I receipt logs model android app . Receipt hasMany logs . I made query group_by logs sortID grade . //recieve RecepitID query group logs final long forwardedId = ( long ) getIntent ( ) .getExtras ( ) .get ( String.valueOf ( `` recepitID '' ) ) ; List < Logs > logsList = new Select ( ) .from ( Logs.class ) .where ( `` Receipt = `` + forwardedId ) .groupBy ( `` SortID , Grade '' ) .execute ( ) ; This grouping works fine . Problem next . I need output like ( part red circle ) : [ ! [ enter image description ] ( https : //i.stack.imgur.com/l0Y8T.png ) ] ( https : //i.stack.imgur.com/l0Y8T.png ) I get like : [ ! [ enter image description ] ( https : //i.stack.imgur.com/d0OzX.png ) ] ( https : //i.stack.imgur.com/d0OzX.png ) And part code I done . public class LogsRecapitulation extends AppCompatActivity { private ListView mainListView ; private BaseAdapter listAdapter ; private TextView logsCount ; @ Override protected void onCreate ( Bundle savedInstanceState ) { super.onCreate ( savedInstanceState ) ; setContentView ( R.layout.recapitulation_listview ) ; mainListView = ( ListView ) findViewById ( R.id.ListViewItem ) ; //recieve RecepitID query group logs final long forwardedId = ( long ) getIntent ( ) .getExtras ( ) .get ( String.valueOf ( `` recepitID '' ) ) ; List < Logs > logsList = new Select ( ) .from ( Logs.class ) .where ( `` Receipt = `` + forwardedId ) .groupBy ( `` SortID , Grade '' ) .execute ( ) ; TextView result = ( TextView ) findViewById ( R.id.LogMassResult ) ; double sum = 0.0 ; ( int = 0 ; < logsList.size ( ) ; i++ ) { sum += logsList.get ( ) .getM3 ( ) ; } result.setText ( String.format ( `` % .2f m3 '' , sum ) ) ; ( int = 0 ; < logsList.size ( ) ; i++ ) { ( logsList.get ( ) .receipt.priceType.equals ( `` Na panju '' ) ) { TextView stumpPriceKN = ( TextView ) findViewById ( R.id.sumPriceKN ) ; double sumPricekn = 0.0 ; ( int j = 0 ; j < logsList.size ( ) ; j++ ) { sumPricekn += logsList.get ( j ) .price.stumpPrice_kn * logsList.get ( j ) .getM3 ( ) ; } stumpPriceKN.setText ( String.format ( `` % .2f KN '' , sumPricekn ) ) ; } else { TextView roadKN = ( TextView ) findViewById ( R.id.sumPriceKN ) ; double roadPrKn = 0.0 ; ( int j = 0 ; j < logsList.size ( ) ; j++ ) { roadPrKn += logsList.get ( j ) .price.roadPrice_kn * logsList.get ( j ) .getM3 ( ) ; } roadKN.setText ( String.format ( `` % .2f KN '' , roadPrKn ) ) ; } } ( int = 0 ; < logsList.size ( ) ; i++ ) { ( logsList.get ( ) .receipt.priceCorrection > 0 & & logsList.get ( ) .receipt.priceType.equals ( `` Na panju '' ) ) { TextView corecctionPriceKn = ( TextView ) findViewById ( R.id.correctionPriceKN ) ; double correcSumKN = 0.0 ; ( int j = 0 ; j < logsList.size ( ) ; j++ ) { correcSumKN += ( logsList.get ( j ) .price.stumpPrice_kn * logsList.get ( j ) .getM3 ( ) ) + ( ( logsList.get ( j ) .price.stumpPrice_kn * logsList.get ( j ) .getM3 ( ) ) * logsList.get ( j ) .receipt.priceCorrection / 100 ) ; } corecctionPriceKn.setText ( String.format ( `` % .2f KN '' , correcSumKN ) ) ; } else ( logsList.get ( ) .receipt.priceCorrection > 0 & & logsList.get ( ) .receipt.priceType.equals ( `` Šumska cesta '' ) ) { TextView corecctionPriceKn = ( TextView ) findViewById ( R.id.correctionPriceKN ) ; double correcSumKN = 0.0 ; ( int j = 0 ; j < logsList.size ( ) ; j++ ) { correcSumKN += ( logsList.get ( j ) .price.roadPrice_kn * logsList.get ( j ) .getM3 ( ) ) + ( ( logsList.get ( j ) .price.roadPrice_kn * logsList.get ( j ) .getM3 ( ) ) * logsList.get ( j ) .receipt.priceCorrection / 100 ) ; } corecctionPriceKn.setText ( String.format ( `` % .2f KN '' , correcSumKN ) ) ; } else { TextView priceHolder = ( TextView ) findViewById ( R.id.KorekcijaCijene ) ; TextView corecctionPriceKn = ( TextView ) findViewById ( R.id.correctionPriceKN ) ; priceHolder.setText ( `` '' ) ; corecctionPriceKn.setText ( `` '' ) ; } } listAdapter = new RecapitulationArrayAdapter ( logsList ) ; mainListView.setAdapter ( listAdapter ) ; //display logs count logsCount = ( TextView ) findViewById ( R.id.logsCount ) ; logsCount.setText ( String.valueOf ( logsList.size ( ) ) ) ; } private class RecapitulationArrayAdapter extends BaseAdapter { private LayoutInflater inflater ; private List < Logs > logsList ; public RecapitulationArrayAdapter ( List < Logs > logsList ) { inflater = LayoutInflater.from ( LogsRecapitulation.this ) ; this.logsList = logsList ; } @ Override public int getCount ( ) { return logsList.size ( ) ; } @ Override public Object getItem ( int position ) { return logsList.get ( position ) ; } @ Override public long getItemId ( int position ) { return logsList.get ( position ) .getId ( ) ; } @ Override public View getView ( int position , View convertView , ViewGroup parent ) { ( convertView == null ) { convertView = inflater.inflate ( R.layout.logs_recapitulation , parent , false ) ; } Logs log = logsList.get ( position ) ; ( ( TextView ) convertView.findViewById ( R.id.rec_log_sort ) ) .setText ( log.sort_id ) ; ( ( TextView ) convertView.findViewById ( R.id.rec_log_class ) ) .setText ( log.grade ) ; ( ( TextView ) convertView.findViewById ( R.id.rec_log_count ) ) .setText ( String.valueOf ( logsList.size ( ) ) ) ; ( ( TextView ) convertView.findViewById ( R.id.rec_logs_mass ) ) .setText ( String.format ( `` % .2f m3 '' , log.getM3 ( ) ) ) ; ( log.receipt.priceType.equals ( `` Na panju '' ) ) { ( ( TextView ) convertView.findViewById ( R.id.rec_log_price_default ) ) .setText ( String.valueOf ( log.price.stumpPrice_kn ) ) ; } else { ( ( TextView ) convertView.findViewById ( R.id.rec_log_price_default ) ) .setText ( String.valueOf ( log.price.roadPrice_kn ) ) ; } ( log.receipt.priceType.equals ( `` Na panju '' ) ) { ( ( TextView ) convertView.findViewById ( R.id.rec_calculated_price ) ) .setText ( String.format ( `` % .2f KN '' , log.price.stumpPrice_kn * log.getM3 ( ) ) ) ; } else { ( ( TextView ) convertView.findViewById ( R.id.rec_calculated_price ) ) .setText ( String.format ( `` % .2f KN '' , log.price.roadPrice_kn * log.getM3 ( ) ) ) ; } return convertView ; } } } I 'm using ActiveAndroid displaying listview . **_PROBLEM displaying grouped items . I ’ displaying BaseAdapter listView shows one item per group needs show multiple items ( 4 items group ) ._** **Can anyone give advice I get output like first image ? **","Correct code would : query.prepare ( QString ( `` UPDATE table SET % 1 = : value WHERE id = : id `` ) .arg ( column ) ) ; query.bindValue ( `` : value '' , value ) ; You bind values , fields names . P.S . : answered reading whole question . Yes , right - way use bindValues bind columns , sqlite , db ."
Stackoverflow,"I downloaded installed SQL Server 2014 Express ( site : < http : //www.microsoft.com/en-us/server-cloud/products/sql-server-editions/sql- server-express.aspx # Installation_Options > ) . The problem I ca n't connect/find local DB server , I can't develop DB local PC . How I reach local server ? My system consists Windows 8.1 ( Pro Enterprise editions ) 64 bits Checking configuration SQL Server ` SQL Server 2014 Configuration Manager ` tool , I see empty list selecting `` SQL Server Services '' tree left . Below find screenshot . ! [ enter image description ] ( https : //i.stack.imgur.com/UBKCY.jpg ) In Windows Services list , one service : ** '' SQL Server VSS Writer '' ** **EDIT** My installation window SQL Server 2014 following : ! [ enter image description ] ( https : //i.stack.imgur.com/HcVJd.jpg )","Most probably , n't install SQL Server Engine service . If SQL Server engine installed , service appear SQL Server Configuration Manager tool . Consider packages ` SQLManagementStudio_Architecture_Language.exe ` ` SQLEXPR_Architecture_Language.exe ` , available [ Microsoft site ] ( http : //www.microsoft.com/en-US/download/details.aspx ? id=42299 ) contain , respectively Management Studio GUI Tools SQL Server engine . If want full featured SQL Server installation , database engine Management Studio , [ download installer file **SQL Server Advanced Services** ] ( http : //www.microsoft.com/en- US/download/details.aspx ? id=42299 ) . Moreover , sample database order perform local tests , use [ Adventure Works database ] ( https : //msftdbprodsamples.codeplex.com/releases/view/125550 ) . Considering package **SQL Server Advanced Services** , beginning installation see something like ( screenshot SQL Server 2008 Express , feature selection similar ) . The checkbox next `` Database Engine Services '' must checked . In next steps , able configure instance settings options . Execute installation process select database engine services feature selection step . At end installation , able see SQL Server services SQL Server Configuration Manager . ! [ enter image description ] ( https : //i.stack.imgur.com/Kyaiu.png )"
Stackoverflow,I need know given Job currently running Ms SQL 2008 server . So invoke job may lead concurrency issues .,"It looks like use ` msdb.dbo.sysjobactivity ` , checking record non-null start_execution_date null stop_execution_date , meaning job started , yet completed . This would give currently running jobs : SELECT sj.name , sja . * FROM msdb.dbo.sysjobactivity AS sja INNER JOIN msdb.dbo.sysjobs AS sj ON sja.job_id = sj.job_id WHERE sja.start_execution_date IS NOT NULL AND sja.stop_execution_date IS NULL"
Stackoverflow,"I 'm using MySQL Query Browser ( part [ MySQL GUI Tools ] ( http : //dev.mysql.com/downloads/gui-tools/ ) ) need change field NULL , I ca n't figure - I delete value tries update ` `` ` . Typing `` NULL '' makes try update ` 'NULL ' ` ( string ) . I know I could write query , defeats entire purpose tool , ?","In MySQL Query Browser , right click cell select 'Clear field content ' focus another cell . In MySQL Workbench , right click cell select 'Set Field NULL ' ."
Stackoverflow,I pulled ` mysql-connector-python ` code I run ` python ./setup.py build ` I get following error : Unable find Protobuf include directory . ` pip install Protobuf ` useless How I solve problem ?,I found error occurs since version 2.2.3 . You avoid issue using version 2.1.6 . ` pip install mysql-connector==2.1.6 ` try .
Stackoverflow,"I 'm trying code : import sqlite connection = sqlite.connect ( 'cache.db ' ) cur = connection.cursor ( ) cur.execute ( `` 'create table item ( id integer primary key , itemno text unique , scancode text , descr text , price real ) ' '' ) connection.commit ( ) cur.close ( ) I 'm catching exception : Traceback ( recent call last ) : File `` cache_storage.py '' , line 7 , < module > scancode text , descr text , price real ) ' '' ) File `` /usr/lib/python2.6/dist-packages/sqlite/main.py '' , line 237 , execute self.con._begin ( ) File `` /usr/lib/python2.6/dist-packages/sqlite/main.py '' , line 503 , _begin self.db.execute ( `` BEGIN '' ) _sqlite.OperationalError : database locked Permissions cache.db ok. Any ideas ?","I 'm presuming actually using sqlite3 even though code says otherwise . Here things check : 1 . That n't hung process sitting file ( unix : ` $ fuser cache.db ` say nothing ) 2 . There n't cache.db-journal file directory cache.db ; would indicate crashed session n't cleaned properly . 3 . Ask database shell check : ` $ sqlite3 cache.db `` pragma integrity_check ; '' ` 4 . Backup database ` $ sqlite3 cache.db `` .backup cache.db.bak '' ` 5 . Remove cache.db probably nothing ( learning ) try code 6 . See backup works ` $ sqlite3 cache.db.bak `` .schema '' ` Failing , read [ Things That Can Go Wrong ] ( http : //www.sqlite.org/atomiccommit.html # sect_9_0 ) [ How Corrupt Your Database Files ] ( http : //www.sqlite.org/lockingv3.html # how_to_corrupt )"
Stackoverflow,Whats best browser tool HSQLDB databases ?,HSQL built-in GUI query tool called [ Database manager ] ( http : //hsqldb.org/doc/2.0/util-guide/dbm-chapt.html ) . In directory database files run : java -cp ../lib/hsqldb.jar org.hsqldb.util.DatabaseManager setting config : jdbc : hsqldb : file : databaseName Allowed edit I needed really convenient way .
Stackoverflow,I added following simple test event mysql database via phpmyadmin : CREATE DEFINER= ` root ` @ ` localhost ` EVENT ` my_event ` ON SCHEDULE EVERY 1 MINUTE STARTS '2013-05-27 00:00:00 ' ON COMPLETION NOT PRESERVE ENABLE DO BEGIN UPDATE ` test ` SET ` name ` = '' z '' ; END My environment mac + MAMP Pro . I expecting change rows 'test ' table name ' z ' within minute . But happening . Do I something additional get events start working ? Output `` SHOW PROCESSLIST '' : ! [ enter image description ] ( https : //i.stack.imgur.com/Xj2j9.png ) Thanks .,"Events run scheduler , started default . Using ` SHOW PROCESSLIST ` possible check whether started . If , run command SET GLOBAL event_scheduler = ON ; run ."
Stackoverflow,"I 'd like use RazorSQL connect database running remote server . I create SSH tunnel localhost following command : ssh -L 1111 : remote.server.com:5432 myuser @ remote.server.com I configure connection via RazorSQL 's GUI , specifying ` localhost ` host ` 1111 ` port . When I click `` Connect '' , following error message appears : ERROR : An error occurred trying make connection database : JDBC URL : jdbc : postgresql : //localhost:1111/myuser FATAL : pg_hba.conf entry host `` aaa.bbb.ccc.ddd '' , user `` myuser '' , database `` mydatabase '' , SSL ` aaa.bbb.ccc.ddd ` remote server 's IP address . What , I allowed change contents ` pg_hba.conf ` file . That 's look like moment : # TYPE DATABASE USER CIDR-ADDRESS METHOD @ remove-line-for-nolocal @ # `` local '' Unix domain socket connections @ remove-line-for-nolocal @ local @ authmethod @ # IPv4 local connections : host 127.0.0.1/32 @ authmethod @ # IPv6 local connections : host : :1/128 @ authmethod @ Is possible connect database server via SSH tunnel using current setup without modifying server 's configuration ?","Your pg_hba.conf appears permit connections localhost . The easiest way causing SSH tunnel connections appear localhost make **to** localhost . The following SSH command connects remote.example.com user `` user '' , causes ssh client listen localhost , port 1111/tcp . Any connections made port forwarded ssh tunnel , ssh server side connections made localhost , port 5432/tcp . Since we're connecting localhost , connections appear localhost also , match existing pg_hba.conf line . ssh -L 1111 : localhost:5432 user @ remote.example.com If expected long-running tunnel , I would recommend using [ autossh ] ( http : //www.harding.motd.ca/autossh/ `` autossh '' ) To connect using psql client host running ssh client , use something like : psql -h localhost -p 1111 -U your-db-username database-name You prompted database user 's password . Alternately , add line line following file called ` .pgpass ` home directory client 're running psql : localhost:1111 : database-name : your-db-user : your-db-password"
Stackoverflow,I 'm looking good explanation test Oracle stored procedure SQL Developer Embarcardero Rapid XE2 . Thank .,Something like create replace procedure my_proc ( p_rc OUT SYS_REFCURSOR ) begin open p_rc select 1 col1 dual ; end ; / variable rc refcursor ; exec my_proc ( : rc ) ; print rc ; work SQL*Plus SQL Developer . I n't experience Embarcardero Rapid XE2 I idea whether supports SQL*Plus commands like .
Stackoverflow,I 'm looking good explanation test Oracle stored procedure SQL Developer Embarcardero Rapid XE2 . Thank .,Something like create replace procedure my_proc ( p_rc OUT SYS_REFCURSOR ) begin open p_rc select 1 col1 dual ; end ; / variable rc refcursor ; exec my_proc ( : rc ) ; print rc ; work SQL*Plus SQL Developer . I n't experience Embarcardero Rapid XE2 I idea whether supports SQL*Plus commands like .
Stackoverflow,"I get Python work Postgresql I get work MySQL . The main problem shared hosting account I I ability install things Django PySQL , I generally fail installing computer maybe 's good I ca n't install host . I found [ bpgsql ] ( http : //barryp.org/software/bpgsql/ ) really good require install , 's single file I look , read call functions . Does anybody know something like MySQL ?","MySQLdb I used . If host using Python version 2.5 higher , support sqlite3 databases built ( sqlite allows relational database simply file filesystem ) . But buyer beware , sqlite suited production , may depend trying . Another option may call host complain , change hosts . Honestly days , self respecting web host supports python mysql ought MySQLdb pre installed ."
Stackoverflow,"lots questions problem , I could n't solve case . one please take look : I ` Office ` table one-many relationship ` Doctor ` ` Secretary ` tables . Both last tables , derived ` Employee ` table shared-primary-key relationship predefined ` Users ` table created ` sqlmembershipprovider ` . It seems many-many relationship ` Users ` table ` Roles ` table I n't hand . My problem creating ( zero , one ) - ( one ) relationship ` Employee ` table ` Users ` table I ended shared primary key relationship error raised , . ( Is better solution problem ? ) **here error : ** > Introducing FOREIGN KEY constraint 'FK_dbo.aspnet_UsersInRoles_dbo.aspnet_Users_UserId ' table 'aspnet_UsersInRoles ' may cause cycles multiple cascade paths . Specify ON DELETE NO ACTION ON UPDATE NO ACTION , modify FOREIGN KEY constraints . Could create constraint . See previous errors . **here codes membership codes reverse engineering : ** public class Office { public Office ( ) { this.Doctors = new HashSet < Doctor > ( ) ; this.Secretaries = new HashSet < Secretary > ( ) ; } [ Key ] public System.Guid OfficeId { get ; set ; } public virtual ICollection < Doctor > Doctors { get ; set ; } public virtual ICollection < Secretary > Secretaries { get ; set ; } } public class Employee { [ Key , ForeignKey ( `` User '' ) ] [ DatabaseGenerated ( DatabaseGeneratedOption.None ) ] public System.Guid Id { get ; set ; } public string Name { get ; set ; } [ ForeignKey ( `` Office '' ) ] public System.Guid OfficeId { get ; set ; } // shared primary key public virtual aspnet_Users User { get ; set ; } public virtual Office Office { get ; set ; } } public class Doctor : Employee { public Doctor ( ) { this.Expertises = new HashSet < Expertise > ( ) ; } //the rest.. public virtual ICollection < Expertise > Expertises { get ; set ; } } public class Secretary : Employee { // blah blah } public class aspnet_Users { public aspnet_Users ( ) { this.aspnet_Roles = new List < aspnet_Roles > ( ) ; } public System.Guid ApplicationId { get ; set ; } public System.Guid UserId { get ; set ; } //the rest.. public virtual aspnet_Applications aspnet_Applications { get ; set ; } public virtual ICollection < aspnet_Roles > aspnet_Roles { get ; set ; } } public class aspnet_Roles { public aspnet_Roles ( ) { this.aspnet_Users = new List < aspnet_Users > ( ) ; } public System.Guid ApplicationId { get ; set ; } public System.Guid RoleId { get ; set ; } //the rest.. public virtual aspnet_Applications aspnet_Applications { get ; set ; } public virtual ICollection < aspnet_Users > aspnet_Users { get ; set ; } } **EDIT : ** relationships go deeper , many-one relationship ` Users ` table ` Applications ` table , also ` Roles ` ` Applications ` .",You use fluent api specify actions error message suggests . In Context : protected override void OnModelCreating ( DbModelBuilder modelBuilder ) { base.OnModelCreating ( modelBuilder ) ; modelBuilder.Entity < aspnet_UsersInRoles > ( ) .HasMany ( = > i.Users ) .WithRequired ( ) .WillCascadeOnDelete ( false ) ; } Note included definition table aspnet_UsersInRoles code may work . Another option remove CASCADE DELETES adding modelBuilder.Conventions.Remove < OneToManyCascadeDeleteConvention > ( ) ; If need info configuring relationships fluent api I suggest < http : //msdn.microsoft.com/en-US/data/jj591620 >
Stackoverflow,"I MySQL query I want include list ID 's another table . On website , people able add certain items , people add items favourites . I basically want get list ID 's people favourited item ( bit simplified , boils ) . Basically , I something like : SELECT * , GROUP_CONCAT ( ( SELECT userid FROM favourites WHERE itemid = items.id ) SEPARATOR ' , ' ) AS idlist FROM items WHERE id = $ someid This way , I would able show favourited item , splitting idlist later array PHP code , however I getting following MySQL error : > _1242 - Subquery returns 1 row_ I thought kind point using ` GROUP_CONCAT ` instead , example , ` CONCAT ` ? Am I going wrong way ? * * * Ok , thanks answers far , seems work . However , catch . Items also considered favourite added user . So I would need additional check check creator = userid . Can someone help come smart ( hopefully efficient ) way ? Thank ! Edit : I tried : SELECT [ ... ] LEFT JOIN favourites ON ( userid = itemid OR creator = userid ) And **idlist** empty . Note I use ` INNER JOIN ` instead ` LEFT JOIN ` I get empty result . Even though I sure rows meet ON requirement .","OP almost got right . ` GROUP_CONCAT ` wrapping columns subquery [ complete subquery ] ( https : //stackoverflow.com/a/4455991/838733 ) ( I 'm dismissing separator comma default ) : SELECT . * , ( SELECT GROUP_CONCAT ( userid ) FROM favourites f WHERE f.itemid = i.id ) AS idlist FROM items WHERE i.id = $ someid This yield desired result also means accepted answer partially wrong , access outer scope variables subquery ."
Stackoverflow,"If I create Stored Procedure SQL call ( ` EXEC spStoredProcedure ` ) within BEGIN/END TRANSACTION , stored procedure also fall transaction ? I n't know worked like try/catches C # .","Yes , _everything_ Begin Transaction Commit ( Rollback ) part transaction ."
Stackoverflow,"I 'm trying use ` mysqldump ` dump schema , mostly works I ran one curiosity : ` -p ` ` -- password ` option seems like something setting password ( ` man ` page ` -- help ` output say ) . Specifically , looks like 's indicated : < http : //snippets.dzone.com/posts/show/360 > \- , setting database dump . To support somewhat outlandish claim , I tell I specify ` -- password ` ( ` -p ` ) option , command prints usage statement exits error . If I specify , I immediately prompted enter password ( ! ) , database specified ` -- password ` option dumped ( error given usual case password matching database name specified ) . Here 's transcript : $ mysqldump -u test -h myhost -- no-data -- tables -- password lose Enter password : -- MySQL dump 10.10 mysqldump : Got error : 1044 : Access denied user 'test ' @ ' % ' database 'lose ' selecting database So , gives ? Is way supposed work ? It surely appear make sense match official documentation . And finally , way works , I meant specify password used automated job ? Using ` expect ` ? ? ? I 'm using ` mysqldump Ver 10.10 Distrib 5.0.22 , pc-linux-gnu ( i486 ) ` .","From man mysqldump : > \ -- password [ =password ] , -p [ password ] > > The password use connecting server . If use short option form ( -p ) , space option password . If omit password value following \ -- password -p option command line , prompted one . > Specifying password command line considered insecure . See Section 6.6 , `` Keeping Your Password Secure '' . Syntactically , using -- password switch correctly . As , command line parser seeing use `` lose '' stand-alone argument mysqldump interprets database name would attempt simpler command like ` mysqldump lose ` To correct , try using ` -- password=lose ` ` -plose ` simply use ` -p ` ` -- password ` type password prompted ."
Stackoverflow,I following code ( Database SQL Server Compact 4.0 ) : Dim competitor=context.Competitors.Find ( id ) When I profile Find method takes 300+ms retrieve competitor table 60 records . When I change code : Dim competitor=context.Competitors.SingleOrDefault ( function ( c ) c.ID=id ) Then competitor found 3 ms . The Competitor class : Public Class Competitor Implements IEquatable ( Of Competitor ) Public Sub New ( ) CompetitionSubscriptions = New List ( Of CompetitionSubscription ) OpponentMeetings = New List ( Of Meeting ) GUID = GUID.NewGuid End Sub Public Sub New ( name As String ) Me.New ( ) Me.Name = name End Sub 'ID' Public Property ID As Long Public Property GUID As Guid 'NATIVE PROPERTIES' Public Property Name As String 'NAVIGATION PROPERTIES' Public Overridable Property CompetitionSubscriptions As ICollection ( Of CompetitionSubscription ) Public Overridable Property OpponentMeetings As ICollection ( Of Meeting ) End Class I defined many many relations ` CompetitionSubscriptions ` ` OpponentMeetings ` using fluent API . The ID property ` Competitor ` class Long translated Code First Identity column primary key datatable ( SQL Server Compact 4.0 ) What going ? ?,"` Find ` calls ` DetectChanges ` internally , ` SingleOrDefault ` ( generally query ) n't . ` DetectChanges ` expensive operation , 's reason ` Find ` slower ( might become faster entity already loaded context ` Find ` would run query return loaded entity ) . If want use ` Find ` lot entities - loop example - disable automatic change detection like ( ca n't write VB , C # example ) : try { context.Configuration.AutoDetectChangesEnabled = false ; foreach ( var id someIdCollection ) { var competitor = context.Competitors.Find ( id ) ; // ... } } finally { context.Configuration.AutoDetectChangesEnabled = true ; } Now , ` Find ` wo n't call ` DetectChanges ` every call fast ` SingleOrDefault ` ( faster entity already attached context ) . Automatic change detection complex somewhat mysterious subject . A great detailed discussion found four-part series : ( Link part 1 , links parts 2 , 3 4 beginning article ) < http : //blog.oneunicorn.com/2012/03/10/secrets-of-detectchanges-part-1-what- does-detectchanges-do/ >"
Stackoverflow,"I 'm using mongo 2.2.3 java driver . My dilemma , I $ push field value array , I cant seem figure . A sample data : `` _id '' : 1 , `` scores '' : [ { `` type '' : `` homework '' , `` score '' : 78.97979 } , { `` type '' : `` homework '' , `` score '' : 6.99 } , { `` type '' : `` quiz '' , `` score '' : 99 } ] I $ push shell : db.collection.update ( { _id:1 } , { $ push : { scores : { type : '' quiz '' , score:99 } } } ) 's I translate java I confuse self chuck keyboard wall . java code ( incomplete wrong ) far : DBObject find = new BasicDBObject ( `` _id '' , 1 ) ; DBObject push = new BasicDBObject ( `` $ push '' , new BasicDBObject ( `` scores '' , new BasicDBObject ( ) ) ) ;","DBObject listItem = new BasicDBObject ( `` scores '' , new BasicDBObject ( `` type '' , '' quiz '' ) .append ( `` score '' ,99 ) ) ; DBObject updateQuery = new BasicDBObject ( `` $ push '' , listItem ) ; myCol.update ( findQuery , updateQuery ) ;"
Stackoverflow,"I trying run mysql client terminal . I installed latest mysql gem . ➜ ~ git : ( master ) ✗ ruby -v ruby 1.8.7 ( 2010-01-10 patchlevel 249 ) [ universal-darwin11.0 ] ➜ ~ git : ( master ) ✗ rails -v Rails 2.3.14 ➜ ~ git : ( master ) ✗ mysql mysql : aliased nocorrect mysql ➜ ~ git : ( master ) ✗ ruby /usr/bin/ruby ➜ ~ git : ( master ) ✗ rails /usr/bin/rails ➜ ~ git : ( master ) ✗ gem list *** LOCAL GEMS *** actionmailer ( 2.3.14 ) actionpack ( 2.3.14 ) activerecord ( 2.3.14 ) activeresource ( 2.3.14 ) activesupport ( 2.3.14 ) builder ( 2.1.2 ) bundler ( 1.0.21 ) capistrano ( 2.9.0 ) capybara ( 0.3.9 ) cgi_multipart_eof_fix ( 2.5.0 ) childprocess ( 0.2.2 ) columnize ( 0.3.4 , 0.3.3 ) cucumber ( 0.9.4 ) cucumber-rails ( 0.3.2 ) culerity ( 0.2.15 ) daemons ( 1.1.4 ) database_cleaner ( 0.6.7 ) diff-lcs ( 1.1.3 ) expertiza-authlogic ( 2.1.6.1 ) fastercsv ( 1.5.4 ) fastthread ( 1.0.7 ) ffi ( 1.0.10 , 1.0.9 ) gdata ( 1.1.2 ) gem_plugin ( 0.2.3 ) gherkin ( 2.2.9 ) highline ( 1.6.2 ) hoptoad_notifier ( 2.4.11 ) json ( 1.4.6 ) json_pure ( 1.6.1 ) linecache ( 0.46 ) mime-types ( 1.16 ) mongrel ( 1.1.5 ) mysql ( 2.8.1 ) mysql2 ( 0.3.7 ) net-scp ( 1.0.4 ) net-sftp ( 2.0.5 ) net-ssh ( 2.2.1 ) net-ssh-gateway ( 1.1.0 ) nokogiri ( 1.5.0 ) rack ( 1.1.2 ) rack-test ( 0.6.1 ) rails ( 2.3.14 ) rake ( 0.9.2 ) rbx-require-relative ( 0.0.5 ) rdoc ( 3.11 ) RedCloth ( 4.2.8 ) rgl ( 0.4.0 ) ruby-debug ( 0.10.4 ) ruby-debug-base ( 0.10.4 ) rubyzip ( 0.9.4 ) selenium-webdriver ( 2.8.0 , 2.7.0 ) stream ( 0.5 ) term-ansicolor ( 1.0.7 , 1.0.6 ) ➜ expertiza git : ( master ) ✗ sudo su Password : sh-3.2 # mysql -u root -p Enter password : ERROR 2002 ( HY000 ) : Ca n't connect local MySQL server socket '/tmp/mysql.sock ' ( 2 ) sh-3.2 # I able get rid error . I created ` mysql.sock file ` ` /Users/HPV/expertiza/tmp/sockets ` . In file I written ` mysql.default_socket =/expertiza/tmp/sockets/mysql.sock ` . What I wrong ? Thanks !","You need [ follow directions ] ( https : //dev.mysql.com/doc/refman/5.7/en/osx-installation-pkg.html ) install start server . The command varies depending installed MySQL . Try first : > sudo /Library/StartupItems/MySQLCOM/MySQLCOM start If fails : > cd /usr/local/mysql > sudo ./bin/mysqld_safe > ( Enter password , necessary ) > ( Press Control-Z ) > bg"
Stackoverflow,"I 'm trying select column single table ( joins ) I need count number rows , ideally I begin retrieving rows . I come two approaches provide information I need . **Approach 1 : ** SELECT COUNT ( my_table.my_col ) AS row_count FROM my_table WHERE my_table.foo = 'bar' Then SELECT my_table.my_col FROM my_table WHERE my_table.foo = 'bar' Or **Approach 2** SELECT my_table.my_col , ( SELECT COUNT ( my_table.my_col ) FROM my_table WHERE my_table.foo = 'bar ' ) AS row_count FROM my_table WHERE my_table.foo = 'bar' I SQL driver ( SQL Native Client 9.0 ) allow use SQLRowCount SELECT statement I need know number rows result order allocate array assigning information . The use dynamically allocated container , unfortunately , option area program . I concerned following scenario might occur : * SELECT count occurs * Another instruction occurs , adding removing row * SELECT data occurs suddenly array wrong size . -In worse case , attempt write data beyond arrays limits crash program . Does Approach 2 prohibit issue ? Also , Will one two approaches faster ? If , ? Finally , better approach I consider ( perhaps way instruct driver return number rows SELECT result using SQLRowCount ? ) For asked , I using Native C++ aforementioned SQL driver ( provided Microsoft . )","If 're using SQL Server , query select [ @ @ RowCount ] ( https : //docs.microsoft.com/en-us/sql/t-sql/functions/rowcount- transact-sql ) function ( result set might 2 billion rows use [ RowCount_Big ( ) ] ( https : //docs.microsoft.com/en- us/sql/t-sql/functions/rowcount-big-transact-sql ) function ) . This return number rows selected previous statement number rows affected insert/update/delete statement . SELECT my_table.my_col FROM my_table WHERE my_table.foo = 'bar' SELECT @ @ Rowcount Or want row count included result sent similar Approach # 2 , use [ OVER clause ] ( https : //docs.microsoft.com/en- us/sql/t-sql/queries/select-over-clause-transact-sql ) . SELECT my_table.my_col , count ( * ) OVER ( PARTITION BY my_table.foo ) AS 'Count' FROM my_table WHERE my_table.foo = 'bar' Using OVER clause much better performance using subquery get row count . Using @ @ RowCount best performance wo n't query cost select @ @ RowCount statement Update response comment : The example I gave would give # rows partition - defined case `` PARTITION BY my_table.foo '' . The value column row # rows value my_table.foo . Since example query clause `` WHERE my_table.foo = 'bar ' '' , rows resultset value my_table.foo therefore value column rows equal ( case ) # rows query . Here better/simpler example include column row total # rows resultset . Simply remove optional Partition By clause . SELECT my_table.my_col , count ( * ) OVER ( ) AS 'Count' FROM my_table WHERE my_table.foo = 'bar '"
Stackoverflow,"I starting project involving small database I considering use SQLite . If I create table I defined one columns text , values stored integers - way change data type column ? I using SQLite Manager I ca n't find function allows . Can I using SQL command , restriction SQLite ? Probably , even restriction , one find way around create new table required column types import data . Regards , Nick","Are sure [ foreign key support ] ( http : //www.sqlite.org/foreignkeys.html # fk_enable ) enabled ? > Assuming library compiled foreign key constraints enabled , must still enabled application runtime , using PRAGMA foreign_keys command . For example : sqlite > PRAGMA foreign_keys = ON ;"
Stackoverflow,"Is easy way determine traces set ` sp_trace_create ` SQL Server 2000 ? How SQL Server 2005 , 2008 , 2012 , 2014 ?",**SQL Server 2005** ( onwards ) : SELECT * FROM sys.traces **SQL Server 2000** : USE msdb SELECT * FROM fn_trace_getinfo ( default ) ; Ref : [ ` fn_trace_getinfo ` ] ( http : //msdn.microsoft.com/en- us/library/ms173875.aspx ) Column descriptions ` sys.traces ` DMV found : [ ` sys.traces ` ] ( https : //docs.microsoft.com/en-us/sql/relational- databases/system-catalog-views/sys-traces-transact-sql )
Stackoverflow,"Here 's strange one : I filter ` NOT NULLS ` SQLite , ` NULLS ` : **This works : ** SELECT * FROM project WHERE parent_id NOT NULL ; **These n't : ** SELECT * FROM project WHERE parent_id IS NULL ; SELECT * FROM project WHERE parent_id ISNULL ; SELECT * FROM project WHERE parent_id NULL ; All return : > There problem syntax query ( Query executed ) ... **UPDATE** : I PHP- code ezSQl using [ PHPLiteAdmin ] ( http : //www.danedesigns.com/phpliteadmin.php ) interface Using [ PHPLiteAdmin demo ] ( http : //www.danedesigns.com/phpliteadmin.php ? table=Customers & action=table_sql ) , expression works- I 'm suspecting version issue PHP's SQLite ? Could ? Was n't expression always valid ? **UPDATE 2** : When I run code PHP using ezSQL , PHP warning : > PHP Warning : SQL logic error missing database Is way get information PHP ? This maddeningly opaque weird , especially statement CLI works fine ... **UPDATE 3** The possible clue I databases I create PHP read CLI , vice versa . I get : > Error : file encrypted database So 's definitly two SQlite flavors butting heads . ( [ See ] ( https : //stackoverflow.com/questions/1513849/error-file-is-encrypted-or- is-not-a-database ) ) Still , invalid statment ? ? **UPDATE 4** OK I think I 've traced problem culprit , reason- The DB I created PHP ezSQL one IS NULL statement fails . If I create DB using PHP 's SQLite3 class , statement works fine , moreover , I access DB CLI , whereas ezSQL created DB gave ` file encrypted ` error . So I little digging ezSQL code- Off bat I see uses PDO methods , newer SQLite3 class . Maybe 's something- I 'm gon na waste time ... In case , I 've found solution , steer clear ezSQL , use PHPs SQLite3 class .","` IS b ` ` IS NOT b ` general form ` ` ` b ` expressions . This generally seen ` IS NULL ` ` IS NOT NULL ` cases . There also ` ISNULL ` ` NOTNULL ` ( also ` NOT NULL ` ) operators short- hands previous expressions , respectively ( take single operand ) . The SQL understood SQLite expressions covered [ SQLite Query Language : Expressions ] ( http : //www.sqlite.org/lang_expr.html ) . Make sure ( previous ) statements terminated ` ; ` first using CLI . These valid negate `` null match '' : expr NOT NULL expr NOTNULL expr IS NOT NULL These valid `` match null '' : expr ISNULL expr IS NULL Since constructs expressions negations also valid ( e.g . ` NOT ( expr NOT NULL ) ` equivalent ` expr IS NULL ` ) . Happy coding . * * * The proof pudding : SQLite version 3.7.7.1 2011-06-28 17:39:05 Enter `` .help '' instructions Enter SQL statements terminated `` ; '' sqlite > create table x ( int null ) ; sqlite > select * x isnull ; sqlite > select * x notnull ; sqlite > select * x null ; sqlite > select * x null ; sqlite > select * x null ; sqlite >"
Stackoverflow,"Was trying select ... temp Table # TempTable sp_Executedsql . Not successfully inserted Messages written ( 359 row ( ) affected ) mean successful inserted ? Script DECLARE @ Sql NVARCHAR ( MAX ) ; SET @ Sql = 'select distinct Coloum1 , Coloum2 # TempTable SPCTable ( nolock ) Convert ( varchar ( 10 ) , Date_Tm , 120 ) Between @ Date_From And @ Date_To ' ; SET @ Sql = 'DECLARE @ Date_From VARCHAR ( 10 ) ; DECLARE @ Date_To VARCHAR ( 10 ) ; SET @ Date_From = `` '+CONVERT ( VARCHAR ( 10 ) , DATEADD ( , DATEDIFF ( d,0 , GETDATE ( ) ) ,0 ) -1,120 ) + '' ' ; SET @ Date_To = `` '+CONVERT ( VARCHAR ( 10 ) , DATEADD ( , DATEDIFF ( d,0 , GETDATE ( ) ) ,0 ) -1,120 ) + '' ' ; '+ @ Sql ; EXECUTE sp_executesql @ Sql ; After executed , return messages ( 359 row ( ) affected ) . Next trying select data # TempTable . Select * From # TempTable ; Its return : Msg 208 , Level 16 , State 0 , Line 2 Invalid object name ' # TempTable ' . Suspected working 'select ' section . The insert working . fix ?","Here simple example : EXEC sp_executesql @ sql , N ' @ p1 INT , @ p2 INT , @ p3 INT ' , @ p1 , @ p2 , @ p3 ; Your call something like EXEC sp_executesql @ statement , N ' @ LabID int , @ BeginDate date , @ EndDate date , @ RequestTypeID varchar ' , @ LabID , @ BeginDate , @ EndDate , @ RequestTypeID"
Stackoverflow,I ’ trying create FULLTEXT index attribute table . Mysql returns > ERROR 1214 : The used table type ’ support FULLTEXT indexes . Any idea I ’ wrong ?,"You ’ using wrong type table . Mysql supports different types tables , commonly used MyISAM InnoDB . [ MyISAM ( MySQL 5.6+also InnoDB tables ) types tables Mysql supports Full- text indexes . ] ( http : //dev.mysql.com/doc/refman/5.1/en/fulltext- restrictions.html ) To check table ’ type issue following sql query : SHOW TABLE STATUS Looking result returned query , find table corresponding value Engine column . If value anything except MyISAM InnoDB Mysql throw error trying add FULLTEXT indexes . To correct , use sql query change engine type : ALTER TABLE < table name > ENGINE = [ MYISAM | INNODB ] Additional information ( thought might useful ) : Mysql using different engine storage types optimize needed functionality specific tables . Example MyISAM default type operating systems ( besides windows ) , preforms SELECTs INSERTs quickly ; handle transactions . InnoDB default windows , used transactions . But InnoDB require disk space server ."
Stackoverflow,"I 've successfully publishing DACPACs SQL Server 2008-2012 instances using SqlPackage.exe , installed SQL Server Data Tools ( typically found ` C : \Program Files ( x86 ) \Microsoft SQL Server\110\DAC\bin ` ) . However , attempting publish 2014-targeted DACPAC SQL Server 2014 instance using SqlPackage.exe , I get following : *** Could deploy package . Internal Error . The database platform service type Microsoft.Data.Tools . Schema.Sql.Sql120DatabaseSchemaProvider valid . You must make sure service loaded , must provide full type name valid database platform service . I 've found minimal info regarding ; closest I found [ problem ] ( http : //social.msdn.microsoft.com/Forums/windowsazure/en- US/8ac0ae34-e614-4d35-8a50-46931cdad20a/internal-error-import-bacpac- file ? forum=ssdsgetstarted ) publishing Azure . I 've kept date SSDT patches would guess SqlPackage.exe I ( shows 11.0.2902.0 version ) simply incompatible . I able publish instance using Visual Studio 2012 's Publish command instance seem issue . Is newer version SqlPackage available would support publishing 2014 DACPAC 2014 server ? Or another scriptable way ?","Yes , new version supporting SQL Server 2005-2016 available installs different location previous ( SQL Server 2012 lower ) version . In fact , 'll different install locations depending use SSDT install part SSMS standalone installer . * SSDT installs Dac DLLs inside Visual Studio latest releases . This avoid side side issues ( Visual Studio 2012 vs 2013 vs SSMS ) required updated use latest code . * If [ updated latest SSDT ] ( https : //msdn.microsoft.com/en-us/library/mt204009.aspx ) , 'll find SqlPackage.exe related DLLs **VS Install Directory\Common7\IDE\Extensions\Microsoft\SQLDB\DAC\130** . For VS2013 VS install directory **C : \Program Files ( x86 ) \Microsoft Visual Studio 12.0** , 's 14.0 VS2015 . * [ SQL Server Management Studio ( SSMS ) ] ( https : //msdn.microsoft.com/en-us/library/mt238290.aspx ) standalone [ Dac Framework MSI ] ( https : //www.microsoft.com/en-us/download/details.aspx ? id=53013 ) install system-wide location . This **C : \Program Files ( x86 ) \Microsoft SQL Server\130\Dac\bin** ."
Stackoverflow,"I ran mysql import mysql dummyctrad < dumpfile.sql server taking long complete . The dump file 5G . The server Centos 6 , memory=16G 8core processors , mysql v 5.7 x64- Are normal messages/status `` waiting table flush '' message ` InnoDB : page_cleaner : 1000ms intended loop took 4013ms . The settings might optimal ` mysql log contents 2016-12-13T10:51:39.909382Z 0 [ Note ] InnoDB : page_cleaner : 1000ms intended loop took 4013ms . The settings might optimal . ( flushed=1438 evicted=0 , time . ) 2016-12-13T10:53:01.170388Z 0 [ Note ] InnoDB : page_cleaner : 1000ms intended loop took 4055ms . The settings might optimal . ( flushed=1412 evicted=0 , time . ) 2016-12-13T11:07:11.728812Z 0 [ Note ] InnoDB : page_cleaner : 1000ms intended loop took 4008ms . The settings might optimal . ( flushed=1414 evicted=0 , time . ) 2016-12-13T11:39:54.257618Z 3274915 [ Note ] Aborted connection 3274915 db : 'dummyctrad ' user : 'root ' host : 'localhost ' ( Got error writing communication packets ) Processlist : mysql > show processlist \G ; *************************** 1. row *************************** Id : 3273081 User : root Host : localhost db : dummyctrad Command : Field List Time : 7580 State : Waiting table flush Info : *************************** 2. row *************************** Id : 3274915 User : root Host : localhost db : dummyctrad Command : Query Time : 2 State : update Info : INSERT INTO ` radacct ` VALUES ( 351318325 , 'kxid ge:7186 ' , 'abcxyz5976c ' , 'user100 *************************** 3. row *************************** Id : 3291591 User : root Host : localhost db : NULL Command : Query Time : 0 State : starting Info : show processlist *************************** 4. row *************************** Id : 3291657 User : remoteuser Host : portal.example.com:32800 db : ctradius Command : Sleep Time : 2 State : Info : NULL 4 rows set ( 0.00 sec ) **Update-1** [ mysqlforum ] ( https : //bugs.mysql.com/bug.php ? id=81899 ) , [ innodb_lru_scan_depth ] ( http : //dev.mysql.com/doc/refman/5.7/en/innodb- parameters.html # sysvar_innodb_lru_scan_depth ) changing innodb_lru_scan_depth value 256 improved insert queries execution time + warning message log , default innodb_lru_scan_depth=1024 ; ` SET GLOBAL innodb_lru_scan_depth=256 ; `","**On Windows** : 0 ) shut service ` mysql56 ` 1 ) go ` C : \ProgramData\MySQL\MySQL Server 5.6 ` , note ` ProgramData ` hidden folder 2 ) looking file ` my.ini ` , open add one line ` skip-grant-tables ` ` [ mysqld ] ` , save [ mysqld ] skip-grant-tables 3 ) start service ` mysql56 ` 4 ) right , access database , run ` mysql ` 5 ) use query update password update mysql.user set password=PASSWORD ( 'NEW PASSWORD ' ) user='root ' ; **note** : newer version , use ` authentication_string ` instead ` password ` 6 ) shut service , remove line ` skip-grant-tables ` save , start service . try use password set login . * * * **On Mac** : 0 ) stop service sudo /usr/local/mysql/support-files/mysql.server stop 1 ) skip grant table sudo /usr/local/mysql/bin/mysqld_safe -- skip-grant-tables 's running , n't close , open new terminal window 2 ) go mysql terminal /usr/local/mysql/bin/mysql -u root 3 ) update password UPDATE mysql.user SET Password=PASSWORD ( 'password ' ) WHERE User='root ' ; newer version like 5.7 , use UPDATE mysql.user SET authentication_string=PASSWORD ( 'password ' ) WHERE User='root ' ; 4 ) run ` FLUSH PRIVILEGES ; ` 5 ) run ` \q ` quit 6 ) start mysql server sudo /usr/local/mysql/support-files/mysql.server start"
Stackoverflow,"I need connect SQL 2008 R2 Server Linux box 's registered company 's domain . I 'm trying use SQuirreL SQL version 3.2.1 . I downloaded [ Microsoft SQL Server JDBC Driver 3.0 ] ( http : //www.microsoft.com/download/en/details.aspx ? displaylang=en & id=21599 ) assigned SQuirreL Drivers tab . Now , I try create Alias SQuirreL , I select SQL Server driver adjust URL . For credentials I use domain registered username password . When I try test connection , I always get error : < `` database-name '' > : Logon failure user ' < `` domain '' > \ < `` domain-user '' > ' . How I get working ? Thanks advance !","I ran similar issue following change fixed issue . Open Application Folder finder open App Package Contents navigate Contents/MacOS/ . Open squirrel-sql.sh file update value '' SQUIRREL_SQL_HOME '' around line 56 . Out box , value would > SQUIRREL_SQL_HOME= ` dirname `` $ 0 '' ` /Contents/Resources/Java Update > SQUIRREL_SQL_HOME='/Applications/SQuirreLSQL.app/Contents/Resources/Java' Thanks < https : //sourceforge.net/p/squirrel-sql/bugs/1232/ # 6bc6 >"
Stackoverflow,"My standard disclaimer : I n't worked Java 10 years , probable I 'm something elementary wrong . I writing `` server-side extension '' [ SmartFoxServer ] ( http : //smartfoxserver.com ) ( SFS ) . In login script , I need make connection MS SQL Server , I attempting using JDBC . I tested JDBC code debug environment , works fine . BUT When I put server-side extension SFS `` extensions '' folder ( per spec ) , I 'm getting ` com.microsoft.sqlserver.jdbc.SQLServerException ` : > `` This driver configured integrated authentication. '' . I Googled error , found 's usually file ` sqljdbc_auth.dll ` system path ; I copied file folder system path , still work ! Any suggestions ?","There different versions sqljdbc_auth.dll different processor architectures ( x86/x64/ia64 ) . Which one using SFS server ? You must choose one match architecture JVM SFS running . So , 're running 32-bit Java 64-bit machine , 'll need x86 version , x64 version . I 've used SFS , I n't know whether writes logs anywhere . If , might worth taking look logs see anything helpful written . **EDIT** : I ca n't 100 % sure SFS using 64-bit Java runs C : \Program Files opposed C : \Program Files ( x86 ) . I found following line [ SFS docs ] ( http : //smartfoxserver.com/docs/ ) Introduction > Requirements Installation . Whilst line applies Linux opposed Windows , might suggest SFS Windows also uses 32-bit Java : > Since version 1.5 SmartFoxServer comes x86 32-bit Sun Java Runtime . One quick way determine version ( ) Java installed see whether either folders ` C : \Program Files\Java ` ` C : \Program Files ( x86 ) \Java ` exist . Of course , folders exist , that's much help . Does application work use x86 version sqljdbc_auth.dll instead x64 version ? If suddenly starts working x86 DLL , SFS must using 32-bit Java . Is batch-file used start SFS ? If , reading might help point SFS running Java . Also look changes ` PATH ` . Java load DLLs ` java.library.path ` system property , Windows , set value ` PATH ` environment variable . If still ca n't determine whether SFS using 32-bit 64-bit Java , try using Process Explorer look environment java.exe process running SFS started ."
Stackoverflow,"I got confused manual , work like : { QSqlDatabase db = QSqlDatabase : :addDatabase ( ... ) ; QSqlQuery query ( db ) ; query.exec ( ... ) ; } QSqlDatabase : :removeDatabase ( ... ) ; As document points , ` query ` ` db ` deconstructed automatically . But efficient ? Well , cache ` db ` inside class , like following : class Dummy { Dummy ( ) { db = QSqlDatabase : :addDatabase ( ... ) ; } ~Dummy ( ) { db.close ( ) ; } bool run ( ) { QSqlQuery query ( db ) ; bool retval = query.exec ( ... ) ; blabla ... } private : QSqlDatabase db ; } ; Sometimes could see warnings like : QSqlDatabasePrivate : :removeDatabase : connection 'BLABLA ' still use , queries cease work . Even n't call ` run ( ) ` .","When create ` QSqlDatabase ` object ` addDatabase ` call ` removeDatabase ` , merely associating disassociating tuple _ ( driver , hostname : port , database name , username/password ) _ name ( default connection name n't specify connection name ) . The SQL driver instantiated , database opened call ` QSqlDatabase : :open ` . That connection name defined application-wide . So call ` addDatabase ` objects use , changing ` QSqlDatabase ` objects uses connection name invalidating queries active . The first code example cited shows correctly disassociate connection name , ensuring : * ` QSqlQuery ` detached ` QSqlDatabase ` closing database calling ` QSqlQuery : :finish ( ) ` , automatic ` QSqlQuery ` object goes scope , * ` QSqlDatabase ` connection name ` close ( ) ` call ` QSqlDatabase : :removeDatabase ` ( ` close ( ) ` also called automatically ` QSqlDatabase ` object goes scope ) . When create QSqlDatabase , depending whether want connection stay open application lifetime ( 1 ) needed ( 2 ) , : 1. keep single ` QSqlDatabase ` instance one single class ( example , mainwindow ) , use objects needs either passing ` QSqlDatabase ` directly connection name pass ` QSqlDatabase : :database ` get ` QSqlDatabase ` instance back . ` QSqlDatabase : :database ` uses ` QHash ` retrieve ` QSqlDatabase ` name , probably negligibly slower passing ` QSqlDatabase ` object directly objects functions , use default connection , n't even pass anything anywhere , call ` QSqlDatabase : :database ( ) ` without parameter . // In object lifetime application // ( global variable , since almost goal ) QSqlDatabase db ; // In constructor initialization function object db = QSqlDatabase : :addDatabase ( `` QSQLDRIVER '' , `` connection-name '' ) ; db.setHostname ( ... ) ; // ... ( ! this- > db.open ( ) ) // open keep opened { // Error handling ... } // -- -- -- -- // Anywhere need , use `` global '' db object // get database connection connection name QSqlDatabase db = QSqlDatabase : :database ( `` connection-name '' ) ; QSqlQuery query ( db ) ; 2. configure ` QSqlDatabase ` , open test parameters correct , ditch instance . The connection name , still accessible anywhere , database reopened : { // Allocated stack QSqlDatabase db = QSqlDatabase : :addDatabase ( `` QSQLDRIVER '' , `` connection-name '' ) ; db.setHostname ( ... ) ; // ... ( ! this- > db.open ( ) ) // test connection { // Error handling } // db closed goes scope } { // Same thing ( 1 ) , default database ( ) opens // connection n't already opened QSqlDatabase db = QSqlDatabase : :database ( `` connection-name '' ) ; QSqlQuery query ( db ) ; // connection open connection name , // connection closed db goes scope } In case , note n't close database explicitly , multiple objects using database connection reentrant manner ( example , function A use connection calls B also use connection . If B closes connection returning control A , connection also closed A , probably bad thing ) ."
Stackoverflow,"I working social network , one procedures returns VARCHAR output . So I wrote : SqlParameter job1 = cmd2.Parameters.Add ( `` @ job '' , SqlDbType.VarChar ) ; job1.Direction = ParameterDirection.Output ; However error comes : > String [ 1 ] : Size property invalid size 0 .","You need **define length** specifying ` varchar ` parameter : SqlParameter job1 = cmd2.Parameters.Add ( `` @ job '' , SqlDbType.VarChar , 50 ) ; You use length defined SQL Server stored procedure . And btw : stored procedure also length defined ( something like ` @ job VARCHAR OUTPUT ` ) - 've defined ` varchar ` string 1 character length ... ..."
Stackoverflow,"Long time listener , first time caller ( finally made account ! ) ... I using **Visual Studio 2013** **.NET 4.5.1** **Entity Framework 6** ( final releases , RC beta ) . When trying add DbGeography property entity , I get error upon execution : One validation errors detected model generation : Geocoder.DbGeography : : EntityType 'DbGeography ' key defined . Define key EntityType . DbGeographies : EntityType : EntitySet 'DbGeographies ' based type 'DbGeography ' keys defined . I already confirmed I references older versions Entity Framework ( discussed [ ] ( http : //entityframework.codeplex.com/workitem/1535 ) ) . I using [ post ] ( https : //stackoverflow.com/questions/7409051/why-use-the-sql- server-2008-geography-data-type/7409237 ) [ MSDN article ] ( http : //msdn.microsoft.com/en-us/data/hh859721.aspx ) examples/information first foray spatial types .NET ( SQL Server , matter ) . Here entity : public class Location { public int Id { get ; set ; } public string Name { get ; set ; } public string Address1 { get ; set ; } public string Address2 { get ; set ; } public string City { get ; set ; } public virtual State State { get ; private set ; } public string ZipCode { get ; set ; } public string ZipCodePlus4 { get ; set ; } public DbGeography Geocode { get ; set ; } public Hours Hours { get ; set ; } public virtual ICollection < Language > Languages { get ; private set ; } public virtual OfficeType OfficeType { get ; private set ; } [ JsonIgnore ] public virtual ICollection < ProviderLocation > Providers { get ; private set ; } } What I wrong ?","This turned opposite I read Microsoft 's response similar issue Codeplex [ ] ( http : //entityframework.codeplex.com/workitem/1535 ) , even [ documentation ] ( http : //msdn.microsoft.com/en- us/library/system.data.spatial\ ( v=vs.110\ ) .aspx ) . Did I interpret wrong ? Both links indicate EF 6 , DbGeography datatype moved System.Data.Entity.Spatial System.Data.Spatial , reverse seems true . I changed using System.Data.Spatial ; using System.Data.Entity.Spatial ; works ."
Stackoverflow,"I want perform query would look like native SQL : SELECT AVG ( t.column ) AS average_value FROM table WHERE YEAR ( t.timestamp ) = 2013 AND MONTH ( t.timestamp ) = 09 AND DAY ( t.timestamp ) = 16 AND t.somethingelse LIKE 'somethingelse' GROUP BY t.somethingelse ; If I trying implement Doctrine 's query builder like : $ qb = $ this- > getDoctrine ( ) - > createQueryBuilder ( ) ; $ qb- > select ( ' e.column AS average_value ' ) - > ( 'MyBundle : MyEntity ' , ' e ' ) - > ( 'YEAR ( e.timestamp ) = 2013 ' ) - > andWhere ( 'MONTH ( e.timestamp ) = 09 ' ) - > andWhere ( 'DAY ( e.timestamp ) = 16 ' ) - > andWhere ( ' u.somethingelse LIKE somethingelse ' ) - > groupBy ( 'somethingelse ' ) ; I get error exception > [ Syntax Error ] line 0 , col 63 : Error : Expected known function , got 'YEAR' How I implement query Doctrines query builder ? **Notes : ** * I know [ Doctrine 's Native SQL ] ( http : //docs.doctrine-project.org/en/latest/reference/native-sql.html ) . I 've tried , leads problem productive development database tables different names . I want work database agnostic , option . * Although I want work db agnostic : FYI , I using MySQL . * There way extend Doctrine `` learn '' ` YEAR ( ) ` etc . statements , e.g . [ seen ] ( http : //www.simukti.net/blog/2012/04/05/how-to-select-year-month-day-in-doctrine2/ ) . But I looking way avoid including third party plugins .","First day next month : sql-server 2012+ DATEADD ( , 1 , EOMONTH ( current_timestamp ) ) sql-server 2008 older : DATEADD ( , DATEDIFF ( , -1 , current_timestamp ) , 0 )"
Stackoverflow,"I doubt , I 've searched web answers seem diversified . Is better use mysql_pconnect mysql_connect connecting database via PHP ? I read pconnect scales much better , hand , persistent connection ... 10 000 connections time , persistent , n't seem scalable . Thanks advance .","Persistent connections unnecessary MySQL . In databases ( Oracle ) , making connection expensive time-consuming , re-use connection 's big win . But brands database offer connection pooling , solves problem better way . Making connection MySQL database quick compared brands , using persistent connections gives proportionally less benefit MySQL would another brand database . Persistent connections downside . The database server allocates resources connection , whether connections needed . So see lot wasted resources purpose connections idle . I n't know 'll reach 10,000 idle connections , even couple hundred costly . Connections state , would inappropriate PHP request '' inherit '' information session previously used another PHP request . For example , temporary tables user variables normally cleaned connection closes , use persistent connections . Likewise session-based settings like character set collation . Also , ` LAST_INSERT_ID ( ) ` would report id last generated session -- even prior PHP request . For MySQL least , downside persistent connections probably outweighs benefits . And , better techniques achieve high scalability . * * * Update March 2014 : MySQL connection speed always low compared brands RDBMS , 's getting even better . See < http : //mysqlserverteam.com/improving-connectdisconnect-performance/ > > In MySQL 5.6 started working optimizing code handling connects disconnects . And work accelerated MySQL 5.7 . In blog post I first show results achieved describe done get . Read blog details speed comparisons ."
Stackoverflow,"I get error ` ERROR 1066 ( 42000 ) : Not unique table/alias : ` I cant figure whats wrong . SELECT Project_Assigned.ProjectID , Project_Title , Account.Account_ID , Username , Access_Type FROM Project_Assigned JOIN Account ON Project_Assigned.AccountID = Account.Account_ID JOIN Project ON Project_Assigned.ProjectID = Project.Project_ID Access_Type = 'Client ' ;","You need give user table alias second time join e.g . SELECT article . * , section.title , category.title , user.name , u2.name FROM article INNER JOIN section ON article.section_id = section.id INNER JOIN category ON article.category_id = category.id INNER JOIN user ON article.author_id = user.id LEFT JOIN user u2 ON article.modified_by = u2.id WHERE article.id = ' 1 '"
Stackoverflow,"I connected Google Cloud SQL database eclipse using Data Source explorer . But I generate DDL database using option ` Generate DDL ` , I ca n't get ` AUTO_INCREMENT ` script get corresponding primary key . How would go getting ` AUTO_INCREMENT ` script ?","**Update** Google Cloud SQL supports direct access , ` MySQLdb ` dialect used . The recommended connection via mysql dialect using URL format : mysql+mysqldb : //root @ / < dbname > ? unix_socket=/cloudsql/ < projectid > : < instancename > ` mysql+gaerdbms ` deprecated SQLAlchemy since version 1.0 I 'm leaving original answer case others still find helpful . * * * For visit question later ( n't want read comments ) , SQLAlchemy supports Google Cloud SQL version 0.7.8 using connection string / dialect ( see : [ docs ] ( http : //docs.sqlalchemy.org/en/rel_0_7/dialects/mysql.html # module- sqlalchemy.dialects.mysql.gaerdbms ) ) : mysql+gaerdbms : /// < dbname > E.g . : create_engine ( 'mysql+gaerdbms : ///mydb ' , connect_args= { `` instance '' : '' myinstance '' } ) * * * I proposed [ update ] ( http : //www.sqlalchemy.org/trac/ticket/2649 ) ` mysql+gaerdmbs : // ` dialect support Google Cloud SQL APIs ( ` rdbms_apiproxy ` ` rdbms_googleapi ` ) connecting Cloud SQL non-Google App Engine production instance ( ex . development workstation ) . The change also modify connection string slightly including project instance part string , require passed separately via ` connect_args ` . E.g . mysql+gaerdbms : /// < dbname > ? instance= < project : instance > This also make easier use Cloud SQL Flask-SQLAlchemy extension n't explicitly make ` create_engine ( ) ` call . If trouble connecting Google Cloud SQL development workstation , might want take look answer - < https : //stackoverflow.com/a/14287158/191902 > ."
Stackoverflow,"Via MySQL command line client , I trying set global mysql_mode : SET GLOBAL sql_mode = TRADITIONAL ; This works current session , I restart server , sql_mode goes back default : `` , empty string . How I permanently set sql_mode TRADITIONAL ? If relevant , MySQL part WAMP package . Thank .","This problem scuppered well . None answers far addressed original problem I believe mine I 'll post case helps anyone else . I MySQL ( mysql.com ) Community Edition 5.7.10 installed OS X 10.10.3 In end I created ` /etc/mysql/my.cnf ` following contents : - [ mysqld ] sql_mode=NO_ENGINE_SUBSTITUTION After restarting server ` SHOW VARIABLES LIKE 'sql_mode ' ; ` gave : - + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- + | Variable_name | Value | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- + | sql_mode | NO_ENGINE_SUBSTITUTION | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- + 1 row set ( 0.00 sec ) Finally , strict mode !"
Stackoverflow,"After several days passed investigate issue , I decided submit question sense apparently happening . **The Case** My computer configured local Oracle Express database . I JAVA project several JUnit Tests extend parent class ( I know `` best practice '' ) opens OJDBC Connection ( using static Hikari connection pool 10 Connections ) @ Before method rolled Back @ After . public class BaseLocalRollbackableConnectorTest { private static Logger logger = LoggerFactory.getLogger ( BaseLocalRollbackableConnectorTest.class ) ; protected Connection connection ; @ Before public void setup ( ) throws SQLException { logger.debug ( `` Getting connection setting autocommit FALSE '' ) ; connection = StaticConnectionPool.getPooledConnection ( ) ; } @ After public void teardown ( ) throws SQLException { logger.debug ( `` Rollback connection '' ) ; connection.rollback ( ) ; logger.debug ( `` Close connection '' ) ; connection.close ( ) ; } StacicConnectionPool public class StaticConnectionPool { private static HikariDataSource ds ; private static final Logger log = LoggerFactory.getLogger ( StaticConnectionPool.class ) ; public static Connection getPooledConnection ( ) throws SQLException { ( ds == null ) { log.debug ( `` Initializing ConnectionPool '' ) ; HikariConfig config = new HikariConfig ( ) ; config.setMaximumPoolSize ( 10 ) ; config.setDataSourceClassName ( `` oracle.jdbc.pool.OracleDataSource '' ) ; config.addDataSourceProperty ( `` url '' , `` jdbc : oracle : thin : @ localhost:1521 : XE '' ) ; config.addDataSourceProperty ( `` user '' , `` MyUser '' ) ; config.addDataSourceProperty ( `` password '' , `` MyPsw '' ) ; config.setAutoCommit ( false ) ; ds = new HikariDataSource ( config ) ; } return ds.getConnection ( ) ; } } This project hundreds tests ( parallel ) use connection ( localhost ) execute queries ( insert/update select ) using Sql2o transaction clousure connection managed externally ( test ) . The database completely empty ACID tests . So expected result insert something DB , makes assertions rollback . way second test find data added previous test order maintain isolation level . **The Problem** Running tests together ( sequentially ) , 90 % times work properly . 10 % one two tests , randomly , fail , dirty data database ( duplicated unique example ) previous tests . looking logs , rollbacks previous tests done properly . In fact , I check database , empty ) If I execute tests server higher performance JDK , Oracle DB XE , failure ratio increased 50 % . This strange I idea connections different tests rollback called time . The JDBC Isolation level READ COMMITTED even used connection , create problem even using connection . So question : Why happen ? idea ? Is JDBC rollback synchronous I know could cases go forward even though fully completed ? These main DB params : processes 100 sessions 172 transactions 189","Try storing bytes : UUID uuid = UUID.randomUUID ( ) ; byte [ ] uuidBytes = new byte [ 16 ] ; ByteBuffer.wrap ( uuidBytes ) .order ( ByteOrder.BIG_ENDIAN ) .putLong ( uuid.getMostSignificantBits ( ) ) .putLong ( uuid.getLeastSignificantBits ( ) ) ; con.createQuery ( `` INSERT INTO TestTable ( ID , Name ) VALUES ( : id , : name ) '' ) .addParameter ( `` id '' , uuidBytes ) .addParameter ( `` name '' , `` test1 '' ) .executeUpdate ( ) ; A bit explanation : table using BINARY ( 16 ) , serializing UUID raw bytes really straightforward approach . UUIDs essentially 128-bit ints reserved bits , code writes big- endian 128-bit int . The ByteBuffer easy way turn two longs byte array . Now practice , conversion effort headaches wo n't worth 20 bytes save per row ."
Stackoverflow,I 've searching ca n't seem find answers goes ... I 've got CSV file I want import table Oracle ( 9i/10i ) . Later I plan use table lookup another use . This actually workaround I 'm working since fact querying using IN clause 1000 values possible . How done using SQLPLUS ? Thanks time ! : ),"**SQL Loader** helps load csv files tables : [ SQL*Loader ] ( http : //www.orafaq.com/wiki/SQL*Loader_FAQ ) If want sqlplus , gets bit complicated . You need locate sqlloader script csv file , run sqlldr command ."
Stackoverflow,"I using MySQL version 5.1.66 . I saw [ long_query_time ] ( http : //dev.mysql.com/doc/refman/5.1/en/server-system- variables.html # sysvar_long_query_time ) variable dynamic , I tried set GLOBAL long_query_time=1 ; After operation I tried mysql > show variables like 'long_query_time ' ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | Variable_name | Value | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | long_query_time | 10.000000 | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ 1 row set ( 0.00 sec ) From mysql console getting altered , ?","You setting GLOBAL system variable , querying SESSION variable . For GLOBAL variable setting take effect current session , need reconnect , set @ @ SESSION.long_query_time variable . ( Note SHOW VARIABLES default shows session variables . ) Here example : mysql > SHOW SESSION VARIABLES LIKE `` long_query_time '' ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | Variable_name | Value | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | long_query_time | 10.000000 | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ mysql > SET @ @ GLOBAL.long_query_time = 1 ; mysql > SHOW GLOBAL VARIABLES LIKE `` long_query_time '' ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + | Variable_name | Value | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + | long_query_time | 1.000000 | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + mysql > SHOW VARIABLES LIKE `` long_query_time '' ; + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | Variable_name | Value | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+ | long_query_time | 10.000000 | + -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -+"
Stackoverflow,"I access command line isql I like get Meta-Data tables given database , possibly formatted file . How I achieve ? Thanks .",Check [ sysobjects ] ( http : //infocenter.sybase.com/help/index.jsp ? topic=/com.sybase.infocenter.dc36274.1570/html/tables/X14933.htm ) [ syscolumns ] ( http : //infocenter.sybase.com/help/index.jsp ? topic=/com.sybase.help.ase_15.0.tables/html/tables/tables46.htm ) tables . [ Here ] ( http : //download.sybase.com/pdfdocs/asg1250e/poster.pdf ) diagram Sybase system tables . List user tables : SELECT * FROM sysobjects WHERE type = ' U' You change ' U ' objects : * C – computed column * D – default * F – SQLJ function * L – log * N – partition condition * P – Transact-SQL SQLJ procedure * PR – prepare objects ( created Dynamic SQL ) * R – rule * RI – referential constraint * S – system table * TR – trigger * U – user table * V – view * XP – extended stored procedure List columns table : SELECT sc . * FROM syscolumns sc INNER JOIN sysobjects ON sc.id = so.id WHERE so.name = 'my_table_name '
Stackoverflow,"I try avoid Count ( _ ) performance issue . ( i.e . SELECT COUNT ( _ ) FROM Users ) If I run followings phpMyAdmin , ok : SELECT SQL_CALC_FOUND_ROWS * FROM Users ; SELECT FOUND_ROWS ( ) ; It return # rows . i.e . # Users . However , I run PHP , I : $ query = 'SELECT SQL_CALC_FOUND_ROWS * FROM Users ; SELECT FOUND_ROWS ( ) ; ' ; mysql_query ( $ query ) ; It seems like PHP n't like two queries passing . So , I ?","` SQL_CALC_FOUND_ROWS ` useful 're using ` LIMIT ` clause , still want know many rows would 've found without ` LIMIT ` . Think works : SELECT SQL_CALC_FOUND_ROWS * FROM Users ; You 're forcing database retrieve/parse ALL data table , throw away . Even n't going retrieve rows , DB server still start pulling actual data disk assumption want data . In human terms , bought entire contents super grocery store , threw away everything except pack gum stand cashier . Whereas , : SELECT count ( * ) FROM users ; lets DB engine know want know many rows , could n't care less actual data . On intelligent DBMS , engine retrieve count table 's metadata , simple run table 's primary key index , without ever touching on-disk row data ."
Stackoverflow,"I 've found answers using mySQL alone , I hoping someone could show way get ID last inserted updated row mysql DB using PHP handle inserts/updates . Currently I something like , column3 unique key , 's also id column 's autoincremented primary key : $ query = '' INSERT INTO TABLE ( column1 , column2 , column3 ) VALUES ( value1 , value2 , value3 ) ON DUPLICATE KEY UPDATE SET column1=value1 , column2=value2 , column3=value3 '' ; mysql_query ( $ query ) ; $ my_id = mysql_insert_id ( ) ; $ my_id correct INSERT , incorrect 's updating row ( ON DUPLICATE KEY UPDATE ) . I seen several posts people advising use something like INSERT INTO table ( ) VALUES ( 0 ) ON DUPLICATE KEY UPDATE id=LAST_INSERT_ID ( id ) get valid ID value ON DUPLICATE KEY invoked -- return valid ID PHP ` mysql_insert_id ( ) ` function ?","Here 's answer , suggested Alexandre : use id=LAST_INSERT_ID ( id ) sets value mysql_insert_id = updated ID -- final code look like : < ? $ query = mysql_query ( `` INSERT INTO table ( column1 , column2 , column3 ) VALUES ( value1 , value2 , value3 ) ON DUPLICATE KEY UPDATE column1 = value1 , column2 = value2 , column3 = value3 , id=LAST_INSERT_ID ( id ) `` ) ; $ my_id = mysql_insert_id ( ) ; This return right value $ my_id regardless update insert ."
Stackoverflow,"I would like send large ` pandas.DataFrame ` remote server running MS SQL . The way I converting ` data_frame ` object list tuples send away pyODBC 's ` executemany ( ) ` function . It goes something like : import pyodbc pdb list_of_tuples = convert_df ( data_frame ) connection = pdb.connect ( cnxn_str ) cursor = connection.cursor ( ) cursor.fast_executemany = True cursor.executemany ( sql_statement , list_of_tuples ) connection.commit ( ) cursor.close ( ) connection.close ( ) I started wonder things sped ( least readable ) using ` data_frame.to_sql ( ) ` method . I came following solution : import sqlalchemy sa engine = sa.create_engine ( `` mssql+pyodbc : /// ? odbc_connect= % '' % cnxn_str ) data_frame.to_sql ( table_name , engine , index=False ) Now code readable , upload **at least 150 times slower** ... Is way flip ` fast_executemany ` using SQLAlchemy ? I using pandas-0.20.3 , pyODBC-4.0.21 sqlalchemy-1.1.13 .","**EDIT ( 08/03/2019 ) : ** Gord Thompson commented good news update logs sqlalchemy : _Since SQLAlchemy 1.3.0 , released 2019-03-04 , sqlalchemy supports ` engine = create_engine ( sqlalchemy_url , fast_executemany=True ) ` ` mssql+pyodbc ` dialect . I.e. , longer necessary define function use ` @ event.listens_for ( engine , 'before_cursor_execute ' ) ` _ Meaning function removed flag needs set create_engine statement - still retaining speed-up . **Original Post : ** Just made account post . I wanted comment beneath thread 's followup already provided answer . The solution worked Version 17 SQL driver Microsft SQL storage writing Ubuntu based install . The complete code I used speed things significantly ( talking > 100x speed-up ) . This turn-key snippet provided alter connection string relevant details . To poster , thank much solution I looking quite time already . import pandas pd import numpy np import time sqlalchemy import create_engine , event urllib.parse import quote_plus conn = `` DRIVER= { ODBC Driver 17 SQL Server } ; SERVER=IP_ADDRESS ; DATABASE=DataLake ; UID=USER ; PWD=PASS '' quoted = quote_plus ( conn ) new_con = 'mssql+pyodbc : /// ? odbc_connect= { } '.format ( quoted ) engine = create_engine ( new_con ) @ event.listens_for ( engine , 'before_cursor_execute ' ) def receive_before_cursor_execute ( conn , cursor , statement , params , context , executemany ) : print ( `` FUNC call '' ) executemany : cursor.fast_executemany = True table_name = 'fast_executemany_test' df = pd.DataFrame ( np.random.random ( ( 10**4 , 100 ) ) ) = time.time ( ) df.to_sql ( table_name , engine , if_exists = 'replace ' , chunksize = None ) print ( time.time ( ) - ) Based comments I wanted take time explain limitations pandas ` to_sql ` implementation way query handled . There 2 things might cause ` MemoryError ` raised afaik : 1 ) Assuming 're writing remote SQL storage . When try write large pandas DataFrame ` to_sql ` method converts entire dataframe list values . This transformation takes way RAM original DataFrame ( top , old DataFrame still remains present RAM ) . This list provided final ` executemany ` call ODBC connector . I think ODBC connector troubles handling large queries . A way solve provide ` to_sql ` method chunksize argument ( 10**5 seems around optimal giving 600 mbit/s ( ! ) write speeds 2 CPU 7GB ram MSSQL Storage application Azure - ca n't recommend Azure btw ) . So first limitation , query size , circumvented providing ` chunksize ` argument . However , enable write dataframe size 10**7 larger , ( least VM I working ~55GB RAM ) , issue nr 2 . This circumvented breaking DataFrame ` np.split ` ( 10**6 size DataFrame chunks ) These written away iteratively . I try make pull request I solution ready ` to_sql ` method core pandas wo n't pre-breaking every time . Anyhow I ended writing function similar ( turn-key ) following : import pandas pd import numpy np def write_df_to_sql ( df , **kwargs ) : chunks = np.split ( df , df.shape ( ) [ 0 ] / 10**6 ) chunk chunks : chunk.to_sql ( **kwargs ) return True A complete example snippet viewed : < https : //gitlab.com/timelord/timelord/blob/master/timelord/utils/connector.py > It 's class I wrote incorporates patch eases necessary overhead comes setting connections SQL . Still write documentation . Also I planning contributing patch pandas n't found nice way yet . I hope helps ."
Stackoverflow,I 'm using SQL Server Profiler figure process consuming SQL process I found event class ` Audit Logout ` causing huge number reads consume cpu process . Is normal ? Or I something wrong SQL Server configuration ?,"The audit logout event aggregates lot values like reads/writes , connection times , etc . time connection opened . See < http : //msdn.microsoft.com/en-us/library/ms175827.aspx > \- definition specific question added : ` Reads ` ` Number logical read I/Os issued user connection. ` So basically , number seeing audit event , actions done connection logging ."
Stackoverflow,"When I try execute following ` SQL ` ` MySQL ` , I 'm getting error : SQL : SQL = `` CREATE TABLE Ranges ( `` ; SQL += `` ID varchar ( 20 ) NOT NULL , `` ; SQL += `` Descriptions longtext NULL , `` ; SQL += `` Version_Number int NULL , `` ; SQL += `` Row_Updated bigint NULL , `` ; SQL += `` Last_Updated datetime NULL , `` ; SQL += `` XML longtext NULL , `` ; SQL += `` PRIMARY KEY ( ID ) '' ; SQL += `` ) `` + `` TYPE = InnoDB '' ; Error : **You error SQL syntax ; check manual corresponds MySQL server version right syntax use near `` TYPE = InnoDB '' ** But I remove ` `` TYPE = InnoDB '' ` , query works fine . Previously query worked fine , ie ` MySQL 5.0 ` . But I upgraded ` MySQL 5.6 ` , I 'm getting error . Any Suggestions / Alternatives ... ? ?",Use ` ENGINE = Innodb ` instead ` TYPE = InnoDB ` . ` TYPE ` removed 5.1 .
Stackoverflow,"I getting following error , I try import MYSQL database : Error Code : 2013 - Lost connection MySQL server queryQuery : Error Code : 2006 - MySQL server gone away Can someone let know wrong ?",Investigation shows many solutions correctly talking setting max_allowed_packet wait_timeout mysql my.cnf ; small addendum default install mysql mac osx n't appear include file . You may first need create /etc/my.cnf ( issue 're using default install mysql instead mamp stack similar ) contents /etc/my.cnf corrected issue : [ mysqld ] max_allowed_packet= 64M wait_timeout= 6000
Stackoverflow,"I SQL Server Database Azure Cloud I want get report server runnning using SSRS would accesses data database . Does anyone experience scenario , could provide guidance go . I 'm reading SSRS would run Azure VM SQL Server 's respective Database ( ) would installed Azure VM . That 's situation I existing SQL Database exists outside VM I spin . Thanks ,","You need host SSRS either Azure VM premise . In [ link ] ( https : //web.archive.org/web/20140523110804/http : //msdn.microsoft.com/en- us/library/azure/jj992719.aspx ) deployment topologies SSRS Azure VM discussed . One strategy deploy SSRS VM use Azure SQL Database data source . Once deploy VM containing SSRS , connect SSRS Azure SQL DB . This artcle discusses [ connecting Azure SQL Database SSRS ] ( https : //msdn.microsoft.com/en-us/library/ff519560.aspx ) . Hope helps !"
Stackoverflow,"I 'm ussing Appache Jackrabbit JCA 2.7.5 , problem files .docx .xlsx indexed . My steps : * Deploy [ Jackrabbit JCA ] ( https : //archive.apache.org/dist/jackrabbit/2.7.5/jackrabbit-jca-2.7.5.rar ) ` resource adapter ` glassfish * create ` Connector Connection Pool ` ` resource adapter ` indicating ` ConfigFile=path/to/the/repository.xml ` ` HomeDir=path/to/the //miss repository.xml ` * create ` Connector Resources ` connector pool ( jndi ) * create web application * create class get session connector ressources ( code ) import java.io.Serializable ; import java.net.MalformedURLException ; import javax.annotation.Resource ; import javax.ejb.Stateless ; import javax.jcr.LoginException ; import javax.jcr.Repository ; import javax.jcr.RepositoryException ; import javax.jcr.Session ; import javax.jcr.SimpleCredentials ; import javax.naming.InitialContext ; import javax.naming.NamingException ; @ Stateless public class OcmRepository implements Serializable { public Repository repository ; public Session session ; public OcmRepository ( ) { } public Session getSession ( String log , String mdp ) throws LoginException , RepositoryException , NamingException , MalformedURLException { InitialContext initalContext = new InitialContext ( ) ; repository = ( Repository ) initalContext.lookup ( `` jndi/jca '' ) ; session = repository.login ( new SimpleCredentials ( log , mdp.toCharArray ( ) ) , null ) ; return session ; } } * Create custom filetype import javax.jcr.PropertyType ; import javax.jcr.Session ; import javax.jcr.nodetype.NodeType ; import javax.jcr.nodetype.NodeTypeManager ; import javax.jcr.nodetype.NodeTypeTemplate ; import javax.jcr.nodetype.PropertyDefinitionTemplate ; /** * * @ author nathan */ public class FileType { public static void RegisterFileType ( Session session ) throws Exception { NodeTypeManager nodeTypeManager = session.getWorkspace ( ) .getNodeTypeManager ( ) ; NodeTypeTemplate nodeType = nodeTypeManager.createNodeTypeTemplate ( ) ; nodeType.setName ( `` FileType '' ) ; String [ ] str = { `` nt : resource '' } ; nodeType.setDeclaredSuperTypeNames ( str ) ; nodeType.setMixin ( false ) ; nodeType.setQueryable ( true ) ; PropertyDefinitionTemplate path = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; path.setName ( `` jcr : path '' ) ; path.setRequiredType ( PropertyType.PATH ) ; path.setQueryOrderable ( false ) ; path.setFullTextSearchable ( false ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( path ) ; PropertyDefinitionTemplate nom = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; nom.setName ( `` jcr : nom '' ) ; nom.setRequiredType ( PropertyType.STRING ) ; nom.setQueryOrderable ( true ) ; nom.setFullTextSearchable ( true ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( nom ) ; PropertyDefinitionTemplate description = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; description.setName ( `` jcr : description '' ) ; description.setRequiredType ( PropertyType.STRING ) ; description.setQueryOrderable ( true ) ; description.setFullTextSearchable ( true ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( description ) ; PropertyDefinitionTemplate motsCles = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; motsCles.setName ( `` jcr : motsCles '' ) ; motsCles.setRequiredType ( PropertyType.STRING ) ; motsCles.setQueryOrderable ( true ) ; motsCles.setFullTextSearchable ( true ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( motsCles ) ; PropertyDefinitionTemplate size = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; size.setName ( `` jcr : size '' ) ; size.setRequiredType ( PropertyType.STRING ) ; size.setQueryOrderable ( true ) ; size.setFullTextSearchable ( false ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( size ) ; PropertyDefinitionTemplate users = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; users.setName ( `` jcr : users '' ) ; users.setRequiredType ( PropertyType.STRING ) ; users.setQueryOrderable ( true ) ; users.setFullTextSearchable ( false ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( users ) ; PropertyDefinitionTemplate groupe = nodeTypeManager.createPropertyDefinitionTemplate ( ) ; groupe.setName ( `` jcr : groupe '' ) ; groupe.setRequiredType ( PropertyType.STRING ) ; groupe.setQueryOrderable ( true ) ; groupe.setFullTextSearchable ( false ) ; nodeType.getPropertyDefinitionTemplates ( ) .add ( groupe ) ; NodeType newnodetype = nodeTypeManager.registerNodeType ( nodeType , true ) ; session.save ( ) ; } } * Create abstract class persistence import java.util.ArrayList ; import java.util.List ; import java.util.Map ; import javax.jcr.Session ; import org.apache.jackrabbit.ocm.query.Filter ; import org.apache.jackrabbit.ocm.query.impl.FilterImpl ; import org.apache.jackrabbit.ocm.query.impl.QueryImpl ; import org.apache.jackrabbit.ocm.query.Query ; import org.apache.jackrabbit.ocm.query.QueryManager ; import org.apache.jackrabbit.ocm.manager.ObjectContentManager ; import org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl ; import org.apache.jackrabbit.ocm.mapper.Mapper ; import org.apache.jackrabbit.ocm.mapper.impl.annotation.AnnotationMapperImpl ; import org.apache.jackrabbit.ocm.reflection.ReflectionUtils ; /** * * @ author nathan */ public abstract class AbstractBean < T > { private Class < T > entityClass ; private ObjectContentManager ocm ; private Mapper mapper ; public AbstractBean ( Class < T > entityClass ) { this.entityClass = entityClass ; } /** * Construct Bean according extended class * This also construct ObjectContentManager nammed ocm default Mapper * @ param session javax.jcr.Session attached Bean * @ return The mapping class found desired java bean class */ public AbstractBean ( Class < T > entityClass , Session session ) { this.entityClass = entityClass ; ocm = new ObjectContentManagerImpl ( session , this.getDefaultMapper ( ) ) ; } /** * @ return ObjectContentManager Bean */ public ObjectContentManager getOcm ( ) throws Exception { return ocm ; } /** * Construct Bean according extended class * This also construct ObjectContentManager nammed ocm param Mapper given * @ param session `` javax.jcr.Session '' attached Bean * @ param map `` org.apache.jackrabbit.ocm.mapper.Mapper '' * use map entity apllication The repository * @ return ObjectContentManager Bean */ public ObjectContentManager getOcm ( Session session , Mapper map ) throws Exception { return new ObjectContentManagerImpl ( session , map ) ; } public void setOcm ( ObjectContentManager ocm ) { this.ocm = ocm ; } private Mapper getDefaultMapper ( ) { ReflectionUtils.setClassLoader ( com.ged.ocm.entity.Groupe.class.getClassLoader ( ) ) ; List < Class > classes = new ArrayList < Class > ( ) ; classes.add ( com.ged.ocm.entity.Fichier.class ) ; classes.add ( com.ged.ocm.entity.Dossier.class ) ; classes.add ( com.ged.ocm.entity.Groupe.class ) ; classes.add ( com.ged.ocm.entity.SimpleNode.class ) ; return new AnnotationMapperImpl ( classes ) ; } public Mapper getMapper ( ) { return mapper ; } public void setMapper ( Mapper mapper ) { this.mapper = mapper ; } public void setLoader ( Class classe ) { ReflectionUtils.setClassLoader ( classe.getClassLoader ( ) ) ; } public void create ( T entity ) { ocm.insert ( entity ) ; ocm.save ( ) ; } public void edit ( T entity ) { ocm.update ( entity ) ; ocm.save ( ) ; } public void remove ( T entity ) { ocm.remove ( entity ) ; ocm.save ( ) ; } public void refresh ( ) { ocm.refresh ( true ) ; ocm.save ( ) ; } public void copy ( String orgPath , String destPath ) { ocm.copy ( orgPath , destPath ) ; ocm.save ( ) ; } public void move ( String orgPath , String destPath ) { ocm.move ( orgPath , destPath ) ; ocm.save ( ) ; } public void removeByPath ( String path ) { ocm.remove ( path ) ; ocm.save ( ) ; } public void removeAllByEqual ( Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; ocm.remove ( query ) ; ocm.save ( ) ; } public void removeAllByEqual ( String nodePath , Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; ocm.remove ( query ) ; ocm.save ( ) ; } public boolean isPathExist ( String path ) { return ocm.objectExists ( path ) ; } public T findByPath ( String path ) { try { return ( T ) ocm.getObject ( path ) ; } catch ( Exception e ) { return null ; } } public T findOneByEqual ( Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; T result = null ; try { result = results.get ( 0 ) ; } catch ( Exception e ) { } return result ; } public List < T > findAllByEqual ( Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( `` // '' ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public List < T > findAllByLike ( Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( `` // '' ) ; ( String key : filters.keySet ( ) ) filter.addLike ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public List < T > findAllByLikeScoped ( String scope , Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( scope ) ; ( String key : filters.keySet ( ) ) filter.addLike ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public List < T > findAllByOrLike ( String attr , String [ ] val ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( `` // '' ) ; filter.addOrFilter ( attr , val ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public T findOneByEqual ( String nodePath , Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; T result = results.get ( 0 ) ; return result ; } public List < T > findAllByEqual ( String nodePath , Map < String , String > filters ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; ( String key : filters.keySet ( ) ) filter.addEqualTo ( key , filters.get ( key ) ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public List < T > findAllByString ( String query ) { List < T > results = ( List < T > ) ocm.getObjects ( query , javax.jcr.query.Query.JCR_SQL2 ) ; return results ; } public List < T > findAllByParentPath ( String nodePath ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public List < T > findAllByParentPathOrder ( String nodePath , String ordering ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; Query query = queryManager.createQuery ( filter ) ; // query.addOrderByDescending ( ordering ) ; query.addOrderByAscending ( ordering ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results ; } public int coutChild ( String nodePath ) { QueryManager queryManager = ocm.getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( entityClass ) ; filter.setScope ( nodePath ) ; Query query = queryManager.createQuery ( filter ) ; List < T > results = ( List < T > ) ocm.getObjects ( query ) ; return results.size ( ) ; } public boolean ifExistByPath ( String path ) { return ocm.objectExists ( path ) ; } public String getParentPath ( String path ) { String parent= '' '' ; String [ ] tmp=path.split ( `` / '' ) ; ( int = 1 ; < ( tmp.length-1 ) ; i++ ) { parent+= '' / '' +tmp [ ] ; } return parent ; } } * Create bean import javax.ejb.Stateless ; import com.ged.ocm.entity.Fichier ; import java.io.InputStream ; import java.util.ArrayList ; import java.util.List ; import java.util.Map ; import javax.jcr.Node ; import javax.jcr.NodeIterator ; import javax.jcr.Session ; import javax.jcr.Workspace ; import javax.jcr.query.QueryResult ; import javax.jcr.query.qom.FullTextSearch ; import javax.jcr.query.qom.StaticOperand ; import org.apache.jackrabbit.ocm.query.Filter ; import org.apache.jackrabbit.ocm.query.Query ; import org.apache.jackrabbit.ocm.query.QueryManager ; @ Stateless public class FichierBean extends AbstractBean < Fichier > { public FichierBean ( ) { super ( Fichier.class ) ; } public FichierBean ( Session session ) { super ( Fichier.class , session ) ; } public List < Fichier > findAllByContains ( String motCles ) throws Exception { String requette = `` SELECT * FROM FileType AS Res WHERE CONTAINS ( Res . * , '* '' +motCles+ '' * ' ) '' ; List < Fichier > results = ( List < Fichier > ) this.getOcm ( ) .getObjects ( requette , javax.jcr.query.Query.JCR_SQL2 ) ; return results ; } public List < Fichier > findAllByContains ( String path , String motCles ) throws Exception { String requette = `` SELECT * FROM FileType AS Res WHERE CONTAINS ( Res . * , '* '' +motCles+ '' * ' ) ORDER BY Res.nom '' ; List < Fichier > tmp = ( List < Fichier > ) this.getOcm ( ) .getObjects ( requette , javax.jcr.query.Query.JCR_SQL2 ) ; List < Fichier > results = new ArrayList < Fichier > ( ) ; ( Fichier fichier : tmp ) { ( fichier.getPath ( ) .startsWith ( path ) ) results.add ( fichier ) ; } return results ; } public List < Fichier > fulltextByOCM ( String motCles ) throws Exception { QueryManager queryManager = this.getOcm ( ) .getQueryManager ( ) ; Filter filter ; filter = queryManager.createFilter ( com.ged.ocm.entity.Fichier.class ) ; filter.addContains ( `` . `` , `` * '' +motCles+ '' * '' ) ; Query query = queryManager.createQuery ( filter ) ; List < Fichier > results = ( List < Fichier > ) this.getOcm ( ) .getObjects ( query ) ; return results ; } } My configuration files : * repository.xml < ? xml version= '' 1.0 '' ? > < ! DOCTYPE Repository PUBLIC `` -//The Apache Software Foundation//DTD Jackrabbit 1.6//EN '' `` http : //jackrabbit.apache.org/dtd/repository-1.6.dtd '' > < Repository > < FileSystem class= '' org.apache.jackrabbit.core.fs.local.LocalFileSystem '' > < param name= '' path '' value= '' $ { rep.home } /repository '' / > < /FileSystem > -- > < FileSystem class= '' org.apache.jackrabbit.core.fs.db.DbFileSystem '' > < param name= '' driver '' value= '' com.mysql.jdbc.jdbc2.optional.MysqlDataSource '' / > < param name= '' url '' value= '' jdbc : mysql : //:3306/db_ged_mysql '' / > < param name= '' user '' value= '' root '' / > < param name= '' password '' value= '' root '' / > < param name= '' schema '' value= '' mysql '' / > < param name= '' schemaObjectPrefix '' value= '' J_R_FS_ '' / > < /FileSystem > < ! -- security configuration -- > < Security appName= '' Jackrabbit '' > < AccessManager class= '' org.apache.jackrabbit.core.security.SimpleAccessManager '' / > < LoginModule class= '' org.apache.jackrabbit.core.security.SimpleLoginModule '' > < param name= '' anonymousId '' value= '' anonymous '' / > < /LoginModule > < /Security > < ! -- location workspaces root directory name default workspace -- > < Workspaces rootPath= '' $ { rep.home } /workspaces '' defaultWorkspace= '' default '' / > < ! -- workspace configuration template : used create initial workspace 's workspace yet -- > < Workspace name= '' $ { wsp.name } '' > < PersistenceManager class= '' org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager '' > < param name= '' driver '' value= '' com.mysql.jdbc.jdbc2.optional.MysqlDataSource '' / > < param name= '' url '' value= '' jdbc : mysql : //:3306/db_ged_mysql '' / > < param name= '' user '' value= '' root '' / > < param name= '' password '' value= '' root '' / > < param name= '' schema '' value= '' mysql '' / > < param name= '' schemaObjectPrefix '' value= '' J_PM_ $ { wsp.name } _ '' / > < param name= '' externalBLOBs '' value= '' false '' / > < /PersistenceManager > < FileSystem class= '' org.apache.jackrabbit.core.fs.db.DbFileSystem '' > < param name= '' driver '' value= '' com.mysql.jdbc.jdbc2.optional.MysqlDataSource '' / > < param name= '' url '' value= '' jdbc : mysql : //:3306/db_ged_mysql '' / > < param name= '' user '' value= '' root '' / > < param name= '' password '' value= '' root '' / > < param name= '' schema '' value= '' mysql '' / > < param name= '' schemaObjectPrefix '' value= '' J_FS_ $ { wsp.name } _ '' / > < /FileSystem > < ! -- Search index file system uses . class : FQN class implementing QueryHandler interface -- > < SearchIndex class= '' org.apache.jackrabbit.core.query.lucene.SearchIndex '' > < param name= '' path '' value= '' $ { rep.home } /workspaces/ $ { wsp.name } /index '' / > < param name= '' tikaConfigPath '' value= '' $ { rep.home } /tika-config.xml '' / > < param name= '' useCompoundFile '' value= '' true '' / > < param name= '' minMergeDocs '' value= '' 100 '' / > < param name= '' volatileIdleTime '' value= '' 3 '' / > < param name= '' maxMergeDocs '' value= '' 2147483647 '' / > < param name= '' mergeFactor '' value= '' 10 '' / > < param name= '' maxFieldLength '' value= '' 10000 '' / > < param name= '' bufferSize '' value= '' 10 '' / > < param name= '' cacheSize '' value= '' 1000 '' / > < param name= '' forceConsistencyCheck '' value= '' false '' / > < param name= '' enableConsistencyCheck '' value= '' false '' / > < param name= '' autoRepair '' value= '' true '' / > < param name= '' analyzer '' value= '' org.apache.lucene.analysis.standard.StandardAnalyzer '' / > < param name= '' queryClass '' value= '' org.apache.jackrabbit.core.query.QueryImpl '' / > < param name= '' respectDocumentOrder '' value= '' true '' / > < param name= '' resultFetchSize '' value= '' 2147483647 '' / > < param name= '' extractorPoolSize '' value= '' 0 '' / > < param name= '' extractorTimeout '' value= '' 100 '' / > < param name= '' extractorBackLogSize '' value= '' 100 '' / > < param name= '' supportHighlighting '' value= '' true '' / > < param name= '' excerptProviderClass '' value= '' org.apache.jackrabbit.core.query.lucene.DefaultXMLExcerpt '' / > < /SearchIndex > < /Workspace > < ! -- Configures versioning -- > < Versioning rootPath= '' $ { rep.home } /version '' > < FileSystem class= '' org.apache.jackrabbit.core.fs.db.DbFileSystem '' > < param name= '' driver '' value= '' com.mysql.jdbc.jdbc2.optional.MysqlDataSource '' / > < param name= '' url '' value= '' jdbc : mysql : //:3306/db_ged_mysql '' / > < param name= '' user '' value= '' root '' / > < param name= '' password '' value= '' root '' / > < param name= '' schema '' value= '' mysql '' / > < param name= '' schemaObjectPrefix '' value= '' J_V_FS_ '' / > < /FileSystem > < PersistenceManager class= '' org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager '' > < param name= '' driver '' value= '' com.mysql.jdbc.jdbc2.optional.MysqlDataSource '' / > < param name= '' url '' value= '' jdbc : mysql : //:3306/db_ged_mysql '' / > < param name= '' user '' value= '' root '' / > < param name= '' password '' value= '' root '' / > < param name= '' schema '' value= '' mysql '' / > < param name= '' schemaObjectPrefix '' value= '' J_V_PM_ '' / > < param name= '' externalBLOBs '' value= '' false '' / > < /PersistenceManager > < /Versioning > < ! -- Search index content shared repository wide ( /jcr : system tree , contains mainly versions ) < SearchIndex class= '' org.apache.jackrabbit.core.query.lucene.SearchIndex '' > < param name= '' path '' value= '' $ { rep.home } /repository/index '' / > < param name= '' extractorPoolSize '' value= '' 2 '' / > < param name= '' supportHighlighting '' value= '' true '' / > < /SearchIndex > -- > < ! -- Cluster configuration system variables . -- > < RepositoryLockMechanism class= '' org.apache.jackrabbit.core.util.CooperativeFileLock '' / > < /Repository > * tika-config.xml < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < properties > < mimeTypeRepository resource= '' /org/apache/tika/mime/tika-mimetypes.xml '' magic= '' false '' / > < parsers > < parser name= '' parse-dcxml '' class= '' org.apache.tika.parser.xml.DcXMLParser '' > < mime > application/xml < /mime > < mime > image/svg+xml < /mime > < /parser > < parser name= '' parse-office '' class= '' org.apache.tika.parser.microsoft.OfficeParser '' > < mime > application/x-tika-msoffice < /mime > < mime > application/msword < /mime > < mime > application/vnd.ms-excel < /mime > < mime > application/vnd.ms-excel.sheet.binary.macroenabled.12 < /mime > < mime > application/vnd.ms-powerpoint < /mime > < mime > application/vnd.visio < /mime > < mime > application/vnd.ms-outlook < /mime > < /parser > < parser name= '' parse-ooxml '' class= '' org.apache.tika.parser.microsoft.ooxml.OOXMLParser '' > < mime > application/x-tika-ooxml < /mime > < mime > application/vnd.openxmlformats-package.core-properties+xml < /mime > < mime > application/vnd.openxmlformats-officedocument.spreadsheetml.sheet < /mime > < mime > application/vnd.openxmlformats-officedocument.spreadsheetml.template < /mime > < mime > application/vnd.ms-excel.sheet.macroenabled.12 < /mime > < mime > application/vnd.ms-excel.template.macroenabled.12 < /mime > < mime > application/vnd.ms-excel.addin.macroenabled.12 < /mime > < mime > application/vnd.openxmlformats-officedocument.presentationml.presentation < /mime > < mime > application/vnd.openxmlformats-officedocument.presentationml.template < /mime > < mime > application/vnd.openxmlformats-officedocument.presentationml.slideshow < /mime > < mime > application/vnd.ms-powerpoint.presentation.macroenabled.12 < /mime > < mime > application/vnd.ms-powerpoint.slideshow.macroenabled.12 < /mime > < mime > application/vnd.ms-powerpoint.addin.macroenabled.12 < /mime > < mime > application/vnd.openxmlformats-officedocument.wordprocessingml.document < /mime > < mime > application/vnd.openxmlformats-officedocument.wordprocessingml.template < /mime > < mime > application/vnd.ms-word.document.macroenabled.12 < /mime > < mime > application/vnd.ms-word.template.macroenabled.12 < /mime > < /parser > < parser name= '' parse-html '' class= '' org.apache.tika.parser.html.HtmlParser '' > < mime > text/html < /mime > < mime > application/xhtml+xml < /mime > < mime > application/x-asp < /mime > < /parser > < parser mame= '' parse-rtf '' class= '' org.apache.tika.parser.rtf.RTFParser '' > < mime > application/rtf < /mime > < /parser > < parser name= '' parse-pdf '' class= '' org.apache.tika.parser.pdf.PDFParser '' > < mime > application/pdf < /mime > < /parser > < parser name= '' parse-txt '' class= '' org.apache.tika.parser.txt.TXTParser '' > < mime > text/plain < /mime > < /parser > < parser name= '' parse-openoffice '' class= '' org.apache.tika.parser.opendocument.OpenOfficeParser '' > < mime > application/vnd.sun.xml.writer < /mime > < mime > application/vnd.oasis.opendocument.text < /mime > < mime > application/vnd.oasis.opendocument.graphics < /mime > < mime > application/vnd.oasis.opendocument.presentation < /mime > < mime > application/vnd.oasis.opendocument.spreadsheet < /mime > < mime > application/vnd.oasis.opendocument.chart < /mime > < mime > application/vnd.oasis.opendocument.image < /mime > < mime > application/vnd.oasis.opendocument.formula < /mime > < mime > application/vnd.oasis.opendocument.text-master < /mime > < mime > application/vnd.oasis.opendocument.text-web < /mime > < mime > application/vnd.oasis.opendocument.text-template < /mime > < mime > application/vnd.oasis.opendocument.graphics-template < /mime > < mime > application/vnd.oasis.opendocument.presentation-template < /mime > < mime > application/vnd.oasis.opendocument.spreadsheet-template < /mime > < mime > application/vnd.oasis.opendocument.chart-template < /mime > < mime > application/vnd.oasis.opendocument.image-template < /mime > < mime > application/vnd.oasis.opendocument.formula-template < /mime > < mime > application/x-vnd.oasis.opendocument.text < /mime > < mime > application/x-vnd.oasis.opendocument.graphics < /mime > < mime > application/x-vnd.oasis.opendocument.presentation < /mime > < mime > application/x-vnd.oasis.opendocument.spreadsheet < /mime > < mime > application/x-vnd.oasis.opendocument.chart < /mime > < mime > application/x-vnd.oasis.opendocument.image < /mime > < mime > application/x-vnd.oasis.opendocument.formula < /mime > < mime > application/x-vnd.oasis.opendocument.text-master < /mime > < mime > application/x-vnd.oasis.opendocument.text-web < /mime > < mime > application/x-vnd.oasis.opendocument.text-template < /mime > < mime > application/x-vnd.oasis.opendocument.graphics-template < /mime > < mime > application/x-vnd.oasis.opendocument.presentation-template < /mime > < mime > application/x-vnd.oasis.opendocument.spreadsheet-template < /mime > < mime > application/x-vnd.oasis.opendocument.chart-template < /mime > < mime > application/x-vnd.oasis.opendocument.image-template < /mime > < mime > application/x-vnd.oasis.opendocument.formula-template < /mime > < /parser > < parser name= '' parse-image '' class= '' org.apache.tika.parser.image.ImageParser '' > < mime > image/bmp < /mime > < mime > image/gif < /mime > < mime > image/jpeg < /mime > < mime > image/png < /mime > < mime > image/tiff < /mime > < mime > image/vnd.wap.wbmp < /mime > < mime > image/x-icon < /mime > < mime > image/x-psd < /mime > < mime > image/x-xcf < /mime > < /parser > < parser name= '' parse-class '' class= '' org.apache.tika.parser.asm.ClassParser '' > < mime > application/x-tika-java-class < /mime > < /parser > < parser name= '' parse-mp3 '' class= '' org.apache.tika.parser.mp3.Mp3Parser '' > < mime > audio/mpeg < /mime > < /parser > < parser name= '' parse-midi '' class= '' org.apache.tika.parser.audio.MidiParser '' > < mime > application/x-midi < /mime > < mime > audio/midi < /mime > < /parser > < parser name= '' parse-audio '' class= '' org.apache.tika.parser.audio.AudioParser '' > < mime > audio/basic < /mime > < mime > audio/x-wav < /mime > < mime > audio/x-aiff < /mime > < /parser > < /parsers > < /properties > All query bean work except I call function ` public List < Fichier > findAllByContains ( String path , String motCles ) ` fulltext search .docx .xslx document . Fulltext search .pdf , .txt , .xml , .xls , .doc , ... work perfectly .","You treat criteria multi-valued properties like criteria . For example , following query find nodes value 'white dog ' 'someProp ' property : SELECT * FROM [ nt : unstructured ] WHERE someProp = 'white dog' If 'someProp ' property multiple values , node least one value satisfies criteria included results . To find nodes multiple values multi-valued property , simply AND together multiple criteria . For example , following query return nodes specified values : SELECT * FROM [ nt : unstructured ] WHERE someProp = 'white dog ' AND someProp = 'black dog' Any operators work , including 'LIKE ' : SELECT * FROM [ nt : unstructured ] WHERE someProp LIKE ' % white % ' AND someProp LIKE ' % black % ' Other combinations possible , course ."
Stackoverflow,Ive actually problem I 've finally decided take . Postgres initially installed using Brew . After upgrade OSX 10.8.2 receive psql : could connect server : No file directory Is server running locally accepting connections Unix domain socket `` /tmp/.s.PGSQL.5432 '' ? error I typed $ psql command . I see following processes : $ ps auxw | grep post Tulsa 59222 0.0 0.2 2483256 14808 ? ? Ss Thu03PM 0:02.61 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid 470DA5CC-1602-4D69-855F-F365A6512F90 -post-exec 4 Tulsa 57940 0.0 0.2 2498852 13648 ? ? Ss Wed10PM 0:00.61 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid FAFAAAB4-0F67-42C8-864E-EF8C31A42EE3 -post-exec 4 root 24447 0.0 0.1 2476468 10080 ? ? Ss 8Feb13 0:03.40 /System/Library/PrivateFrameworks/DiskImages.framework/Resources/diskimages-helper -uuid CC768720-12C2-436C-9020-548C275A6B0C -post-exec 4 Tulsa 74224 0.0 0.0 2432768 596 s002 R+ 7:24PM 0:00.00 grep post $ psql /usr/local/bin/psql $ pg_ctl start pg_ctl : database directory specified environment variable PGDATA unset Try `` pg_ctl -- help '' information .,"You n't started Postgres server . Some dmg packages Postgres set run service startup . But however install . You need init data directory , start postgres , go . initdb /some/directory # ONCE pg_ctl -D /some/directory start # many options , e.g . logging , available psql postgres You set environment variable data directory wo n't need ` -D ` flag later . You look later ."
Stackoverflow,"I 'm R user , I frequently find I need write functions require subsetting large datasets ( 10s millions rows ) . When I apply functions large number observations , get time consuming I 'm careful I implement . To , I sometimes used data.table package , provides much faster speeds subsetting using data frames . Recently , I 've started experimenting packages like RMySQL , pushing tables mysql , using package run sql queries return results . I found mixed performance improvements . For smaller datasets ( millions ) , seems loading data data.table setting right keys makes faster subsetting . For larger datasets ( 10s 100s millions ) , appears sending query mysql moves faster . Was wondering anyone insight technique return simple subsetting aggregation queries faster , whether depend size data ? I understand setting keys data.table somewhat analogous creating index , I n't much intuition beyond .","Maybe open new DB connection ` obj < - dbConnect ( ... ) ` every time send query code . You simply call ` dbDisconnect ( obj ) ` object created kill respective connection everytime query executed . Also use function kill open connections : library ( RMySQL ) killDbConnections < - function ( ) { all_cons < - dbListConnections ( MySQL ( ) ) print ( all_cons ) ( con all_cons ) + dbDisconnect ( con ) print ( paste ( length ( all_cons ) , `` connections killed . '' ) ) } I 'd recommed write small function outside shiny handles whole opening closing thing : library ( RMySQL ) sqlQuery < - function ( query ) { # creating DB connection object RMysql package DB < - dbConnect ( MySQL ( ) , user= '' youruser '' , password='yourpassword ' , dbname='yourdb ' , host='192.168.178.1 ' ) # close db connection function call exits on.exit ( dbDisconnect ( DB ) ) # send Query btain result set rs < - dbSendQuery ( DB , query ) # get elements result sets convert dataframe result < - fetch ( rs , -1 ) # return dataframe return ( result ) } Hope helps !"
Stackoverflow,"Our database seems broken , normally uses 1-2 % cpu , run additional backend services making UPDATE INSERT queries 10M rows table ( 1 query per 3 second ) everything going hell ( including CPU increase 2 % 98 % usage ) . We decided debug 's going , run VACUUM ANALYZE learn 's wrong db ... production= # ANALYZE VERBOSE users_user ; INFO : analyzing `` public.users_user '' INFO : `` users_user '' : scanned 280 280 pages , containing 23889 live rows 57 dead rows ; 23889 rows sample , 23889 estimated total rows INFO : analyzing `` public.users_user '' INFO : `` users_user '' : scanned 280 280 pages , containing 23889 live rows 57 dead rows ; 23889 rows sample , 23889 estimated total rows ERROR : tuple already updated self able finish ANALYZE ANY tables could find information issue . Any suggestions wrong ? PostgreSQL 9.6.8 x86_64-pc-linux-gnu , compiled gcc ( GCC ) 4.8.5 20150623 ( Red Hat 4.8.5-16 ) , 64-bit **Additional info requested comments : ** > Maybe corrupted pg_class SELECT * FROM pg_class WHERE relname = 'users_user ' ; Output : < https : //pastebin.com/WhmkH34U > > So first thing would kick sessions try There additional sessions , dumped whole DB new testing server , issue still occur , clients connected DB",` query.on ` removed node-pg 7 . See < https : //node-postgres.com/guides/upgrading > properly handle rows . The usual way use promises async/await ( using promises clearer way ) : await client.connect ( ) ; var res = await client.query ( `` SELECT * FROM json_test '' ) ; res.rows.forEach ( row= > { console.log ( row ) ; } ) ; await client.end ( ) ;
Stackoverflow,"I certain table mySQL field called `` image '' datatype `` BLOB '' . I wondering possible upload image field directly Command Line Client rather php ... If possible , exactly I place image files ?","Please try code INSERT INTO xx_BLOB ( ID , IMAGE ) VALUES ( 1 , LOAD_FILE ( ' E : /Images/jack.jpg ' ) ) ;"
Stackoverflow,"I ASP.NET MVC workflow configured two websites managed load balancer . The websites use Sql Server session state provider authentication switch ( required ) . Now , sporadically I appear losing session state , I believe request handled alternative server , essentially user jumping server server , depending load balancer sees fit . I always `` losing session state '' stage workflow , I believe something related web farm configuration + sql server session state . Both applications use machine key encrypt decrypt session state stored sql server . The configuration servers follows : < authentication mode= '' None '' / > < sessionState mode= '' SQLServer '' sqlConnectionString= '' { connection-string } '' / > < machineKey decryptionKey= '' 777CB456774AF02F7F1AC8570FAF31545B156354D9E2DAAD '' validationKey= '' 89B5B536D5D17B8FE6A53CBB3CA8B8695289BA3DF0B1370BC47D362D375CF91525DDB5307D8A288230DCD4B3931D23AED4E223955C45CFF2AF66BCC422EC7ECD '' / > I 've confirmed identical servers , something I missing ? This occur development environment I using single server . I fear I suffering Friday blues , doubt figure answer next week , sadly I n't want wait ! Any ideas ?","Found issue . When create applications expect share session state using Sql Server , need ID configured IIS . This session ID generated generated based application ID . ( Internally application ID something like _LM/W3SVC/1_ The two servers different IDs application IIS . The resolution change ID ` Manage Website - > Advanced Settings ' server . ! [ enter image description ] ( https : //i.stack.imgur.com/KsaLW.png )"
Stackoverflow,"I 've made web application using SQLite ( 2.8.17 ) , I 've discovered 's SQLite3 . It somehow eluded attention making web application , probably due lack documentation php functions . I 'm wondering , benefits using SQLite3 SQLite ? Is considerably faster ?","SQLite2 internally stores every value string , regardless type . Upgrading SQLite3 certainly shrink database size since numbers BLOBS get stored native formats , could make things run faster . Another big advantage opinion recent versions sqlite , ( starting 3.6.23 ) support foreign keys . Since using PHP , I would suggest look [ PDO ] ( http : //www.php.net/manual/en/book.pdo.php ) . It could prove helpful case need change DBMS application"
Stackoverflow,"I database updated datasets time time . Here may happen dataset delivered already exists database . Currently I 'm first SELECT FROM ... WHERE val1= ... AND val2= ... check , dataset data already exists ( using data WHERE-statement ) . If return value , I 'm INSERT . But seems bit complicated . So question : kind conditional INSERT adds new dataset case exist ? I 'm using ** [ SmallSQL ] ( http : //www.smallsql.de/ ) **","You single statement subquery nearly relational databases . INSERT INTO targetTable ( field1 ) SELECT field1 FROM myTable WHERE NOT ( field1 IN ( SELECT field1 FROM targetTable ) ) Certain relational databases improved syntax , since describe fairly common task . SQL Server ` MERGE ` syntax kinds options , MySQL optional ` INSERT OR IGNORE ` syntax . **Edit : ** [ SmallSQL's documentation ] ( http : //www.smallsql.de/doc/sqlsyntax.html ) fairly sparse parts SQL standard implements . It may implement subqueries , may unable follow advice , anywhere else , need stick SmallSQL ."
Stackoverflow,Why I get # 1060 - Duplicate column name 'id' SELECT COUNT ( * ) FROM ( SELECT * FROM ` tips ` ` ` LEFT JOIN tip_usage ON tip_usage.tip_id=t.id GROUP BY t.id ) sq,Probably * ` select * ` selects two columns name ` tip_usage ` ` tips ` .
Stackoverflow,"I one entity class public class someclass { public string property1 { get ; set ; } public string property2 { get ; set ; } public string property3 { get ; set ; } } using sqlite connection class obj DB I creating table Db.CreateTableAsync < someclass > ( ) .GetAwaiter ( ) .GetResult ( ) ; What I want achieve , I n't want sqlite create column table ` property3 ` . Is way achieve ? I using SQLiteAsync library windows store apps .",You use ` Ignore ` attribute : public class someclass { public string property1 { get ; set ; } public string property2 { get ; set ; } [ Ignore ] public string property3 { get ; set ; } }
Stackoverflow,"I 've using SQLite.swift lately build app database . And I 'm defining ` INTEGER ` columns ` Int64 ` type , like documentation explains . But every I need ` Int64 ` ` Int ` . So question , I : //Create table Int instead Int64 let test_id = Expression < Int > ( `` test_id '' ) let tests = db [ `` tests '' ] db.create ( table : tests , ifNotExists : true ) { t.column ( test_id ) } class func insertTest ( : Int ) - > Int { //insert.rowid returns Int64 type let insert = tests.insert ( test_id < - ) let rowid = insert.rowid { //directly cast Int64 Int return Int ( rowid ) } return 0 } Will correct ? Of course I tested . And works , I reading [ question Stackoverflow ] ( https : //stackoverflow.com/questions/29055820/using-variables- in-filters-in-sqlite-swift/29064444 # 29064444 ) And seems I could problem 32 bits devices ... If wrong , I cast ` Int64 ` ` Int ` ?","Converting ` Int64 ` ` Int ` passing ` Int64 ` value ` Int ` initializer always work 64-bit machine , crash 32-bit machine integer outside range ` Int32.min ... Int32.max ` . For safety use ` init ( truncatingIfNeeded : ) ` initializer ( formerly known ` init ( truncatingBitPattern : ) ` earlier Swift versions ) convert value : return Int ( truncatingIfNeeded : rowid ) On 64-bit machine , ` truncatingIfNeeded ` nothing ; get ` Int ` ( size ` Int64 ` anyway ) . On 32-bit machine , throw away top 32 bits , zeroes , n't lost data . So long value fit 32-bit ` Int ` , without losing data . If value outside range ` Int32.min ... Int32.max ` , change value ` Int64 ` something fits 32-bit ` Int ` , crash . * * * You see works Playground . Since ` Int ` Playground 64-bit ` Int ` , explicitly use ` Int32 ` simulate behavior 32-bit system . let : Int64 = 12345678901 // value bigger maximum 32-bit Int let j = Int32 ( truncatingIfNeeded : ) // j = -539,222,987 let k = Int32 ( ) // crash ! * * * **Update Swift 3/4** In addition ` init ( truncatingIfNeeded : ) ` still works , Swift 3 introduces failable initializers safely convert one integer type another . By using ` init ? ( exactly : ) ` pass one type initialize another , returns ` nil ` initialization fails . The value returned optional must unwrapped usual ways . For example : let : Int64 = 12345678901 let j = Int32 ( exactly : ) { print ( `` \ ( j ) fits Int32 '' ) } else { // initialization returned nil print ( `` \ ( ) large Int32 '' ) } This allows apply _nil coalescing operator_ supply default value conversion fails : // return 0 rowid big fit Int device return Int ( exactly : rowid ) ? ? 0"
Stackoverflow,"**When using Entity Framework , ESQL perform better Linq Entities ? ** I 'd prefer use Linq Entities ( mainly strong-type checking ) , team members citing performance reason use ESQL . I would like get full idea pro's/con 's using either method .","I find ESQL good edge cases , example : 1 . Where 's really hard express something LINQ . 2 . For building _very_ dynamic searches . 3 . If want use database specific function exposed provider using . Also , know Entity SQL , able express QueryViews Model-Defined Queries ."
Stackoverflow,I 've installed MySql 5.6 Workbench MySql Notifier . I start/stop MySql Service ( Service name : MySql56 ) Services.msc . But I 'm able start/stop MySql Notifier . I'm following error . ! [ enter image description ] ( https : //i.stack.imgur.com/RA40I.png ) I n't 's going . I confirm service named `` MySql56 '' present starts/stops successfully services.msc My system Windows 7 Professional 64 bit,You must access location : % APPDATA % \Oracle\MySQL Notifier\settings.config ( C : \Users\YourUsername\AppData\Roaming\Oracle\MySQL Notifier\settings.config ) change ServiceName MySQL56 settings.config file See : < http : //i.stack.imgur.com/eSXKl.png >
Stackoverflow,"I 'm situation I would generate script database I could run another server get database identical original one , without data . In essence , I want end big create script captures database schema . I working environment SQL Server 2000 installed , I unable install 2005 client tools ( event would help ) . I ca n't afford RedGate , I really would like database identical schema another server . Any suggestions ? Any simple .exe ( installation required ) tools , tips , T-SQL tricks would much appreciated . **Update : ** The database I 'm working 200+ tables several foreign- key relationships constraints , manually scripting table pasting together script viable option . I 'm looking something better manual solution **Additional Update** Unless I 'm completely missing something , viable solution using SQL 2000 tools . When I select option generate create script database . I end script contains CREATE DATABASE command , creates none objects - tables , constraints , etc . SQL 2005 's Management studio may handle objects well , database environment way connect installation Management Studio .","You 'd use ` REGEXP_REPLACE ` order remove non-digit characters string : select regexp_replace ( column_name , ' [ ^0-9 ] ' , `` ) mytable ; select regexp_replace ( column_name , ' [ ^ [ : digit : ] ] ' , `` ) mytable ; Of course write function ` extract_number ` . It seems bit like overkill though , write funtion consists one function call . create function extract_number ( in_number varchar2 ) return varchar2 begin return regexp_replace ( in_number , ' [ ^ [ : digit : ] ] ' , `` ) ; end ;"
Stackoverflow,"Background : The [ original case ] ( https : //stackoverflow.com/questions/36452626/selecting-a- subset-of-rows-that-exceed-a-percentage-of-total-values ) simple . Calculate running total per user highest revenue lowest : CREATE TABLE ( Customer INTEGER NOT NULL PRIMARY KEY , '' User '' VARCHAR ( 5 ) NOT NULL , Revenue INTEGER NOT NULL ) ; INSERT INTO ( Customer , '' User '' , Revenue ) VALUES ( 001 , 'James',500 ) , ( 002 , 'James',750 ) , ( 003 , 'James',450 ) , ( 004 , 'Sarah',100 ) , ( 005 , 'Sarah',500 ) , ( 006 , 'Sarah',150 ) , ( 007 , 'Sarah',600 ) , ( 008 , 'James',150 ) , ( 009 , 'James',100 ) ; Query : SELECT * , 1.0 * Revenue/SUM ( Revenue ) OVER ( PARTITION BY `` User '' ) AS percentage , 1.0 * SUM ( Revenue ) OVER ( PARTITION BY `` User '' ORDER BY Revenue DESC ) /SUM ( Revenue ) OVER ( PARTITION BY `` User '' ) AS running_percentage FROM ; ` ** [ ` LiveDemo ` ] ( http : //rextester.com/ZMUJW38298 ) ** ` Output : ╔════╦═══════╦═════════╦════════════╦════════════════════╗ ║ ID ║ User ║ Revenue ║ percentage ║ running_percentage ║ ╠════╬═══════╬═════════╬════════════╬════════════════════╣ ║ 2 ║ James ║ 750 ║ 0.38 ║ 0.38 ║ ║ 1 ║ James ║ 500 ║ 0.26 ║ 0.64 ║ ║ 3 ║ James ║ 450 ║ 0.23 ║ 0.87 ║ ║ 8 ║ James ║ 150 ║ 0.08 ║ 0.95 ║ ║ 9 ║ James ║ 100 ║ 0.05 ║ 1 ║ ║ 7 ║ Sarah ║ 600 ║ 0.44 ║ 0.44 ║ ║ 5 ║ Sarah ║ 500 ║ 0.37 ║ 0.81 ║ ║ 6 ║ Sarah ║ 150 ║ 0.11 ║ 0.93 ║ ║ 4 ║ Sarah ║ 100 ║ 0.07 ║ 1 ║ ╚════╩═══════╩═════════╩════════════╩════════════════════╝ It could calculated differently using specific windowed functions . * * * Now let 's assume use windowed ` SUM ` rewrite : SELECT c.Customer , c. '' User '' , c. '' Revenue '' ,1.0 * Revenue / NULLIF ( c3.s,0 ) AS percentage ,1.0 * c2.s / NULLIF ( c3.s,0 ) AS running_percentage FROM c CROSS APPLY ( SELECT SUM ( Revenue ) AS FROM c2 WHERE c. '' User '' = c2 . `` User '' AND c2.Revenue > = c.Revenue ) AS c2 CROSS APPLY ( SELECT SUM ( Revenue ) AS FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS c3 ORDER BY `` User '' , Revenue DESC ; ` ** [ ` LiveDemo ` ] ( http : //rextester.com/KPEK4632 ) ** ` I used ` CROSS APPLY ` I like correlated subqueries ` SELECT ` colums list ` c3 ` used twice . Everything work . But look closer ` c2 ` ` c3 ` similiar . So combine use simple conditional aggregation : SELECT c.Customer , c. '' User '' , c. '' Revenue '' ,1.0 * Revenue / NULLIF ( c2.sum_total,0 ) AS percentage ,1.0 * c2.sum_running / NULLIF ( c2.sum_total,0 ) AS running_percentage FROM c CROSS APPLY ( SELECT SUM ( Revenue ) AS sum_total , SUM ( CASE WHEN c2.Revenue > = c.Revenue THEN Revenue ELSE 0 END ) AS sum_running FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS c2 ORDER BY `` User '' , Revenue DESC ; Unfortunately possible . > Multiple columns specified aggregated expression containing outer reference . If expression aggregated contains outer reference , outer reference must column referenced expression . Of course I could circumvent wrapping another subquery , becomes bit `` ugly '' : SELECT c.Customer , c. '' User '' , c. '' Revenue '' ,1.0 * Revenue / NULLIF ( c2.sum_total,0 ) AS percentage ,1.0 * c2.sum_running / NULLIF ( c2.sum_total,0 ) AS running_percentage FROM c CROSS APPLY ( SELECT SUM ( Revenue ) AS sum_total , SUM ( running_revenue ) AS sum_running FROM ( SELECT Revenue , CASE WHEN c2.Revenue > = c.Revenue THEN Revenue ELSE 0 END AS running_revenue FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS sub ) AS c2 ORDER BY `` User '' , Revenue DESC ` ** [ ` LiveDemo ` ] ( http : //rextester.com/YRKO76442 ) ** ` * * * ` Postgresql ` version . The difference ` LATERAL ` instead ` CROSS APPLY ` . SELECT c.Customer , c. '' User '' , c.Revenue ,1.0 * Revenue / NULLIF ( c2.sum_total,0 ) AS percentage ,1.0 * c2.running_sum / NULLIF ( c2.sum_total,0 ) AS running_percentage FROM c , LATERAL ( SELECT SUM ( Revenue ) AS sum_total , SUM ( CASE WHEN c2.Revenue > = c.Revenue THEN c2.Revenue ELSE 0 END ) AS running_sum FROM c2 WHERE c. '' User '' = c2 . `` User '' ) c2 ORDER BY `` User '' , Revenue DESC ; ` ** [ ` SqlFiddleDemo ` ] ( http : //sqlfiddle.com/ # ! 15/8cab1/2/0 ) ** ` It works nice . * * * ` SQLite ` / ` MySQL ` version ( I prefer ` LATERAL/CROSS APPLY ` ) : SELECT c.Customer , c. '' User '' , c.Revenue , 1.0 * Revenue / ( SELECT SUM ( Revenue ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS percentage , 1.0 * ( SELECT SUM ( CASE WHEN c2.Revenue > = c.Revenue THEN c2.Revenue ELSE 0 END ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) / ( SELECT SUM ( c2.Revenue ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS running_percentage FROM c ORDER BY `` User '' , Revenue DESC ; ` ** [ ` SQLFiddleDemo-SQLite ` ] ( http : //sqlfiddle.com/ # ! 5/94a35/1/0 ) ** ` ` ** [ ` SQLFiddleDemo-MySQL ` ] ( http : //sqlfiddle.com/ # ! 9/c85e83/5/0 ) ** ` * * * I 've read [ Aggregates Outer Reference ] ( http : //sqlmag.com/blog/aggregates-outer-reference ) : > The source restriction ` SQL-92 ` standard , ` SQL Server ` inherited ` Sybase ` codebase . The problem SQL Server needs figure query compute aggregate . I search answers **only** show circumvent . The questions : 1 . Which part standard disallow interfere ? 2 . Why RDBMSes problem kind outer dependency ? 3 . Do extend ` SQL Standard ` ` SQL Server ` behaves ` SQL Server ` implement fully ( correctly ? ) ? . I would grateful references : * ` ISO standard ` ( 92 newer ) * [ SQL Server Standards Support ] ( https : //msdn.microsoft.com/en-us/library/hh501587 % 28v=sql.105 % 29.aspx ) * official documenation RDBMS explains ( ` SQL Server/Postgresql/Oracle/ ... ` ) . **EDIT : ** I know ` SQL-92 ` concept ` LATERAL ` . But version subqueries ( like ` SQLite/MySQL ` ) work . ` ** [ ` LiveDemo ` ] ( http : //rextester.com/ZAOJ48675 ) ** ` **EDIT 2 : ** To simplify bit , let 's check correlated subquery : SELECT c.Customer , c. '' User '' , c.Revenue , 1.0* ( SELECT SUM ( CASE WHEN c2.Revenue > = c.Revenue THEN c2.Revenue ELSE 0 END ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) / ( SELECT SUM ( c2.Revenue ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS running_percentage FROM c ORDER BY `` User '' , Revenue DESC ; The version works fine ` MySQL/SQLite/Postgresql ` . In ` SQL Server ` get error . After wraping subquery `` flatten '' one level works : SELECT c.Customer , c. '' User '' , c.Revenue , 1.0 * ( SELECT SUM ( CASE WHEN r1 > = r2 THEN r1 ELSE 0 END ) FROM ( SELECT c2.Revenue AS r1 , c.Revenue r2 FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS S ) / ( SELECT SUM ( c2.Revenue ) FROM c2 WHERE c. '' User '' = c2 . `` User '' ) AS running_percentage FROM c ORDER BY `` User '' , Revenue DESC ; The point question ` SQL standard ` regulate . ` ** [ ` LiveDemo ` ] ( http : //rextester.com/WFYP90759 ) ** `","It 's hard know exactly designers SQL language thinking wrote standard , _but 's opinion_ . SQL , general rule , requires explicitly state expectations intent . The language try _ '' guess meant '' _ , automatically fill blanks . **This good thing** . **When write query important consideration _yields correct results_ . ** If made mistake , 's probably better SQL parser informs , rather making guess intent returning results may correct . The declarative nature SQL ( state want retrieve rather steps retrieve ) already makes easy inadvertently make mistakes . **Introducing fuzziniess language syntax would make better** . In fact , every case I think language allows **shortcuts** caused problems . Take , instance , natural joins - omit names columns want join allow database infer based column names . Once column names change ( naturally time ) _\- semantics existing queries changes them_ . **This bad ... bad** \- really n't want kind _magic_ happening behind scenes database code . **One consequence design choice , however , SQL verbose language must explicitly express intent . ** This result write code may like , gripe certain constructs verbose ... end day - ."
Stackoverflow,"Hi All getting error message running phoenix count query large table . 0 : jdbc : phoenix : hadoopm1:2181 > select Count ( * ) PJM_DATASET ; + -- -- -- -- -- -- + | COUNT ( 1 ) | + -- -- -- -- -- -- + java.lang.RuntimeException : org.apache.phoenix.exception.PhoenixIOException : org.apache.phoenix.exception.PhoenixIOException : Failed attempts=36 , exceptions : Fri Jan 09 02:18:10 CST 2015 , null , java.net.SocketTimeoutException : callTimeout=60000 , callDuration=62365 : row `` table 'PJM_DATASET ' region=PJM_DATASET , ,1420633295836.4394a3aa2721f87f3e6216d20ebeec44. , hostname=hadoopctrl,60020,1420790733247 , seqNum=27753 sqlline.SqlLine $ IncrementalRows.hasNext ( SqlLine.java:2440 ) sqlline.SqlLine $ TableOutputFormat.print ( SqlLine.java:2074 ) sqlline.SqlLine.print ( SqlLine.java:1735 ) sqlline.SqlLine $ Commands.execute ( SqlLine.java:3683 ) sqlline.SqlLine $ Commands.sql ( SqlLine.java:3584 ) sqlline.SqlLine.dispatch ( SqlLine.java:821 ) sqlline.SqlLine.begin ( SqlLine.java:699 ) sqlline.SqlLine.mainWithInputRedirection ( SqlLine.java:441 ) sqlline.SqlLine.main ( SqlLine.java:424 ) 0 : jdbc : phoenix : hadoopm1:2181 > please help .","After several hours frustration resignation I found solution : You use IP-Adress machine , name . I edited hbase-site.xml follows : < property > < name > hbase.zookeeper.quorum < /name > < value > 192.168.59.81 < /value > < /property > My regionservers - file looks like : localhost And zoo.cfg contains : # Define Quorum-Servers server.1=192.168.59.81:2888:3888 Hope help others problem connecting pseudo-distributed hbase cluster remotely ."
Stackoverflow,"I 'm looking way parse / tokenize SQL statement within Node.js application , order : * Tokenize `` basics '' SQL keywords defined [ ISO/IEC 9075 ] ( https : //github.com/marijnh/CodeMirror/tree/master/mode/sql ) standard [ ] ( http : //codemirror.net/ ) . * Validate SQL syntax . * Find query gon na ( e.g . read write ? ) . Do solution advises peeps ? Linked : [ Any Javascript/Jquery Library To validate SQL statment ? ] ( https : //stackoverflow.com/questions/13169783/any-javascript-jquery- library-to-validate-sql-statment ) * * * I 've done research I found ways : **Using existing node.js libraries** I [ Google search ] ( http : //fr.wikipedia.org/wiki/Microsoft_SQL_Server ) I n't found consensual popular library use . I found ones : * [ simple-sql-parser ] ( http : //www.mysql.fr/ ) ( 22 stars [ github ] ( http : //fr.wikipedia.org/wiki/PL/SQL ) , 16 daily download [ npm ] ( http : //cassandra.apache.org/ ) ) * Supports SELECT , INSERT , UPDATE DELETE * There [ v2 branch ] ( https : //mariadb.org/ ) road * [ sql-parser ] ( https : //github.com/forward/sql-parser ) ( 90 stars [ github ] ( https : //github.com/forward/sql-parser ) , 6 daily downloads [ npm ] ( https : //www.npmjs.org/package/sql-parser ) ) * Only supports basic SELECT statements * Based [ jison ] ( http : //jison.org ) * [ sqljs ] ( https : //github.com/langpavel/node-sqljs ) ( 17 stars [ github ] ( https : //github.com/langpavel/node-sqljs ) , 5 daily downloads [ npm ] ( https : //www.npmjs.org/package/sqljs ) ) * v0.0.0-3 , development ... No documentation : ) Unfortunately , none libraries seams complete trustful . **Doing based node.js low level tokenizer library** I self low level tokenizer library like : * [ jison ] ( http : //jison.org ) ( 1,457 stars [ github ] ( https : //github.com/zaach/jison ) , 240 daily downloads [ npm ] ( https : //www.npmjs.org/package/jison ) ) * [ tokenizer ] ( https : //github.com/floby/node-tokenizer ) ( 44 stars [ github ] ( https : //github.com/Floby/node-tokenizer ) , 10 daily downloads [ npm ] ( https : //www.npmjs.org/package/tokenizer ) ) **Doing based existing Javascript code beautifier** [ CodeMirror ] ( http : //codemirror.net/ ) pretty cool Javascript library ( browser side ) recognize SQL keywords , strings , etc . Check ou [ demo ] ( http : //codemirror.net/1/contrib/sql/ ) . I build node.js library tokenizer based CodeMirror . The [ SQL mode github ] ( https : //github.com/marijnh/CodeMirror/tree/master/mode/sql ) , I maybe adapt get tokens within node application . PS : [ CodeMirror ] ( http : //codemirror.net/ ) 5,046 stars [ github ] ( https : //github.com/marijnh/codemirror ) well maintained . * * * I figured 2 distinct problems : Tokenization Syntax validation ( related tokenization ) . I made **SQL tokenizer** Node.js based [ SQL mode ] ( https : //github.com/marijnh/CodeMirror/tree/master/mode/sql ) excellent [ CodeMirror ] ( http : //codemirror.net/ ) ( 5,046 stars github , well maintained ) . CodeMirror 's SQL mode take charge `` generic '' SQL SQL particularities like [ MSSQL ] ( http : //fr.wikipedia.org/wiki/Microsoft_SQL_Server ) , [ MySQL ] ( http : //www.mysql.fr/ ) , [ PL/SQL ] ( http : //fr.wikipedia.org/wiki/PL/SQL ) , [ Cassandra ] ( http : //cassandra.apache.org/ ) , Hive [ MariaDB ] ( https : //mariadb.org/ ) . When project mature enough , I ( probably ) put public GitHub let know . About **SQL syntax** validation , I found JavaScript tool ( open source project adapt JS ) yet ...","You might like take look [ sqlparse ] ( http : //code.google.com/p/python- sqlparse/ ) Blatantly stolen homepage : > > > # Parsing > > > res = sqlparse.parse ( 'select * `` someschema '' . `` mytable '' id = 1 ' ) > > > res < < < ( < Statement 'select ... ' 0x9ad08ec > , ) > > > stmt = res [ 0 ] > > > stmt.to_unicode ( ) # converting back unicode < < < u'select * `` someschema '' . `` mytable '' id = 1' > > > # This internal representation looks like : > > > stmt.tokens < < < ( < DML 'select ' 0x9b63c34 > , < Whitespace ' ' 0x9b63e8c > , < Operator '* ' 0x9b63e64 > , < Whitespace ' ' 0x9b63c5c > , < Keyword 'from ' 0x9b63c84 > , < Whitespace ' ' 0x9b63cd4 > , < Identifier ' '' somes ... ' 0x9b5c62c > , < Whitespace ' ' 0x9b63f04 > , < Where 'where ... ' 0x9b5caac > ) > > >"
Stackoverflow,"Is way _not_ fetch specific column using linqtosql without use anonymous type specify returned filed individually ? We use SQLMetal generate dbml file contains types put queried data results . However , select columns included linq query , results go anonymous type instead type declared dbml file . I would like select _all one_ columns particular table , still results returned related dbml type . Any ideas appreciated .","LINQ SQL supports [ lazy loading ] ( http : //en.wikipedia.org/wiki/Lazy_loading ) individual properties . In DBML designer , set ` Delay Loaded ` ` true ` column 's properties . Columns delay loaded included initial ` SELECT ` . If try access object 's property loaded yet , another ` SELECT ` statement executed bring value DB . If editing DBML file hand , set ` < Column IsDelayLoaded= '' true '' > ` . If writing LINQ SQL classes hand , simple declaring property 's backing field ` Link < T > ` instead ` T ` . For example : [ Table ] public class Person { private Link < string > _name ; [ Column ( Storage = `` _name '' ) ] public string Name { get { return _name.Value ; } set { _name.Value = value ; } } } See also `` Delay/Lazy Loading '' section [ post Scott Guthrie ] ( http : //weblogs.asp.net/scottgu/archive/2007/05/29/linq-to-sql- part-2-defining-our-data-model-classes.aspx ) . * * * Update : The answer applies want column still available need . It included ` SELECT ` unless specifically ask ( see ` LoadOptions ` ) try access . If n't want use access column , n't want available class property , go [ Serapth's answer ] ( https : //stackoverflow.com/questions/852679/linq-to-sql-dont-fetch-a- particular-column/852934 # 852934 ) removing column DBML file . Be sure understand implications , loss concurrency checking column ."
Stackoverflow,"We trying setup sql project , new machine Windows 7 , VS 2010 SP1 & SSDT 2010 ( installed SSDT 2010 iso image ) . But getting message I open sqlproj . **'Verifying model synchronized source files . Your database ready 12734 operations completed . ' And number keeps increasing** . And keeps running background . Tried re-installing SSDT , VS 2010 help . Created new database project Northwind db , issue . Ran procmon saw 's going files . It works fine another system similar configuration . **EDIT** The issue seems related TFS , unbind TFS works fine . But sure exact cause . Any suggestion would really helpful .","> Open MS Ticket : < https : //developercommunity.visualstudio.com/content/problem/67789/visual- studio-hangs-when-closing-sql-files.html > The real solution come update MS. For , however , turning participation ‘ Visual Studio Experience Improvement Program ’ seemed solve . You check whether 're signed program clicking Help - > Send Feedback - > Settings ( 2017 , sure 2015 ) ."
Stackoverflow,"I question MySqlParameter .NET connector . I query : SELECT * FROM table WHERE id IN ( @ parameter ) And MySqlParameter : intArray = new List < int > ( ) { 1,2,3,4 } ; ... connection.Command.Parameters.AddWithValue ( `` parameter '' , intArray ) ; This possible ? Is possible pass array int single MySqlParameter ? The solution convert array int string like `` 1,2,3,4 '' , , pass MySqlParameter recognized string , puts sql query like `` 1\,2\,3\,4 '' return expected values . @ UPDATE : Seems like mysql connector team work little bit harder .","> pass MySqlParameter recognized string , puts sql query like `` 1\,2\,3\,4 '' return expected values . I ran last night . I found FIND_IN_SET works : SELECT * FROM table WHERE FIND_IN_SET ( id , @ parameter ) ! = 0 ... intArray = new List < int > ( ) { 1,2,3,4 } ; conn.Command.Parameters.AddWithValue ( `` parameter '' , string.Join ( `` , '' , intArray ) ) ; Apparently length limitations ( I found post looking alternate solution ) , may work ."
Stackoverflow,"I would like create MySQL table Pandas ' to_sql function primary key ( usually kind good primary key mysql table ) : group_export.to_sql ( con = db , name = config.table_group_export , if_exists = 'replace ' , flavor = 'mysql ' , index = False ) creates table without primary key , ( even without index ) . The documentation mentions parameter 'index_label ' combined 'index ' parameter could used create index n't mention option primary keys . [ Documentation ] ( http : //pandas.pydata.org/pandas- docs/dev/generated/pandas.DataFrame.to_sql.html )","Simply add primary key uploading table pandas . group_export.to_sql ( con=engine , name=example_table , if_exists='replace ' , flavor='mysql ' , index=False ) engine.connect ( ) con : con.execute ( 'ALTER TABLE ` example_table ` ADD PRIMARY KEY ( ` ID_column ` ) ; ' )"
Stackoverflow,"In Rails application I want use ` will_paginate ` gem paginate SQL query . Is possible ? I tried something like n't work : @ users = User.find_by_sql ( `` SELECT u.id , u.first_name , u.last_name , CASE WHEN r.user_accepted =1 AND ( r.friend_accepted =0 || r.friend_accepted IS NULL ) ... ... ... '' ) .paginate ( : page = > @ page , : per_page = > @ per_page , : conditions = > conditions_hash , : order = > 'first_name ASC ' ) If , recommend way around ? I n't want write pagination .","Use ` paginate_by_sql ` , i.e . sql = `` SELECT * FROM users WHERE created_at > = ' # { 2.weeks.ago } ' ORDER BY created_at DESC `` @ users = User.paginate_by_sql ( sql , : page = > @ page , : per_page = > @ per_page ) As general rule finder paginated replacing ` find* ` ` paginate* ` ."
Stackoverflow,"I new reading XML file table SQL Server Management Studio . There probably better ways I would like use approach . Currently I reading standard XML file records people . A ` < record > ` tag highest level row data . I want read records separate rows SQL table . I gotten along fine far using following approach follows : SELECT -- Record category , editor , entered , subcategory , uid , updated , -- Person first_name , last_name , ssn , ei , title , POSITION , FROM OPENXML ( @ hDoc , 'records/record/person/names ' ) WITH ( -- Record category [ varchar ] ( 100 ) '../../ @ category ' , editor [ varchar ] ( 100 ) '../../ @ editor ' , entered Datetime '../../ @ entered ' , subcategory [ varchar ] ( 100 ) '../../ @ subcategory ' , uid BIGINT '../../ @ uid ' , updated [ varchar ] ( 100 ) '../../ @ updated ' , -- Person first_name [ varchar ] ( 100 ) 'first_name ' , last_name [ varchar ] ( 100 ) 'last_name ' , ssn [ varchar ] ( 100 ) '../ @ ssn ' , ei [ varchar ] ( 100 ) '../ @ e-i ' , title [ varchar ] ( 100 ) '../title ' , Position [ varchar ] ( 100 ) '../position ' , ) However approach worked fine tag names unique record/person . The issue I within ` < Person > ` tag I ` < Aliases > ` tag contains list 1 ` < Alias > test name < /Alias > ` tags . If I use approach & reference **'../aliases'** I get Alias elements one long String row mixed together . If I try **'../aliases/alias'** ONLY first element returned per record row . If 10 Alias elements within Aliases tag set I would like 10 rows returned example . Is way specify multiple tags name within higher level tag , return & one row ? The following example block within XML I referring : - < aliases > < alias > test 1 < /alias > < alias > test 2 < /alias > < alias > test 3 < /alias > < /aliases > I would like following SQL table : Record Aliases Record 1 test 1 Record 1 test 2 Record 1 test 3 Record 2 test 4 Record 2 test 5 I get : Record 1 test 1 Record 2 test 4 Apologies I explained correctly - help would greatly appreciated .","SELECT * FROM OPENXML ( @ index , 'rootnode/group/anothernode ' ) WITH ( id int '../id ' , anothernode varchar ( 30 ) ' . ' ) Or use XML datatype instead like : SELECT G.N.value ( ' ( id/text ( ) ) [ 1 ] ' , 'int ' ) AS id , A.N.value ( 'text ( ) [ 1 ] ' , 'varchar ( 30 ) ' ) AS anothernode FROM @ XMLDoc.nodes ( 'rootnode/group ' ) AS G ( N ) CROSS APPLY G.N.nodes ( 'anothernode ' ) AS A ( N )"
Stackoverflow,"I need use SQLCipher android ... 've already made app using SQLite want convert SQLCipher . The problem , I know nothing SQLCipher . I read link : < http : //sqlcipher.net/sqlcipher-for- android/ > But 'm clear , still . I wondering could provide basic sqlcipher android tutorials , everything taught easy way absolute basics . Thanks !",# Download Build sqlcipher # # # \ -- Skip sqlcipher already installed Pull code < https : //github.com/sqlcipher/sqlcipher > directory ( say ~/sqlcipher ) mkdir ~/bld ; # Build occur sibling directory cd ~/bld ; # Change build directory ../sqlcipher/configure -- enable-tempstore=yes CFLAGS= '' -DSQLITE_HAS_CODEC '' LDFLAGS= '' -lcrypto '' ; # configure sqlcipher make install ; # Install build products # Decrypt database plaintext database $ cd ~/ ; $ ./sqlcipher encrypted.db sqlite > PRAGMA key = 'testkey ' ; sqlite > ATTACH DATABASE 'plaintext.db ' AS plaintext KEY `` ; -- empty key disable encryption sqlite > SELECT sqlcipher_export ( 'plaintext ' ) ; sqlite > DETACH DATABASE plaintext ; Find decrypted database ~/plaintext.db use sqlite browser like [ ] ( http : //sqlitebrowser.org/ ) . # **Update** : September 2015 < http : //sqlitebrowser.org > supports sqlcipher databases . That 's neat .
Stackoverflow,"I found lot comparisions , one ; So , best one ?","There 's full comparison [ SQLite's site ] ( http : //www.sqlite.org/cvstrac/wiki ? p=SqliteVersusDerby ) . SQLite much restricted , supports small subset SQL92 , whereas Derby ( JavaDB ) full support SQL92 SQL99 ."
Stackoverflow,"I questions around FILESTREAM capability SQL Server 2008 . 1 . What would difference performance returning file streamed SQL Server 2008 using FILESTREAM capability vs. directly accessing file shared directory ? 2 . If 100 users requested 100 100Mb files ( stored via FILESTREAM ) within 10 second window , would SQL Server 2008 performance slow crawl ?","_If 100 users requested 100 100Mb files ( stored via FILESTREAM ) within 10 second window , would SQL Server 2008 performance slow crawl ? _ On kind server ? ? What kind hardware serve files ? What kind disks , network etc. ? ? So many questions ... ... . There 's really good blog post Paul Randal [ SQL Server 2008 : FILESTREAM Performance ] ( http : //www.sqlskills.com/BLOGS/PAUL/post/SQL- Server-2008-FILESTREAM-performance.aspx ) \- check . There 's also [ 25-page whitepaper FILESTREAM ] ( http : //msdn.microsoft.com/en- us/library/cc949109.aspx ) available - also covering performance tuning tips . * * * But also check Microsoft Research TechReport [ To BLOB Not To BLOB ] ( http : //research.microsoft.com/apps/pubs/default.aspx ? id=64525 ) . It 's profound well based article put questions paces . Their conclusion : > The study indicates objects **larger one megabyte average** , NTFS clear advantage SQL Server . If objects **under 256 kilobytes , database clear advantage** . Inside range , depends write intensive workload , storage age typical replica system . So judging - blobs typically less 1 MB , store VARBINARY ( MAX ) database . If 're typically larger , FILESTREAM feature . I would n't worry much performance rather benefits FILESTREAM `` unmanaged '' storage NTFS file folder : storing files outside database without FILESTREAM , control : * access control provided database * files n't part SQL Server backup * files n't handled transactionally , e.g . could end `` zombie '' files n't referenced database anymore , `` skeleton '' entries database without corresponding file disk Those features alone make absolutely worthwhile use FILESTREAM ."
Stackoverflow,"I difficulty querieing users , defined : type User struct { ID int ` db : '' id '' json : '' id '' ` UserName string ` db : '' username '' json : '' username '' ` Email string ` db : '' email '' json : '' email '' ` CreatedAt time.Time ` db : '' created_at '' json : '' created_at '' ` StatusID uint8 ` db : '' status_id '' json : '' status_id '' ` Deleted uint8 ` db : '' deleted '' json : '' deleted '' ` ... } And table MariaDB defined : + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- + | Field | Type | Null | Key | Default | Extra | + -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- + | id | int ( 10 ) unsigned | NO | PRI | NULL | auto_increment | | username | varchar ( 50 ) | NO | | NA | | | email | varchar ( 255 ) | NO | | NULL | | | created_at | datetime | NO | | CURRENT_TIMESTAMP | | | status_id | tinyint ( 1 ) | NO | | 0 | | | deleted | tinyint ( 1 ) | NO | | 0 | | ... | However I query like : func GetUsers ( c *gin.Context ) { var users [ ] model.User err : = shared.Dbmap.Select ( & users , `` SELECT * FROM user '' ) err == nil { c.JSON ( 200 , users ) } else { fmt.Println ( `` % v \n '' , err ) c.JSON ( http.StatusInternalServerError , gin.H { `` error '' : `` user ( ) table problem query '' } ) } // curl -i http : //127.0.0.1:8080/api/v1/users } I get error : sql : Scan error column index 3 : unsupported Scan , storing driver.Value type [ ] uint8 type *time.Time rows table . I also tried ` created_at ` ` timestamp ` still get error . So I 'm left clueless wrong ? How I fix ? **P.S . ** Though question turned answer [ ] ( https : //stackoverflow.com/questions/29341590/go-parse-time-from- database ) context different ( ` sqlx ` instead ` go-sql- driver/mysql ` ) . Also since error subject probably searchable people google error . So perhaps worth keeping separate question .","Alright , I found solution , thanks [ ] ( https : //stackoverflow.com/a/29343013/5774375 ) answer . The problem goes adding ` ? parseTime=true ` db mapper . Like : db , err : = sqlx.Connect ( `` mysql '' , `` myuser : mypass @ tcp ( 127.0.0.1:3306 ) /mydb ? parseTime=true '' )"
Stackoverflow,"In MySQL I tried define trigger like : DELIMITER $ $ CREATE TRIGGER vipInvite AFTER INSERT ON meetings FOR EACH ROW BEGIN IF ( NOT EXISTS ( SELECT * FROM participants WHERE meetid = NEW.meetid AND pid ='vip ' ) ) THEN IF ( EXISTS ( SELECT * FROM meetings WHERE meetid = NEW.meetid AND slot > 16 ) ) THEN INSERT INTO participants ( meetid , pid ) VALUES ( NEW.meetid , ( SELECT userid FROM people WHERE people.group = 'tap ' GROUP BY invite ) ) ; END IF ; END IF ; END $ $ DELIMITER ; Produces error : > This version MySQL n't yet support 'multiple triggers action time event one table . Is way work around I define multiple triggers ?",This need proceed . See example I 've worked . mysql > select * test ; + -- -- -- + -- -- -- -+ | id | name | + -- -- -- + -- -- -- -+ | 1 | name1 | | 2 | name2 | | 3 | name3 | | 4 | name4 | + -- -- -- + -- -- -- -+ 4 rows set ( 0.00 sec ) mysql > select * test1 ; + -- -- -- + -- -- -- + -- -- -- -- + | id | tid | name2 | + -- -- -- + -- -- -- + -- -- -- -- + | 1 | 2 | name11 | | 2 | 3 | name12 | | 3 | 4 | name13 | + -- -- -- + -- -- -- + -- -- -- -- + 3 rows set ( 0.00 sec ) mysql > select - > t1.name - > - > test t1 - > join - > test1 t2 t2.tid = t1.id - > join - > ( select id test id < 4 limit 3 ) tempt tempt.id = t1.id ; + -- -- -- -+ | name | + -- -- -- -+ | name2 | | name3 | + -- -- -- -+ 2 rows set ( 0.00 sec ) Hope helps .
Stackoverflow,I planning moving main project Postgres 10 point . I like keep local dev 's database version close running prod . Currently prod database Google Cloud SQL PostgreSQL 9.6 . I heard anything Google managed cloud sql product offer Postgres 10.x addition 9.6 . Does anyone know Postgres 10 supported option GCP 's managed SQL product ? I would like start planning .,"After I got information @ clemens make research I found , dump file section ` CREATE SEQUENCE table_id_seq ` statement ` AS integer ` I restored new database create ` nextval ( ) ` sequence . If I remove statement ` AS integer ` ` CREATE SEQUENCE ` section works find . In dump file : CREATE SEQUENCE table_id_seq AS integer START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1 ; Remove ` AS integer ` dump file CREATE SEQUENCE table_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1 ;"
Stackoverflow,"I trying use PROC SQL query DB2 table hundreds millions records . During development stage , I want run query arbitrarily small subset records ( say , 1000 ) . I 've tried using INOBS limit observations , I believe parameter simply limiting number records SAS processing . I want SAS fetch arbitrary number records database ( process ) . If I writing SQL query , I would simply use ` SELECT * FROM x FETCH FIRST 1000 ROWS ONLY ... ` ( equivalent ` SELECT TOP 1000 * FROM x ` SQL Server ) . But PROC SQL n't seem option like . It's taking extremely long time fetch records . The question : How I instruct SAS arbitrarily limit number records **return database** . I 've read PROC SQL uses ANSI SQL , n't specification row limiting keyword . Perhaps SAS n't feel like making effort translate SQL syntax vendor-specific keywords ? Is work around ?","Have tried using ` outobs ` option ` proc sql ` ? For example , proc sql outobs=10 ; create table test select * schema.HUGE_TABLE order n ; quit ; Alternatively , use SQL passthrough write query using DB2 syntax ( ` FETCH FIRST 10 ROWS ONLY ` ) , although requires store data database , least temporarily . Passthrough looks something like : proc sql ; connect db2 ( user= & userid . password= & userpw . database=MY_DB ) ; create table test select * connection db2 ( select * schema.HUGE_TABLE order n FETCH FIRST 10 ROWS ONLY ) ; quit ; It requires syntax ca n't access sas datasets , ` outobs ` works , I would recommend ."
Stackoverflow,"How I update table 's column trigger update table ? Here 's trigger : CREATE TRIGGER upd_total_votes AFTER UPDATE ON products_score FOR EACH ROW UPDATE products_score SET products_score.votes_total = ( SELECT ( votes_1 + votes_2 + votes_3 + votes_4 + votes_5 ) FROM products_score WHERE id = new.id ) Now I update table like UPDATE products_score SET votes_1 = 5 WHERE id = 0 n't work , I get following : # 1442 - Ca n't update table 'products_score ' stored function/trigger already used statement invoked stored function/trigger . So earth I get work ?","MySQL triggers ca n't manipulate table assigned . All major DBMS support feature hopefully MySQL add support soon . < http : //forums.mysql.com/read.php ? 99,122354,240978 # msg-240978 >"
Stackoverflow,"I 'm running R Windows machine directly linked PostgreSQL database . I 'm using RODBC . My database encoded UTF-8 confirmed following R command : dbGetQuery ( con , `` SHOW CLIENT_ENCODING '' ) # client_encoding # 1 UTF8 However , text read R , displays strange text R. For example , following text shown PostgreSQL database : `` Stéphane '' After exporting R 's shown : `` StÃ©phane '' ( **é** encoded **Ã©** ) When importing R I use ` dbConnect ` command establish connection ` dbGetQuery ` command query data using SQL . I specify text encoding anywhere connecting database running query . I 've searched online ca n't find direct resolution issue . I found [ link ] ( https : //stackoverflow.com/questions/7167298/rodbc-character- encoding-error-with-postgresql ) , issue RODBC , I 'm using . [ This link ] ( http : //www.i18nqa.com/debug/utf8-debug.html ) helpful identifying symbols , I n't want find & replace R ... way much data . I try running following commands I arrived warning . Sys.setlocale ( `` LC_ALL '' , `` en_US.UTF-8 '' ) # [ 1 ] `` '' # Warning message : # In Sys.setlocale ( `` LC_ALL '' , `` en_US.UTF-8 '' ) : # OS reports request set locale `` en_US.UTF-8 '' honored Sys.setenv ( LANG= '' en_US.UTF-8 '' ) Sys.setenv ( LC_CTYPE= '' UTF-8 '' ) The warning occurs ` Sys.setlocale ( `` LC_ALL '' , `` en_US.UTF-8 '' ) ` command . My intuition Windows specific issue n't occur Mac/Linux/Unix .",Followup info I problem . Installing ` yum install postgresql-devel ` resolves error .
Stackoverflow,"run shark queries , memory gets hoarded main memory This top command result . * * * Mem : 74237344k total , 70080492k used , 4156852k free , 399544k buffers Swap : 4194288k total , 480k used , 4193808k free , 65965904k cached * * * n't change even kill/stop shark , spark , hadoop processes . Right , way clear cache reboot machine . anyone faced issue ? configuration problem known issue spark/shark ?","I share thoughts based experience . But , possible , please let us know use-case . It 'll help us answering queries better manner . 1- If going writes reads , Cassandra obviously good choice . Having said , coming SQL background planning use Cassandra 'll definitely find CQL helpful . But need perform operations like JOIN GROUP BY , even though CQL solves primitive GROUP BY use cases write time compact time sorts implements one-to-many relationships , CQL answer . 2- Spark SQL ( Formerly Shark ) fast two reasons , in-memory processing planning data pipelines . In-memory processing makes ~100x faster Hive . Like Hive , Spark SQL handles larger memory data types well 10x faster thanks planned pipelines . Situation shifts Spark SQL benefit multiple data pipelines like filter groupBy present . Go need ad-hoc real time querying . Not suitable need long running jobs gigantic amounts data . 3- Hive basically warehouse runs top existing Hadoop cluster provides SQL like interface handle data . But Hive suitable real-time needs . It best suited offline batch processing . Does n't need additional infra uses underlying HDFS data storage . Go perform operations like JOIN , GROUP BY etc large dataset OLAP . ` Note : ` Spark SQL emulates Apache Hive behavior top Spark , supports virtually Hive features potentially faster . It supports existing Hive Query language , Hive data formats ( SerDes ) , user-defined functions ( UDFs ) , queries call external scripts . But I think able evaluate pros cons tools properly getting hands dirty . I could suggest based questions . Hope answers queries . P.S . : The answer based solely experience . Comments/corrections welcome ."
Stackoverflow,"I 've never really understood difference two indexes , someone please explain difference ( performance-wise , index structure look like db , storage-wise etc ) ? I understand question broad , please bear . I really know scope . Perhaps guys start explaining know-hows I 'll get pointers right direction enabling make question narrow ? **Included index** CREATE NONCLUSTERED INDEX IX_Address_PostalCode ON Person.Address ( PostalCode ) INCLUDE ( AddressLine1 , AddressLine2 , City , StateProvinceID ) ; **'Normal ' index** CREATE NONCLUSTERED INDEX IX_Address_PostalCode ON Person.Address ( PostalCode , AddressLine1 , AddressLine2 , City , StateProvinceID ) ;",You see reports SSMS : Right-click instance name / reports / standard / top sessions You see top CPU consuming sessions . This may shed light SQL processes using resources . There CPU related reports look around . I going point DMVs 've looked already I 'll skip . You use [ sp_BlitzCache ] ( http : //www.brentozar.com/archive/2014/05/introducing- sp_blitzcache/ ) find top CPU consuming queries . You also sort IO things well . This using DMV info accumulates restarts . [ This article ] ( http : //www.mssqltips.com/sqlservertip/2454/how-to-find-out-how- much-cpu-a-sql-server-process-is-really-using/ ) looks promising . Some [ stackoverflow ] ( https : //stackoverflow.com/questions/28952/cpu- utilization-by-database ) goodness Mr. Ozar .
Stackoverflow,I 've already looked [ ] ( https : //stackoverflow.com/questions/11506224/connection-for- controluser-as-defined-in-your-configuration-failed-phpmyadmin-xa ) since I running Ubuntu 10.04 instead XAMPP already created phpmyadmin database I log terminal root phpmyadmin users . How I installed : sudo apt-get install lamp-server^ sudo apt-get install phpmyadmin I login locally terminal using root phpmyadmin users . I view phpmyadmin login page . A phpinfo.php page I posted server works . * Ubuntu Version 10.04.4 Lucid Lynx * Apache 2.2.14 * MySQL 5.1.73 * PHP 5.3.6 * Phpmyadmin 3.3.2,"As stated [ manual ] ( http : //dev.mysql.com/doc/refman/5.1/en/condition- handling.html ) MySQL version 5.1 : > Other statements related conditions ` SIGNAL ` , ` RESIGNAL ` , ` GET DIAGNOSTICS ` . The ` SIGNAL ` ` RESIGNAL ` statements supported MySQL 5.5 . The ` GET DIAGNOSTICS ` statement supported MySQL 5.6 . To raise error older versions MySQL , deliberately issue erroneous command . I often ` CALL ` non-existent procedure , example : CALL raise_error ;"
Stackoverflow,"# Final solution : The connection added connection pool . So I closed , still remained physically open . With ConnectionString Parameter `` Pooling=false '' static methods MySqlConnection.ClearPool ( connection ) MySqlConnection.ClearAllPools problem avoided . Note problem , connection still alive I closed application . Even though I closed . So either I n't use connection pooling I clear specific pool closing connection problem solved . I 'll take time figuring 's best solution case . Thanks answered ! It helped understand concepts C # better I learned alot useful input . : ) === # Original Problem : I 've searched n't found solution problem : I 'm new C # try write class make MySql Connections easier . My problem , I open connection close . It still open Database gets aborted . I 'm using 'using ' statement ' course , connection still open gets aborted I exit program . Here 's code looks like : using ( DatabaseManager db = new DatabaseManager ( ) ) { using ( MySqlDataReader result = db.DataReader ( `` SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC '' ) ) { foreach ( MySqlDataReader result db.DataReader ( `` SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC '' ) ) { //Do stuff } } } The class Database manager opens connection closes disposed : public DatabaseManager ( ) { this.connectionString = new MySqlConnectionStringBuilder ( `` Server=localhost ; Database=businessplan ; Uid=root ; '' ) ; connect ( ) ; } private bool connect ( ) { bool returnValue = true ; connection = new MySqlConnection ( connectionString.GetConnectionString ( false ) ) ; connection.Open ( ) ; } public void Dispose ( ) { Dispose ( true ) ; } public void Dispose ( bool disposing ) { ( disposing ) { ( connection.State == System.Data.ConnectionState.Open ) { connection.Close ( ) ; connection.Dispose ( ) ; } } //GC.SuppressFinalize ( ) ; //Updated } //Updated //~DatabaseManager ( ) // { // Dispose ( false ) ; // } So , I checked debugger Dispose ( ) -method called executes correctly . What I missing ? Is something I wrong misunderstood ? Any help appreciated ! Greetings , Simon P.S . : Just case , DataReader ( ) -method ( Updated version ) : public IEnumerable < IDataReader > DataReader ( String query ) { using ( MySqlCommand com = new MySqlCommand ( ) ) { com.Connection = connection ; com.CommandText = query ; using ( MySqlDataReader result = com.ExecuteReader ( System.Data.CommandBehavior.CloseConnection ) ) { ( result.Read ( ) ) { yield return ( IDataReader ) result ; } } } } * * * Ok , I tried use yield return : foreach ( MySqlDataReader result db.DataReader ( `` SELECT * FROM module WHERE Active=1 ORDER BY Sequence ASC '' ) ) { // ... } And I changed DataReader-method : public IEnumerable < IDataReader > DataReader ( String query ) { using ( MySqlCommand com = new MySqlCommand ( ) ) { com.Connection = connection ; com.CommandText = query ; using ( MySqlDataReader result = com.ExecuteReader ( ) ) { ( result.Read ( ) ) { yield return ( IDataReader ) result ; } } } } It works way I retrieve data , yet I still problem : The connection n't closed properly . : (","Im unsure mysqlconnection sql server counter part uses Connection pooling close call close instead puts connection pool ! Edit : Make sure dispose Reader , Command , Connection object ! Edit : Solved ConnectionString Parameter `` Pooling=false '' static methods MySqlConnection.ClearPool ( connection ) MySqlConnection.ClearAllPools ( )"
Stackoverflow,"I need SQL guru help speed query . I 2 tables , quantities prices . quantities records quantity value 2 timestamps , 15 minutes apart . prices records price given timestamp , given price type price 5 record every 5 minutes . I need 2 work total price period , e.g . hour day , two timestamps . This calculated sum ( quantity multiplied average 3 prices 15 minute quantity window ) period . For example , let 's say I want see total price hour 1 day . The total price value row result set sum total prices four 15 minute periods hour . And total price 15 minute period calculated multiplying quantity value period average 3 prices ( one 5 minutes ) quantity 's period . Here 's query I 'm using , results : SELECT MIN ( ` quantities ` . ` start_timestamp ` ) AS ` start ` , MAX ( ` quantities ` . ` end_timestamp ` ) AS ` end ` , SUM ( ` quantities ` . ` quantity ` * ( SELECT AVG ( ` prices ` . ` price ` ) FROM ` prices ` WHERE ` prices ` . ` timestamp ` > = ` quantities ` . ` start_timestamp ` AND ` prices ` . ` timestamp ` < ` quantities ` . ` end_timestamp ` AND ` prices ` . ` type_id ` = 1 ) ) AS total FROM ` quantities ` WHERE ` quantities ` . ` start_timestamp ` > = '2010-07-01 00:00:00' AND ` quantities ` . ` start_timestamp ` < '2010-07-02 00:00:00' GROUP BY HOUR ( ` quantities ` . ` start_timestamp ` ) ; + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + | start | end | total | + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + | 2010-07-01 00:00:00 | 2010-07-01 01:00:00 | 0.677733 | | 2010-07-01 01:00:00 | 2010-07-01 02:00:00 | 0.749133 | | 2010-07-01 02:00:00 | 2010-07-01 03:00:00 | 0.835467 | | 2010-07-01 03:00:00 | 2010-07-01 04:00:00 | 0.692233 | | 2010-07-01 04:00:00 | 2010-07-01 05:00:00 | 0.389533 | | 2010-07-01 05:00:00 | 2010-07-01 06:00:00 | 0.335300 | | 2010-07-01 06:00:00 | 2010-07-01 07:00:00 | 1.231467 | | 2010-07-01 07:00:00 | 2010-07-01 08:00:00 | 0.352800 | | 2010-07-01 08:00:00 | 2010-07-01 09:00:00 | 1.447200 | | 2010-07-01 09:00:00 | 2010-07-01 10:00:00 | 0.756733 | | 2010-07-01 10:00:00 | 2010-07-01 11:00:00 | 0.599467 | | 2010-07-01 11:00:00 | 2010-07-01 12:00:00 | 1.056467 | | 2010-07-01 12:00:00 | 2010-07-01 13:00:00 | 1.252600 | | 2010-07-01 13:00:00 | 2010-07-01 14:00:00 | 1.285567 | | 2010-07-01 14:00:00 | 2010-07-01 15:00:00 | 0.442933 | | 2010-07-01 15:00:00 | 2010-07-01 16:00:00 | 0.692567 | | 2010-07-01 16:00:00 | 2010-07-01 17:00:00 | 1.281067 | | 2010-07-01 17:00:00 | 2010-07-01 18:00:00 | 0.652033 | | 2010-07-01 18:00:00 | 2010-07-01 19:00:00 | 1.721900 | | 2010-07-01 19:00:00 | 2010-07-01 20:00:00 | 1.362400 | | 2010-07-01 20:00:00 | 2010-07-01 21:00:00 | 1.099300 | | 2010-07-01 21:00:00 | 2010-07-01 22:00:00 | 0.646267 | | 2010-07-01 22:00:00 | 2010-07-01 23:00:00 | 0.873100 | | 2010-07-01 23:00:00 | 2010-07-02 00:00:00 | 0.546533 | + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- + 24 rows set ( 5.16 sec ) I need query run lot faster , would though would possible . Here 's results EXPLAIN EXTENDED ... + -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | + -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + | 1 | PRIMARY | quantities | range | start_timestamp | start_timestamp | 8 | NULL | 89 | Using ; Using temporary ; Using filesort | | 2 | DEPENDENT SUBQUERY | prices | ref | timestamp , type_id | type_id | 4 | const | 22930 | Using | + -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- + -- -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -+ -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + 2 rows set , 3 warnings ( 0.00 sec ) I noticed dependent sub query using timestamp field key query scanning loads rows . Can anyone help get running hell lot faster ? Here SQL statements required create schema fill lot data ( 2 months worth ) # Create prices table CREATE TABLE ` prices ` ( ` id ` int ( 11 ) NOT NULL AUTO_INCREMENT , ` timestamp ` datetime NOT NULL , ` type_id ` int ( 11 ) NOT NULL , ` price ` float ( 8,2 ) NOT NULL , PRIMARY KEY ( ` id ` ) , KEY ` timestamp ` ( ` timestamp ` ) , KEY ` type_id ` ( ` type_id ` ) ) ENGINE=MyISAM ; # Create quantities table CREATE TABLE ` quantities ` ( ` id ` int ( 11 ) NOT NULL AUTO_INCREMENT , ` start_timestamp ` datetime NOT NULL , ` end_timestamp ` datetime NOT NULL , ` quantity ` float ( 7,2 ) NOT NULL , PRIMARY KEY ( ` id ` ) , KEY ` start_timestamp ` ( ` start_timestamp ` ) , KEY ` end_timestamp ` ( ` end_timestamp ` ) ) ENGINE=MyISAM ; # Insert first 2 rows prices , one 2 types , starting 64 days ago INSERT INTO ` prices ` ( ` id ` , ` timestamp ` , ` type_id ` , ` price ` ) VALUES ( NULL , DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 1 ' , RAND ( ) ) , ( NULL , DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 2 ' , RAND ( ) ) ; # Fill prices table record type , every 5 minutes , next 64 days INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 32 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 16 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 8 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 4 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 2 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 1 DAY ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 12 HOUR ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 6 HOUR ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 3 HOUR ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 90 MINUTE ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 45 MINUTE ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 20 MINUTE ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 10 MINUTE ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_ADD ( ` timestamp ` , INTERVAL 5 MINUTE ) , ` type_id ` , RAND ( ) FROM prices ; INSERT INTO prices ( ` timestamp ` , ` type_id ` , ` price ` ) SELECT DATE_SUB ( ` timestamp ` , INTERVAL 5 MINUTE ) , ` type_id ` , RAND ( ) FROM prices WHERE MOD ( ( TIME_TO_SEC ( ` timestamp ` ) - TIME_TO_SEC ( CONCAT ( DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 00:00:00 ' ) ) ) , 45 *60 ) = 0 AND ` timestamp ` > CONCAT ( DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 00:00:00 ' ) ; # Insert first row quantities , start timestamp 64 days ago , end timestamp start timestamp plus 15 minutes INSERT INTO ` quantities ` ( ` id ` , ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) VALUES ( NULL , DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , DATE_SUB ( CURDATE ( ) , INTERVAL '63 23:45 ' DAY_MINUTE ) , RAND ( ) ) ; # Fill quantities table record 15 minute period next 64 days INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 32 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 32 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 16 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 16 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 8 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 8 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 4 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 4 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 2 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 2 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 1 DAY ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 1 DAY ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 12 HOUR ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 12 HOUR ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 6 HOUR ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 6 HOUR ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 3 HOUR ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 3 HOUR ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 90 MINUTE ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 90 MINUTE ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 45 MINUTE ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 45 MINUTE ) , RAND ( ) FROM quantities ; INSERT INTO ` quantities ` ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_ADD ( ` start_timestamp ` , INTERVAL 15 MINUTE ) , DATE_ADD ( ` end_timestamp ` , INTERVAL 15 MINUTE ) , RAND ( ) FROM quantities ; INSERT INTO quantities ( ` start_timestamp ` , ` end_timestamp ` , ` quantity ` ) SELECT DATE_SUB ( ` start_timestamp ` , INTERVAL 15 MINUTE ) , DATE_SUB ( ` end_timestamp ` , INTERVAL 15 MINUTE ) , RAND ( ) FROM quantities WHERE MOD ( ( TIME_TO_SEC ( ` start_timestamp ` ) - TIME_TO_SEC ( CONCAT ( DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 00:00:00 ' ) ) ) , 45 * 60 ) = 0 AND ` start_timestamp ` > CONCAT ( DATE_SUB ( CURDATE ( ) , INTERVAL 64 DAY ) , ' 00:00:00 ' ) ;","Here first attempt . This one dirty uses following properties data : * three 5 minute prices quarter quantities ( violated data query work ) * notice cardinality three , guaranteed data integrity checks therefore I call dirty * also flexible changes periods Query 1 : SELECT sql_no_cache min ( q.start_timestamp ) start , max ( q.end_timestamp ) end , sum ( ( p1.price + p2.price + p3.price ) /3*q.quantity ) total FROM quantities q join prices p1 q.start_timestamp = p1.timestamp p1.type_id = 1 join prices p2 p2.timestamp = adddate ( q.start_timestamp , interval 5 minute ) p2.type_id = 1 join prices p3 p3.timestamp = adddate ( q.start_timestamp , interval 10 minute ) p3.type_id = 1 WHERE q.start_timestamp '2010-07-01 00:00:00 ' '2010-07-01 23:59:59 ' GROUP BY hour ( q.start_timestamp ) ; This one returns results 0.01 sec slow testing machine , original query runs ~6 sec , gnarf 's query ~0.85 sec ( queries always tested ` SQL_NO_CACHE ` keyword reuse results , warm database ) . EDIT : Here version sensitive missing rows price side Query 1a SELECT sql_no_cache min ( q.start_timestamp ) start , max ( q.end_timestamp ) end , sum ( ( COALESCE ( p1.price,0 ) + COALESCE ( p2.price,0 ) + COALESCE ( p3.price,0 ) ) / ( 3 - COALESCE ( p1.price-p1.price,1 ) - COALESCE ( p2.price-p2.price,1 ) - COALESCE ( p3.price-p3.price,1 ) ) *q.quantity ) total FROM quantities q LEFT JOIN prices p1 q.start_timestamp = p1.timestamp p1.type_id = 1 LEFT JOIN prices p2 p2.timestamp = adddate ( q.start_timestamp , interval 5 minute ) p2.type_id = 1 LEFT JOIN prices p3 p3.timestamp = adddate ( q.start_timestamp , interval 10 minute ) p3.type_id = 1 WHERE q.start_timestamp '2010-07-01 00:00:00 ' '2010-07-01 23:59:59 ' GROUP BY hour ( q.start_timestamp ) ; EDIT2 : Query 2 : Here direct improvement , different approach , query minimal changes brings execuction time ~0.22 sec machine SELECT sql_no_cache MIN ( ` quantities ` . ` start_timestamp ` ) AS ` start ` , MAX ( ` quantities ` . ` end_timestamp ` ) AS ` end ` , SUM ( ` quantities ` . ` quantity ` * ( SELECT AVG ( ` prices ` . ` price ` ) FROM ` prices ` WHERE ` prices ` . ` timestamp ` > = '2010-07-01 00:00:00 ' AND ` prices ` . ` timestamp ` < '2010-07-02 00:00:00 ' AND ` prices ` . ` timestamp ` > = ` quantities ` . ` start_timestamp ` AND ` prices ` . ` timestamp ` < ` quantities ` . ` end_timestamp ` AND ` prices ` . ` type_id ` = 1 ) ) AS total FROM ` quantities ` WHERE ` quantities ` . ` start_timestamp ` > = '2010-07-01 00:00:00' AND ` quantities ` . ` start_timestamp ` < '2010-07-02 00:00:00' GROUP BY HOUR ( ` quantities ` . ` start_timestamp ` ) ; That mysql 5.1 , I think I read 5.5 kind thing ( merging indexes ) available query planner . Also , could make start_timestamp timestamp related foreign key allow kind correlated queries make use indexes ( would need modify design establish sort timeline table could referenced quantities prices ) . Query 3 : Finally , last version ~0.03 sec , robust flexible Query 2 SELECT sql_no_cache MIN ( start ) , MAX ( end ) , SUM ( subtotal ) FROM ( SELECT sql_no_cache q. ` start_timestamp ` AS ` start ` , q. ` end_timestamp ` AS ` end ` , AVG ( p. ` price ` * q. ` quantity ` ) AS ` subtotal ` FROM ` quantities ` q LEFT JOIN ` prices ` p ON p.timestamp > = q.start_timestamp AND p.timestamp < q.end_timestamp AND p.timestamp > = '2010-07-01 00:00:00 ' AND p. ` timestamp ` < '2010-07-02 00:00:00 ' WHERE q. ` start_timestamp ` > = '2010-07-01 00:00:00 ' AND q. ` start_timestamp ` < '2010-07-02 00:00:00' AND p.type_id = 1 GROUP BY q. ` start_timestamp ` ) forced_tmp GROUP BY hour ( start ) ; **NOTE : ** Do forget remove **sql_no_cache** keywords production . There many counter intuitive tricks applied queries ( sometimes conditions repeated join condition speed queries , sometimes slow ) . Mysql great little RDBMS really fast comes relatively simple queries , complexity increases easy run scenarios . So general , I apply following principle set expectations regarding performance query : * base result set < 1,000 rows query business ~0.01 sec ( base result set number rows functionally determine resulting set ) In particular case start less 1000 rows ( prices quantities one day , 15 minutes precision ) able compute final results ."
Stackoverflow,"I 've got list objects I 've got db table full records . My list objects title attribute I want remove objects duplicate titles list ( leaving original ) . Then I want check list objects duplicates records database , remove items list adding database . I seen solutions removing duplicates list like : ` myList = list ( set ( myList ) ) ` , 'm sure list objects ? I need maintain order list objects . I also thinking maybe I could use ` difflib ` check differences titles .","The ` set ( list_of_objects ) ` remove duplicates know duplicate , , 'll need define uniqueness object . In order , 'll need make object hashable . You need define ` __hash__ ` ` __eq__ ` method , : < http : //docs.python.org/glossary.html # term-hashable > Though , 'll probably need define ` __eq__ ` method . **EDIT** : How implement ` __eq__ ` method : You 'll need know , I mentioned , uniqueness definition object . Supposed Book attributes author_name title combination unique , ( , many books Stephen King authored , many books named The Shining , one book named The Shining Stephen King ) , implementation follows : def __eq__ ( self , ) : return self.author_name==other.author_name\ self.title==other.title Similarly , I sometimes implement ` __hash__ ` method : def __hash__ ( self ) : return hash ( ( 'title ' , self.title , 'author_name ' , self.author_name ) ) You check create list 2 books author title , book objects ~~be ( ` ` operator ) and~~ equal ( ` == ` operator ) . Also , ` set ( ) ` used , remove one book . **EDIT** : This one old anwser mine , I notice error corrected strikethrough last paragraph : objects ` hash ( ) ` wo n't give ` True ` compared ` ` . Hashability object used , however , intend use elements set , keys dictionary ."
Stackoverflow,"I downloaded developer edition SQL Anywhere . How I get list tables database I 'm connected ? . Also particular table , I get meta-data table ( column names , types , etc ) ?","You use ini_set ( `` memory_limit '' , -1 ) ; `` memory_set '' . Also , look : < https : //stackoverflow.com/a/5263981/2729140 > ( Suhosin extension 's memory limit setting ) . If script needs lot memory , try revise . One thing aware unlimited SQL queries . Your tables A LOT records , 's wise always limit queries . If need fetch records table , pages , using ` LIMIT ... OFFSET ` SQL constructs ."
Stackoverflow,"I two tables Employee Department following entity classes Department.java @ Entity @ Table ( name = `` DEPARTMENT '' ) public class Department { @ Id @ Column ( name = `` DEPARTMENT_ID '' ) @ GeneratedValue ( strategy = GenerationType.AUTO ) private Integer departmentId ; @ Column ( name = `` DEPARTMENT_NAME '' ) private String departmentName ; @ Column ( name = `` LOCATION '' ) private String location ; @ OneToMany ( cascade = CascadeType.ALL , mappedBy = `` department '' , orphanRemoval = true ) @ Fetch ( FetchMode.SUBSELECT ) // @ Fetch ( FetchMode.JOIN ) private List < Employee > employees = new ArrayList < > ( ) ; } Employee.java @ Entity @ Table ( name = `` EMPLOYEE '' ) public class Employee { @ Id @ SequenceGenerator ( name = `` emp_seq '' , sequenceName = `` seq_employee '' ) @ GeneratedValue ( generator = `` emp_seq '' ) @ Column ( name = `` EMPLOYEE_ID '' ) private Integer employeeId ; @ Column ( name = `` EMPLOYEE_NAME '' ) private String employeeName ; @ ManyToOne @ JoinColumn ( name = `` DEPARTMENT_ID '' ) private Department department ; } Below queries fired I ` em.find ( Department.class , 1 ) ; ` **\ -- fetch mode = fetchmode.join** SELECT department0_.DEPARTMENT_ID AS DEPARTMENT_ID1_0_0_ , department0_.DEPARTMENT_NAME AS DEPARTMENT_NAME2_0_0_ , department0_.LOCATION AS LOCATION3_0_0_ , employees1_.DEPARTMENT_ID AS DEPARTMENT_ID3_1_1_ , employees1_.EMPLOYEE_ID AS EMPLOYEE_ID1_1_1_ , employees1_.EMPLOYEE_ID AS EMPLOYEE_ID1_1_2_ , employees1_.DEPARTMENT_ID AS DEPARTMENT_ID3_1_2_ , employees1_.EMPLOYEE_NAME AS EMPLOYEE_NAME2_1_2_ FROM DEPARTMENT department0_ LEFT OUTER JOIN EMPLOYEE employees1_ ON department0_.DEPARTMENT_ID =employees1_.DEPARTMENT_ID WHERE department0_.DEPARTMENT_ID= ? **\ -- fetch mode = fetchmode.subselect** SELECT department0_.DEPARTMENT_ID AS DEPARTMENT_ID1_0_0_ , department0_.DEPARTMENT_NAME AS DEPARTMENT_NAME2_0_0_ , department0_.LOCATION AS LOCATION3_0_0_ FROM DEPARTMENT department0_ WHERE department0_.DEPARTMENT_ID= ? SELECT employees0_.DEPARTMENT_ID AS DEPARTMENT_ID3_1_0_ , employees0_.EMPLOYEE_ID AS EMPLOYEE_ID1_1_0_ , employees0_.EMPLOYEE_ID AS EMPLOYEE_ID1_1_1_ , employees0_.DEPARTMENT_ID AS DEPARTMENT_ID3_1_1_ , employees0_.EMPLOYEE_NAME AS EMPLOYEE_NAME2_1_1_ FROM EMPLOYEE employees0_ WHERE employees0_.DEPARTMENT_ID= ? I wanted know one prefer ` FetchMode.JOIN ` ` FetchMode.SUBSELECT ` ? one opt scenario ?","The SUBQUERY strategy Marmite refers related FetchMode.SELECT , SUBSELECT . The console output 've posted **fetchmode.subselect** curious way supposed work . The [ FetchMode.SUBSELECT ] ( https : //docs.jboss.org/hibernate/annotations/3.5/api/org/hibernate/annotations/FetchMode.html # SUBSELECT ) > use subselect query load additional collections Hibernate [ docs ] ( http : //docs.jboss.org/hibernate/orm/4.3/manual/en- US/html_single/ # performance-fetching-subselect ) : > If one lazy collection single-valued proxy fetched , Hibernate load , re-running original query subselect . This works way batch-fetching without piecemeal loading . FetchMode.SUBSELECT look something like : SELECT < employees columns > FROM EMPLOYEE employees0_ WHERE employees0_.DEPARTMENT_ID IN ( SELECT department0_.DEPARTMENT_ID FROM DEPARTMENT department0_ ) You see second query bring memory **all** employees belongs departament ( i.e . employee.department_id null ) , n't matter department retrieve first query . So potentially major issue table employees large may [ accidentially loading whole database memory ] ( http : //www.christophbrill.de/de_DE/hibernate-fetch-subselect- performance/ ) . However , FetchMode.SUBSELECT reduces significatly number queries takes two queries comparisson N+1 queries FecthMode.SELECT . You may thinking FetchMode.JOIN makes even less queries , 1 , use SUBSELECT ? Well , 's true cost duplicated data heavier response . If single-valued proxy fetched JOIN , query may retrieve : + -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -+ | DEPARTMENT_ID | BOSS_ID | BOSS_NAME | + -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -+ | 1 | 1 | GABRIEL | | 2 | 1 | GABRIEL | | 3 | 2 | ALEJANDRO | + -- -- -- -- -- -- -- -+ -- -- -- -- -+ -- -- -- -- -- -+ The employee data boss duplicated directs one department cost bandwith . If lazy collection fetched JOIN , query may retrieve : + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ | DEPARTMENT_ID | DEPARTMENT_ID | EMPLOYEE_ID | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ | 1 | Sales | GABRIEL | | 1 | Sales | ALEJANDRO | | 2 | RRHH | DANILO | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -+ The department data duplicated contains one employee ( natural case ) . We n't suffer cost bandwidth also get duplicate [ duplicated Department objects ] ( https : //howtoprogramwithjava.com/how-to-fix-duplicate-data-from- hibernate-queries/ ) must use SET [ DISTINCT_ROOT_ENTITY ] ( https : //docs.jboss.org/hibernate/orm/current/javadocs/org/hibernate/criterion/CriteriaSpecification.html # DISTINCT_ROOT_ENTITY ) de-duplicate . However , duplicate data pos lower latency good trade many cases , like Markus Winand [ says ] ( http : //use-the-index- luke.com/sql/join/nested-loops-join-n1-problem ) . > An SQL join still efficient nested selects approach—even though performs index lookups—because **avoids lot network communication** . It **is even faster total amount transferred data bigger duplication** employee attributes sale . That two dimensions performance : response time throughput ; computer networks call latency bandwidth . Bandwidth minor impact response time **latencies huge impact** . That means number database round trips important response time amount data transferred . So , main issue using SUBSELECT [ hard control ] ( https : //stackoverflow.com/a/7239455/3517383 ) may loading whole graph entities memory . With Batch fetching fetch associated entity separate query SUBSELECT ( n't suffer duplicates ) , gradually important query related entities ( n't suffer potentially load huge graph ) IN subquery filtered IDs retrieved outter query ) . Hibernate : select ... mkyong.stock stock0_ Hibernate : select ... mkyong.stock_daily_record stockdaily0_ stockdaily0_.STOCK_ID ( ? , ? , ? , ? , ? , ? , ? , ? , ? , ? ) ( It may interesting test Batch fetching high batch size would act like SUBSELECT without issue load whole table ) A couple posts showing different fetching strategies SQL logs ( important ) : * [ Hibernate – fetching strategies examples ] ( http : //www.mkyong.com/hibernate/hibernate-fetching-strategies-examples/ ) * [ Hibernate FetchMode explained example ] ( http : //www.solidsyntax.be/2013/10/17/fetching-collections-hibernate/ ) * [ Investigating Hibernate fetch strategies – A tutorial ] ( https : //technicalmumbojumbo.wordpress.com/2011/08/24/investigating-hibernate-fetch-strategy-tutorial/ ) Summary : * JOIN : avoids major issue N+1 queries may retrieve data duplicated . * SUBSELECT : avoids N+1 n't duplicate data loads entities associated type memory . The tables built using [ ascii-tables ] ( https : //ozh.github.io/ascii- tables/ ) ."
Stackoverflow,"I created web app uses MySQL database , I migrate database **Microsoft SQL Server 2008 R2** I 'm using **SQL Server Migration Assistant** ( SSMA ) . I 'm getting errors report tables use foreign keys . # # 1\ . Self-referencing foreign keys I one table parent-child relationship rows ; **map** table : | map_id | map_title | latitude | longitude | map_zoom | map_parent | | : -- -- -- : | : -- -- -- -- -- -- -- -- -- - : | : -- -- -- -- - : | : -- -- -- -- -- : | : -- -- -- -- : | : -- -- -- -- -- : | | 1 | My Parent Map | 50.364829 | -52.635623 | 17 | NULL | | 2 | Some Child Map | 50.366916 | -52.634718 | | 1 | | 3 | Another Child Map | 50.364898 | -52.634543 | | 1 | | 4 | My Last Example Map | 50.361986 | -52.638891 | | 3 | The report generated SQL Server Migration Assistant ( SSMA ) shows SQL would used create table SQL Server . **MySQL** ( source ) : 1 CREATE 2 TABLE ` map ` 3 ( 4 ` map_id ` int ( 11 ) UNSIGNED NOT NULL AUTO_INCREMENT , 5 ` map_title ` varchar ( 50 ) DEFAULT NULL , 6 ` latitude ` varchar ( 12 ) DEFAULT NULL , 7 ` longitude ` varchar ( 12 ) DEFAULT NULL , 8 ` map_zoom ` varchar ( 5 ) NOT NULL , 9 ` map_parent ` int ( 11 ) UNSIGNED DEFAULT NULL , 10 PRIMARY KEY ( ` map_id ` ) , 11 KEY ` map_parent ` ( ` map_parent ` ) , 12 CONSTRAINT ` map_ibfk_2 ` FOREIGN KEY ( ` map_parent ` ) REFERENCES ` map ` ( ` map_id ` ) ON DELETE CASCADE ON UPDATE CASCADE 13 ) ENGINE = InnoDB AUTO_INCREMENT = 12 DEFAULT CHARSET = utf8 ; **SQL Server** ( target , SQL generated SSMA ) : 1 CREATE TABLE dbo.map 2 ( 3 map_id bigint NOT NULL IDENTITY ( 12 , 1 ) , 4 map_title nvarchar ( 50 ) NULL DEFAULT NULL , 5 latitude nvarchar ( 12 ) NULL DEFAULT NULL , 6 longitude nvarchar ( 12 ) NULL DEFAULT NULL , 7 map_zoom nvarchar ( 5 ) NOT NULL , 8 map_parent bigint NULL DEFAULT NULL , 9 CONSTRAINT PK_map_map_id PRIMARY KEY ( map_id ) , 10 /* 11 * SSMA error messages : 12 * M2SS0040 : ON DELETE CASCADE|SET NULL|SET DEFAULT action changed NO ACTION avoid circular references cascaded foreign keys . 13 14 CONSTRAINT map $ map_ibfk_2 FOREIGN KEY ( map_parent ) REFERENCES dbo.map ( map_id ) 15 ON DELETE NO ACTION 16 /* 17 * SSMA error messages : 18 * M2SS0036 : ON UPDATE CASCADE|SET NULL|SET DEFAULT action changed NO ACTION avoid circular references cascaded foreign keys . 19 20 ON UPDATE NO ACTION 21 */ 22 23 24 */ 25 26 27 ) 28 GO 29 CREATE NONCLUSTERED INDEX map_parent 30 ON dbo.map ( map_parent ASC ) 31 GO As see gives error indicating changed ` ON UPDATE CASCADE ` ` ON DELETE CASCADE ` ` NO ACTION ` order `` avoid circular references cascaded foreign keys . '' # # 2\ . Many-to-many tables I two tables got error `` multiple paths '' similarly changed ` NO ACTION ` . **asset_property** table : | asset_id | property_id | property_value | | : -- -- -- -- : | : -- -- -- -- -- - : | : -- -- -- -- -- -- -- - : | | 933 | 1 | Joseph | | 933 | 2 | Green | | 936 | 1 | Jacob | | 936 | 2 | Yellow | | 942 | 1 | Susan | | 942 | 2 | Blue | **MySQL** ( source ) : 1 CREATE 2 TABLE ` asset_property ` 3 ( 4 ` asset_id ` int ( 11 ) NOT NULL , 5 ` property_id ` int ( 11 ) NOT NULL , 6 ` property_value ` varchar ( 100 ) DEFAULT NULL , 7 PRIMARY KEY ( ` asset_id ` , ` property_id ` ) , 8 KEY ` asset_id ` ( ` asset_id ` ) , 9 KEY ` property_id ` ( ` property_id ` ) , 10 CONSTRAINT ` asset_property_ibfk_1 ` FOREIGN KEY ( ` asset_id ` ) REFERENCES ` asset ` ( ` asset_id ` ) ON DELETE CASCADE ON UPDATE CASCADE , 11 CONSTRAINT ` asset_property_ibfk_2 ` FOREIGN KEY ( ` property_id ` ) REFERENCES ` property ` ( ` property_id ` ) ON DELETE CASCADE ON UPDATE CASCADE 12 ) ENGINE = InnoDB DEFAULT CHARSET = utf8 ; **SQL Server** ( target , SQL generated SSMA ) : 1 CREATE TABLE dbo.asset_property 2 ( 3 asset_id int NOT NULL , 4 property_id int NOT NULL , 5 property_value nvarchar ( 100 ) NULL DEFAULT NULL , 6 CONSTRAINT PK_asset_property_asset_id PRIMARY KEY ( asset_id , property_id ) , 7 /* 8 * SSMA error messages : 9 * M2SS0041 : ON DELETE CASCADE|SET NULL|SET DEFAULT action changed NO ACTION avoid multiple paths cascaded foreign keys . 10 11 CONSTRAINT asset_property $ asset_property_ibfk_1 FOREIGN KEY ( asset_id ) REFERENCES dbo.asset ( asset_id ) 12 ON DELETE NO ACTION 13 /* 14 * SSMA error messages : 15 * M2SS0037 : ON UPDATE CASCADE|SET NULL|SET DEFAULT action changed NO ACTION avoid multiple paths cascaded foreign keys . 16 17 ON UPDATE NO ACTION 18 */ 19 20 21 */ 22 23 , 24 CONSTRAINT asset_property $ asset_property_ibfk_2 FOREIGN KEY ( property_id ) REFERENCES dbo.property ( property_id ) 25 ON DELETE CASCADE 26 ON UPDATE CASCADE 27 ) 28 GO 29 CREATE NONCLUSTERED INDEX asset_id 30 ON dbo.asset_property ( asset_id ASC ) 31 GO 32 CREATE NONCLUSTERED INDEX property_id 33 ON dbo.asset_property ( property_id ASC ) 34 GO I 've found one [ article ] ( http : //blogs.msdn.com/b/ssma/archive/2011/03/19/mysql-to-sql-server- migration-method-for-correcting-schema-issues.aspx ) talks errors . The article 's solution self-referencing table error seem apply , many-to-many error solution remove constraint `` application user ’ modifying values . '' Thanks help ! ! * * * ! [ db diagram ] ( https : //i.stack.imgur.com/hbQUC.png )",After spent literally hours trying find n't work I found solution I started give use linked table Access pass-through query MSSQL . I using wrong ODBC driver . It turns 2 MySQL ODBC drivers installed ` ANSI ` ` Unicode ` driver . I using ` ANSI ` . When I swapped ` Unicode ` well ! ! [ Unicode ODBC driver ] ( https : //i.stack.imgur.com/CU5rP.png )
Stackoverflow,"I following sample code . The objective run SQL statement multiple input parameters . [ < Literal > ] let connectionString = @ '' Data Source=Localhost ; Initial Catalog=Instrument ; Integrated Security=True '' [ < Literal > ] let query = `` SELECT MacroName , MacroCode FROM Instrument WHERE MacroCode IN ( @ codeName ) '' type MacroQuery = SqlCommandProvider < query , connectionString > let cmd = new MacroQuery ( ) let res = cmd.AsyncExecute ( codeName= [ | '' CPI '' ; '' GDP '' | ] ) | > Async.RunSynchronously However , codeName inferred string type instead array list give error . Alternatively , I could run query without statement filter based result . However , lots cases returns millions rows , I would prefer filter data SQL server level efficient . I n't find relevant samples documentation fsharp.data.sqlclient . Please help !",`` See Table-valued parameters ( TVPs ) '' section documentation : < http : //fsprojects.github.io/FSharp.Data.SqlClient/configuration % 20and % 20input.html >
Stackoverflow,"I wanted work custom DB provider Visual Studio . I need use Entity Framework . For example , I downloaded NpgSQL , registered GAC : gacutil -i c : \temp\npgsql.dll gacutil -i c : \temp\mono.security.dll added machine.config file : < add name= '' Npgsql Data Provider '' invariant= '' Npgsql '' support= '' FF '' description= '' .Net Framework Data Provider Postgresql Server '' type= '' Npgsql.NpgsqlFactory , Npgsql , Version=2.0.6.0 , Culture=neutral , PublicKeyToken=5d8b90d52f46fda7 '' / > But Npgsql appear Datasource list Visual Studio : ! [ Data source VS ] ( https : //i.stack.imgur.com/KUT60.png ) How add custom DB provider list ? UPD : If I use command string edmgen.exe I got error : > error 7001 : Failed find load registered .Net Framework Data Provider .","Pass array : string [ ] numbers = new string [ ] { `` 123 '' , `` 234 '' } ; NpgsqlCommands cmd = new NpgsqlCommands ( `` select * products number = ANY ( : numbers ) '' ) ; NpgsqlParameter p = new NpgsqlParameter ( `` numbers '' , NpgsqlDbType.Array | NpgsqlDbType.Text ) ; p.value = numbers ; command.Parameters.Add ( p ) ;"
Stackoverflow,I run jboss standalone mode set datasource ` standalone.xml ` following : < datasource jndi-name= '' MyDenaliDS '' pool-name= '' MyDenaliDs_Pool '' enabled= '' true '' jta= '' true '' use-java-context= '' true '' use-ccm= '' true '' > < connection-url > jdbc : sqlserver : //myip:1433 ; databaseName=mydb ; integratedSecurity=true < /connection-url > < driver > sqljdbc < /driver > < security > < user-name > username < /user-name > < password > password < /password > < /security > < /datasource > < drivers > < driver name= '' sqljdbc '' module= '' com.microsoft.sqlserver.jdbc '' > < driver-class > com.microsoft.sqlserver.jdbc.SQLServerDataSource < /driver-class > < /driver > < /drivers > folder ` % jbosshome % \modules\com\microsoft\sqlserver\jdbc\ ` I ` sqljdb4.jar ` following ` module.xml ` : < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < module name= '' com.microsoft.sqlserver.jdbc '' xmlns= '' urn : jboss : module:1.0 '' > < resources > < resource-root path= '' sqljdbc4.jar '' / > < /resources > < dependencies > < module name= '' javax.api '' / > < module name= '' javax.transaction.api '' / > < /dependencies > < /module > When I start jboss gives following error : > New missing/unsatisfied dependencies : service > jboss.jdbc-driver.sqljdbc ( missing ) Anyone know I 've done incorrect I 'm missing ?,"Comment line ` setEncrypt ( true ) ` : ... dSource.setDatabaseName ( REDACTED ) ; //dSource.setEncrypt ( true ) ; dSource.setTrustServerCertificate ( true ) ; ... You might trouble encryption setting . From [ setEncrypt ( ... ) ] ( http : //msdn.microsoft.com/en-us/library/bb879920.aspx ) documentation : > If encrypt property set **true** , Microsoft SQL Server JDBC Driver uses JVM 's default JSSE security provider negotiate SSL encryption SQL Server . The default security provider may support features required negotiate SSL encryption successfully . For example , default security provider may support size RSA public key used SQL Server SSL certificate . In case , default security provider might raise error cause JDBC driver terminate connection . In order resolve issue , one following : > > * Configure SQL Server server certificate smaller RSA public key > > * Configure JVM use different JSSE security provider '' /lib/security/java.security '' security properties file > > * Use different JVM > > **Update** With Java versions 1.6.0_29 7.0.0_1 Oracle introduced security fix SSL/TLS BEAST attack likely cause problem . The security fix known make trouble database connections MSSQL Server jTDS driver Microsoft driver . You either * decide use encryption using ` setEncrypt ( true ) ` ( specified ) * , enforced MSSQL Server , could turn Java fix JVM setting ` -Djsse.enableCBCProtection=false ` system property . Be warned , affect SSL connections within VM ."
Stackoverflow,"I currently database table setup follows ( EAV - business reasons valid ) : * Id - int ( PK ) * Key - unique , varchar ( 15 ) * Value - varchar ( 1000 ) This allows add mixed values databse key/value pairs . For example : 1 | 'Some Text ' | 'Hello World' 2 | 'Some Number ' | '123456' etc . In C # code I use ADO.Net using ` reader.GetString ( 2 ) ; ` retrieve value string , code elsewhere convert needed , example ... ` Int32.ParseInt ( myObj.Value ) ; ` . I 'm looking enhancing table possibly changing value column ` sql_variant ` datatype , I n't know benefit would ? Basically , advantage _value_ column ` sql_variant ` vs ` varchar ( 1000 ) ` ? * * * To clear , I read somewhere sql_variant gets returned nvarchar ( 4000 ) back client making call ( ouch ) ! But , could n't I cast 's type returning ? Obviously code would adjusted store value object instead string value . I guess , advantages/disadvantages using ` sql_variant ` versus type current situation ? Oh , worth mentioning I plan store datetimes , strings , numerical types ( int , decimal , etc ) value column ; I n't plan storing blob images etc .",One word : **DON'T** This bad practice - columns **ONE SINGLE DATATYPE** reason . Do abuse make everything variants ... ... . Marc
Stackoverflow,"SQLite experimental JSON1 extention work JSON fields . The functions choose look promising , I n't get use context query . Suppose I created following table : sqlite > create table user ( name , phone ) ; sqlite > insert user values ( 'oz ' , json_array ( [ '+491765 ' , '+498973 ' ] ) ) ; The documentation shows use ` json_each ` query , functions lack context documentation . Can someone SQLite experience provide examples use : * ` json_extract ` * ` json_set `","So , first example use ` json_extract ` . First , data inserted bit different way : insert user ( name , phone ) values ( `` oz '' , json ( ' { `` cell '' : '' +491765 '' , `` home '' : '' +498973 '' } ' ) ) ; Now , select users phone numbers normal sql : sqlite > select user.phone user user.name=='oz ' ; { `` cell '' : '' +491765 '' , '' home '' : '' +498973 '' } sqlite > But , n't care land lines want cell phones ? Enter ` json_extract ` : sqlite > select json_extract ( user.phone , ' $ .cell ' ) user ; +491765 And use ` json_extract ` . Using ` json_set ` similar . Given want update cell phone : sqlite > select json_set ( json ( user.phone ) , ' $ .cell ' , 123 ) \ user ; { `` cell '' :123 , '' home '' : '' +498973 '' } You combine function calls SQL queries . Thus , use SQLite structured data unstructured data form JSON . Here update user cell phone : sqlite > update user ... > set phone = ( select json_set ( json ( user.phone ) , ' $ .cell ' , 721 ) user ) ... > name == 'oz ' ; sqlite > select * user ; oz| { `` cell '' :721 , '' home '' : '' +498973 '' }"
Stackoverflow,[ ! [ Here error ] ( https : //i.stack.imgur.com/RLwBc.png ) ] ( https : //i.stack.imgur.com/RLwBc.png ) > TITLE : Microsoft SQL Server Management Studio > > Attach database failed Server ' ( localdb ) \mssqllocaldb ' . ( Microsoft.SqlServer.Smo ) > > ADDITIONAL INFORMATION : > > At least one file needed Database Attach . ( Microsoft.SqlServer.Smo ) I trying attach ` .mdf ` database file LocalDb instance . It's fine I SQL Server . I ` .ldf ` file directory,"For completion 's sake - [ Jim's comment ] ( https : //stackoverflow.com/questions/40020271/trying-to-attach-mdf- file-to-localdb-throws-error-at-least-one-file-is- required # comment68313278_40020271 ) solves ( half ) problem gets going . The `` half '' problem - _ultimately_ want rename physical database file ? The answer available [ CodeProject post ] ( https : //www.codeproject.com/Tips/345942/Rename-a-Database-and-its-MDF- and-LDF-files-in-SQL ) . * * * Steps : 1 . ** ` ALTER DATABASE ` set new _physical_ filenames ( data file log file ) ** _Wo n't take effect SQL Server restarted database taken offline brought back online_ * ` ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName ' , FILENAME = ' < Full-Path-Required > \NewDbName.mdf ' ) ; ` * ` ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName_log ' , FILENAME = ' < Full-Path-Required > \NewDbName_log.ldf ' ) ; ` 2 . ** ` ALTER DATABASE ` set new _logical_ file names ( , data log files ) ** _Takes effect immediately_ * ` ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName ' , NEWNAME = 'NewDbName ' ) ; ` * ` ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName_log ' , NEWNAME = 'NewDbName_log ' ) ; ` 3 . **Take offline bring back online restart SQL Server** * **Using SQL Server Management Studio : ** 1 . Right-click renamed database click ` Take Offline ` ` Tasks ` . 2 . Right-click ( offline ) database click ` Bring Online ` ` Tasks ` . * **Using T-SQL : ** 1 . ` ALTER DATABASE [ CurrentName ] SET OFFLINE WITH ROLLBACK IMMEDIATE ; ` ( sets offline disconnects clients ) 2 . ` ALTER DATABASE [ CurrentName ] SET ONLINE ; ` * * * Full code : -- Find `` CurrentName '' ( without quotes ) replace current database name -- Find `` NewDbName '' ( without quotes ) replace new database name USE [ CurrentName ] ; -- Change physical file names : ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName ' , FILENAME = ' < Full-Path-Required > \NewDbName.mdf ' ) ; ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName_log ' , FILENAME = ' < Full-Path-Required > \NewDbName_log.ldf ' ) ; -- Change logical names : ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName ' , NEWNAME = 'NewDbName ' ) ; ALTER DATABASE [ CurrentName ] MODIFY FILE ( NAME = 'CurrentName_log ' , NEWNAME = 'NewDbName_log ' ) ; -- Take offline back online USE [ master ] GO ALTER DATABASE [ CurrentName ] SET OFFLINE WITH ROLLBACK IMMEDIATE ; -- Then navigate < Full-Path-Required > rename files ALTER DATABASE [ CurrentName ] SET ONLINE ;"
Stackoverflow,"I VS2015 [ SSDT ] ( https : //docs.microsoft.com/en-us/sql/ssdt/download- sql-server-data-tools-ssdt ) installed , along [ SSMS ] ( https : //docs.microsoft.com/en-us/sql/ssms/download-sql-server- management-studio-ssms ) [ SqlServer ] ( https : //www.powershellgallery.com/packages/SqlServer/21.0.17152 ) PowerShell module ( includes ` invoke-sqlcmd ` comand ) , yet If I try execute query Azure SQL Data Warehouse like : invoke-sqlcmd -Query `` Select top 5 * customer '' -ConnectionString `` Server=tcp : my.database.windows.net,1433 ; Database=Customer ; Authentication=Active Directory Integrated ; Encrypt=True ; `` I get following error : invoke-sqlcmd : Unable load adalsql.dll ( Authentication=ActiveDirectoryIntegrated ) . Error code : 0x2 . For information , see http : //go.microsoft.com/fwlink/ ? LinkID=513072 At line:1 char:1 + invoke-sqlcmd -Query `` Select top 5 * vwOffer '' -ConnectionStrin ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation : ( : ) [ Invoke-Sqlcmd ] , SqlException + FullyQualifiedErrorId : SqlExectionError , Microsoft.SqlServer.Management.PowerShell.GetScriptCommand invoke-sqlcmd : At line:1 char:1 + invoke-sqlcmd -Query `` Select top 5 * vwOffer '' -ConnectionStrin ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ParserError : ( : ) [ Invoke-Sqlcmd ] , ParserException + FullyQualifiedErrorId : ExecutionFailureException , Microsoft.SqlServer.Management.PowerShell.GetScriptComman If I try install [ adalsql.dll ] ( http : //go.microsoft.com/fwlink/ ? LinkID=513072 ) directly , I get message stating ` A higher version already exists ` I see versions dll found : C : \Windows\SysWOW64\adalsql.dll C : \Windows\System32\adalsql.dll yet , invoke-sqlcmd cant find . Any idea either ( A ) register existing dll invoke-sqlcmd find ( B ) uninstall re-installed ? Incidentally , I _am_ able use Active Directory Authenticatoin 32-bit SQLCMD.exe , I know 32 bit dll working fine . It 's 64 bit dll n't loading properly ...","So , problem vexed well . I 'm unclear happened , maybe 's coincidence happened I installed latest version SSMS . My fix : 1 ) Navigate Add Remove Programs 2 ) In little search window type sql , go find : `` Active Directory Authentication Library SQL Server '' . 3 ) Uninstall little guy 4 ) Navigate download latest ADAL library ( pick x64 ) : < https : //www.microsoft.com/en-us/download/confirmation.aspx ? id=48742 > 5 ) Just kicks , reboot 6 ) Your stuff properly load adalsql.dll !"
Stackoverflow,"I 'm starting simple java test project using sqlite4java building using java . I get core sqlite4java library downloaded easily , I 'm sure best ( ! ) way get gradle download native libraries put right place . This build.gradle file : apply plugin : 'java' /* We use Java 1.7 */ sourceCompatibility = 1.7 targetCompatibility = 1.7 version = ' 1.0' repositories { mavenCentral ( ) } sourceSets { main { java.srcDir 'src' output.classesDir = 'build/main' } test { java.srcDir 'test' output.classesDir = 'build/test' } } dependencies { testCompile group : 'junit ' , name : 'junit ' , version : ' 4.11' compile `` com.almworks.sqlite4java : sqlite4java:1.0.392 '' compile `` com.almworks.sqlite4java : libsqlite4java-osx:1.0.392 '' } But I run simple test I get : TestTest > testSqlite4Basic FAILED com.almworks.sqlite4java.SQLiteException TestTest.java:15 Caused : java.lang.UnsatisfiedLinkError TestTest.java:15 ( I 'm building within IntelliJ , using gradle build options - I n't think 's ItelliJ stuffing class path running tests ... ) I 'm pretty sure first time I tried build I got message unable unpack libsqlite4java-osx ZIP file , ( surprisingly maven central says dylib file ) . What I need make gradle right thing ? **ASIDE** : I 'm able get code run manually copying downloaded ` .dylib ` maven cache somewhere listed ` java.library.path ` ( Think I used ` $ HOME/Library/Java/Extensions ` mac ) . But seems contrary whole point packaging setup dependencies ` .gradle ` file , wont let distribute anything easily later .","I suppose , need use additional gradle plugin handle native libraries make specific tasks , upload put native libs right place , order could found linked . At moment I know one plugin , hope solve problem < https : //github.com/cjstehno/gradle-natives > **Edit : ** The problem plugin case fact , dependecny `` com.almworks.sqlite4java : libsqlite4java-osx:1.0.392 '' native lib , jar included native lib I supposed . So , case , simply add dependency dependencies par build script , ' already done , create custom copy task , put place need . Tried gradle 2.6 Win7 , look like : task copyNtiveDeps ( type : Copy ) { ( configurations.compile+configurations.testCompile ) { include `` libsqlite4java-osx-1.0.392.dylib '' } `` c : \\tmp '' } In case , need set `` '' property path java.library.path . And second , make task runs automaticaly gradle task properties dependsOn mustRunAfter ."
Stackoverflow,I piece code needs run every day specified time . The code right sitting part web application . There 2 stored procedures get/save data code uses . How I setup Microsoft SQL Server Management Studio 2008 R2 execute code well stored procs SQL Agent Job . I never done seem find documentation .,"This solution would work : SELECT DATEDIFF ( SECOND , aj.start_execution_date , GetDate ( ) ) AS Seconds FROM msdb..sysjobactivity aj JOIN msdb..sysjobs sj sj.job_id = aj.job_id WHERE aj.stop_execution_date IS NULL -- job n't stopped running AND aj.start_execution_date IS NOT NULL -- job currently running AND sj.name = 'JobX' exists ( -- make sure recent run select 1 msdb..sysjobactivity new new.job_id = aj.job_id new.start_execution_date > aj.start_execution_date ) This general check dependent system tables . If 'd prefer custom route , could job insert job log table created instead ."
Stackoverflow,"Recently I 'm learning use node [ node- sqlite3 ] ( https : //github.com/mapbox/node-sqlite3 ) manipulate sqlite3 , sample . var sqlite3 = require ( 'sqlite3 ' ) ; var db = new sqlite3.Database ( ' : memory : ' ) ; db.serialize ( function ( ) { db.run ( `` CREATE TABLE test ( info TEXT ) '' ) ; db.run ( `` INSERT INTO test ( info ) VALUES ( 'info1 ' ) '' ) ; } ) db.close ( ) ; The documentation said ` db.serialized ` used ensure SQL lines executed order , I confused , would n't get executed order without ` db.serialize ` , would pulled event queue executed order ? How work ? And one sql executed , safe run without ` db.serialize ` follows ? var sqlite3 = require ( 'sqlite3 ' ) ; var db = new sqlite3.Database ( ' : memory : ' ) ; db.run ( `` CREATE TABLE test ( info TEXT ) '' ) ; db.close ( ) ;","Each command inside ` serialize ( ) ` function guaranteed **finish** executing next one starts . In example , ` CREATE TABLE ` finish ` INSERT ` gets run . If n't use ` serialize ( ) ` ` CREATE TABLE ` ` INSERT ` statements would run parallel . They would start quickly one ` INSERT ` may actually finish table created , giving error trying insert data table exist . This called _race condition_ , every time run program might get different winner . If ` CREATE TABLE ` wins race program work fine . But ` INSERT ` wins race , program break error . Since ca n't control wins race , ` serialize ( ) ` stop ` INSERT ` even starting ` CREATE TABLE ` reached end , ensuring get outcome every time . In second example one statement ` serialize ( ) ` still required . This ` run ( ) ` starts SQL query **returns immediately** , leaving query run background . Since next command one ` close ( ) ` database , 'll cut query still running . Since ` serialize ( ) ` n't return last internal queries completed , using hold ` close ( ) ` query completed . If using different type query ( say response user clicking button web page , database left open calls ) probably would n't need ` serialize ( ) ` . It depends whether code follows query requires queries completed . When deciding whether use ` serialize ( ) ` , helpful think non-serialized queries commented , see code would still work . In first example , removing ` CREATE TABLE ` command would break following ` INSERT ` statement ( 'd table insert ) , therefore need serialised . But two ` CREATE TABLE ` commands removing one would affect , two commands would serialized . _ ( This tip n't apply ` close ( ) ` however - rule thumb call ` close ( ) ` everything finished running . ) _"
Stackoverflow,"I newbie ( 1 week ) Python world . I tried installing django- mssql , I tried import library ( using ` import sqlserver_ado.dbapi ` ) , I got error message : ImportError : No module named pythoncom I tried look library without success . Can guys point right direction ?",You missing ` pythoncom ` package . It comes [ ActivePython ] ( http : //www.activestate.com/activepython ) get separately [ GitHub ] ( https : //github.com/mhammond/pywin32 ) ( previously [ SourceForge ] ( https : //sourceforge.net/projects/pywin32/files/ ) ) part pywin32 .
Stackoverflow,"I 'm working OS X 10.6.4 . I 've using clbuild install supporting libraries SBCL ( including clsql ) , I work Aquamacs . I installed MySQL using [ excellent instructions Hive Logic ] ( http : //hivelogic.com/articles/installing-mysql-on-mac-os-x ) . But I call ` ( require 'clsql ) ` \ -- seems work fine -- try execute ` ( clsql : connect ' ( nil `` lisp '' `` root '' `` '' ) : database-type : mysql ) ` connect local running database , I get error message : erred invoking # < COMPILE-OP ( : VERBOSE NIL ) { 12096109 } > # < CLSQL-MYSQL-SOURCE-FILE `` clsql_mysql '' { 1208E071 } > [ Condition type ASDF : OPERATION-ERROR ] From research problem , I think comes compiled version libmysqlclient.dylib file , I copy /usr/local/mysql/lib/ , I 'm clear go compiling . [ This forum post ] ( http : //osdir.com/ml/lisp.clsql.general/2006-11/msg00022.html ) seems say 's exactly I need , 's make file directory .","I would find much less surprising defined traits classes used normal inheritance : ( def-view-class trait-mapsto-company ( ) ( ( company-id : type integer : initarg : company-id ) ( company : accessor company : db-kind : join : db-info ( : join-class company : home-key company-id : foreign-key company-id : set nil ) ) ) ) ( def-view-class trait-mapsto-manager ( ) ( ( manager-id : type integer : initarg : manager-id ) ( manager : accessor manager : db-kind : join : db-info ( : join-class manager : home-key managerid : foreign-key emplid : set nil ) ) ) ( def-view-class employee ( trait-mapsto-company trait-mapsto-manager ) ( ( employee-id : db-kind : key : db-constraints ( : not-null ) : type integer ) ( first-name : accessor employee-first-name : type ( string 30 ) : initarg : first-name ) ( last-name : accessor employee-last-name : type ( string 30 ) : initarg : last-name ) ( email : accessor employee-email : type ( string 100 ) : initarg : email ) ) ) This certainly make accessor name dependent name inheriting class , really want ? My view way write shows would actually break decoupling principle ."
Stackoverflow,"I trying restore database using Restore-SqlDatabase cmdlet . I need relocate files I 'm getting following errror Restore-SqlDatabase : Can bind parameter 'RelocateFile ' . Can convert `` Microsoft.SqlServer.Management.Smo.RelocateFile '' value type `` Microsoft.SqlServer.Management.Smo.RelocateFile '' type `` Microsoft.SqlServer.Management.Smo.RelocateFile '' . At line:25 char:108 + ... e -RelocateFil $ RelocateData + ~~~~~~~~~~~~~ + CategoryInfo : InvalidArgument : ( : ) [ Restore-SqlDatabase ] , ParameterBindingException + FullyQualifiedErrorId CannotConvertArgumentNoMessage , Microsoft.SqlServer.Management.PowerShell.RestoreSqlDatabaseCommand My powershell code look like $ RelocateData = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile ( `` MyDB_Data '' , `` c : \data\MySQLServerMyDB.mdf '' ) $ RelocateLog = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile ( `` MyDB_Log '' , `` c : \data\MySQLServerMyDB.ldf '' ) $ file = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile ( $ RelocateData , $ RelocateLog ) $ myarr= @ ( $ RelocateData , $ RelocateLog ) Restore-SqlDatabase -ServerInstance DEV\DEMO -Database `` test '' -BackupFile $ backupfile -RelocateFile $ myarr","For solution # 1 , need specify assembly qualified name instanciate relocate file use correct assembly . $ RelocateData = New-Object 'Microsoft.SqlServer.Management.Smo.RelocateFile , Microsoft.SqlServer.SmoExtended , Version=11.0.0.0 , Culture=neutral , PublicKeyToken=89845dcd8080cc91 ' -ArgumentList `` MyDB_Data '' , `` c : \data\MySQLServerMyDB.mdf '' $ RelocateLog = New-Object 'Microsoft.SqlServer.Management.Smo.RelocateFile , Microsoft.SqlServer.SmoExtended , Version=11.0.0.0 , Culture=neutral , PublicKeyToken=89845dcd8080cc91 ' -ArgumentList `` MyDB_Log '' , `` c : \data\MySQLServerMyDB.ldf '' $ file = New-Object Microsoft.SqlServer.Management.Smo.RelocateFile ( $ RelocateData , $ RelocateLog ) $ myarr= @ ( $ RelocateData , $ RelocateLog ) Restore-SqlDatabase -ServerInstance DEV\DEMO -Database `` test '' -BackupFile $ backupfile -RelocateFile $ myarr Hope helps !"
Stackoverflow,"I executing SQL query via jcc run report . When I opened error log file program examined SQL query , everything seems fine ( There extra missing brackets , commas , etc syntax good ) however I execute I getting error : [ Report.execute ( ) ] DB2 SQL Error : SQLCODE=-104 , SQLSTATE=42601 , SQLERRMC= , ; ATE IN ( 1,2,3,10,1 ) ; , DRIVER=4.12.55 When I researched SQLCODE I found means illegal symbol query . What I look find illegal symbol ? This query ! [ enter image description ] ( https : //i.stack.imgur.com/Z5ixS.png ) Sorry tiny font zoom 200 % see query better . Thanks lot : )","You comma ( n't ) end line : AND Tick.STATE IN ( 1,2,3,10,1 ) , The following line also problem ."
Stackoverflow,"Here setup * Windows Server 2008 R2 64 bit * Apache 2.4.4 64 bit * PHP 5.4.15 32 bit ( 64 bit still experimental ) , thread safe , VC9 compiler * Microsoft SQL Server 2012 Native Client 64-bit * Microsoft Visual C++ 2010 x86 x64 I need load Microsoft 's SQLSRV library . I added 'extension=php_sqlsrv_54_ts.dll ' php.ini copied 'php_sqlsrv_54_ts.dll ' ext folder PHP installed . When I restart apache , I get following error php error log , SQLSRV listed phpinfo . PHP Warning : PHP Startup : Unable load dynamic library ' C : \php5\ext\php_sqlsrv_54_ts.dll ' - % 1 valid Win32 application . Where I going wrong ? EDIT For testing purposes I 've installed PHP 5.5.10 64 bit VC 2012 error remains : (","It ` sqlsrv_query ( ) ` uses ** ` SQLSRV_CURSOR_FORWARD ` ** cursor type default . However , order get result ` sqlsrv_num_rows ( ) ` , choose one cursor types : * SQLSRV_CURSOR_STATIC * SQLSRV_CURSOR_KEYSET * SQLSRV_CURSOR_CLIENT_BUFFERED For information , check : [ Cursor Types ( SQLSRV Driver ) ] ( http : //technet.microsoft.com/en- us/library/hh487160 % 28v=sql.105 % 29.aspx ) In conclusion , use query like : $ query = sqlsrv_query ( $ conn , $ result , array ( ) , array ( `` Scrollable '' = > 'static ' ) ) ; get result : $ row_count = sqlsrv_num_rows ( $ query ) ;"
Stackoverflow,"I taking backup SQLite DB using cp commmand running wal_checkpoint ( FULL ) . The DB used WAL mode files like -shm -wal folder . When I run wal_checkpoint ( FULL ) , changes WAL file get committed database . I waqs wondering whether -wal -shm files get deelted running checkpoint . If , contain ? I know backup process good since I using SQLite backup APIs . This bug code . Can anyone please suggest content -shm -wal files contain running checkpoint . Any link provided would helpful . Thanks","After searching numerous sources , I believe following true : 1 . The ` -shm ` file contains index ` -wal ` file . The ` -shm ` file improves performance reading ` -wal ` file . 2 . If ` -shm ` file gets deleted , get created next database access . 3 . If ` checkpoint ` run , ` -wal ` file deleted . To perform safe backups : 1 . It recommended use SQLite backup functions making backups . SQLite library even make backups online database . 2 . If n't want use ( 1 ) , best way close database handles . This ensures clean consistent state database file , deletes ` -shm ` ` -wal ` files . A backup made using ` cp ` , ` scp ` etc . 3 . If SQLite database file intended transmitted network , **vacuum** command run ` checkpoint ` . This removes fragmentation database file thereby reducing size , transfer less data network ."
Stackoverflow,"I 've trying `` read lines '' original ( and/or current ) motivation NetSqlAzMan project . Was written ? 1 . An adapter Windows Authorization Manager ( AzMan ) . Where methods NetSqlAzMan passes calls ( Windows Authorization Manager ( AzMan ) ) , perhaps nicer/cleaner methods ? 2 . A replacement ( Windows Authorization Manager ( AzMan ) ) . Where ( ) features available ( Windows Authorization Manager ( AzMan ) ) recreated NetSqlAzMan , code developed independently . ( Perhaps provide DotNet 4.0 support ? ? ? ) ( Perhaps remove COM dependencies ) 3 . To provide features ( Windows Authorization Manager ( AzMan ) ) provided . Aka , `` smarter '' / '' better '' version ( Windows Authorization Manager ( AzMan ) ) . 4 . To rewrite also keep semi-dead project alive open-source . ( As , perhaps ( Windows Authorization Manager ( AzMan ) ) ) dead abandoned project Microsoft ) . 5 . Other ? ... ... ... ... ... . I like object model NetSqlAzMan . But I need defend decision use project manager ( ) developers . The object model seems '' right '' ( think [ goldilocks ] ( http : //www.pmcgee.net/prodesign/goldilocks_large.jpg ) middle bed ) far I desire security . I NOT want role based security . I want right ( task permission ) based security . ( See : < http : //lostechies.com/derickbailey/2011/05/24/dont-do-role-based- authorization-checks-do-activity-based-checks/ > < http : //granadacoder.wordpress.com/2010/12/01/rant-hard-coded-security-roles/ > ) And basically question came : `` What advantage using NetSqlAzMan instead ( Windows Authorization Manager ( AzMan ) ) ? '' And sub question `` Is Windows Authorization Manager ( AzMan ) dead ? '' . ( And something along lines Long Live NetSqlAzMan ! ) . ... ... ... ... ... ... My in-general requirements : Non Active-Directory users . ( Down road Active Directory and/or LDAP support would nice , requirement ) . Passwords stored plain text . Be able handle RIGHTS security checks . Group rights role . Assign roles users . ( But , code check right , role performing action . ) Allow ( occasion ) rights assigned users . With Deny override . ( Aka , single user stupid thing ( like `` Delete Employee '' ) right revoked . ) Roles Rights maintained multiple applications . So ideas welcome . But [ Windows Identity Foundation ] ( http : //blogs.msdn.com/b/card/archive/2009/10/20/how-geneva-helps- with-access-control-and-what-is-its-relationship-to-azman.aspx ) seems like little overkill . Thanks .","I finally found `` compare '' article last night . < http : //www.c-sharpcorner.com/uploadfile/a.ferendeles/netsqlazman12122006123316pm/netsqlazman.aspx > I going paste relevant portion ( ) . ( Just case website ceases exist future . Small chance , I know , I hate `` The answer '' links , hit link , dead one . ) From I tell . NetSqlAzMan provides ( table ) user-defined-function overload provide list users ( assigned roles/tasks ) . NetSqlAzMan provides `` Yeah '' mappings ( Grant ) , also Deny Grant-With- Delegate well . NetSqlAzMan Azman allows users ( groups ) role mappings . Only NetSqlAzMan allows users Task mappings . After looking samples ... object model NetSqlAzMan clean . ======================================================= > Ms Authorization Manager ( AzMan ) vs .NET Sql Authorization Manager ( NetSqlAzMan ) > > As pointed , analogous Microsoft product already exists called Authorization Manager ( AzMan ) ; AzMan present , default , Windows Server 2003 , Admin Pack setup , Windows XP . > > The important difference AzMan NetSqlAzMan first Role-based , , based belonging - Role concept operations container role , second Item-based ( prefer Operation-based ) , users users group group groups belong Roles execute Task and/or Operations ( Items ) . > > Here important features differences two products : > > Ms AzMan : > > > * It 's COM . > * It 's equipped MMC 2.0 ( COM ) console . > * Its storage XML file ADAM ( Active Directory Application Mode - e un LDAP ) . > * It 's role-based . > * It supports static/dynamic applicative groups , members/not-members . > * Structure based Roles - > Tasks - > Operations . ( Hierarchical Roles Tasks , none Operations ) . > * Authorizations added Roles . > * It n't implement `` delegate '' concept . > * It n't manage authorizations `` time '' . > * It n't trigger events . > * The type authorization `` Allow '' . > ( `` deny '' needs remove user/group Role ) . > * It supports Scripting / Biz rules . > * It supports Active Directory users/groups ADAM users . > > > NetSqlAzMan : > > > * It 's .NET 2.0 . > * It 's equipped MMC 3.0 ( .NET ) console . > * Its storage Sql Server database ( 2000/MSDE/2005/Express ) . > * It 's based Tdo - Typed Data Object technology . > * It 's Item-based . > * Structure based Roles - > Tasks - > Operations . ( hierarchical ones ) . > * Authorizations added Roles , Task Operations . > * It supports static/dynamic applicative groups , members/not-members . > * LDAP query testing directly console . > * It 's time-dependant . > * It 's delegate-compliant . > * It triggers events ( ENS ) . > * It supports 4 authorization types : > Allow delegation ( authorized authorized delegate ) . > Allow ( authorized ) . > Deny ( authorized ) . > Neutral ( neutral permission , depends higher level Item permission ) . > * Hierarchical authorizations . > * It supports Scripting / Biz rules ( compiled .NET - C # - VB - interpreted ) > * It supports Active Directory users/groups custom users defined SQL Server Database . > Here 's another gotcha . Azman sample code : < http : //channel9.msdn.com/forums/sandbox/252978-AzMan-in- the-Enterprise-Sample-Code > < http : //channel9.msdn.com/forums/sandbox/252973-Programming-AzMan-Sample-Code > using System ; using System.Security.Principal ; using System.Runtime.InteropServices ; using AZROLESLib ; namespace TreyResearch { public class AzManHelper : IDisposable { AzAuthorizationStore store ; IAzApplication app ; string appName ; public AzManHelper ( string connectionString , string appName ) { this.appName = appName ; try { // load initialize AzMan runtime store = new AzAuthorizationStore ( ) ; store.Initialize ( 0 , connectionString , null ) ; // drill application app = store.OpenApplication ( appName , null ) ; } catch ( COMException x ) { throw new AzManException ( `` Failed initizlize AzManHelper '' , x ) ; } catch ( System.IO.FileNotFoundException x ) { throw new AzManException ( string.Format ( `` Failed load AzMan policy { 0 } - make sure connection string correct . `` , connectionString ) , x ) ; } } public void Dispose ( ) { ( null == app ) return ; Marshal.ReleaseComObject ( app ) ; Marshal.ReleaseComObject ( store ) ; app = null ; store = null ; } public bool AccessCheck ( string audit , Operations op , WindowsIdentity clientIdentity ) { try { // first step create AzMan context client // looks security identifiers ( SIDs ) user's // access token maps onto AzMan roles , tasks , operations IAzClientContext ctx = app.InitializeClientContextFromToken ( ( ulong ) clientIdentity.Token.ToInt64 ( ) , null ) ; // next step see user authorized // requested operation . Note AccessCheck allows // check multiple operations desire object [ ] scopes = { `` '' } ; object [ ] operations = { ( int ) op } ; object [ ] results = ( object [ ] ) ctx.AccessCheck ( audit , scopes , operations , null , null , null , null , null ) ; int result = ( int ) results [ 0 ] ; return 0 == result ; } catch ( COMException x ) { throw new AzManException ( `` AccessCheck failed '' , x ) ; } } public bool AccessCheckWithArg ( string audit , Operations op , WindowsIdentity clientIdentity , string argName , object argValue ) { try { // first step create AzMan context client // looks security identifiers ( SIDs ) user's // access token maps onto AzMan roles , tasks , operations IAzClientContext ctx = app.InitializeClientContextFromToken ( ( ulong ) clientIdentity.Token.ToInt64 ( ) , null ) ; // next step see user authorized // requested operation . Note AccessCheck allows // check multiple operations desire object [ ] scopes = { `` '' } ; object [ ] operations = { ( int ) op } ; object [ ] argNames = { argName } ; object [ ] argValues = { argValue } ; object [ ] results = ( object [ ] ) ctx.AccessCheck ( audit , scopes , operations , argNames , argValues , null , null , null ) ; int result = ( int ) results [ 0 ] ; return 0 == result ; } catch ( COMException x ) { throw new AzManException ( `` AccessCheckWithArg failed '' , x ) ; } } // use update running app // change AzMan policy public void UpdateCache ( ) { try { store.UpdateCache ( null ) ; Marshal.ReleaseComObject ( app ) ; app = store.OpenApplication ( appName , null ) ; } catch ( COMException x ) { throw new AzManException ( `` UpdateCache failed '' , x ) ; } } } public class AzManException : Exception { public AzManException ( string message , Exception innerException ) : base ( message , innerException ) { } } } That Azman helper code . That ugly COM/Interopish stuff . : < Now check NetSqlAzMan code samples : < http : //netsqlazman.codeplex.com/wikipage ? title=Samples > /// < summary > /// Create Full Storage .NET code /// < /summary > private void CreateFullStorage ( ) { // USER MUST BE A MEMBER OF SQL DATABASE ROLE : NetSqlAzMan_Administrators //Sql Storage connection string string sqlConnectionString = `` data source= ( local ) ; initial catalog=NetSqlAzManStorage ; user id=netsqlazmanuser ; password=password '' ; //Create instance SqlAzManStorage class IAzManStorage storage = new SqlAzManStorage ( sqlConnectionString ) ; //Open Storage Connection storage.OpenConnection ( ) ; //Begin new Transaction storage.BeginTransaction ( AzManIsolationLevel.ReadUncommitted ) ; //Create new Store IAzManStore newStore = storage.CreateStore ( `` My Store '' , `` Store description '' ) ; //Create new Basic StoreGroup IAzManStoreGroup newStoreGroup = newStore.CreateStoreGroup ( SqlAzManSID.NewSqlAzManSid ( ) , `` My Store Group '' , `` Store Group Description '' , String.Empty , GroupType.Basic ) ; //Retrieve current user SID IAzManSid mySid = new SqlAzManSID ( WindowsIdentity.GetCurrent ( ) .User ) ; //Add sid `` My Store Group '' IAzManStoreGroupMember storeGroupMember = newStoreGroup.CreateStoreGroupMember ( mySid , WhereDefined.Local , true ) ; //Create new Application IAzManApplication newApp = newStore.CreateApplication ( `` New Application '' , `` Application description '' ) ; //Create new Role IAzManItem newRole = newApp.CreateItem ( `` New Role '' , `` Role description '' , ItemType.Role ) ; //Create new Task IAzManItem newTask = newApp.CreateItem ( `` New Task '' , `` Task description '' , ItemType.Task ) ; //Create new Operation IAzManItem newOp = newApp.CreateItem ( `` New Operation '' , `` Operation description '' , ItemType.Operation ) ; //Add `` New Operation '' sid `` New Task '' newTask.AddMember ( newOp ) ; //Add `` New Task '' sid `` New Role '' newRole.AddMember ( newTask ) ; //Create authorization `` New Role '' IAzManAuthorization auth = newRole.CreateAuthorization ( mySid , WhereDefined.Local , mySid , WhereDefined.Local , AuthorizationType.AllowWithDelegation , null , null ) ; //Create custom attribute IAzManAttribute < IAzManAuthorization > attr = auth.CreateAttribute ( `` New Key '' , `` New Value '' ) ; //Create authorization DB User `` Andrea '' `` New Role '' IAzManAuthorization auth2 = newRole.CreateAuthorization ( mySid , WhereDefined.Local , storage.GetDBUser ( `` Andrea '' ) .CustomSid , WhereDefined.Local , AuthorizationType.AllowWithDelegation , null , null ) ; //Commit transaction storage.CommitTransaction ( ) ; //Close connection storage.CloseConnection ( ) ; } That tells story ."
Stackoverflow,"I 've searched google thoroughly definitive solution set steps resolve issue , n't seem many high quality results , I n't found question stack overflow . We 're trying set MySQL replication using one slave . The slave appears replicating fine , following error occurs : > Could parse relay log event entry . The possible reasons : master 's binary log corrupted ( check running 'mysqlbinlog' binary log ) , slave 's relay log corrupted ( check running 'mysqlbinlog ' relay log ) , network problem , bug master 's slave 's MySQL code . If want check master 's binary log slave 's relay log , able know names issuing 'SHOW SLAVE STATUS ' slave . In order benefit large number people inevitably stumble upon question search , would helpful someone responds provided overview could going wrong steps take resolve issue , I also provide details related particular situation hopes someone help solve . * * * The dump imported slave get started created using following command master : mysqldump -- opt -- allow-keywords -q -uroot -ppassword dbname > E : \Backups\dbname.sql The script performs backup also logs master 's current binary log position . We took following steps start replication slave : 1 . STOP SLAVE ; 2 . DROP DATABASE dbname ; 3 . SOURCE dbname.sql ; ( ... waited hours 10gb dump import ) 4 . RESET SLAVE ; 5 . CHANGE MASTER TO MASTER_HOST= ' [ masterhostname ] ' , MASTER_USER= ' [ slaveusername ] ' , MASTER_PASSWORD= ' [ slaveuserpassword ] ' , MASTER_PORT= [ port ] , MASTER_LOG_FILE= ' [ masterlogfile ] ' , MASTER_LOG_POS= [ masterlogposition ] ; 6 . START SLAVE ; After day replication working fine , failed 3:43 AM . The first thing appeared MySQL 's error log error . Then another generic error appeared timestamp : Error running query , slave SQL thread aborted . Fix problem , restart slave SQL thread `` SLAVE START '' . We stopped log ' [ masterlogfile ] ' position [ masterlogpos ] For logging information , I set batch script run `` SHOW SLAVE STATUS '' `` SHOW FULL PROCESSLIST '' every hour . Here results failure : -- Monitoring : 3:00:00.15 Slave Status : *************************** 1. row *************************** Slave_IO_State : Waiting master send event Master_Host : 192.168.xxx.xxx Master_User : slave_user Master_Port : xxxx Connect_Retry : 60 Master_Log_File : mysql-bin.000xxx Read_Master_Log_Pos : 316611912 Relay_Log_File : dbname-relay-bin.00000x Relay_Log_Pos : 404287513 Relay_Master_Log_File : mysql-bin.000xxx Slave_IO_Running : Yes Slave_SQL_Running : Yes Replicate_Do_DB : dbname Replicate_Ignore_DB : Replicate_Do_Table : Replicate_Ignore_Table : Replicate_Wild_Do_Table : Replicate_Wild_Ignore_Table : Last_Errno : 0 Last_Error : Skip_Counter : 0 Exec_Master_Log_Pos : 316611912 Relay_Log_Space : 404287513 Until_Condition : None Until_Log_File : Until_Log_Pos : 0 Master_SSL_Allowed : No Master_SSL_CA_File : Master_SSL_CA_Path : Master_SSL_Cert : Master_SSL_Cipher : Master_SSL_Key : Seconds_Behind_Master : 0 *************************** 1. row *************************** Id : 98 User : system user Host : db : NULL Command : Connect Time : 60547 State : Waiting master send event Info : NULL *************************** 2. row *************************** Id : 99 User : system user Host : db : NULL Command : Connect Time : 5 State : Has read relay log ; waiting slave I/O thread update Info : NULL *************************** 3. row *************************** Id : 119 User : root Host : localhost : xxxx db : NULL Command : Query Time : 0 State : NULL Info : SHOW FULL PROCESSLIST -- Monitoring : 4:00:02.71 Slave Status : *************************** 1. row *************************** Slave_IO_State : Waiting master send event Master_Host : 192.168.xxx.xxx Master_User : slave_user Master_Port : xxxx Connect_Retry : 60 Master_Log_File : mysql-bin.000xxx Read_Master_Log_Pos : 324365637 Relay_Log_File : dbname-relay-bin.00000x Relay_Log_Pos : 410327741 Relay_Master_Log_File : mysql-bin.000xxx Slave_IO_Running : Yes Slave_SQL_Running : No Replicate_Do_DB : dbname Replicate_Ignore_DB : Replicate_Do_Table : Replicate_Ignore_Table : Replicate_Wild_Do_Table : Replicate_Wild_Ignore_Table : Last_Errno : 0 Last_Error : Could parse relay log event entry . The possible reasons : master 's binary log corrupted ( check running 'mysqlbinlog ' binary log ) , slave 's relay log corrupted ( check running 'mysqlbinlog ' relay log ) , network problem , bug master 's slave 's MySQL code . If want check master 's binary log slave 's relay log , able know names issuing 'SHOW SLAVE STATUS ' slave . Skip_Counter : 0 Exec_Master_Log_Pos : 322652140 Relay_Log_Space : 412041238 Until_Condition : None Until_Log_File : Until_Log_Pos : 0 Master_SSL_Allowed : No Master_SSL_CA_File : Master_SSL_CA_Path : Master_SSL_Cert : Master_SSL_Cipher : Master_SSL_Key : Seconds_Behind_Master : NULL *************************** 1. row *************************** Id : 98 User : system user Host : db : NULL Command : Connect Time : 64149 State : Waiting master send event Info : NULL *************************** 2. row *************************** Id : 122 User : root Host : localhost:3029 db : NULL Command : Query Time : 0 State : NULL Info : SHOW FULL PROCESSLIST I tried following instructions error ran mysqlbinlog slave 's relay log start_position thousands statements , stop_position thousands statements point failure , redirected output text file . I see corruption errors command line log file . This log file said around point failure : ... # 410327570 # 120816 3:43:26 server id 1 log_pos 322651969 Intvar SET INSERT_ID=3842697 ; # 410327598 # 120816 3:43:26 server id 1 log_pos 322651997 Query thread_id=762340 exec_time=0 error_code=0 SET TIMESTAMP=1345113806 insert LOGTABLENAME ( UpdateDate , Description ) values ( ( ) , `` Invalid floating point operation '' ) ; # 410327741 # 120816 3:44:26 server id 1 log_pos 322754486 Intvar SET INSERT_ID=3842701 ; # 410327769 # 120816 3:43:26 server id 1 log_pos 322754514 Query thread_id=762340 exec_time=0 error_code=0 SET TIMESTAMP=1345113866 ; insert LOGTABLENAME ( UpdateDate , Description ) values ( ( ) , `` Invalid floating point operation '' ) ; # 410327912 ... Interesting 's logging Invalid floating point operation point , I 'm sure could cause replication break position . I ran mysqlbinlog master 's binary log found SHOW SLAVE STATUS , see errors command line ( get chance open 100mb log file generated since I n't want bog production server ) . So right I 'm loss else try . I 'm basically looking insights might going wrong suggestions steps take next . Thanks !","I 'm sure root cause may . But recover situation , 'd want instruct MySQL clear relay-bin-logs beyond following point * Relay_Master_Log_File : mysql-bin.000xxx * Exec_Master_Log_Pos : 322652140 following : ` STOP SLAVE ; CHANGE MASTER TO MASTER_LOG_FILE = 'mysql-bin.000xxx ' , MASTER_LOG_POS = 322652140 ; START SLAVE ; ` **NOTE : ** To readers , confused Relay_Master_Log_File , NOT Read_Master_Log_Pos . And confuse Exec_Master_Log_Pos Read_Master_Log_Pos . The Read_* read-ahead strategy MySQL download replication bin logs master ahead actual implementation replication executed locally ."
Stackoverflow,I large XML note many nodes . way I select single node contents larger XML ? using sql 2005,"You use [ query ( ) Method ] ( http : //msdn.microsoft.com/en- us/library/ms191474.aspx ) want get part XML . declare @ XML xml set @ XML = ' < root > < row1 > < value > 1 < /value > < /row1 > < row2 > < value > 2 < /value > < /row2 > < /root > ' select @ XML.query ( '/root/row2 ' ) Result : < row2 > < value > 2 < /value > < /row2 > If want value specific node use [ value ( ) Method ] ( http : //msdn.microsoft.com/en-us/library/ms178030.aspx ) . select @ XML.value ( ' ( /root/row2/value ) [ 1 ] ' , 'int ' ) Result : 2 **Update : ** If want shred XML multiple rows use [ nodes ( ) Method ] ( http : //msdn.microsoft.com/en-us/library/ms188282.aspx ) . To get values : declare @ XML xml set @ XML = ' < root > < row > < value > 1 < /value > < /row > < row > < value > 2 < /value > < /row > < /root > ' select T.N.value ( 'value [ 1 ] ' , 'int ' ) @ XML.nodes ( '/root/row ' ) T ( N ) Result : ( No column name ) 1 2 To get entire XML : select T.N.query ( ' . ' ) @ XML.nodes ( '/root/row ' ) T ( N ) Result : ( No column name ) < row > < value > 1 < /value > < /row > < row > < value > 2 < /value > < /row >"
Stackoverflow,"When I compare 2 databases Red Gate 's SQL compare getting error . `` ExecuteReader : CommandText property initialized '' I using SQL server 2012 express Red Gate 's SQL Compare 8 Is solution ? Thanks ,",SQL Compare 8 ( current 10.4 ) work SQL Server 2012 [ http : //www.red- gate.com/messageboard/viewtopic.php ? t=15296 & highlight=commandtext ] ( http : //www.red- gate.com/messageboard/viewtopic.php ? t=15296 & highlight=commandtext )
Stackoverflow,"I get connection failure I try connect postgres server Azure app/client , SSL enabled . _Unable connect server : FATAL : SSL connection required . Please specify SSL options retry._ Is strong requirement ? Is way I circumvent requirement ?","As noted error text , required follow ` < username @ hostname > ` format trying connect postgresql server , whether psql client using pgadmin . Using ` < username @ hostname > ` format instead ` < username > ` get rid error . Read quick-start documents [ Azure portal ] ( https : //docs.microsoft.com/en-us/azure/postgresql/quickstart-create- server-database-portal ) [ CLI ] ( https : //docs.microsoft.com/en- us/azure/postgresql/quickstart-create-server-database-azure-cli ) understand create configure postgres server ."
Stackoverflow,I wondering 's best practice moving documentDB Azure Data Lake Storage . Should I create file document collection move entire documentDB ? Also I n't find much information I access documentDB using U-SQL ? Input would appreciated .,"You think U-SQL Microsoft 's version Spark SQL , write SQL Server styled SQL extend User-Defined Functions C # . While Spark write Semi MySQL styled SQL extend either Scala Python . If familiar Scala Python choosing HDInsight might best choice . Spark comes GraphX MLLib moment analogues Data Lake Analytics . Also need something works outside Azure SparkSQL option . Another important dimension think pricing . Data Lake Analytics costs money query executing , HDInsight costs money long cluster running . Depending size data complexity queries Data Lake Analytics cheaper n't charged 's provisioning ."
Stackoverflow,I looking using [ Sails ] ( http : //sailsjs.com ) app developing . I 'm using sails-postgresql adapter uses waterline orm . I existing database I want connect . If I create model using ` generate something ` model I attributes : { title : { type : 'String ' } } If I browse localhost/something orm deletes columns ` something ` table except title . Is way stop ? This app delete columns database . Thanks !,"I author Sails-Postgresql . Sails ORM called Waterline uses managing data . The default setting assumes would want ` auto-migrate ` database match model attributes . Because Postgresql SQL database Sails-Postgresql adapter setting called syncable defaults true . This would false NoSQL database like redis . This easy turn want manage database columns . You add ` migrate : safe ` model wo n't try update database schema start Sails . module.exports = { adapter : 'postgresql ' , migrate : 'safe ' , attributes : { title : { type : 'string ' } } } ; Sails n't anything like migrations Rails . It uses auto-migrations attempt remove development process leaves updating production schema ."
Stackoverflow,"There 's another question similar , n't seem answer _my_ question . My question : I getting back error ` ERROR 1222 ( 21000 ) : The used SELECT statements different number columns ` following SQL SELECT * FROM friends LEFT JOIN users AS u1 ON users.uid = friends.fid1 LEFT JOIN users AS u2 ON users.uid = friends.fid2 WHERE ( friends.fid1 = 1 ) AND ( friends.fid2 > 1 ) UNION SELECT fid2 FROM friends WHERE ( friends.fid2 = 1 ) AND ( friends.fid1 < 1 ) ORDER BY RAND ( ) LIMIT 6 ; Here 's ` users ` : + -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | Field | Type | Null | Key | Default | Extra | + -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | uid | int ( 11 ) | NO | PRI | NULL | auto_increment | | first_name | varchar ( 50 ) | NO | | NULL | | | last_name | varchar ( 50 ) | NO | | NULL | | | email | varchar ( 128 ) | NO | UNI | NULL | | | mid | varchar ( 40 ) | NO | | NULL | | | active | enum ( ' N ' , ' Y ' ) | NO | | NULL | | | password | varchar ( 64 ) | NO | | NULL | | | sex | enum ( 'M ' , ' F ' ) | YES | | NULL | | | created | datetime | YES | | NULL | | | last_login | datetime | YES | | NULL | | | pro | enum ( ' N ' , ' Y ' ) | NO | | NULL | | + -- -- -- -- -- -- + -- -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + Here 's ` friends ` : + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | Field | Type | Null | Key | Default | Extra | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | friendship_id | int ( 11 ) | NO | MUL | NULL | auto_increment | | fid1 | int ( 11 ) | NO | PRI | NULL | | | fid2 | int ( 11 ) | NO | PRI | NULL | | | status | enum ( 'pending ' , 'accepted ' , 'ignored ' ) | NO | | NULL | | + -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + If want give feedback anything crazy see going , well , please feel free . I 'll take lumps .","UNIONs ( ` UNION ` ` UNION ALL ` ) require queries UNION'd : 1 . The number columns SELECT clause 2 . The column data type match position Your query : SELECT f.* , u1 . * , u2 . * ... UNION SELECT fid2 FROM friends The easiest re-write I : SELECT f.* , u . * FROM FRIENDS AS f JOIN USERS AS u ON u.uid = f.fid2 WHERE f.fid1 = 1 AND f.fid2 > 1 UNION SELECT f.* , u . * FROM FRIENDS AS f JOIN USERS AS u ON u.uid = f.fid1 WHERE f.fid2 = 1 AND f.fid1 < 1 ORDER BY RAND ( ) LIMIT 6 ; You 've LEFT JOIN 'd ` USERS ` table twice , n't appear using information ."
Stackoverflow,I upgrading mysql-5.5 docker container database mysql-5.6 docker container . I able fix problems . Finally server running 5.6 . But run mysql_upgrade getting following error . ERROR : root @ 17aa74cbc5e2 # mysql_upgrade -uroot -password Warning : Using password command line interface insecure . Looking 'mysql ' : mysql Looking 'mysqlcheck ' : mysqlcheck Running 'mysqlcheck ' connection arguments : ' -- port=3306 ' ' -- socket=/var/run/mysqld/mysqld.sock ' Warning : Using password command line interface insecure . Running 'mysqlcheck ' connection arguments : ' -- port=3306 ' ' -- socket=/var/run/mysqld/mysqld.sock ' Warning : Using password command line interface insecure . mysql.columns_priv OK mysql.db OK mysql.event OK mysql.func OK mysql.general_log OK mysql.help_category OK mysql.help_keyword OK mysql.help_relation OK mysql.help_topic OK mysql.innodb_index_stats Error : Table 'mysql.innodb_index_stats ' n't exist status : Operation failed mysql.innodb_table_stats Error : Table 'mysql.innodb_table_stats ' n't exist status : Operation failed mysql.ndb_binlog_index OK mysql.plugin OK mysql.proc OK mysql.procs_priv OK mysql.proxies_priv OK mysql.servers OK mysql.slave_master_info Error : Table 'mysql.slave_master_info ' n't exist status : Operation failed mysql.slave_relay_log_info Error : Table 'mysql.slave_relay_log_info ' n't exist status : Operation failed mysql.slave_worker_info Error : Table 'mysql.slave_worker_info ' n't exist status : Operation failed mysql.slow_log OK mysql.tables_priv OK mysql.time_zone OK mysql.time_zone_leap_second OK mysql.time_zone_name OK mysql.time_zone_transition OK mysql.time_zone_transition_type OK mysql.user OK Repairing tables mysql.innodb_index_stats Error : Table 'mysql.innodb_index_stats ' n't exist status : Operation failed mysql.innodb_table_stats Error : Table 'mysql.innodb_table_stats ' n't exist status : Operation failed mysql.slave_master_info Error : Table 'mysql.slave_master_info ' n't exist status : Operation failed mysql.slave_relay_log_info Error : Table 'mysql.slave_relay_log_info ' n't exist status : Operation failed mysql.slave_worker_info Error : Table 'mysql.slave_worker_info ' n't exist status : Operation failed Running 'mysql_fix_privilege_tables ' ... Warning : Using password command line interface insecure . ERROR 1146 ( 42S02 ) line 62 : Table 'mysql.innodb_table_stats ' n't exist ERROR 1243 ( HY000 ) line 63 : Unknown prepared statement handler ( stmt ) given EXECUTE ERROR 1243 ( HY000 ) line 64 : Unknown prepared statement handler ( stmt ) given DEALLOCATE PREPARE ERROR 1146 ( 42S02 ) line 66 : Table 'mysql.innodb_index_stats ' n't exist ERROR 1243 ( HY000 ) line 67 : Unknown prepared statement handler ( stmt ) given EXECUTE ERROR 1243 ( HY000 ) line 68 : Unknown prepared statement handler ( stmt ) given DEALLOCATE PREPARE ERROR 1146 ( 42S02 ) line 81 : Table 'mysql.slave_relay_log_info ' n't exist ERROR 1243 ( HY000 ) line 82 : Unknown prepared statement handler ( stmt ) given EXECUTE ERROR 1243 ( HY000 ) line 83 : Unknown prepared statement handler ( stmt ) given DEALLOCATE PREPARE ERROR 1146 ( 42S02 ) line 110 : Table 'mysql.slave_master_info ' n't exist ERROR 1243 ( HY000 ) line 111 : Unknown prepared statement handler ( stmt ) given EXECUTE ERROR 1243 ( HY000 ) line 112 : Unknown prepared statement handler ( stmt ) given DEALLOCATE PREPARE ERROR 1146 ( 42S02 ) line 128 : Table 'mysql.slave_worker_info ' n't exist ERROR 1243 ( HY000 ) line 129 : Unknown prepared statement handler ( stmt ) given EXECUTE ERROR 1243 ( HY000 ) line 130 : Unknown prepared statement handler ( stmt ) given DEALLOCATE PREPARE ERROR 1146 ( 42S02 ) line 1896 : Table 'mysql.slave_master_info ' n't exist ERROR 1146 ( 42S02 ) line 1897 : Table 'mysql.slave_master_info ' n't exist ERROR 1146 ( 42S02 ) line 1898 : Table 'mysql.slave_master_info ' n't exist ERROR 1146 ( 42S02 ) line 1899 : Table 'mysql.slave_worker_info ' n't exist ERROR 1146 ( 42S02 ) line 1900 : Table 'mysql.slave_relay_log_info ' n't exist ERROR 1146 ( 42S02 ) line 1904 : Table 'mysql.innodb_table_stats ' n't exist ERROR 1146 ( 42S02 ) line 1908 : Table 'mysql.innodb_index_stats ' n't exist FATAL ERROR : Upgrade failed,"This known bug MySQL 5.6 , [ documented ] ( https : //bugs.mysql.com/bug.php ? id=67179 ) . According replies bug report , manually create missing tables . The structure missing tables provided attachment [ ] ( https : //bugs.mysql.com/file.php ? id=19725 & bug_id=67179 ) . Steps follow : 1 ) Drop tables Mysql : innodb_index_stats innodb_table_stats slave_master_info slave_relay_log_info slave_worker_info 2 ) Delete ` *.frm ` ` *.ibd ` files 5 tables . 3 ) Create tables running following queries : CREATE TABLE ` innodb_index_stats ` ( ` database_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` table_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` index_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` last_update ` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP , ` stat_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` stat_value ` bigint ( 20 ) unsigned NOT NULL , ` sample_size ` bigint ( 20 ) unsigned DEFAULT NULL , ` stat_description ` varchar ( 1024 ) COLLATE utf8_bin NOT NULL , PRIMARY KEY ( ` database_name ` , ` table_name ` , ` index_name ` , ` stat_name ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0 ; CREATE TABLE ` innodb_table_stats ` ( ` database_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` table_name ` varchar ( 64 ) COLLATE utf8_bin NOT NULL , ` last_update ` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP , ` n_rows ` bigint ( 20 ) unsigned NOT NULL , ` clustered_index_size ` bigint ( 20 ) unsigned NOT NULL , ` sum_of_other_index_sizes ` bigint ( 20 ) unsigned NOT NULL , PRIMARY KEY ( ` database_name ` , ` table_name ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0 ; CREATE TABLE ` slave_master_info ` ( ` Number_of_lines ` int ( 10 ) unsigned NOT NULL COMMENT 'Number lines file . ' , ` Master_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name master binary log currently read master . ' , ` Master_log_pos ` bigint ( 20 ) unsigned NOT NULL COMMENT 'The master log position last read event . ' , ` Host ` char ( 64 ) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT `` COMMENT 'The host name master . ' , ` User_name ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used connect master . ' , ` User_password ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used connect master . ' , ` Port ` int ( 10 ) unsigned NOT NULL COMMENT 'The network port used connect master . ' , ` Connect_retry ` int ( 10 ) unsigned NOT NULL COMMENT 'The period ( seconds ) slave wait trying reconnect master . ' , ` Enabled_ssl ` tinyint ( 1 ) NOT NULL COMMENT 'Indicates whether server supports SSL connections . ' , ` Ssl_ca ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used Certificate Authority ( CA ) certificate . ' , ` Ssl_capath ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path Certificate Authority ( CA ) certificates . ' , ` Ssl_cert ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name SSL certificate file . ' , ` Ssl_cipher ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name cipher use SSL connection . ' , ` Ssl_key ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name SSL key file . ' , ` Ssl_verify_server_cert ` tinyint ( 1 ) NOT NULL COMMENT 'Whether verify server certificate . ' , ` Heartbeat ` float NOT NULL , ` Bind ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays interface employed connecting MySQL server ' , ` Ignored_server_ids ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number server IDs ignored , followed actual server IDs ' , ` Uuid ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid . ' , ` Retry_count ` bigint ( 20 ) unsigned NOT NULL COMMENT 'Number reconnect attempts , master , giving . ' , ` Ssl_crl ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used Certificate Revocation List ( CRL ) ' , ` Ssl_crlpath ` text CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used Certificate Revocation List ( CRL ) files ' , ` Enabled_auto_position ` tinyint ( 1 ) NOT NULL COMMENT 'Indicates whether GTIDs used retrieve events master . ' , PRIMARY KEY ( ` Host ` , ` Port ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Master Information ' ; CREATE TABLE ` slave_relay_log_info ` ( ` Number_of_lines ` int ( 10 ) unsigned NOT NULL COMMENT 'Number lines file rows table . Used version table definitions . ' , ` Relay_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name current relay log file . ' , ` Relay_log_pos ` bigint ( 20 ) unsigned NOT NULL COMMENT 'The relay log position last executed event . ' , ` Master_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name master binary log file events relay log file read . ' , ` Master_log_pos ` bigint ( 20 ) unsigned NOT NULL COMMENT 'The master log position last executed event . ' , ` Sql_delay ` int ( 11 ) NOT NULL COMMENT 'The number seconds slave must lag behind master . ' , ` Number_of_workers ` int ( 10 ) unsigned NOT NULL , ` Id ` int ( 10 ) unsigned NOT NULL COMMENT 'Internal Id uniquely identifies record . ' , PRIMARY KEY ( ` Id ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Relay Log Information ' ; CREATE TABLE ` slave_worker_info ` ( ` Id ` int ( 10 ) unsigned NOT NULL , ` Relay_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL , ` Relay_log_pos ` bigint ( 20 ) unsigned NOT NULL , ` Master_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL , ` Master_log_pos ` bigint ( 20 ) unsigned NOT NULL , ` Checkpoint_relay_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL , ` Checkpoint_relay_log_pos ` bigint ( 20 ) unsigned NOT NULL , ` Checkpoint_master_log_name ` text CHARACTER SET utf8 COLLATE utf8_bin NOT NULL , ` Checkpoint_master_log_pos ` bigint ( 20 ) unsigned NOT NULL , ` Checkpoint_seqno ` int ( 10 ) unsigned NOT NULL , ` Checkpoint_group_size ` int ( 10 ) unsigned NOT NULL , ` Checkpoint_group_bitmap ` blob NOT NULL , PRIMARY KEY ( ` Id ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT='Worker Information ' ; 4 ) Restart MySQL server ."
Stackoverflow,I relational model Oracle SQL Developer Data Modeler tables relationships . Is possible export relational model image file ?,"It possible , looks like intuitive . 1 . Zoom relational model expected resolution ( least readable ) . 2 . Go **File** , **Data Modeler** , **Print Diagram** selected desired format . This generate image file relational model current zoom level resolution ."
Stackoverflow,"I need **make audit module** Java Web App . I use EclipseLink , Hibernate ( ca n't use Envers ) . I searched lot way get SQL JPQL JPA executing , I could log something like : System.out.println ( `` User `` + user + `` `` + ip_address + `` executed `` + jpa_statement + `` `` + new Date ( ) ) ; Actually , I 'll save info history database 's table . So I easily retrieve info time I want . That 's `` **SHOW SQL** `` parameters enough . I really need SQL string , I manipulate source code . I found JPA spec **EntityListener** feature thought perfect place put logging code . For example , postUpdate method could log time object updated . But problem I ca n't SQL executed . Here example I mean : public class AuditListener { @ PostUpdate public void postUpdate ( Object object ) { User user = ( User ) FacesContext.getCurrentInstance ( ) .getExternalContext ( ) .getSessionMap ( ) .get ( `` user '' ) ; String ip_address = ( User ) FacesContext.getCurrentInstance ( ) .getExternalContext ( ) .getSessionMap ( ) .get ( `` ip_address '' ) ; String jpa_statement = object.getSQL ( ) ; System.out.println ( `` User `` + user + `` `` + ip_address + `` executed `` + jpa_statement + `` `` + new Date ( ) ) ; } } **But `` object.getSQL ( ) '' n't exists . So I get SQL statement ? ** If anyone could point right direction , I would appreciate !","EclipseLink full support history tracking . See , < http : //wiki.eclipse.org/EclipseLink/Examples/JPA/History > JPA events contain changed , object . EclipseLink also supports DesriptorEvents ( see DescriptorEventListener ) also define postUpdate include ObjectChangeSet describe changes , also UpdateObjectQuery contains SQL DatabaseRecord ."
Stackoverflow,"I would like record id user session/transaction , using ` SET ` , I could able access later trigger function , using ` current_setting ` . Basically , I 'm trying option n2 [ similar ticket posted previously ] ( https : //stackoverflow.com/questions/13172524/passing-user- id-to-postgresql-triggers ) , difference I 'm using PG 10.1 . I 've trying 3 approaches setting variable : * ` SET local myvars.user_id = 4 ` , thereby setting locally transaction ; * ` SET myvars.user_id = 4 ` , thereby setting session ; * ` SELECT set_config ( 'myvars.user_id ' , ' 4 ' , false ) ` , depending last argument , shortcut previous 2 options . None usable trigger , receives ` NULL ` getting variable ` current_setting ` . Here script I 've devised troubleshoot ( easily used postgres docker image ) : database= $ POSTGRES_DB user= $ POSTGRES_USER [ -z `` $ user '' ] & & user= '' postgres '' psql -v ON_ERROR_STOP=1 -- username `` $ user '' $ database < < -EOSQL DROP TRIGGER IF EXISTS add_transition1 ON houses ; CREATE TABLE IF NOT EXISTS houses ( id SERIAL NOT NULL , name VARCHAR ( 80 ) , created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT ( ) , PRIMARY KEY ( id ) ) ; CREATE TABLE IF NOT EXISTS transitions1 ( id SERIAL NOT NULL , house_id INTEGER , user_id INTEGER , created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT ( ) , PRIMARY KEY ( id ) , FOREIGN KEY ( house_id ) REFERENCES houses ( id ) ON DELETE CASCADE ) ; CREATE OR REPLACE FUNCTION add_transition1 ( ) RETURNS TRIGGER AS \ $ \ $ DECLARE user_id integer ; BEGIN user_id : = current_setting ( 'myvars.user_id ' ) : :integer || NULL ; INSERT INTO transitions1 ( user_id , house_id ) VALUES ( user_id , NEW.id ) ; RETURN NULL ; END ; \ $ \ $ LANGUAGE plpgsql ; CREATE TRIGGER add_transition1 AFTER INSERT OR UPDATE ON houses FOR EACH ROW EXECUTE PROCEDURE add_transition1 ( ) ; BEGIN ; % 1 % SELECT current_setting ( 'myvars.user_id ' ) ; % 2 % SELECT set_config ( 'myvars.user_id ' , '55 ' , false ) ; % 3 % SELECT current_setting ( 'myvars.user_id ' ) ; INSERT INTO houses ( name ) VALUES ( 'HOUSE PARTY ' ) RETURNING houses.id ; SELECT * houses ; SELECT * transitions1 ; COMMIT ; DROP TRIGGER IF EXISTS add_transition1 ON houses ; DROP FUNCTION IF EXISTS add_transition1 ; DROP TABLE transitions1 ; DROP TABLE houses ; EOSQL The conclusion I came function triggered different transaction different ( ? ) session . Is something one configure , happens within context ?","It clear trying concat ` NULL ` ` user_id ` obviously cause problem . Get rid : CREATE OR REPLACE FUNCTION add_transition1 ( ) RETURNS TRIGGER AS $ $ DECLARE user_id integer ; BEGIN user_id : = current_setting ( 'myvars.user_id ' ) : :integer ; INSERT INTO transitions1 ( user_id , house_id ) VALUES ( user_id , NEW.id ) ; RETURN NULL ; END ; $ $ LANGUAGE plpgsql ; Note SELECT 55 || NULL always gives ` NULL ` ."
Stackoverflow,"We 've got couple on-premises dbs 're seeing migrate SQL Azure . Some dbs couple user defined functions written C # assembly ( SAFE ) . After running search , I 've found couple posts contradict . Some say v12 supports CLR code . Others say n't . So , questions : * Does V12 supports embedding clr assemblies ? * How export generation script azure assembly ? Whenever set export option azure end getting error saying clr user defined functions supported azure . Thanks guys ! Luis","CLR Functions supported Azure : Check : [ Azure SQL Database Transact-SQL differences ] ( https : //azure.microsoft.com/en- gb/documentation/articles/sql-database-transact-sql-information/ ) Under unsupported features mentions `` .NET Framework CLR integration SQL Server '' I believe may confusion whether n't support used one version , removed support . Here link detailing fact supported , got pulled , apparently due security issue : [ Breaking News , Literally : SQL CLR Support Removed Azure SQL DB ] ( https : //www.brentozar.com/archive/2016/04/breaking-news-literally-sql-clr- support-removed-azure-sql-db/ )"
Stackoverflow,We Old 5.1 Mysql server running server 2003 . Recently move newer environment Mysql 5.6 server 2008 . Now new server keep getting errors inserting special chars like ' Ã ' . Now I checked source encoding UTF-8 . But old Mysql server configured latin1 ( Server / tables / colonms ) collation latin_swedish_ci receive errors old environment . Now I done testing since live new environment . I tried setting tables tables / colonms well latin1 . In cases I keep getting errors . What I noticed old server servers default char-set latin1 new server utf-8 . Could problem ? I find strange source utf-8 . Is maybe option handle could turned old environment ? I 'm sure something like exists . I compare settings within mysql admin tool apart default char-set looks . **EDIT : ** > SHOW VARIABLES LIKE 'char % ' ; **Old server : ** + -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | Variable_name | Value | + -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | character_set_client | utf8 | * | character_set_connection | utf8 | * | character_set_database | latin1 | | character_set_filesystem | binary | | character_set_results | utf8 | * | character_set_server | latin1 | | character_set_system | utf8 | **New Server : ** + -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | Variable_name | Value | + -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | character_set_client | utf8mb4 | * | character_set_connection | utf8mb4 | * | character_set_database | utf8 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | * | character_set_server | utf8 | | character_set_system | utf8 | As far I understand article MySQL site utf8mb4 super-set utf8 create problem encoding I think since basically identical encoding right ?,"The key question whether ODBC client executable -- thing that's going load driver library use data -- 32-bit 64-bit . 64-bit Windows ( XP , Vista , 7 , 8 , Server 2003 , Server 2008 , variants date ) supports 32-bit 64-bit binary executables/libraries . 32-bit executables ( usually found ` Program Files ( x86 ) ` ) use 32-bit drivers ; 64-bit executables ( usually found ` Program Files ` ) use 64-bit drivers . Once 've figured part , install matching 32-bit 64-bit driver MySQL , configure right ODBC Administrator . The 32-bit ODBC Administrator counterintuitively found ` C : \Windows\SysWow64\odbcad32.exe ` , 64-bit ODBC Administrator likewise counterintuitively found ` C : \Windows\System32\odbcad32.exe ` . ( Yes , named ` odbcad32 ` directory names suggest bitness -- I 've said accurate . ) For , read [ article ] ( http : //wikis.openlinksw.com/UdaWikiWeb/Win32vs64OdbcAdmin ) posted [ employer ] ( http : //www.openlinksw.com/ ) 's website Note added fun , Microsoft 's ODBC driver manager ( MDAC ) bug [ shows 32-bit User DSNs 64-bit client executables ( including Administrator ) , shows 64-bit User DSNs 32-bit client executables ( , including Administrator ) ] ( http : //support.microsoft.com/kb/942976 ) \ -- even though mis-matches work together . For reason , I strongly recommend using _only_ System DSNs environment may mix 32-bit 64-bit executables/drivers/DSNs ."
Stackoverflow,"I using ` QSqlTableModel ` ` QTableView ` view SQLite database table . I would like table auto refresh every second ( 's going large table - couple hundred rows ) . And - like : QTimer *updateInterval = new QTimer ( ) ; updateInterval- > setInterval ( 1000 ) ; updateInterval- > start ( ) ; connect ( updateInterval , SIGNAL ( timeout ( ) ) , , SLOT ( update_table ( ) ) ) ; ... void MainWindow : :update_table ( ) { model- > select ( ) ; //QSqlTableModel* sqlTable- > reset ( ) ; //QTableView* } But removes selection I , selections last second . This annoying , another pane GUI depends selected . If nothing selected , resets explanation splash page . I tried somewhat hacky approach , gets selected row number , resets table , selects row . But n't work either , selected row move based additions table . I know classes ` dataChanged ( ) ` signal , would ideal . Do know **how I could table refresh reflect changes database** ( either command line usage , instances program ) **AND keep current selection ? ** I know I could get data current selection , reset search row reselect , seems like counter productive bad solution problem . EDIT : Current attempt solution : void MainWindow : :update_table ( ) { QList < QModelIndex > selection = sqlTable- > selectionModel ( ) - > selection ( ) .indexes ( ) ; QList < int > selectedIDs ; bool somethingSelected = true ; ( QList < QModelIndex > : :iterator = selection.begin ( ) ; ! = selection.end ( ) ; ++i ) { int col = i- > column ( ) ; QVariant data = i- > data ( Qt : :DisplayRole ) ; ( col == 0 ) { selectedIDs.append ( data.toInt ( ) ) ; } } ( selectedIDs.empty ( ) ) somethingSelected = false ; model- > select ( ) ; sqlTable- > reset ( ) ; ( somethingSelected ) { QList < int > selectedRows ; int rows = model- > rowCount ( QModelIndex ( ) ) ; ( int = 0 ; < rows ; ++i ) { sqlTable- > selectRow ( ) ; ( selectedIDs.contains ( sqlTable- > selectionModel ( ) - > selection ( ) .indexes ( ) .first ( ) .data ( Qt : :DisplayRole ) .toInt ( ) ) ) selectedRows.append ( ) ; } ( QList < int > : :iterator = selectedRows.begin ( ) ; ! = selectedRows.end ( ) ; ++i ) { sqlTable- > selectRow ( *i ) ; } } } Okay less works ...","The view draws background based ` Qt : :BackgroundRole ` role cell ` QBrush ` value returned ` QAbstractItemModel : :data ( index , role ) ` role . You subclass ` QSqlQueryModel ` redefine ` data ( ) ` return calculated color , Qt > 4.8 , use [ ` QIdentityProxyModel ` ] ( http : //qt- project.org/doc/qt-4.8/qidentityproxymodel.html ) : class MyModel : public QIdentityProxyModel { QColor calculateColorForRow ( int row ) const { ... } QVariant data ( const QModelIndex & index , int role ) { ( role == Qt : :BackgroundRole ) { int row = index.row ( ) ; QColor color = calculateColorForRow ( row ) ; return QBrush ( color ) ; } return QIdentityProxyModel : :data ( index , role ) ; } } ; And use model view , sql model set source ` QIdentityProxyModel : :setSourceModel ` . # OR You keep model unchanged modify background delegate set view ` QAbstractItemView : :setItemDelegate ` : class BackgroundColorDelegate : public QStyledItemDelegate { public : BackgroundColorDelegate ( QObject *parent = 0 ) : QStyledItemDelegate ( parent ) { } QColor calculateColorForRow ( int row ) const ; void initStyleOption ( QStyleOptionViewItem *option , const QModelIndex & index ) const { QStyledItemDelegate : :initStyleOption ( option , index ) ; QStyleOptionViewItemV4 *optionV4 = qstyleoption_cast < QStyleOptionViewItemV4* > ( option ) ; optionV4- > backgroundBrush = QBrush ( calculateColorForRow ( index.row ( ) ) ) ; } } ; As last method always obvious translate C++ code , equivalent python : def initStyleOption ( self , option , index ) : super ( BackgroundColorDelegate , self ) .initStyleOption ( option , index ) option.backgroundBrush = calculateColorForRow ( index.row ( ) )"
Stackoverflow,"I 'm trying store query result temporary table processing . create temporary table tmpTest ( FLOAT , b FLOAT , c FLOAT ) engine = memory ; insert tmpTest ( select , b , c someTable ... ) ; But reason insert takes minute , whereas subselect alone takes seconds . Why would take much longer write data temporary table instead printing SQL management tool's output ? ? ? **UPDATE** My Setup : MySQL 7.3.2 Cluster 8 Debian Linux ndb data nodes 1 SQL Node ( Windows Server 2012 ) The table I 'm running select ndb table . I tried find , execution plan would differ using 'insert into.. ' , look : ( sorry formatting , stackoverflow n't tables ) id select_type table type possible_keys key key_len ref rows Extra 1 PRIMARY < subquery3 > ALL \N \N \N \N \N \N 1 PRIMARY foo ref PRIMARY PRIMARY 3 < subquery3 > .fooId 9747434 Using 2 SUBQUERY someTable range PRIMARY PRIMARY 3 \N 136933000 Using pushed condition ; Using MRR ; Using temporary ; Using filesort 3 MATERIALIZED tmpBar ALL \N \N \N \N 1000 \N CREATE TABLE ... SELECT slow , . 47 seconds vs. 5 seconds without table insert/create .","Error 157 actually 'could connect storage engine ' fact MySQL fails report error correctly bug : < http : //bugs.mysql.com/bug.php ? id=44817 > The case described bug mentions get error try query table NDB cluster still . So I 'm guessing , I would conclude cluster started . Either missed starting one nodes , else something went wrong starting one nodes ."
Stackoverflow,"I 'm able connect SQL server 2008 R2 I use ` Provider=SQLOLEDB ` connection string . But I use ` Provider=SQLNCLI ` connection string I'm unable connect . > ADODB.Connection error '800a0e7a' > > Provider found . It may properly installed . > > /test.asp , line 7 Code written within ` test.asp ` < % Set cn = Server.CreateObject ( `` ADODB.Connection '' ) 'Does n't work cn.Open `` Provider=SQLNCLI ; Server=remoteServer\SQL2008R2 ; Database=DB ; UID=MyUser ; PWD=pa55word ; '' 'Works Perfectly 'cn.Open `` Provider=SQLOLEDB ; Server=remoteServer\SQL2008R2 ; Database=DB ; UID=MyUser ; PWD=pa55word ; '' cn.CommandTimeout = 900 cn.Close Response.write ( `` dfjslkfsl '' ) % > The SQL Server I 'm trying connect ( classic ASP Page within IIS 7 windows 7 ) located different server different network I'm connecting using VPN . I tested sql native client creating sql native client System DSN connection said Sql server 2008 R2 ( connected VPN ) ODBC datasource administrator . And got connected successfully . These snaps windows 7 system ! [ Appwiz.cpl snap ] ( https : //i.stack.imgur.com/HI3Ks.png ) ! [ IIS 7 features ] ( https : //i.stack.imgur.com/RgOfM.png ) ! [ enter image description ] ( https : //i.stack.imgur.com/r5crO.png ) * Windows 7 * IIS 7 * Classic ASP page ( .asp )",Try changing provider ` sqlncli10 ` : cn.Open `` Provider=SQLNCLI10 ; Server=remoteServer\SQL2008R2 ; Database=DB ; UID=MyUser ; PWD=pa55word ; '' Maybe name differet machine . : )
Stackoverflow,"Hi wondering perhaps someone could shed light error . The sql works fine locally get error remotely . SQL query : SELECT COUNT ( node.nid ) , node.nid AS nid , node_data_field_update_date.field_update_date_value AS node_data_field_update_date_field_update_date_value FROM node node LEFT JOIN content_type_update node_data_field_update_date ON node.vid = node_data_field_update_date.vid WHERE node.type IN ( 'update ' ) ORDER BY node_data_field_update_date_field_update_date_value DESC MySQL said : > # 1140 - Mixing GROUP columns ( MIN ( ) , MAX ( ) , COUNT ( ) , ... ) GROUP columns illegal GROUP BY clause `","When using aggregate function , ` COUNT ` , need include [ ` GROUP BY ` ] ( http : //dev.mysql.com/doc/refman/5.0/en/group-by-functions.html ) clause . SELECT type type , COUNT ( * ) AS total FROM page AS p WHERE p.parent_id = p.page_id GROUP BY type As far worked locally , live server ; [ MySql require complete listing non-aggregate columns ` GROUP BY ` clause default ] ( http : //dev.mysql.com/doc/refman/5.0/en/group-by-hidden-columns.html ) , live server probably [ ` ONLY_FULL_GROUP_BY ` ] ( http : //dev.mysql.com/doc/refman/5.1/en/server-sql- mode.html # sqlmode_only_full_group_by ) option turned ."
Stackoverflow,"I set call detail records , records , I 'm supposed determine average concurrent active calls per system , per hour ( precision one minute ) . If I query 7pm 8pm , I see average concurrent calls hour ( averaging concurrent calls minute ) within hour ( system ) . So , I need way check count active calls 7:00-7:01 , 7:01-7:02 , etc average numbers . A call considered active call 's time duration fall within current minute checked . What makes even difficult needs span SQL 7.0 SQL 2000 ( functions 2000 n't available 7.0 , GetUTCTime ( ) ) , I get 2000 working I 'll happy . # # What approaches problem I take ? I thought looping minutes ( 60 ) hour checked adding count calls fall minute somehow cross referencing duration make sure call starts 7:00 pm duration 300 seconds shows active 7:04 , I ca n't imagine approach problem . I tried figure way weight call particular minute would tell call active minute , could n't come effective solution . The data types I query . I n't control schema ( possibly converting data inserting another table appropriate data types ) . I 've provided example data I know concurrent active calls . CREATE TABLE Records ( seconds char ( 10 ) , time char ( 4 ) , date char ( 8 ) , dur int , system int , port int , ) -- seconds stime value . It 's difference seconds UTC 1/1/1970 00:00:00 current UTC time , use identifier ( like epoch ) . -- time time call made . -- date day call made . -- dur duration call seconds . -- system system number . -- port port system ( particularly relevant question ) . INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924228 ' , '1923 ' , '20090416',105,2,2 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239923455 ' , '1910 ' , '20090416',884,1,97 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924221 ' , '1923 ' , '20090416',116,2,15 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924259 ' , '1924 ' , '20090416',90,1,102 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239923458 ' , '1910 ' , '20090416',891,2,1 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924255 ' , '1924 ' , '20090416',99,2,42 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924336 ' , '1925 ' , '20090416',20,2,58 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924293 ' , '1924 ' , '20090416',64,2,41 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239923472 ' , '1911 ' , '20090416',888,2,27 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924347 ' , '1925 ' , '20090416',25,1,100 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924301 ' , '1925 ' , '20090416',77,2,55 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924332 ' , '1925 ' , '20090416',52,2,43 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924240 ' , '1924 ' , '20090416',151,1,17 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924313 ' , '1925 ' , '20090416',96,2,62 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924094 ' , '1921 ' , '20090416',315,2,16 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239923643 ' , '1914 ' , '20090416',788,2,34 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924447 ' , '1927 ' , '20090416',6,2,27 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924342 ' , '1925 ' , '20090416',119,2,15 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924397 ' , '1926 ' , '20090416',76,2,41 ) INSERT INTO Records ( seconds , time , date , dur , system , port ) VALUES ( '1239924457 ' , '1927 ' , '20090416',23,2,27 )","Check estimated query plan sort , perform one joins may choosing merge join example , achieve requires data sorted first prior merge - point 've sorted ."
Stackoverflow,"If I SQL Server 2008 SQL Server 2012 installed locally , I would simply try ; however I newer version installed would like keep way . * SQL Server 2008 comes assembly ` Microsoft.SqlServer.Types.dll ` , major version 10 . * SQL Server 2012 comes assembly ` Microsoft.SqlServer.Types.dll ` , major version 11 . Among things , assemblies expose [ ` SqlGeometryBuilder ` type ] ( http : //msdn.microsoft.com/en- us/library/microsoft.sqlserver.types.sqlgeometrybuilder.aspx ) . The one notable difference two assembly versions 2012 type additional overloaded method ` AddCircularArc ` , 2008 type . Since [ 's exactly trivial ( perhaps bad idea ) reference assemblies parallel ] ( http : //kentb.blogspot.com/2008/11/visual-studio- referencing-same-assembly.html ) , I wonder whether I use 2012 version -- even SQL Server 2008 instance , long I n't make use ` AddCircularArc ` . Can anyone share experience tried ?","By default SqlClient uses version 10.0 Microsoft.SqlServer.Types assembly ( even reference newer version project ) . When two different versions assembly loaded time may see strange runtime exceptions like `` System.InvalidCastException : Unable cast object type 'Microsoft.SqlServer.Types.SqlGeometry ' type 'Microsoft.SqlServer.Types.SqlGeometry'. '' ... The following article describes possibilities use newer Microsoft.SqlServer.Types assemblies SqlClient : [ Breaking Changes Database Engine Features SQL Server 2012 ] ( http : //technet.microsoft.com/en-us/library/ms143179.aspx ) The options : * Calling GetSqlBytes method , instead Get methods ( e.g . SqlGeometry.Deserialize ( reader.GetSqlBytes ( 0 ) ) ) * Using assembly redirection application configuration * Specifying value `` SQL Server 2012 '' `` Type System Version '' attribute force SqlClient load version 11.0 assembly I personally favor `` Type System Version '' connection string keyword . See MSDN article : [ SqlConnection.ConnectionString Property ] ( http : //msdn.microsoft.com/en- us/library/system.data.sqlclient.sqlconnection.connectionstring.aspx ) search 'Type System Version ' ."
Stackoverflow,"I table SQL Server 2005 approx 4 billion rows . I need delete approximately 2 billion rows . If I try single transaction , transaction log fills fails . I n't extra space make transaction log bigger . I assume best way forward batch delete statements ( batches ~ 10,000 ? ) . I probably using cursor , standard/easy/clever way ? P.S . This table identity column PK . The PK made integer foreign key date .",Every database SQL Server 2000 [ sysusers system table ] ( http : //msdn.microsoft.com/en-us/library/aa260592 % 28v=sql.80 % 29.aspx ) Probably something like Use < MyDatabase > Select [ name ] From sysusers Where issqlrole = 1 trick
Stackoverflow,"I MySQL proxy running I LUA function ` read_auth ( ) ` however passwords passed authentication hashed ( expected ) . I require format I work post onwards , cleartext . Enabling cleartext plugin MySQL client effect , I suspect MySQL proxy demanding client sends cleartext defaults hashing . So basically : ideas I would able get clear text authentication details within ` read_auth ( ) ` function MySQL proxy ? Note : end goal auth LDAP , however way I get password ( hashed ) actually binding LDAP , obtained searching .","MySQL treats logins specific host originate . You different password home machine one use server , entirely different sets permissions granted username different origin hosts . On PHPMyadmin , database running server web server , therefore refers ` localhost ` , IP ` 127.0.0.1 ` . Your machine Workbench installed must access MySQL different credentials ` username @ localhost ` . The server requires grant access username host intend connect . In PhpMyAdmin , need grant access database remote host : ( See also Pekka 's answer allow connections _any_ host ) GRANT ALL PRIVILEGES dbname . * TO yourusername @ your_remote_hostname IDENTIFIED BY 'yourpassword ' ; To see grants currently ` localhost ` duplicate remote host : SHOW GRANTS FOR yourusername @ localhost ; Additionally , MySQL server needs setup accept remote connections first place . This n't always case , especially web hosting platforms . In ` my.cnf ` file , ` skip-networking ` line removed commented . If ` skip-networking ` line , must comment line : bind-address = 127.0.0.1 ... restart MySQL ."
Stackoverflow,"I issue SQLite database . I using SQLite ODBC < http : //www.ch-werner.de/sqliteodbc/ > Installed 64-bit version created ODBC settings : ! [ enter image description ] ( https : //i.stack.imgur.com/TEcXd.png ) I open Access database link datasource . I open table , add records , delete edit records . Is something I need fix ODBC side allow ? The error I get I try delete record : > The Microsoft Access database engine stopped process another user attempting change data time . When I edit record I get : > The record changed another user since started editing . If save record , overwrite changed user made . Save record disabled . Only copy clipboard drop changes available .","My initial attempt recreate issue unsuccessful . I used following 32-bit test VM : * Access 2010 * SQLite 3.8.2 * SQLite ODBC Driver 0.996 I created populated test table [ tbl1 ] documented [ ] ( http : //www.sqlite.org/sqlite.html ) . I created Access linked table prompted I chose columns ( [ one ] [ two ] ) Primary Key . When I opened linked table Datasheet View I able add , edit , delete records without incident . The difference I see setup ( apart fact I 32-bit 64-bit ) ODBC DSN settings I left ` Sync.Mode ` setting default value ` NORMAL ` , whereas appears set ` OFF ` . Try setting ` Sync.Mode ` ` NORMAL ` see makes difference . **Edit : comments** The solution case following : > One possible workaround would create new SQLite table columns plus new INTEGER PRIMARY KEY column Access `` see '' AutoNumber . You create unique index ( currently ) first four columns ensure remain unique , new new `` identity '' ( ROWID ) column Access would use identify rows CRUD operations ."
Stackoverflow,"I know 's stupid question , I could find answer anywhere . How set default value column **sqlite.net** model ? Here model class : public class ItemTaxes { [ PrimaryKey ] public string Sku { get ; set ; } public bool IsTaxable { get ; set ; } // How set IsTaxable 's default value true ? public decimal PriceTaxExclusive { get ; set ; } } I wan na set default value **Not Null** column **IsTaxable** **true** , I achieve ? Btw I want use raw sql statement , i.e . ` conn.execute ( ) ; ` Thank .","If change Key nullable type ( int ? ) work . SQLite sees null coming generates new id needed . public class LogEntry { [ PrimaryKey , AutoIncrement ] public int ? Key { get ; set ; } public DateTime Date { get ; set ; } }"
Stackoverflow,"I using **Room Persistence Library 1.1.0** . I could find database file ` /data/data/ < package_name > /databases/ ` using Android Studio 's Device File Explorer . It contains multiple tables I access contents tables without problem using ` room-DAO ` . However opening ` sqlite- browser ` , shows table . What might reason ? Is possible resolve issue without switching back old ` SQLiteOpenHelper ` ` room ` ?","# Solution To open _such databases_ * ` sqlite-browser ` , **you need copy three files** . All must directory . * Databases stored multiple files stated question . # Why three files ? As per docs , Starting version ` 1.1.0 ` , Room uses ` write-ahead logging ` default journal mode devices sufficient RAM running API Level 16 higher . It ` Truncate ` devices version . ` write-ahead logging ` different internal structure compared ` Truncate ` . Take look files temporary files used ` SQLite ` : **Until version 1.1.0** [ ! [ enter image description ] ( https : //i.stack.imgur.com/IqfWG.png ) ] ( https : //i.stack.imgur.com/IqfWG.png ) **From version 1.1.0** [ ! [ enter image description ] ( https : //i.stack.imgur.com/Hc6em.png ) ] ( https : //i.stack.imgur.com/Hc6em.png ) If want change journal mode explicitly ` Truncate ` , way . But , **it recommended** ` WAL ` much better compared ` Truncate ` . public static void initialize ( Context context ) { sAppDatabase = Room.databaseBuilder ( context , AppDatabase.class , DATABASE_NAME ) .setJournalMode ( JournalMode.TRUNCATE ) .build ( ) ; } # Is possible move single file without changing ` Truncate ` ? Yes , . Query following statement database . @ Query ( `` pragma wal_checkpoint ( full ) '' ) void checkpoint ( ) ; [ Here ] ( https : //stackoverflow.com/a/51560124/7594961 ) related post ."
Stackoverflow,I looking definite reference SQL understood Microsoft Access . All links I find talk bits pieces . Ideally I looking grammar specification details different keywords . Motivation : I trying write parser Microsoft Access SQL statements .,"`` Note '' reserved word Microsoft Access . You need surround square brackets : INSERT INTO Grocery_Store_Prices ( Store , Item , Brand , Price , Unit , Quantity , [ Note ] ) VALUES ( `` Kroger '' , '' Cheesy Poof '' , '' Cartman '' ,0.51 , '' fart '' ,15 , '' ____ '' ) ; Helpful list reserved words : < http : //support.microsoft.com/kb/286335 > Some consider best practice _always_ encase field names square brackets , n't worry . Good luck !"
Stackoverflow,How I create stored procedure start SQL Server job ?,"If I open package BIDS ( `` Business Intelligence Development Studio '' , tool use design packages ) , select item , I `` Properties '' pane bottom right containing - among others , ` MaximumErrorCount ` property . If see , maybe minimized open ( look tabs right ) . If find way , try menu : View/Properties Window . Or try F4 key ."
Stackoverflow,"I implementing repository pattern RxJava using SqlBrite/SqlDelight offline data storage retrofit Http requests Here 's sample : protected Observable < List < Item > > getItemsFromDb ( ) { return database.createQuery ( tableName ( ) , selectAllStatement ( ) ) .mapToList ( cursor - > selectAllMapper ( ) .map ( cursor ) ) ; } public Observable < List < Item > > getItems ( ) { Observable < List < Item > > server = getRequest ( ) .doOnNext ( items - > { BriteDatabase.Transaction transaction = database.newTransaction ( ) ; ( Item item : items ) { database.insert ( tableName ( ) , contentValues ( item ) ) ; } transaction.markSuccessful ( ) ; transaction.end ( ) ; } ) .flatMap ( items - > getItemsFromDbById ( ) ) .delaySubscription ( 200 , TimeUnit.MILLISECONDS ) ; Observable < List < Item > > db = getItemsFromDbById ( id ) .filter ( items - > items ! = null & & items.size ( ) > 0 ) ; return Observable.amb ( db , server ) .doOnSubscribe ( ( ) - > server.subscribe ( items - > { } , throwable - > { } ) ) ; } The current implementation uses ` Observable.amb ` get latest 2 streams returns ` db ` stream case ` db ` data server otherwise . To prevent early failure case internet , ` server ` ` delaySubscription ` ` 200ms ` . I tried using ` Observable.concat ` SqlBrite stream never calls ` onComplete ` ` server ` observable never triggered . I also tried ` Observable.combineLatest ` n't work keeps waiting ` server ` observable return data emitting anything ` Observable.switchOnNext ` n't work either . What I looking repository : * Keeps subscription SqlBrite ( DB ) open , case DB updates * Always fetches data server writes database * Should emit empty result case nothing database network request still going . This , user see progress bar case first load .",**EDIT December 27th 2016 : ** SQLBrite version 1.1.0 exposes ` getWritableDatabase ( ) ` It 's worth noting calling ` getWritableDatabase ( ) ` potentially create migrate database make sure 're calling background thread !
Stackoverflow,"I implementing repository pattern RxJava using SqlBrite/SqlDelight offline data storage retrofit Http requests Here 's sample : protected Observable < List < Item > > getItemsFromDb ( ) { return database.createQuery ( tableName ( ) , selectAllStatement ( ) ) .mapToList ( cursor - > selectAllMapper ( ) .map ( cursor ) ) ; } public Observable < List < Item > > getItems ( ) { Observable < List < Item > > server = getRequest ( ) .doOnNext ( items - > { BriteDatabase.Transaction transaction = database.newTransaction ( ) ; ( Item item : items ) { database.insert ( tableName ( ) , contentValues ( item ) ) ; } transaction.markSuccessful ( ) ; transaction.end ( ) ; } ) .flatMap ( items - > getItemsFromDbById ( ) ) .delaySubscription ( 200 , TimeUnit.MILLISECONDS ) ; Observable < List < Item > > db = getItemsFromDbById ( id ) .filter ( items - > items ! = null & & items.size ( ) > 0 ) ; return Observable.amb ( db , server ) .doOnSubscribe ( ( ) - > server.subscribe ( items - > { } , throwable - > { } ) ) ; } The current implementation uses ` Observable.amb ` get latest 2 streams returns ` db ` stream case ` db ` data server otherwise . To prevent early failure case internet , ` server ` ` delaySubscription ` ` 200ms ` . I tried using ` Observable.concat ` SqlBrite stream never calls ` onComplete ` ` server ` observable never triggered . I also tried ` Observable.combineLatest ` n't work keeps waiting ` server ` observable return data emitting anything ` Observable.switchOnNext ` n't work either . What I looking repository : * Keeps subscription SqlBrite ( DB ) open , case DB updates * Always fetches data server writes database * Should emit empty result case nothing database network request still going . This , user see progress bar case first load .","The solution problem actually super easy clean change way thinking bit . I using exact data interaction ( Retrofit + Sqlbrite ) solution works perfectly . What use two separate observable subscriptions , take care completely different processes . 1 . ` Database ` ` - > ` ` View ` : This one used attach ` View ` ( ` Activity ` , ` Fragment ` whatever displays data ) persisted data db . You subscribe ONCE created ` View ` . dbObservable .subscribeOn ( Schedulers.io ( ) ) .observeOn ( AndroidSchedulers.mainThread ( ) ) .subscribe ( data - > { displayData ( data ) ; } , throwable - > { handleError ( throwable ) ; } ) ; 2 . ` API ` ` - > ` ` Database ` : The one fetch data api persist db . You subscribe every time want refresh data database . apiObservable .subscribeOn ( Schedulers.io ( ) ) .observeOn ( Schedulers.io ( ) ) .subscribe ( data - > { storeDataInDatabase ( data ) ; } , throwable - > { handleError ( throwable ) ; } ) ; * * * EDIT : You n't want `` transform '' observables one , purely reason 've included question . Both observables act completely differently . The ` observable ` Retrofit acts like [ ` Single ` ] ( http : //reactivex.io/RxJava/javadoc/rx/Single.html ) . It needs , finishes ( ` onCompleted ` ) . The ` observable ` Sqlbrite typical ` Observable ` , emit something every time specific table changes . Theoretically finish future . Ofc work around difference , would lead far , far away clean easily readable code . If really , really need expose single ` observable ` , **hide** fact 're actually subscribing observable retrofit subscribing database . 1 . Wrap Api subscription method : public void fetchRemoteData ( ) { apiObservable .subscribeOn ( Schedulers.io ( ) ) .observeOn ( Schedulers.io ( ) ) .subscribe ( data - > { persistData ( data ) ; } , throwable - > { handleError ( throwable ) ; } ) ; } 2 . ` fetchRemoteData ` subscription dbObservable .subscribeOn ( Schedulers.io ( ) ) .observeOn ( AndroidSchedulers.mainThread ( ) ) .doOnSubscribe ( ( ) - > fetchRemoteData ( ) ) .subscribe ( data - > { displayData ( data ) ; } , throwable - > { handleError ( throwable ) ; } ) ; I suggest really think . Because fact forcing position need single observable , might restricting quite badly . I believe exact thing force change concept future , instead protecting change ."
Stackoverflow,"I need particular business scenario set field entity ( PK ) number sequence ( sequence number min max I defined sequence like : CREATE SEQUENCE MySequence MINVALUE 65536 MAXVALUE 4294967296 START WITH 65536 INCREMENT BY 1 CYCLE NOCACHE ORDER ; In Java code I retrieve number sequence like : select mySequence.nextval dual My question : If I call `` ` select mySequence.nextval dual ` `` transaction time another transaction method called ( parallel requests ) sure values returned sequence different ? Is possible like read uncommitted value first transaction ? Cause let 's say I would used sequence plain table I would increment sequence , transaction 2 would able read value trasactinalitY default `` READ COMMITTED '' .","The answer NO . Oracle guarantees numbers generated sequence different . Even parallel requests issued , RAC environement rollback commits mixed . Sequences nothing transactions . See [ docs ] ( http : //docs.oracle.com/cd/B28359_01/server.111/b28286/statements_6015.htm ) : > Use CREATE SEQUENCE statement create sequence , database object multiple users may generate **unique** integers . You use sequences automatically generate primary key values . > > When sequence number generated , sequence incremented , **independent** transaction committing rolling back . If two users concurrently increment sequence , sequence numbers user acquires may gaps , sequence numbers generated user . One user never acquire sequence number generated another user . After sequence value generated one user , user continue access value regardless whether sequence incremented another user . > > Sequence numbers generated independently tables , sequence used one multiple tables . It possible individual sequence numbers appear skipped , generated used transaction ultimately rolled back . Additionally , single user may realize users drawing sequence ."
Stackoverflow,Having interesting issue . I 'm reading excel file server via OpenRowset Sql2005 . I 've run query number times without problems . I 've gone quick meeting suddenly I get error '' Can initialize data source object OLE DB provider `` MSDASQL '' linked server `` ( null ) '' '' I 've made sure files use server even deleted recopied onto server still I 'm getting error . UPDATE : This seems happen I join two selects different openrowsets . If I run queries individually still work fine . I done join without issues . Ideas ?,The problem comes Temp folder User SQL server service running n't accessible credentials query running . Try set security temp folder minimal restrictions . The dsn gets created every time run openrowset query recreated without credentials conflict . This worked without restart requirements .
Stackoverflow,"I new whole 'spatial index ' thing , seems best solution filtering based latitude/longitude . So I added column table : So I created ` geometry ` field : ALTER TABLE ` addresses ` ADD ` point ` POINT NOT NULL And I tried add index : ALTER TABLE ` addresses ` ADD SPATIAL INDEX ( ` point ` ) But I get error : # 1416 - Can get geometry object data send GEOMETRY field What I wrong ?","OK I found solution : Ca n't create spatial index column fields contain data . After running UPDATE ` addresses ` SET ` point ` = POINT ( lng , lat ) Everything worked fine ."
Stackoverflow,"I 'm querying big mysql database read privileges , I 'd like set slow query results variable , 'foo ' , I use queries . Basically , I want variable cumbersome subquery , I reuse without cost running every time I want use . I enter : set @ foo : = ( select * table1 join table2 bar = 0 group id ) ; I get : ERROR 1241 ( 21000 ) : Operand contain 1 column ( ) I restrict 1 column , ERROR 1242 ( 21000 ) : Subquery returns 1 row Is way store array table variable ? I n't privileges create temporary tables .",` @ ` ` MySQL ` . set @ foo : = ( select * table1 join table2 bar = 0 group id ) ;
Stackoverflow,The connection SQL Workbench/J gets disconnected frequently . Where I change settings lose connections atleast hour . Thanks,"You n't need change connection profile , change autocommit property inside SQL script `` on-the-fly '' [ set autocommit ] ( http : //www.sql-workbench.net/manual/wb-commands.html # command-set- autocommit ) set autocommit ; vacuum ; set autocommit ; You also toggle current autocommit state menu `` SQL - > Autocommit ''"
Stackoverflow,"I 'd like amend SQL 's generated EF : CF generating database schema ( DDL ) , [ suggested Entity Framework team ] ( http : //entityframework.codeplex.com/workitem/2591 ) . How done ? I could n't find anything appropriate via Google .","You override [ ` MigrationSqlGenerator ` ] ( https : //msdn.microsoft.com/en- us/library/system.data.entity.migrations.sql.migrationsqlgenerator.aspx ) used Entity Framework calling [ DbMigrationsConfiguration.SetSqlGenerator ( ) ] ( https : //msdn.microsoft.com/en- us/library/system.data.entity.migrations.dbmigrationsconfiguration.setsqlgenerator.aspx ) method constructor ` DbMigrationsConfiguration ` class , passing database provider name ( e.g . ` `` System.Data.SqlClient '' ` SQL Server ) , ` MigrationSqlGenerator ` instance use database provider . Consider example [ work item ] ( http : //entityframework.codeplex.com/workitem/2591 ) linked : public class MyEntity { public int Id { get ; set ; } [ Required ] [ MinLength ( 5 ) ] public string Name { get ; set ; } } Suppose table ` MyEntity ` already generated ` Add- Migration ` command used add ` Name ` field . By default , scaffolded migration : public partial class AddMyEntity_Name : DbMigration { public override void Up ( ) { AddColumn ( `` dbo.MyEntity '' , `` Name '' , c = > c.String ( nullable : false ) ) ; } public override void Down ( ) { DropColumn ( `` dbo.MyEntity '' , `` Name '' ) ; } } Notice scaffolder generate anything ` MinLengthAttribute ` . To EF convey minimum length requirement , [ specify attribute-to-column annotation convention ] ( http : //entityframework.codeplex.com/wikipage ? title=Code % 20First % 20Annotations ) . As mentioned documentation page , [ ` AnnotationValues ` ] ( https : //msdn.microsoft.com/en- us/library/system.data.entity.infrastructure.annotations.annotationvalues.aspx ) ignored default SQL generators . Within DbContext 's OnModelCreating ( ) override , add following : modelBuilder.Conventions.Add ( new AttributeToColumnAnnotationConvention < MinLengthAttribute , Int32 > ( `` minLength '' , ( property , attributes ) = > attributes.Single ( ) .Length ) ) ; After adding , regenerate scaffolded migration running ` Add-Migration -Force AddMyEntity_Name ` . Now scaffolded migration : public partial class AddMyEntity_Name : DbMigration { public override void Up ( ) { AddColumn ( `` dbo.MyEntity '' , `` Name '' , c = > c.String ( nullable : false , annotations : new Dictionary < string , AnnotationValues > { { `` minLength '' , new AnnotationValues ( oldValue : null , newValue : `` 5 '' ) } , } ) ) ; } public override void Down ( ) { DropColumn ( `` dbo.MyEntity '' , `` Name '' , removedAnnotations : new Dictionary < string , object > { { `` minLength '' , `` 5 '' } , } ) ; } } Suppose , linked work item , want generate constraint check trimmed ` Name ` value greater minLength ( 5 case ) . You start creating custom ` MigrationSqlGenerator ` extends ` SqlServerMigrationSqlGenerator ` call SetSqlGenerator ( ) install custom ` MigrationSqlGenerator ` : internal class CustomSqlServerMigrationSqlGenerator : SqlServerMigrationSqlGenerator { protected override void Generate ( AddColumnOperation addColumnOperation ) { base.Generate ( addColumnOperation ) ; } } internal sealed class Configuration : DbMigrationsConfiguration < DataContext > { public Configuration ( ) { AutomaticMigrationsEnabled = false ; SetSqlGenerator ( `` System.Data.SqlClient '' , new CustomSqlServerMigrationSqlGenerator ( ) ) ; } protected override void Seed ( DataContext context ) { // ... } } Right , ` CustomSqlServerMigrationSqlGenerator ` overrides Generate ( AddColumnOperation ) method , simply calls base implementation . If look [ documentation ` AddColumnOperation ` ] ( https : //msdn.microsoft.com/en- us/library/system.data.entity.migrations.model.addcolumnoperation.aspx ) , see two important properties , ` Column ` ` Table ` . ` Column ` [ ` ColumnModel ` ] ( https : //msdn.microsoft.com/en- us/library/system.data.entity.migrations.model.columnmodel.aspx ) created lambda Up ( ) , ` c = > c.String ( nullable : false , annotations : ... ) ` . In Generate ( ) method , access custom ` AnnotationValues ` via ` Annotations ` property ` ColumnModel ` . To generate DDL adds constraint , need generate SQL call Statement ( ) method . For example : internal class CustomSqlServerMigrationSqlGenerator : SqlServerMigrationSqlGenerator { protected override void Generate ( AddColumnOperation addColumnOperation ) { base.Generate ( addColumnOperation ) ; var column = addColumnOperation.Column ; ( column.Type == System.Data.Entity.Core.Metadata.Edm.PrimitiveTypeKind.String ) { var annotations = column.Annotations ; AnnotationValues minLengthValues ; ( annotations.TryGetValue ( `` minLength '' , minLengthValues ) ) { var minLength = Convert.ToInt32 ( minLengthValues.NewValue ) ; ( minLength > 0 ) { ( Convert.ToString ( column.DefaultValue ) .Trim ( ) .Length < minLength ) { throw new ArgumentException ( String.Format ( `` minLength { 0 } specified { 1 } . { 2 } , default value , ' { 3 } ' , satisfy requirement . `` , minLength , addColumnOperation.Table , column.Name , column.DefaultValue ) ) ; } using ( var writer = new StringWriter ( ) ) { writer.Write ( `` ALTER TABLE `` ) ; writer.Write ( Name ( addColumnOperation.Table ) ) ; writer.Write ( `` ADD CONSTRAINT `` ) ; writer.Write ( Quote ( `` ML_ '' + addColumnOperation.Table + `` _ '' + column.Name ) ) ; writer.Write ( `` CHECK ( LEN ( LTRIM ( RTRIM ( { 0 } ) ) ) > { 1 } ) '' , Quote ( column.Name ) , minLength ) ; Statement ( writer.ToString ( ) ) ; } } } } } } If run ` Update-Database -Verbose ` , see exception generated ` CustomSqlServerMigrationSqlGenerator ` : minLength 5 specified dbo.MyEntity.Name , default value , `` , satisfy requirement . To fix issue , specify defaultValue Up ( ) method longer minimum length ( e.g . ` `` unknown '' ` ) : public override void Up ( ) { AddColumn ( `` dbo.MyEntity '' , `` Name '' , c = > c.String ( nullable : false , defaultValue : `` unknown '' , annotations : new Dictionary < string , AnnotationValues > { { `` minLength '' , new AnnotationValues ( oldValue : null , newValue : `` 5 '' ) } , } ) ) ; } Now re-run ` Update-Database -Verbose ` , see ` ALTER TABLE ` statement adds column ` ALTER TABLE ` statement adds constraint : ALTER TABLE [ dbo ] . [ MyEntity ] ADD [ Name ] [ nvarchar ] ( max ) NOT NULL DEFAULT 'unknown' ALTER TABLE [ dbo ] . [ MyEntity ] ADD CONSTRAINT [ ML_dbo.MyEntity_Name ] CHECK ( LEN ( LTRIM ( RTRIM ( [ Name ] ) ) ) > 5 ) See also : [ EF6 : Writing Your Own Code First Migration Operations ] ( http : //romiller.com/2013/02/27/ef6-writing-your-own-code-first- migration-operations/ ) , shows implement custom migration operation ."
Stackoverflow,"I 'm trying use EXCEPT keyword Oracle 10.1.0.2.0 , kept getting error 'Unknown Command ' . I 've tried googling around someone said keyword MINUS , I used MINUS , instead , I still got error . Any idea ? Thanks . So 's query . I 'm finding name students enrolls ALL courses course number > 500 SELECT s.name FROM Students WHERE NOT EXISTS ( SELECT c.id FROM Courses c WHERE c.number > 500 MINUS SELECT e.course_id FROM Enrollment e WHERE e.student_id = s.id ) ;","Oracle ` MINUS ` operator ; 's equivalent ` EXCEPT ` SQL Server . [ Here previous post ] ( https : //stackoverflow.com/q/5557991/1275871 ) explaining difference . Here 's trivial example : SELECT , b , c FROM table_a MINUS SELECT , b , c FROM table_b If still problems , add complete query using question ; 's likely simple syntax error ."
Stackoverflow,"I need drop user dbowner schema SQL Server database . I drop since I get error message > Drop failed User 'network service ' . ( Microsoft.SqlServer.Smo ) > > The database principal owns schema database , dropped . ( Microsoft SQL Server , Error : 15138 ) When I try uncheck schema owned user remove db owner nothing . My question I drop user edit name 'network service ' 'NT AUTHORITY\NETWORK SERVICE '",take look : < http : //www.itorian.com/2012/12/the-database-principal-owns-schema-in.html > It suggests need add another owner first
Stackoverflow,"Here 's table : CREATE TABLE ` alums_alumphoto ` ( ` id ` int ( 11 ) NOT NULL auto_increment , ` alum_id ` int ( 11 ) NOT NULL , ` photo_id ` int ( 11 ) default NULL , ` media_id ` int ( 11 ) default NULL , ` updated ` datetime NOT NULL , PRIMARY KEY ( ` id ` ) , KEY ` alums_alumphoto_alum_id ` ( ` alum_id ` ) , KEY ` alums_alumphoto_photo_id ` ( ` photo_id ` ) , KEY ` alums_alumphoto_media_id ` ( ` media_id ` ) , CONSTRAINT ` alums_alumphoto_ibfk_1 ` FOREIGN KEY ( ` media_id ` ) REFERENCES ` media_mediaitem ` ( ` id ` ) , CONSTRAINT ` alum_id_refs_id_706915ea ` FOREIGN KEY ( ` alum_id ` ) REFERENCES ` alums_alum ` ( ` id ` ) , CONSTRAINT ` photo_id_refs_id_63282119 ` FOREIGN KEY ( ` photo_id ` ) REFERENCES ` media_mediaitem ` ( ` id ` ) ) ENGINE=InnoDB AUTO_INCREMENT=63 DEFAULT CHARSET=utf8 I want delete column ` photo_id ` , presumably also require deleting foreign key constraint index . The problem I get errors I try drop column : ERROR 1025 ( HY000 ) : Error rename '.\dbname\ # sql-670_c5c ' '.\dbname\alums_alumphoto ' ( errno : 150 ) ... I try drop index ( ) , I try drop foreign key constraint : ERROR 1091 ( 42000 ) : Ca n't DROP 'photo_id_refs_id_63282119 ' ; check column/key exists ) What order I ? What precise commands I using ?","Precisely , try : First drop Foreign Key Constraint : ALTER TABLE ` alums_alumphoto ` DROP FOREIGN KEY ` photo_id_refs_id_63282119 ` ; The previous command removes Foreign Key Constraint column . Now drop column ` photo_id ` ( index removed MySQL dropping column ) : ALTER TABLE ` alums_alumphoto ` DROP COLUMN ` photo_id ` ; Aternatively , could combine 2 operations one : ALTER TABLE ` alums_alumphoto ` DROP FOREIGN KEY ` photo_id_refs_id_63282119 ` , DROP COLUMN ` photo_id ` ;"
Stackoverflow,"I ran odd situation use case project : ESQL calling java method , sending String input parameter method unmarshal , apply logic , store useful info unmarshalled object . So , method must either throw JAXBException , use try catch order handle possible exceptions . The problem , ESQL invoke java method includes throws signature . BUT , want errors fall back previously calling MBNode handled appropriately , trycatch picture . It struck hey , possible return type Exception encounter issue , null ? So I wrote simple method , though I n't get warnings errors , seemed wrong sense good programming . For example : public Exception doStuffAndCheckForErorrs ( String inString ) { ( inString.equals ( null ) ) { return new Exception ( `` Your string null '' ) ; } else return null ; } But I get terrible feeling anything way . I 'm open thoughts different solutions , especially there's way around ESQL signature issue . **UPDATE** : Adding reference ESQL procedure call java method throws clause signature . Excerpt [ This link ] ( https : //publib.boulder.ibm.com/infocenter/wmbhelp/v6r1m0/index.jsp ? topic= % 2Fcom.ibm.etools.mft.doc % 2Fac06009_.htm ) CREATE PROCEDURE statement section : '' Any Java method want invoke must following basic signature : public static ( < 0 - N parameters > ) must list Java IN data types table ESQL Java data type mapping ( excluding REFERENCE type , permitted return value ) , Java void data type . The parameter data types must also ESQL Java data type mapping table . In addition , **the Java method allowed exception throws clause signature** . ''","XQuery API , standard , full syntax found online : [ XQuery 1.0 ] ( http : //www.w3.org/TR/xquery/ ) [ XQuery 3.0 ] ( http : //www.w3.org/TR/xquery-3/ ) ( 2.0 ) . You 'll also find many manuals , tutorials etc . XQuery relies [ XPath ] ( http : //www.w3.org/TR/xpath20/ ) , even wider used XQuery found libraries almost every general purpose language . Your expression correct XQuery , considers everything sequence , comma concatenates ( flattens ) two sequences . XPath know ` NULL ` , knows ` xsi : nil ` ` ( ) ` , latter _empty sequence_ . An empty sequence removed result . I sure XQuery processor used underneath , correct expression ` ( $ var0 , $ var1 ) [ 1 ] ` 2 , works way ` COALESCE ` operation1 . In XPath XQuery , variables referenced ` $ ` sign . The number variables expressions separated comma unbounded . If empty sequence ( null ) , result empty sequence . Without ` [ 1 ] ` , return items non-null discard rest . You use another index , like ` [ 3 ] ` get third non-null value . If value exists , return null ( empty sequence ) . * * * 1 _which behaves exactly way described . I believe behaves like ` var0 == null var1 else var0 ` , selects first non-null value ( I 've updated OP ) ._ 2 _as Florent explained comments , warning expression place . If ` $ var1 : = ( 1 , 2 ) ` ` $ var2 : = ( 3 , 4 ) ` , expression ` $ var1 , $ var2 ) [ 1 ] ` return ` 1 ` , ` ( 1 , 2 ) ` , sequences contain subsequences , indexing sequence ` [ x ] ` return x th value flattened sequence . You safe-guard expression ` ( zero-or-one ( $ var1 ) , zero-or-one ( $ var2 ) ) [ 1 ] ` ._"
Stackoverflow,Hello I 'm trying import data excel file ` ( xls ) ` new ` SQL table ` I use ` Import Export data 32/bit ` achieve . When I load excel file automatically detects data types columns . e.g . column phone numbers data type new table ` float ` excel Double ( 15 ) I try change ` float ` ` nvarchar ` I get : > Found 2 unknown column type conversion ( ) You selected skip 1 potential lost column conversion ( ) You selected skip 3 safe column conversion ( ) And I 'm allowed continue export . Is way change data types trying import ? Thank time . These data set ` text ` data type excel Sample data one columns excel : 5859031783 5851130582 8811014190 This I get : ! [ enter image description ] ( https : //i.stack.imgur.com/SIth5.jpg ),"Select Column Excel sheet change data type text ! [ enter image description ] ( https : //i.stack.imgur.com/ZxZsU.png ) Then go sql server open import-export wizard steps select source data bla bla get point ` Mapping Column ` , select ` Float ` data type default , You change ` NVARCHAR ( N ) ` test I changed NVARCHAR ( 400 ) , gave warning I might lose data I converting data 1 datatyep another . ! [ enter image description ] ( https : //i.stack.imgur.com/iN4Px.png ) When get ` Data Type Mapping ` page make sure select Convert checkbox . stop process failure appropriate . ! [ enter image description ] ( https : //i.stack.imgur.com/71JrH.png ) Going steps finally got data destination table ` Warning ` I converted data n bla bla microsoft worries much : ) ! [ enter image description ] ( https : //i.stack.imgur.com/8uvi6.png ) **Finally Data Sql-Server Table** ╔═══════╦════════════╦═════════╦════════════════╦══════════════╗ ║ Name ║ City ║ Country ║ Phone ║ Float_Column ║ ╠═══════╬════════════╬═════════╬════════════════╬══════════════╣ ║ Shaun ║ London ║ UK ║ 04454165161665 ║ 5859031783 ║ ║ Mark ║ Newyork ║ USA ║ 16846814618165 ║ 8811014190 ║ ║ Mike ║ Manchester ║ UK ║ 04468151651651 ║ 5851130582 ║ ╚═══════╩════════════╩═════════╩════════════════╩══════════════╝"
Stackoverflow,"I 've never used mysqli_multi_query 's boggling brain , examples I find net n't helping figure exactly I want . Here code : < ? php $ link = mysqli_connect ( `` server '' , `` user '' , `` pass '' , `` db '' ) ; ( mysqli_connect_errno ( ) ) { printf ( `` Connect failed : % s\n '' , mysqli_connect_error ( ) ) ; exit ( ) ; } $ agentsquery = `` CREATE TEMPORARY TABLE LeaderBoard ( ` agent_name ` varchar ( 20 ) NOT NULL , ` job_number ` int ( 5 ) NOT NULL , ` job_value ` decimal ( 3,1 ) NOT NULL , ` points_value ` decimal ( 8,2 ) NOT NULL ) ; '' ; $ agentsquery .= `` INSERT INTO LeaderBoard ( ` agent_name ` , ` job_number ` , ` job_value ` , ` points_value ` ) SELECT agent_name , job_number , job_value , points_value FROM jobs WHERE YEAR ( booked_date ) = $ current_year & & WEEKOFYEAR ( booked_date ) = $ weeknum ; '' ; $ agentsquery .= `` INSERT INTO LeaderBoard ( ` agent_name ` ) SELECT DISTINCT agent_name FROM apps WHERE YEAR ( booked_date ) = $ current_year & & WEEKOFYEAR ( booked_date ) = $ weeknum ; '' ; $ agentsquery .= `` SELECT agent_name , SUM ( job_value ) , SUM ( points_value ) FROM leaderboard GROUP BY agent_name ORDER BY SUM ( points_value ) DESC '' ; $ = 0 ; $ agentsresult = mysqli_multi_query ( $ link , $ agentsquery ) ; ( $ row = mysqli_fetch_array ( $ agentsresult ) ) { $ number_of_apps = getAgentAppsWeek ( $ row [ 'agent_name ' ] , $ weeknum , $ current_year ) ; $ i++ ; ? > < tr class= '' tr < ? php echo ( $ & 1 ) ? > '' > < td style= '' font-weight : bold ; '' > < ? php echo $ row [ 'agent_name ' ] ? > < /td > < td > < ? php echo $ row [ 'SUM ( job_value ) ' ] ? > < /td > < td > < ? php echo $ row [ 'SUM ( points_value ) ' ] ? > < /td > < td > < ? php echo $ number_of_apps ; ? > < /td > < /tr > < ? php } ? > All I 'm trying run multiple query use final results 4 queries put tables . code really n't work , I get following error : > Warning : mysqli_fetch_array ( ) expects parameter 1 mysqli_result , boolean given C : \xampp\htdocs\hydroboard\hydro_reporting_2010.php line 391 help ?","While pipodesign corrected error within $ querystring alleviated problem , actual solution provided regarding Strict Standards error . I disagree SirBT 's advice , changing DO WHILE WHILE necessary . The Strict Standards message receive quite informative . To obey , use : { } ( mysqli_more_results ( $ db ) & & mysqli_next_result ( $ db ) ) ; Then , need write conditional exit break inside loop condition break loop first occurrence error . *note , statement do-while deny entry loop first query error . In example , running INSERT queries , wo n't receive result sets process . If want count many rows 've added , use mysqli_affected_rows ( ) . As complete solution question : ( mysqli_multi_query ( $ db , $ querystring ) ) { { $ cumulative_rows+=mysqli_affected_rows ( $ db ) ; } ( mysqli_more_results ( $ db ) & & mysqli_next_result ( $ db ) ) ; } ( $ error_mess=mysqli_error ( $ db ) ) { echo `` Error : $ error_mess '' ; } echo `` Cumulative Affected Rows : $ cumulative_rows '' ; Output : // errors Cumulative Affected Rows : 2 // error second query Error : [ something ] Cumulative Affected Rows : 1 // error first query Error : [ something ] Cumulative Affected Rows : 0 * * * **_LATE EDIT : _** Since people new mysqli stumbling across post , I 'll offer general yet robust snippet handle queries with/without result sets using multi_query ( ) add feature display query array handled ... **Classic `` IF ( ) { DO { } WHILE } '' Syntax** : ( mysqli_multi_query ( $ mysqli , implode ( ' ; ' , $ queries ) ) ) { { echo `` < br > < br > '' , key ( $ queries ) , '' : `` , current ( $ queries ) ; // display key : value @ pointer ( $ result=mysqli_store_result ( $ mysqli ) ) { // result set ( $ rows=mysqli_fetch_assoc ( $ result ) ) { echo `` < br > Col = { $ rows [ `` Col '' ] } '' ; } mysqli_free_result ( $ result ) ; } echo `` < br > Rows = `` , mysqli_affected_rows ( $ mysqli ) ; // acts like num_rows SELECTs } ( next ( $ queries ) & & mysqli_more_results ( $ mysqli ) & & mysqli_next_result ( $ mysqli ) ) ; } ( $ mysqli_error=mysqli_error ( $ mysqli ) ) { echo `` < br > < br > '' , key ( $ queries ) , '' : `` , current ( $ queries ) , '' Syntax Error : < br > $ mysqli_error '' ; // display array pointer key : value } //if want use snippet ... $ mysqli_error=null ; // clear variables reset ( $ queries ) ; // reset pointer * * * **Reinvented Wheel `` WHILE { } '' Syntax** ( ... n't like post-test loops ) : ( ( isset ( $ multi_query ) & & ( next ( $ queries ) & & mysqli_more_results ( $ mysqli ) & & mysqli_next_result ( $ mysqli ) ) ) || ( ! isset ( $ multi_query ) & & $ multi_query=mysqli_multi_query ( $ mysqli , implode ( ' ; ' , $ queries ) ) ) ) { echo `` < br > < br > '' , key ( $ queries ) , '' : `` , current ( $ queries ) ; // display array pointer key : value ( $ result=mysqli_store_result ( $ mysqli ) ) { ( $ rows=mysqli_fetch_assoc ( $ result ) ) { echo `` < br > Col = { $ rows [ `` Col '' ] } '' ; } mysqli_free_result ( $ result ) ; } echo `` < br > Rows = `` , mysqli_affected_rows ( $ mysqli ) ; // acts like num_rows SELECTs } ( $ mysqli_error=mysqli_error ( $ mysqli ) ) { echo `` < br > < br > '' , key ( $ queries ) , '' : `` , current ( $ queries ) , '' Syntax Error : < br > $ mysqli_error '' ; // display array pointer key : value } //if want use snippet ... $ multi_query= $ mysqli_error=null ; // clear variables reset ( $ queries ) ; // reset pointer So , either snippet given following queries offer output : **Query array : ** $ queries [ ] = '' SELECT * FROM ` TEST ` `` ; $ queries [ ] = '' INSERT INTO ` TEST ` ( Col ) VALUES ( 'string1 ' ) , ( 'string2 ' ) '' ; $ queries [ ] = '' SELECT * FROM ` TEST ` `` ; $ queries [ ] = '' DELETE FROM ` TEST ` WHERE Col LIKE 'string % ' '' ; **Output : ** 0 : SELECT * FROM ` TEST ` Rows = 0 1 : INSERT INTO ` TEST ` ( Col ) VALUES ( 'string1 ' ) , ( 'string2 ' ) Rows = 2 2 : SELECT * FROM ` TEST ` Col = string1 Col = string2 Rows = 2 3 : DELETE FROM ` TEST ` WHERE Col LIKE 'string % ' Rows = 2 Modify snippets per needs . Leave comment discover bug ."
Stackoverflow,"I manually ( GASP ! ) entered MySQL command command line , I received Warning I even begin understand . ( And anyone says anything , yes , I KNOW : 1 . Using command line interface best approach ; 2 . My table NOT named `` TABLE_NAME '' column NOT named `` DateColumn '' RecordID value NOT really `` 1234 '' ; 3 . Maybe column type _should_ TIMESTAMP , , 's . Moving ... . ) Attempting enter value date `` July 26th , 2012 2:27 PM ( GMT ) '' , I keyed : mysql > update TABLE_NAME set DateColumn= '' 2012-07-26 14:27:00 '' RecordID= '' 1234 '' ; I received : Query OK , 1 row affected , 1 warning ( 0.11 sec ) Rows matched : 1 Changed : 1 Warnings : 1 So , I keyed : mysql > show warnings ; + -- -- -- -- -+ -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | Level | Code | Message | + -- -- -- -- -+ -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | Warning | 1264 | Out range value column 'DateColumn ' row 1 | + -- -- -- -- -+ -- -- -- + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ Weird , I thought . So I checked table first confirm column ( field ) type : mysql > describe TABLE_NAME ; + -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -+ | Field | Type | Null | Key | Default | Extra | | DateColumn | datetime | YES | | NULL | | + -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- + -- -- -+ -- -- -- -- -- -- -- -- -- -+ -- -- -- -+ BUT value DOES get written properly database , truncated , AFAIK : mysql > select * TABLE_NAME RecordID= '' 1234 '' ; + -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -+ | RecordID | Date_Column | BlahBlahBlah | + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + | 1234 | 2012-07-26 14:27:00 | something.. | + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- + I 've already searched StackOverflow.com solution . I 've already Googled explanation . I 've already read < http : //dev.mysql.com/doc/refman/5.5/en/datetime.html > says : MySQL retrieves displays DATETIME values 'YYYY-MM-DD HH : MM : SS ' format . The supported range '1000-01-01 00:00:00 ' '9999-12-31 23:59:59 ' . I even slight suspicion something date time I making entry ; I state server database located Pacific Daylight Time ( GMT-8 , except right GMT-7 DST ) ; I logged ( SSH ) client EDT ( matter ) ; I storing Date_Column values GMT . At time I entering value `` 2012-07-26 14:27:00 '' three dates well AFTER , 7/30/12 . Not matter -- I _should_ able enter _future_ dates without getting error -- thought might helpful know . So -- **WHY , OH WHY `` 2012-07-26 14:27:00 '' Out-of-Range Value ? ** My MySQL client API version 5.1.49 . This first time I 've ever posted StackOverflow . Thank advance suggestions .","**Float ( 9,2 ) ** would allow _7 numbers decimal , 2 after_ , **you 8 2 after** . You need increase size filed wish change value require . You look [ Numeric Types ] ( http : //dev.mysql.com/doc/refman/5.0/en/numeric-types.html ) > For example , column defined FLOAT ( 7,4 ) look like -999.9999"
Stackoverflow,"I using Apache Camel SQL batch insertion process . 1 . My application reading tickets Active MQ contains around 2000 tickets . 2 . I updated batch 100 . 3 . The query firing follows : ` sql.subs.insertCdr= insert subscription_logs ( master_id , request_type , req_desc , msisdn , amount , status , resp_code , resp_desc , channel , transaction_id , se_mode , be_mode , sub_type , sub_timeleft , srv_name , srv_id , start_date , end_date , operator , circle , country , time_offset , retry_count , user_status , previous_state , se_reqrecvtime , se_respsenttime , be_reqsenttime , be_resprecvtime , cp_id , cp_name , sub_srvname , sub_srvid , msg_senderid , msg_text , call_back_url , call_back_resp , client_ip , se_sysIp , language , cp_callbackurlhittime , action , alert , notification_url , notification_resp ) values ( : # masterId , : # requestType , : # reqDesc , : # msisdnCdr , : # price , : # status , : # responseCode , : # reason , : # channel , : # transactionId , : # seMode , : # beMode , : # subType , : # subTimeLeft , : # serviceName , : # serviceId , : # subStartDate , : # cdrEndDate , : # operator , : # circle , : # country , : # timeOffset , : # retryCount , : # userStatus , : # previousState , : # seReqRecvTime , : # seRespSentTime , : # beReqSentTime , : # beRespRecvTime , : # cpId , : # cpName , : # subServiceName , : # subServiceId , : # shortCode , : # message , : # callBackUrl , : # callBackResp , : # clientIp , : # seSysIp , : # language , : # cpCallbackUrlHitTime , : # action , : # alert , : # notificationUrl , : # notificationResponse ) ` 4 . The SQL batch route defined follows : < pipeline > < log message= '' Going insert database '' > < /log > < transform > < method ref= '' insertionBean '' method= '' subsBatchInsertion '' > < /method > < /transform > < choice > < > < simple > $ { in.header.subsCount } == $ { properties : batch.size } < /simple > < uri= '' sql : { { sql.subs.insertCdr } } ? batch=true '' > < /to > < log message= '' Inserted rows $ { body } '' > < /log > < /when > < /choice > < /pipeline > 5 . Below java code : public List < Map < String , Object > > subsBatchInsertion ( Exchange exchange ) { ( subsBatchCounter > batchSize ) { subsPayLoad.clear ( ) ; subsBatchCounter = 1 ; } subsPayLoad.add ( generateInsert ( exchange.getIn ( ) .getBody ( SubscriptionCdr.class ) ) ) ; exchange.getIn ( ) .setHeader ( `` subsCount '' , subsBatchCounter ) ; subsBatchCounter++ ; return subsPayLoad ; } public Map < String , Object > generateInsert ( Cdr cdr ) { Map < String , Object > insert = new HashMap < String , Object > ( ) ; try { insert = BeanUtils.describe ( cdr ) ; } catch ( Exception e ) { Logger.sysLog ( LogValues.error , this.getClass ( ) .getName ( ) + '' | `` +Thread.currentThread ( ) .getStackTrace ( ) [ 1 ] .getMethodName ( ) , coreException.GetStack ( e ) ) ; } ( String name : insert.keySet ( ) ) { Logger.sysLog ( LogValues.APP_DEBUG , this.getClass ( ) .getName ( ) , name + `` : '' + insert.get ( name ) + `` \t '' ) ; } return insert ; } Now problem around 120 ticket ActiveMQ , SQL batch started insert values database . But taking lot time . It starts insertion process around 500 tickets ActiveMQ . Can anyboody help optimizing insertion process ? Or approach ?","The problem ActiceMQ consumer numbers . > When changed consumer count back 1 , batch getting updated time . Actually , consumer count 10 , tickets consumed parallel . That means , 100 tickets consumed activemq 10 consumers , approx 10 tickets consumer , thus adding time . When one consumer got 100 tickets , batch getting updated . > So changing consumer count 1 made tickets processed single consumer , thus performing batch update fine ."
Stackoverflow,"I looked documentation google big query data types , checking differences TimeStamp Datetime data types . As I understand main difference : > Unlike Timestamps , DATETIME object refer absolute instance time . Instead , civil time , time user would see watch calendar . So I use Timestamp/Datetime ? Thanks","In cases want use [ timestamp data type ] ( https : //cloud.google.com/bigquery/docs/reference/standard-sql/data- types # timestamp-type ) . It refers actual point time , including timezone . Very rarely would use [ datetime data type ] ( https : //cloud.google.com/bigquery/docs/reference/standard-sql/data- types # datetime-type ) , date time time zone . The example I like give 'd use datetime represent pi day , 2017 , since occurs ` 2017-03-14 15:09:26.535898 ` time zone separately ."
Stackoverflow,"I using greendao ORM . I trying encrypt database using SQLCipher . Greendao automativally supports sqlcipher . So I wrote following code encryption . DaoMaster.DevOpenHelper helper = new DaoMaster.DevOpenHelper ( context , `` encrypted-db '' , null ) ; Database db = helper.getEncryptedWritableDb ( `` mySecretPassword '' ) ; DaoSession session = new DaoMaster ( db ) .newSession ( ) ; return session ; However whenever I perform database operation using session , gives error Caused : java.lang.NoClassDefFoundError : Failed resolution : Lorg/greenrobot/greendao/database/DatabaseOpenHelper $ EncryptedHelper ; org.greenrobot.greendao.database.DatabaseOpenHelper.checkEncryptedHelper ( DatabaseOpenHelper.java:121 ) org.greenrobot.greendao.database.DatabaseOpenHelper.getEncryptedWritableDb ( DatabaseOpenHelper.java:133 ) My gradle dependencies are- > compile fileTree ( include : [ '*.jar ' ] , dir : 'libs ' ) testCompile 'junit : junit:4.12' compile 'com.android.support : appcompat-v7:24.2.0' compile 'org.greenrobot : greendao:3.2.0' compile 'com.google.code.gson : gson:2.8.0' My proguard rules -keepclassmembers class * extends org.greenrobot.greendao.AbstractDao { public static java.lang.String TABLENAME ; } -keep class ** $ Properties # If use Rx : -dontwarn rx . ** So encrypt database using greendao SQLCipher ? PS : ` Database db = helper.getEncryptedWritableDb ( `` mySecretPassword '' ) ; ` line generates error performing database operation . Database db = helper.getEncryptedWritableDb ( `` mySecretPassword '' ) ;","add build.gradle app work box : dependencies { compile 'net.zetetic : android-database-sqlcipher:3.5.2 @ aar' ... } In code , load 'native libraries ' 'aar ' file contains . SQLiteDatabase.loadLibs ( context ) ; NOTE use ` net.sqlcipher.database.SQLiteDatabase ` instead ` android.database.sqlite.SQLiteDatabase ` , like couple SQLite classes ."
Stackoverflow,"I 'm trying use sql-maven-plugin execute PL/SQL script Oracle 11 database . Although script valid PL/SQL ( far I tell ) , execution gives PLS-00103 error : The SQL script : ( drop_all_tables.sql ) BEGIN EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE ' ; EXCEPTION WHEN OTHERS THEN IF SQLCODE ! = -942 THEN RAISE ; END IF ; END ; And plugin configuration : < plugin > < groupId > org.codehaus.mojo < /groupId > < artifactId > sql-maven-plugin < /artifactId > < version > 1.5 < /version > < dependencies > < dependency > < groupId > oracle < /groupId > < artifactId > jdbc < /artifactId > < version > 11.2.0.2.0 < /version > < /dependency > < /dependencies > < executions > < execution > < id > create-schema < /id > < phase > process-test-resources < /phase > < goals > < goal > execute < /goal > < /goals > < configuration > < driver > oracle.jdbc.driver.OracleDriver < /driver > < url > $ { jdbc.url } < /url > < username > $ { jdbc.username } < /username > < password > $ { jdbc.password } < /password > < autocommit > true < /autocommit > < srcFiles > < srcFile > src/main/resources/sql/drop_all_tables.sql < /srcFile > < /srcFiles > < /configuration > < /execution > < /executions > < /plugin > And output Maven execution : [ ERROR ] Failed execute : BEGIN EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE ' ; [ INFO ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ ERROR ] BUILD ERROR [ INFO ] -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- [ INFO ] ORA-06550 : line 2 , column 43 : PLS-00103 : Encountered symbol `` end-of-file '' expecting one following : ( begin case declare end exception exit goto loop mod null pragma raise return select update < identifier > < double-quoted delimited-identifier > < bind variable > < < continue close current delete fetch lock insert open rollback savepoint set sql execute commit forall merge pipe purge","I guess plugin splitting sql script semicolons trying execute part independently . The first statement would BEGIN EXECUTE IMMEDIATE 'DROP TABLE MY_TABLE ' ; Which incomplete far oracle concerned . The plugin two configuration parameters change behaviour , [ delimiter ] ( http : //mojo.codehaus.org/sql-maven-plugin/execute- mojo.html # delimiter ) [ delimiterType ] ( http : //mojo.codehaus.org/sql-maven- plugin/execute-mojo.html # delimiterType ) . By changing configuration like separating ` BEGIN ` blocks ` / ` line able execute script . < configuration > < delimiter > / < /delimiter > < delimiterType > row < /delimiterType > < /configuration >"
Stackoverflow,"I created following MySQL table store latitude/longitude coordinates along name point : CREATE TABLE ` points ` ( ` id ` int ( 10 ) unsigned NOT NULL AUTO_INCREMENT , ` name ` varchar ( 128 ) NOT NULL , ` location ` point NOT NULL , PRIMARY KEY ( ` id ` ) , SPATIAL KEY ` location ` ( ` location ` ) ) ENGINE=MyISAM DEFAULT CHARSET=latin1 AUTO_INCREMENT=1 ; I trying query : * points within _n_ mile radius given point ; * distance returned point given point All examples I found refer using minimum bounding rectangle ( MBR ) rather radius . The table contains approximately 1 million points , need needs efficient possible .","It appears ` PHP ` ` unpack ` function needed extract coordinate information . ` MySQL ` stores geometry fields WKB ( Well Known Binary ) format . The ` unpack ` function able extract information provided proper format specifier . The following code test script successfully extracted desired information . Please note ` format ` unpack differs slightly reference . This WKB string n't retrieved MySQL ` AsWKB ( ) ` function , [ contains extra padding ] ( https : //bugs.mysql.com/bug.php ? id=69798 ) . < ? php $ padding = 0 ; $ order = 1 ; $ gtype = 1 ; $ lon = -73.91353 ; $ lat = 42.80611 ; $ bindata = pack ( 'LcLd2 ' , $ padding , $ order , $ gtype , $ lat , $ lon ) ; printf ( `` Packed : % s\n\n '' , bin2hex ( $ bindata ) ) ; $ result = unpack ( 'Lpadding/corder/Lgtype/dlatitude/dlongitude ' , $ bindata ) ; var_dump ( $ result ) ; ? > References [ MySQL ] ( http : //dev.mysql.com/doc/refman/5.7/en/gis-data- formats.html # gis-wkb-format ) [ ESRI Network ] ( http : //edndoc.esri.com/arcsde/9.0/general_topics/wkb_representation.htm ) ."
Stackoverflow,"I 'm aware [ ] ( https : //stackoverflow.com/questions/1707531/get-max-min- in-one-line-with-linq ) question , I would like obtain something close generated SQL : select MAX ( Column ) , MIN ( Column ) Table WHERE Id = 1 When I try : var query = db.Table d.Id == 1 select new { min = db.Table.Max ( = > s.Column ) , max = db.Table.Min ( = > s.Column ) } ; The generated sql looks like : SELECT [ Extent1 ] . [ Id ] AS [ Id ] , [ GroupBy1 ] . [ A1 ] AS [ C1 ] , [ GroupBy2 ] . [ A1 ] AS [ C2 ] FROM [ dbo ] . [ Table ] AS [ Extent1 ] CROSS JOIN ( SELECT MAX ( [ Extent2 ] . [ Column ] ) AS [ A1 ] FROM [ dbo ] . [ Table ] AS [ Extent2 ] ) AS [ GroupBy1 ] CROSS JOIN ( SELECT MIN ( [ Extent3 ] . [ Column ] ) AS [ A1 ] FROM [ dbo ] . [ Table ] AS [ Extent3 ] ) AS [ GroupBy2 ] WHERE ( [ Extent1 ] . [ Id ] = 1 ) AND ( 1 IS NOT NULL ) I also tried : var query = db.Table d.Id == 1 group d.Id grp let min = grp.Min ( = > s.Column ) let max = grp.Max ( = > s.Column ) select new { min , max } ; Which gives : SELECT [ Project2 ] . [ Id ] AS [ Id ] , [ Project2 ] . [ C1 ] AS [ C1 ] , [ Project2 ] . [ C2 ] AS [ C2 ] FROM ( SELECT [ Project1 ] . [ C1 ] AS [ C1 ] , [ Project1 ] . [ Id ] AS [ Id ] , ( SELECT MAX ( [ Extent2 ] . [ Column ] ) AS [ A1 ] FROM [ dbo ] . [ Table ] AS [ Extent2 ] WHERE ( [ Extent2 ] . [ Id ] = 1 ) AND ( 1 IS NOT NULL ) AND ( [ Project1 ] . [ Id ] = [ Extent2 ] . [ Id ] ) ) AS [ C2 ] FROM ( SELECT [ GroupBy1 ] . [ A1 ] AS [ C1 ] , [ GroupBy1 ] . [ K1 ] AS [ Id ] FROM ( SELECT [ Extent1 ] . [ Id ] AS [ K1 ] , MIN ( [ Extent1 ] . [ Column ] ) AS [ A1 ] FROM [ dbo ] . [ Table ] AS [ Extent1 ] WHERE ( [ Extent1 ] . [ Id ] = 16 ) AND ( 16 IS NOT NULL ) GROUP BY [ Extent1 ] . [ Id ] ) AS [ GroupBy1 ] ) AS [ Project1 ] ) AS [ Project2 ] They work , performance hit probably negligible , 's mostly aesthetic : The two generated queries hurt eyes .","You view generated SQL IQueryable using ` .ToString ( ) ` , e.g . var query = context.People.Where ( x = > x.DomainId == 1 ) ; Console.WriteLine ( query.ToString ( ) ) ;"
Stackoverflow,I using sails js **sails-mssqlserver** adapter . The problem stored procedure returns multiple result sets I receive one result set latest . The stored procedure working fine Java I get iterate relevant result sets . I need know specific way access result sets sails-mssqlserver ?,"The sails-mssqlserver adapter wrapper official Microsoft SQL Server client Node.js available [ ] ( https : //www.npmjs.com/package/mssql # query-command-callback ) dependecy however latest release . **Option 1 : ** As per official documentation MsSQL package , enable multiple recordsets queries request.multiple = true command . To enable multiple queries/recordsets sails-mssqlserver adapter , hackish workaround open sails-mssqlserver/lib/adapter.js edit raw query function . Adding request.multiple = true var request = new mssql.Request ( mssqlConnect ) . As shown example . // Raw Query Interface query : function ( connection , collection , query , data , cb ) { ( _.isFunction ( data ) ) { ( debugging ) { console.log ( 'Data function . A cb passed back ' ) } cb = data data = null } adapter.connectConnection ( connection , function __FIND__ ( err , uniqId ) { ( err ) { console.error ( 'Error inside query __FIND__ ' , err ) return cb ( err ) } uniqId = uniqId || false var mssqlConnect ( ! uniqId ) { mssqlConnect = connections [ connection ] .mssqlConnection } else { mssqlConnect = connections [ connection ] .mssqlConnection [ uniqId ] } var request = new mssql.Request ( mssqlConnect ) // Add request.multiple = true request.query ( query , function ( err , recordset ) { ( err ) return cb ( err ) ( connections [ connection ] & & ! connections [ connection ] .persistent ) { mssqlConnect & & mssqlConnect.close ( ) } cb ( null , recordset ) } ) } ) } , Now returned recordset contain multiple results . **Option 2 : ** A sustainable option use cases running stored procedure returns multiple recordsets , use latest version official Microsoft SQL Server client Node.js . Information running stored procedures available [ ] ( https : //www.npmjs.com/package/mssql # execute-procedure-callback ) First install latest package : npm install mssql -- save In code would like run stored procedure add connection mssql database : // require mssql package const sql = require ( 'mssql ' ) // make connection , use values already stored adapter const pool = new sql.ConnectionPool ( { user : sails.config.connections. < yourMsSQLConnection > .user , password : sails.config.connections. < yourMsSQLConnection > .password , server : sails.config.connections. < yourMsSQLConnection > .server , database : sails.config.connections. < yourMsSQLConnection > .database } ) // connect pool test error pool.connect ( err = > { // ... } ) // run stored procedure using request const request = new sql.Request ( ) request.execute ( 'procedure_name ' , ( err , result ) = > { // ... error checks console.log ( result.recordsets.length ) // count recordsets returned procedure console.log ( result.recordsets [ 0 ] .length ) // count rows contained first recordset console.log ( result.recordset ) // first recordset result.recordsets console.log ( result.returnValue ) // procedure return value console.log ( result.output ) // key/value collection output values console.log ( result.rowsAffected ) // array numbers , number represents number rows affected executed statemens // ... } ) // close pool using pool.close ( ) In cases , sails-* database adapter n't include functionality require . I find best create sails Service wraps additional functionality . It really clean solution ."
Stackoverflow,"I getting weird behavior Pro-C procedure shown : # define BGHCPY_TO_ORA ( dest , source ) \ { \ ( void ) strcpy ( ( void* ) ( dest ) .arr , ( void* ) ( source ) ) ; \ ( dest ) .len = strlen ( ( const char * ) ( dest ) .arr ) ; \ } # define BGHCPY_FROM_ORA ( dest , source ) \ { \ ( void ) memcpy ( ( void* ) ( dest ) , ( void* ) ( source ) .arr , ( size_t ) ( source ) .len ) ; \ ( dest ) [ ( source ) .len ] = '\0 ' ; \ } long fnSQLMarkProcessed ( char *pszRowId , char *pszMarker ) { BGHCPY_TO_ORA ( O_rowid_stack , pszRowId ) ; BGHCPY_TO_ORA ( O_cust_processed , pszMarker ) ; EXEC SQL UPDATE document_all SET processed_by_bgh = : O_cust_processed WHERE rowid = : O_rowid_stack ; return ( sqlca.sqlcode ) ; } The input arguments values passed function pszRowId = [ AAAF1lAAIAABOoRAAB ] , pszMarker=X The query return error code:02115 following message : SQL Error:02115 Code interpretation problem -- check COMMON_NAME usage I using Oracle backend database . **Can anyone provide information possible causes failed query ? ** Any help highly appreciated . Flags used PRO-C Compilation defined : -- -- -- /u01/app/oracle/product/8.1.6/ORACLE_HOME/bin/proc ` echo -Dbscs5 -Dsun5 -I/export/home/bscsobw/bscs6/src/CoreDumpIssue/final_Code_Fix_004641 -DNDEBUG -DSunOS53 -D_POSIX_4SOURCES -I/usr/generic++/generic++2.5.3.64_bit/include -DFEATURE_212298 -DBSCS_CONFIG -I/export/home/bscsobw/bscs6//src/bat/include -DFEATURE_00203808_GMD -DFEATURE_00241737 -DORACLE_DB_BRAND -I/u01/app/oracle/product/8.1.6/ORACLE_HOME/rdbms/demo -I/u01/app/oracle/product/8.1.6/ORACLE_HOME/precomp/public -I/export/home/bscsobw/bscs6/src/CoreDumpIssue/final_Code_Fix_004641/include -I../bat/include -DFEATURE61717 -DFEATURE52824 -DFEATURE56178 -DD236312_d -DSDP -g | sed -e 's/-I/INCLUDE=/g ' -e 's/-D [ ^ ] = [ ^ ] *//g ' -e 's/-D\ ( [ ^ ] *\ ) /DEFINE=\1/g ' ` select_error=no DEFINE=FEATURE61717 DEFINE=FEATURE52824 DEFINE=FEATURE56178 \ lines=yes iname=bgh_esql.pc oname=bgh_esql.c lname=bgh_esql.lis","Embedded SQL one popular way SQL C '' old days '' ( C++ yet invented ) . These days mostly 'll using ORM library . It recommended embedded SQL , put well , depends proprietary pre-processor makes code difficult debug , manage , maintain . It also _hooks_ one single database vendor code extremely difficult move another database backend . Generally , n't '' real life '' . But class , prof probably interested teaching SQL database concepts . Embedded SQL tool . You 're supposed learn SQL databases , embedded SQL C++ . However , I believe 're missing point asking PHP Java . Not mention PHP scripting language , Java another language ( potentially ) write processor embedded SQL . So point embedded SQL really nothing _language_ choices . It tradeoffs balance ( 1 ) proprietary embedded system preprocessor , ( 2 ) using ORM library , data-access library ( e.g . ODBC ) . _Off-Topic : _ I first started using embedded SQL I College ( 30 years ago ! ) . Actually got programming jobs College still used , obviously way . Never seen used ever since 1990 ."
Stackoverflow,"Is way call Backup-SqlDatabase cmdlet connect SQL Server 2012 SQL Server credentials ? The command I currently using : Backup-SqlDatabase -ServerInstance $ serverName -Database $ sqldbname -BackupFile `` $ ( $ backupFolder ) $ ( $ dbname ) _db_ $ ( $ addinionToName ) .bak '' But relies user , call , default , Windows Authentication used .","The backup-sqldatabase cmdlet supports Credential parameter . If look help cmdlet 's even example ( help backup- sqldatabase -full ) : -- -- -- -- -- -- -- -- -- -- -- -- -- EXAMPLE 4 -- -- -- -- -- -- -- -- -- -- -- -- -- C : \PS > Backup-SqlDatabase -ServerInstance Computer\Instance -Database MyDB -Credential ( Get-Credential sa ) Description -- -- -- -- -- - This command creates complete database backup database 'MyDB ' , using sa SQL Server credential . This co mmand prompt password complete SQL Server authentication ."
Stackoverflow,"I 'm looking way multiple row inserts I 'm inserting data single column . Here example table : + -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | Field | Type | Null | Key | Default | Extra | + -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + | id | tinyint ( 4 ) | NO | PRI | NULL | auto_increment | | name | varchar ( 40 ) | NO | UNI | NULL | | + -- -- -- -+ -- -- -- -- -- -- -+ -- -- -- + -- -- -+ -- -- -- -- -+ -- -- -- -- -- -- -- -- + I want able insert something like ( 'admin ' , 'author ' , 'mod ' , 'user ' , 'guest ' ) name column row . The MySQL documentation shows multiple inserts format : INSERT INTO tbl_name ( , b , c ) VALUES ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) ; However statement ends looking like : INSERT INTO User_Role ( name ) VALUES ( 'admin ' , 'author ' , 'mod ' , 'user ' , 'guest ' ) ; And I get following : ERROR 1136 ( 21S01 ) : Column count n't match value count row 1 Meaning thinks I 'm trying single row insert . I 'm sure I 'm missing something simple , I n't see anything particular MySQL docs use case .","syntax bit . put parentheses around data `` set '' ( meaning single value case ) trying insert . INSERT INTO User_Roll ( name ) VALUES ( 'admin ' ) , ( 'author ' ) , ( 'mod ' ) , ( 'user ' ) , ( 'guest ' ) ;"
Stackoverflow,"Note : SQLite , although I expect problem Qt side . First , I set database table SQLite command line tool : sqlite > create table testtable ( id INTEGER PRIMARY KEY NOT NULL , state INTEGER ) ; sqlite > insert testtable ( state ) values ( 0 ) ; sqlite > insert testtable ( state ) values ( 1 ) ; sqlite > insert testtable ( state ) values ( 9 ) ; sqlite > insert testtable ( state ) values ( 20 ) ; Then I test query : sqlite > SELECT id , state FROM testtable WHERE state IN ( 0,1,2 ) ; 1|0 3|1 ( Those expected results . ) Then I run C++ code : void runQuery ( ) { QSqlQuery qq ; qq.prepare ( `` SELECT id , state FROM testtable WHERE state IN ( : states ) '' ) ; QList < QVariant > statesList = QList < QVariant > ( ) ; statesList.append ( 0 ) ; statesList.append ( 1 ) ; statesList.append ( 2 ) ; qq.bindValue ( `` : states '' , statesList ) ; qq.exec ( ) ; qDebug ( ) < < `` '' ; ( qq.next ( ) ) { qDebug ( ) < < qq.value ( 0 ) .toInt ( ) < < qq.value ( 1 ) .toInt ( ) ; } qDebug ( ) < < `` '' ; } prints : > > No rows printed . I assume I ca n't bind list directly placeholder `` '' clause . But way ? I n't able find anything .","` QSqlDriver ` supports notifications emit signal specific event occurred . To subscribe event use ` QSqlDriver : :subscribeToNotification ( const QString & name ) ` . When event ’ subscribing posted database driver emit notification ( ) signal application take appropriate action . db.driver ( ) - > subscribeToNotification ( `` someEventId '' ) ; The message posted automatically trigger stored procedure . The message lightweight : nothing string containing name event occurred . You connect ` notification ( const QString & ) ` signal slot like : QObject : :connect ( db.driver ( ) , SIGNAL ( notification ( const QString & ) ) , , SLOT ( refreshView ( ) ) ) ; I note feature supported MySQL event posting mechanism ."
Stackoverflow,"I working WinRT app . I want use ` sqlite-net-extensions ` support ` OneToMany ` , ` ManyToMany ` . using SQLiteNetExtensions.Attributes ; using SQLite ; [ Table ( `` WorkFlow '' ) ] public class Workflow { [ PrimaryKey , AutoIncrement ] public int WorkflowId { get ; set ; } public string Name { get ; set ; } public int Revision { get ; set ; } [ OneToMany ] public List < Step > Steps { get ; set ; } } [ Table ( `` Step '' ) ] public class Step { public string Type { get ; set ; } public string Description { get ; set ; } [ ManyToOne ] public Workflow Workflow { get ; set ; } } When I try generate tables database , raises exception : > An exception type 'System.NotSupportedException ' occurred app_name.exe handled user code Additional information : Do n't know System.Collections.Generic.List ` 1 [ app_name.Model.modelName ] This coming ` SqlType ` ` SQLite.cs ` . But example ` sqlite-net-extensions ` [ homepage ] ( https : //bitbucket.org/twincoders/sqlite-net-extensions ) , ` List ` property work fine . This copy example : public class Stock { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ MaxLength ( 8 ) ] public string Symbol { get ; set ; } [ OneToMany ] // One many relationship Valuation public List < Valuation > Valuations { get ; set ; } } public class Valuation { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ ForeignKey ( typeof ( Stock ) ) ] // Specify foreign key public int StockId { get ; set ; } public DateTime Time { get ; set ; } public decimal Price { get ; set ; } [ ManyToOne ] // Many one relationship Stock public Stock Stock { get ; set ; } } Can anyone give suggestions solve problem ? Thanks .","Yes , explicitly declare foreign keys inverse properties relationship attribute , otherwise library may get wrong foreign key relationship . public class ClassA { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ OneToMany ( `` O2MClassAKey '' , `` BObjectsInverse '' ) ] public List < ClassB > BObjects { get ; set ; } [ OneToOne ( `` O2OClassAKey '' , `` BObjectInverse '' ) ] public ClassB BObject { get ; set ; } // Other properties public string Bar { get ; set ; } } public class ClassB { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ ForeignKey ( typeof ( ClassA ) ) ] public int O2MClassAKey { get ; set ; } [ ForeignKey ( typeof ( ClassA ) ) ] public int O2OClassAKey { get ; set ; } // Inverse relationships , optional [ ManyToOne ( `` O2MClassAKey '' , `` BObjects '' ) ] public ClassA BObjectsInverse { get ; set ; } [ OneToOne ( `` O2OClassAKey '' , `` BObject '' ) ] public ClassA BObjectInverse { get ; set ; } // Other properties public string Foo { get ; set ; } } Please note foreign key ` O2OClassAKey ` ` OneToOne ` relationship declared classes . If n't need inverse properties , skip relationship attribute : public class ClassA { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ OneToMany ( `` O2MClassAKey '' ) ] public List < ClassB > BObjects { get ; set ; } [ OneToOne ( `` O2OClassAKey '' ) ] public ClassB BObject { get ; set ; } // Other properties public string Bar { get ; set ; } } public class ClassB { [ PrimaryKey , AutoIncrement ] public int Id { get ; set ; } [ ForeignKey ( typeof ( ClassA ) ) ] public int O2MClassAKey { get ; set ; } [ ForeignKey ( typeof ( ClassA ) ) ] public int O2OClassAKey { get ; set ; } // Other properties public string Foo { get ; set ; } }"
Stackoverflow,"I want implement NDB Cluster MySQL Cluster 6 . I want huge data structure minimum 2 million records . I want know limitations implementing NDB cluster . For example , RAM size , number databases , size database NDB cluster .","2 million databases ? I asssume meant `` rows '' . Anyway , concerning limitations : one important things keep mind NDB/MySQL Cluster general purpose database . Most notably , join operations , also subqueries range opertions ( queries like : orders created week ago ) , considerably slower might expect . This part due fact data distributed across multiple nodes . Although improvements made , Join performance still disappointing . On hand , need deal many ( preferably small ) concurrent transactions ( typically single row updates/inserts/delete lookups primary key ) mangage keep data memory , scalable performant solution . You ask want cluster . If simply want ordinary database , except added 99,999 % availability , may disappointed . Certainly MySQL cluster provide great availability uptime , workload app may well suited thtings cluster good . Plus may able use another high availability solution increase uptime otherwise traditional database . BTW - 's list limitations per doc : < http : //dev.mysql.com/doc/refman/5.1/en/mysql-cluster-limitations.html > But whatever , try cluster , see good . MySQL cluster `` MySQL + 5 nines '' . You 'll find try ."
Stackoverflow,"I problem using mySQL . This error pops : ` Invalid use null value ` I trying make two attributes table primary key ; code : alter table contact_info add primary key ( phone_number , contactID ) ; Here alter statements I put contact_info table : alter table contact_info add contactID varchar ( 10 ) ; alter table contact_info add suffixID varchar ( 8 ) ; alter table contact_info add titleID varchar ( 12 ) ; alter table contact_info add companyID varchar ( 12 ) ; alter table contact_info add phonetypeID char ( 2 ) ; Does anybody know 's wrong ? Thanks advance .","Look contact_info null value phone_number contactID . You ca n't add primary key existing null values table . select * contact_info ( phone_number null contactID null ) Run SQL find records values null . Update records , try applying primary key . I 'm sure , may want **back data first ! ! ! ** running updates . Here update might able use set contactID : update contact_info set contactID = ( select max ( contactID ) contact_info ) + 1 contactID null update contact_info set phone_number = '555-1212' phone_number null If duplicates data , need find update well . Here 's find duplicates : -- Assuming phone_number duplicate ( 2 people living house phone number ) select a.phone_number , count ( 1 ) contact_info group a.phone_number count ( 1 ) > 1"
Stackoverflow,"I 'm attempting use cloud sql proxy connect 2 different cloud sql instances ... In docs I found line ` Use -instances parameter . For multiple instances , use comma-separated list. ` sure make look . < https : //cloud.google.com/sql/docs/sql-proxy > . I 'm using Google Container engine , single CloudSQL instance works great : - name : cloudsql-proxy image : b.gcr.io/cloudsql-docker/gce-proxy:1.05 command : [ `` /cloud_sql_proxy '' , `` -- dir=/cloudsql '' , `` -instances=starchup-147119 : us-central1 : first-db=tcp:3306 '' , `` -credential_file=/secrets/cloudsql/credentials.json '' ] volumeMounts : - name : cloudsql-oauth-credentials mountPath : /secrets/cloudsql readOnly : true - name : ssl-certs mountPath : /etc/ssl/certs But multiple I 've tried ` -instances ` section : -instances=starchup-147119 : us-central1 : first-db , starchup-147119 : us-central1 : second-db=tcp:3306 -instances=starchup-147119 : us-central1 : first-db=tcp:3306 , starchup-147119 : us-central1 : second-db=tcp:3306 give various errors ; ` ECONNREFUSED 127.0.0.1:3306 ` , ` ER_DBACCESS_DENIED_ERROR ` , ` ER_ACCESS_DENIED_ERROR ` Any help much appreciated !","You two databases hosted TCP port . Instead , specify ports database comma-separated list : -instances=project : region : db=tcp:3306 , project : region : db-2=tcp:3307 I used 3306 3307 , use ports want ! Make sure rest Container Engine config allows communication nodes ports ( maybe 's true default , I n't use GKE ) . Most mysql drivers connect port 3306 default way specify another port . You 'll arrange code connect different port choose second database ."
Stackoverflow,"I 'd like switch PDO INSERT UPDATE prepared statements INSERT ON DUPLICATE KEY UPDATE since I think 'll lot efficient I'm currently , I 'm trouble figuring correct syntax use named placeholders bindParam . I found several similar question SO , I 'm new PDO successfully adapt code criteria . This I 've tried , n't work ( n't insert update ) : try { $ stmt = $ conn- > prepare ( 'INSERT INTO customer_info ( user_id , fname , lname ) VALUES ( : user_id , : fname , : lname ) ' 'ON DUPLICATE KEY UPDATE customer_info SET fname= : fname , lname= : lname WHERE user_id = : user_id ' ) ; $ stmt- > bindParam ( ' : user_id ' , $ user_id ) ; $ stmt- > bindParam ( ' : fname ' , $ _POST [ 'fname ' ] , PDO : :PARAM_STR ) ; $ stmt- > bindParam ( ' : lname ' , $ _POST [ 'lname ' ] , PDO : :PARAM_STR ) ; $ stmt- > execute ( ) ; } This simplified version code ( I several queries , query 20 - 50 fields ) . I 'm currently updating first checking number rows updated greater 0 running Insert , queries 's set bindParam statements .","It 's parameter query , n't supply value MySQL . $ insert = $ mysqli- > prepare ( `` INSERT INTO posts ( post_name , publish_date ) VALUES ( ? , NOW ( ) ) '' ) ;"
Stackoverflow,"I 'm beginning incorporate Alembic project already uses SQLAlchemy table definitions . At present DB schema managed external application , I want bring entire schema table definitions file . In PostgreSQL I use custom domain storing email addresses . The PostgreSQL DDL : CREATE DOMAIN email_address TEXT CHECK ( value ~ '.+ @ .+ ' ) How I represent creation domain , usage column data type , SQLAlchemy ?","Domain type based basic `` buildin '' type always . This type specifies binary format . So domain based `` text '' type , store data text ."
Stackoverflow,"I want save data system table user_tab_cols , temp table I take dump . There 100,000 rows , I select user_tab_cols 1,000 records save temp table query : create table temp table select * user_tab_cols condition ... I error 'illegal use longtype ' , column DATA_DEFAULT contain type long . Is alterantive way I store long type anotehr table ?","> ORA-00997 : illegal use LONG datatype It **restriction** usage **LONG** data type . _You create object type LONG attribute._ SQL > CREATE TABLE AS SELECT data_default FROM user_tab_cols ; CREATE TABLE AS SELECT data_default FROM user_tab_cols * ERROR line 1 : ORA-00997 : illegal use LONG datatype SQL > Alternatively , could use **TO_LOB** workaround . Which would convert CLOB data type . For example , SQL > CREATE TABLE AS SELECT TO_LOB ( data_default ) data_default FROM user_tab_cols ; Table created . SQL > desc ; Name Null ? Type -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- DATA_DEFAULT CLOB SQL > See examples workarounds [ ] ( http : //www.oracle- developer.net/display.php ? id=430 ) ."
Stackoverflow,"I want add column table using ` ALTER TABLE ` ` UPDATE ` statements recreate full table . When using subquery ` UPDATE ` statement I n't get output I expect . **build reproducible data** library ( dplyr ) library ( dbplyr ) library ( DBI ) con < - DBI : :dbConnect ( RSQLite : :SQLite ( ) , path = `` : memory : '' ) copy_to ( con , iris [ c ( 1,2,51 ) , ] , '' iris '' ) tbl ( con , '' iris '' ) # # Source : table < iris > [ ? ? x 5 ] # # Database : sqlite 3.19.3 [ ] # Sepal.Length Sepal.Width Petal.Length Petal.Width Species # < dbl > < dbl > < dbl > < dbl > < chr > # 1 5.1 3.5 1.4 0.2 setosa # 2 4.9 3.0 1.4 0.2 setosa # 3 7.0 3.2 4.7 1.4 versicolor **create new column separate table** DBI : :dbSendQuery ( con , `` CREATE TABLE new_table AS SELECT t2.new_col iris t1 inner join ( SELECT Species , sum ( ` Sepal.Width ` ) new_col FROM iris GROUP BY Species ) t2 t1.Species = t2.Species '' ) tbl ( con , '' new_table '' ) # # Source : table < new_table > [ ? ? x 1 ] # # Database : sqlite 3.19.3 [ ] # new_col # < dbl > # 1 6.5 # 2 6.5 # 3 3.2 **create new column old table** DBI : :dbSendQuery ( con , `` ALTER TABLE iris ADD COLUMN new_col DOUBLE '' ) **try plug new column ` new_table ` there** DBI : :dbSendQuery ( con , `` UPDATE iris SET new_col = ( SELECT new_col FROM new_table ) '' ) tbl ( con , '' iris '' ) # # Source : table < iris > [ ? ? x 6 ] # # Database : sqlite 3.19.3 [ ] # Sepal.Length Sepal.Width Petal.Length Petal.Width Species new_col # < dbl > < dbl > < dbl > < dbl > < chr > < dbl > # 1 5.1 3.5 1.4 0.2 setosa 6.5 # 2 4.9 3.0 1.4 0.2 setosa 6.5 # 3 7.0 3.2 4.7 1.4 versicolor 6.5 As see ` new_col ` contains value ` 6.5 ` I expected ` 3.2 ` last row . How I fix ?","SQLite see string passed query , something like sqlcmd < - paste ( `` SELECT * FROM annual WHERE fiscal= '' , fiscal.year , sep= '' '' ) data.annual < - dbGetQuery ( db , sqlcmd ) The nice thing use usual way unwind loops . Forgetting second ram restrictions , conceptually years < - seq ( 2000,2010 ) data < - lapply ( years , function ( ) { dbGetQuery ( db , paste ( `` SELECT * FROM annual WHERE fiscal= '' , , sep= '' '' ) } data list containing yearly data sets . Or could keep data , run regression store summary object ."
Stackoverflow,"I trying re-create database ( MyDB ) one SQL server ( **Source** ) another one ( **Target** ) . **Source** located local machine SQL Server 2014 . **Target** located remote machine 's SQL Server 2012 . Here steps I 've taken : 1 . On local machine I go SQL Server Management studio , I right click MyDB go Tasks -- > Generate Scripts . 2 . There I select `` Script entire database database objects '' . 3 . I click Next next page , Advanced , I select `` Schema data '' . 4 . That generates SQL file ( scripts.sql ) contains definition MyDB . 5 . Then I use following osql command re-create database **Target** : > osql -S target -d master -E -i scripts.sql -o output.log 6 . After execution finished I get error log file `` output.log '' : > 1 > 2 > 1 > 2 > 3 > 4 > 5 > 6 > 7 > 8 > Msg 5133 , Level 16 , State 1 , Server Target , Line 2 Directory lookup file `` C : \Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB.mdf '' failed operating system error 3 ( The system find path specified. ) . Msg 1802 , Level 16 , State 1 , Server Target , Line 2 CREATE DATABASE failed . Some file names listed could created . Check related errors . 1 > 2 > Msg 5011 , Level 14 , State 5 , Server Target , Line 1 User permission alter database 'MyDB ' , database exist , database state allows access checks . Msg 5069 , Level 16 , State 1 , Server Target , Line 1 ALTER DATABASE statement failed . Here first lines `` scripts.sql '' : USE [ master ] GO /****** Object : Database [ MyDB ] Script Date : 4/12/2016 4:30:20 PM ******/ CREATE DATABASE [ MyDB ] CONTAINMENT = NONE ON PRIMARY ( NAME = N'MyDB ' , FILENAME = N ' C : \Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB.mdf ' , SIZE = 513024KB , MAXSIZE = UNLIMITED , FILEGROWTH = 262144KB ) LOG ON ( NAME = N'MyDB_log ' , FILENAME = N ' C : \Program Files\Microsoft SQL Server\MSSQL12.MSSQLSERVER\MSSQL\DATA\MyDB_log.ldf ' , SIZE = 1317504KB , MAXSIZE = 2048GB , FILEGROWTH = 131072KB ) GO ALTER DATABASE [ MyDB ] SET COMPATIBILITY_LEVEL = 100 GO IF ( 1 = FULLTEXTSERVICEPROPERTY ( 'IsFullTextInstalled ' ) ) begin EXEC [ MyDB ] . [ dbo ] . [ sp_fulltext_database ] @ action = 'enable' end GO ALTER DATABASE [ MyDB ] SET ANSI_NULL_DEFAULT ON I file MyDB.mdf location 's complaining **Source** , **Target** . There directory '' MSSQL12.MSSQLSERVER '' **Target** . How I fix ?","For interested solution , problem `` MSSQL12.MSSQLSERVER '' directory **Target** 's different version SQL Server , namely 2012 . What I create directory manually started working ."
Stackoverflow,I trying migrate postgresql database mysql following tutorial : < http : //mysqlworkbench.org/2012/11/how-to-migrate-postgresql-databases-to- mysql-using-the-mysql-workbench-migration-wizard/ > I experiencing error I try test connection > Could connect Source DBMS [ IM002 ] [ Microsoft ] [ ODBC Driver Manager ] Data soure name found default driver specified ( 0 ) ( SQLDriverConnect ) ODBC connection string Driver=psqlodc ; SERVER=127.0.0.1 ; PORT=5432 ; DATA ... Does anyone know correct error ?,In case I using Windows 64bit using `` PostgreSQL ANSI ( x64 ) '' connect Postgres database gave errors migrating database Postgres Mysql . I used driver `` PostgreSQL Unicode ( x64 ) '' worked . If data contains unicode characters i.e . non ASCII characters use `` PostgreSQL Unicode ( x64 ) '' drivers . [ ! [ enter image description ] ( https : //i.stack.imgur.com/BMxpl.png ) ] ( https : //i.stack.imgur.com/BMxpl.png )
Stackoverflow,"I created procedure like : CREATE OR REPLACE FUNCTION insert_user_ax_register ( user_name character varying ( 50 ) , password character varying ( 300 ) , role_id character varying ( 10 ) , created_dt date , status boolean , email character varying ( 50 ) , join_date character varying ( 30 ) , phone_no bigint , client_address character varying ( 200 ) , full_name character varying ( 100 ) , financial_year character varying ( 10 ) ) RETURNS void AS $ BODY $ declare begin INSERT INTO ax_register ( user_name , password , role_id , created_dt , status , email , join_date , phone_no , client_address , full_name , financial_year ) VALUES ( user_name , password , role_id , ( ) , true , email , join_date , phone_no , client_address , full_name , financial_year ) ; end $ BODY $ LANGUAGE plpgsql VOLATILE tried execute like : SELECT * insert_user_ax_register ( 'debasrita ' , 'debasrita ' , 'client001 ' , ( ) , 't ' , 'abc @ gmail.com ' , ( ) , 'ctc ' , 'debasrita ' , '2014-15',9090909090 ) ; throws following error : ERROR : function insert_user_ax_register ( unknown , unknown , unknown , timestamp time zone , unknown , unknown , timestamp time zone , unknown , unknown , unknown , bigint ) exist SQL state : 42883 Hint : No function matches given name argument types . You might need add explicit type casts . Character : 16 Please help . I new pgsql able find solution google . I using ` pgsql 9.1.3 ` May I know correct way achieve objective ?","The error message tells need looking : > _ '' No function matches given name **and argument types** `` _ As function _name_ seems correct , parameters passing . So write value passed parameter : 'debasrita ' -- > user_name character varying ( 50 ) 'debasrita ' -- > password character varying ( 300 ) 'client001 ' -- > role_id character varying ( 10 ) created_dt date -- > ( ) status boolean , -- > 't' email varchar ( 50 ) -- > 'abc @ gmail.com' join_date varchar ( 30 ) -- > ( ) ** < < first error : ( ) character constant** phone_no bigint -- > 'ctc ' ** < < second error : 'ctc ' bigint** client_address varchar ( 200 ) -- > 'debasrita' full_name varchar ( 100 ) -- > '2014-15' financial_year varchar ( 10 ) -- > 9090909090 ** < < third error : 9090909090 character literal ** So need either adjust parameter types , e.g . define ` join_date ` date , ` varchar ` adjust values pass parameter . And finally need call function like : SELECT insert_user_ax_register ( ... ) ; rather ` select * ... `"
Stackoverflow,"I asked question days ago ( [ Access SQL Server 2005 non- domain machine using Windows authentication ] ( https : //stackoverflow.com/questions/2795723/access-to-sql- server-2005-from-a-non-domain-machine-using-windows-authentication ) ) got interesting , usable suggestions . I 'd like ask question , make clear constraints : I Windows domain within machine running SQL Server 2005 configured support Windows authentication . I would like run C # client application machine network , NOT domain , access database SQL Server 2005 instance . I CAN NOT create modify OS SQL Server users either machine , I CAN NOT make changes permissions impersonation , I CAN NOT make use runas . I know I write Perl Java applications connect SQL Server database using four parameters : server name , database name , username ( form domain\user ) , password . In C # I tried various things around : string connectionString = `` Data Source=server ; Initial Catalog=database ; User Id=domain\user ; Password=password '' ; SqlConnection connection = new SqlConnection ( connectionString ) ; connection.Open ( ) ; tried setting integrated security true false , nothing seems work . Is I trying simply impossible C # ? Thanks help , Martin","I similar problem I writing tool needed run machine one domain authenticate SQL server another domain using trusted connection . Everything I could find subject said could n't done . Instead must join domain , use SQL authentication , get involved chap called Kerberos , get network guys setup trusted relationship , name alternatives . The thing I knew I could get working way using RUNAS I'd proven SSMS : C : \WINDOWS\system32\runas.exe /netonly /savecred /user : megacorp\joe.bloggs `` C : \Program Files\Microsoft SQL Server\90\Tools\Binn\VSShell\Common7\IDE\SqlWb.exe '' The /netonly flag allowed execute exe local credentials access network remote credentials , I think , anyway I got result set I expected remote server . The problem runas command made difficult debug application , n't smell good . Eventually I found article [ code project ] ( http : //www.codeproject.com/KB/cs/cpimpersonation1.aspx ) talking authenticating manipulate Active Directory , Here main class impersonation : using System ; using System.Runtime.InteropServices ; // DllImport using System.Security.Principal ; // WindowsImpersonationContext namespace TestApp { class Impersonator { // group type enum enum SECURITY_IMPERSONATION_LEVEL : int { SecurityAnonymous = 0 , SecurityIdentification = 1 , SecurityImpersonation = 2 , SecurityDelegation = 3 } // obtains user token [ DllImport ( `` advapi32.dll '' , SetLastError = true ) ] static extern bool LogonUser ( string pszUsername , string pszDomain , string pszPassword , int dwLogonType , int dwLogonProvider , ref IntPtr phToken ) ; // closes open handes returned LogonUser [ DllImport ( `` kernel32.dll '' , CharSet = CharSet.Auto ) ] extern static bool CloseHandle ( IntPtr handle ) ; // creates duplicate token handle [ DllImport ( `` advapi32.dll '' , CharSet = CharSet.Auto , SetLastError = true ) ] extern static bool DuplicateToken ( IntPtr ExistingTokenHandle , int SECURITY_IMPERSONATION_LEVEL , ref IntPtr DuplicateTokenHandle ) ; WindowsImpersonationContext newUser ; /// /// Attempts impersonate user . If successful , returns /// WindowsImpersonationContext new users identity . /// /// Username want impersonate /// Logon domain /// User 's password logon /// public Impersonator ( string sUsername , string sDomain , string sPassword ) { // initialize tokens IntPtr pExistingTokenHandle = new IntPtr ( 0 ) ; IntPtr pDuplicateTokenHandle = new IntPtr ( 0 ) ; pExistingTokenHandle = IntPtr.Zero ; pDuplicateTokenHandle = IntPtr.Zero ; // domain name blank , assume local machine ( sDomain == `` '' ) sDomain = System.Environment.MachineName ; try { const int LOGON32_PROVIDER_DEFAULT = 0 ; // create token // const int LOGON32_LOGON_INTERACTIVE = 2 ; const int LOGON32_LOGON_NEW_CREDENTIALS = 9 ; //const int SecurityImpersonation = 2 ; // get handle token bool bImpersonated = LogonUser ( sUsername , sDomain , sPassword , LOGON32_LOGON_NEW_CREDENTIALS , LOGON32_PROVIDER_DEFAULT , ref pExistingTokenHandle ) ; // impersonation fail ? ( false == bImpersonated ) { int nErrorCode = Marshal.GetLastWin32Error ( ) ; // show reason LogonUser failed throw new ApplicationException ( `` LogonUser ( ) failed error code : `` + nErrorCode ) ; } bool bRetVal = DuplicateToken ( pExistingTokenHandle , ( int ) SECURITY_IMPERSONATION_LEVEL.SecurityImpersonation , ref pDuplicateTokenHandle ) ; // DuplicateToken fail ? ( false == bRetVal ) { int nErrorCode = Marshal.GetLastWin32Error ( ) ; CloseHandle ( pExistingTokenHandle ) ; // close existing handle // show reason DuplicateToken failed throw new ApplicationException ( `` DuplicateToken ( ) failed error code : `` + nErrorCode ) ; } else { // create new identity using new primary token WindowsIdentity newId = new WindowsIdentity ( pDuplicateTokenHandle ) ; WindowsImpersonationContext impersonatedUser = newId.Impersonate ( ) ; newUser = impersonatedUser ; } } finally { // close handle ( ) ( pExistingTokenHandle ! = IntPtr.Zero ) CloseHandle ( pExistingTokenHandle ) ; ( pDuplicateTokenHandle ! = IntPtr.Zero ) CloseHandle ( pDuplicateTokenHandle ) ; } } public void Undo ( ) { newUser.Undo ( ) ; } } } To use : Impersonator impersonator = new Impersonator ( `` username '' , `` domain '' , `` password '' ) ; //Connect use SQL server impersonator.Undo ( ) ; I added Undo method otherwise impersonator object tended get garbage collected . I also altered code use LOGON32_LOGON_NEW_CREDENTIALS poke run make work ; I still need understand fully , I feeling /netonly flag runas . I 'm also going break constructor bit ."
Stackoverflow,"[ Sqlinq ] ( http : //sqlinq.codeplex.com/ ) opensource project create sql queries linq . I 'm using [ Dapper ] ( https : //github.com/SamSaffron/dapper-dot-net/ ) , unfortunately n't support JOINs . Is libraries thing supports JOIN ? If , could solution avoid hard coding sql select queries ?",How build-in linq comes .NET ? < http : //msdn.microsoft.com/en-us/library/bb397927.aspx >
Stackoverflow,"This seems simple question , I wonder disadvantages calling `` close ( ) '' function .","In current usage , close : > If IDbConnection closed Fill called , opened retrieve data closed . If connection open Fill called , remains open . < http : //msdn.microsoft.com/en-us/library/zxkb3c3d.aspx > I think 's always better explicitly cater ` using ` statement : using ( SqlConnection conn = new SqlConnection ( `` '' ) ) { conn.Open ( ) ; // Do Stuff. } // Closes dispose . This often readable n't rely people understanding inner workings ` SqlDataAdapter.Fill ` , ` using ` statement connections . However , _know_ connection closed adapter uses ( , 've created connection ) 's used anything else , code perfectly safe valid . Personally , I 'd write something like : string cnStr = `` Data Source=TEST ; Initial Catalog=Suite ; Persist Security Info=True ; User ID=app ; Password=Immmmmm '' ; DataSet ds = new DataSet ( ) ; using ( SqlConnection cn = new SqlConnection ( cnStr ) ) using ( SqlCommand cmd = new SqlCommand ( `` SELECT TOP 10 * FROM Date '' , cn ) ) using ( SqlDataAdapter adapter = new SqlDataAdapter ( cmd ) ) { conn.Open ( ) ; adapter.Fill ( ds ) ; }"
Stackoverflow,"When trying connect MySQL server Azure mysql client , I get following error , even though I using correct username server name . How I fix ? _The connection string may right . Please visit portal references._","When connecting server instance Azure Database MySQL , required follow ` < username @ hostname > ` format , whether mysqlexe client MySQL workbench . We recommend get complete connection string client Azure portal use connecting MySQL server . Read [ How get Connection Information ] ( https : //docs.microsoft.com/en- us/azure/mysql/quickstart-create-mysql-server-database-using-azure-portal # get- connection-information ) understand connect MySQL server various clients ."
Stackoverflow,"SQLCL exactly I need , I 've big difficulty one little thing : I want make script ( batch file ) Auto connection EXPORT CSV ( remote desktop : server ) . So I 'm using pipe method SQLCL Batch File : echo SET SQLFORMAT CSV < echo SPOOL export.csv < echo SELECT COUNT ( * ) FROM ARTICLE ; < echo SPOOL OFF | C : \Work\Soft\sqlcl\bin\sql.exe login/passwd @ xxx.xxx.xxx.xxx:1521/DB.SCH It 's working ( errors console ) , impossible find file ` export.csv ` : I change destination ` c : \ ... ` 's working impossible find created file . It 's working fine SQL Developer file created dekstop , I n't understand 's case SQLCL .","It looks like SID service name . In SQL Developer seem using SID - least custom JDBC URL showed - denoted colon ` : vdbsl4 ` . Your SQLcl URL using service name , denoted slash ` /vdbsl14 ` . Using SID instead ( i.e . changing / : ) URL work since 's using JDBC : sqlcl username/pass @ delphix-vdb-n-1.va2.b2c.nike.com:1521 : vdbsl14 Alternatively ( preferably , opinion ) find service name actually . If sufficient privileges database ` show parameters service_names ` SQL Devleoper , access server DBA ` lsnrctl services ` , even look ` tnsnames.ora ` case TNS alias defined shows service name . ( ` listener.ora ` n't likely help , could give hints lucky show default service name ) . You use service name JDBC URL , ` /service_name ` . You also use TNS alias SQLcl ( SQL*Plus ) . You may already ` tnsnames.ora ` available ; might able copy server , create . That refer SID service name . You even pass full TNS description SQL*Plus ( sure SQLcl ) 's bit unpleasant . If n't have/want ` tnsnames.ora ` use 'easy connect ' syntax , 're using SQLcl - _has_ service name , n't allow SIDs ."
Stackoverflow,"Do people use .NET 's SqlMembershipProvider , SqlRoleProvider , SqlProfileProvider developing site membership capabilities ? Or many people make providers , even membership systems entirely ? What limitations SQL providers would make roll ? Is easy extend SQL providers provide additional functionality ? **For Reference** [ Per Scott Gu's Blog ] ( http : //weblogs.asp.net/scottgu/archive/2006/04/13/442772.aspx ) , [ Microsoft provides source code SqlMembershipProvider ] ( http : //download.microsoft.com/download/a/b/3/ab3c284b-dc9a-473d-b7e3-33bacfcc8e98/ProviderToolkitSamples.msi ) customize , instead starting scratch . Just FYI .",We use everything except Profile Provider . The Profile Provider completly text based full text seearches - becomes exceedingly slow user base gets larger . We found much better solution '' role '' profile section membership api database keyed userid membership .
Stackoverflow,"I attempting create simple http server one thing . Upon receiving HttpRequest , runs query local database server , returns string based query . I learning Dart , I trouble grasping Futures . I thought I understood , example leads believe I really idea work . So , I looking solution problem , pointers I gladly accept well . Note : This code primitive example I trying accomplish , sake reaching Stackoverflow community , I shortened / simplified much possible , keeping problem intact . Here server.dart code import 'dart : io ' ; import 'package : sqljocky/sqljocky.dart ' ; final connection = new ConnectionPool ( host : 'localhost ' , port : 3306 , user : 'root ' , password : null , db : 'server1 ' ) ; main ( ) { HttpServer.bind ( InternetAddress.ANY_IP_V4 , 9090 ) ..then ( ( server ) { print ( `` serving generic database query localhost:9090 '' ) ; server.listen ( ( request ) { ( request.method == `` GET '' ) { request.response.write ( getResults ( ) ) ; request.response.close ( ) ; } else { request.response.statusCode = HttpStatus.BAD_REQUEST ; } } ) ; } ) ; } String getResults ( ) { StringBuffer sb = new StringBuffer ( ) ; sb.write ( `` START -- '' ) ; connection.query ( `` select name , email , zkey users '' ) ..then ( ( results ) { results.forEach ( ( row ) { sb.write ( row.toString ( ) ) ; print ( row.toString ( ) ) ; } ) ; } ) ; sb.write ( `` -- END '' ) ; print ( sb.toString ( ) ) ; return sb.toString ( ) ; } So I send request server returns `` START -- -- END '' . The server prints expected query result , prints `` START -- -- END '' . This leads believe request response closing returning query result done processing . So whether I curl localhost:9090/asdf actually build client http request sender , I get response I expecting ... database query result . Thanks advance","That ` `` START -- -- END '' ` printed 'out order ' behavior futures confusing developers beginning . A call like ` connection.query ( ) ` returns future executed immediately enlisted queue later execution . The current thread execution continued finished queue processed one another . What n't work code async call ` connection.query ( ) ` continue sync call . That never work Dart . When start async execution cant go back sync . ( far I know planned async/await solve ) . More details dartlang.org [ The Event Loop Dart ] ( https : //www.dartlang.org/articles/event-loop/ ) # # EDIT tested code import 'dart : io ' ; import 'package : sqljocky/sqljocky.dart ' ; final connection = new ConnectionPool ( host : 'localhost ' , port : 3306 , user : 'root ' , password : null , db : 'server1 ' ) ; main ( ) { HttpServer.bind ( InternetAddress.ANY_IP_V4 , 9090 ) ..then ( ( server ) { print ( `` serving generic database query localhost:9090 '' ) ; server.listen ( ( request ) { ( request.method == `` GET '' ) { getResults ( ) .then ( ( result ) { print ( 'Result : $ result ' ) ; request.response.write ( result ) ; request.response.close ( ) ; } ) ; } else { request.response.statusCode = HttpStatus.BAD_REQUEST ; } } ) ; } ) ; } Future < String > getResults ( ) { StringBuffer sb = new StringBuffer ( ) ; sb.write ( `` START -- '' ) ; return connection.query ( `` select name , email , zkey users '' ) .then ( ( Result results ) = > results.toList ( ) ) .then ( ( list ) { list.forEach ( ( row ) { sb.write ( row.toString ( ) ) ; } ) ; sb.write ( `` -- END '' ) ; } ) .then ( ( _ ) = > sb.toString ( ) ) ; } See also Gregs answer question read SqlJocky results [ sqljocky querying database synchronously ] ( https : //stackoverflow.com/questions/20411171 )"
Stackoverflow,"I using JSQLPARSER first time . I SQL files come dynamically , need read table column names SQL . After lot googling I tried JSQLPARSER . I trying read column names file I unable read column names due expression , please one correct code I went wrong . I getting CLASSCASTEXCEPTION code : public static void main ( String [ ] args ) throws JSQLParserException { // TODO Auto-generated method stub String statement= '' SELECT LOCATION_D.REGION_NAME , LOCATION_D.AREA_NAME , COUNT ( DISTINCT INCIDENT_FACT.TICKET_ID ) FROM LOCATION_D , INCIDENT_FACT WHERE ( LOCATION_D.LOCATION_SK=INCIDENT_FACT.LOCATION_SK ) GROUP BY LOCATION_D.REGION_NAME , LOCATION_D.AREA_NAME '' ; CCJSqlParserManager parserManager = new CCJSqlParserManager ( ) ; Select select= ( Select ) parserManager.parse ( new StringReader ( statement ) ) ; PlainSelect plain= ( PlainSelect ) select.getSelectBody ( ) ; List selectitems=plain.getSelectItems ( ) ; System.out.println ( selectitems.size ( ) ) ; ( int i=0 ; < selectitems.size ( ) ; i++ ) { Expression expression= ( ( SelectExpressionItem ) selectitems.get ( ) ) .getExpression ( ) ; System.out.println ( `` Expression : - '' +expression ) ; Column col= ( Column ) expression ; System.out.println ( col.getTable ( ) + '' , '' +col.getColumnName ( ) ) ; } }","You sample code , code based guessing want show count ( * ) actually function call . import java.io.StringReader ; import net.sf.jsqlparser.JSQLParserException ; import net.sf.jsqlparser.expression.Function ; import net.sf.jsqlparser.parser.CCJSqlParserManager ; import net.sf.jsqlparser.statement.select.PlainSelect ; import net.sf.jsqlparser.statement.select.Select ; import net.sf.jsqlparser.statement.select.SelectExpressionItem ; public class MySQLParser { CCJSqlParserManager parserManager = new CCJSqlParserManager ( ) ; public MySQLParser ( ) throws JSQLParserException { String statement = `` SELECT COUNT ( * ) FROM db.table1 '' ; PlainSelect plainSelect = ( PlainSelect ) ( ( Select ) parserManager.parse ( new StringReader ( statement ) ) ) .getSelectBody ( ) ; System.out.format ( `` % function call ? % '' , plainSelect.getSelectItems ( ) .get ( 0 ) , ( ( Function ) ( ( SelectExpressionItem ) plainSelect.getSelectItems ( ) .get ( 0 ) ) .getExpression ( ) ) .isAllColumns ( ) ) ; } public static void main ( String [ ] args ) throws JSQLParserException { new MySQLParser ( ) ; } }"
Stackoverflow,"The background : We application main entity customer . All information applications starts customer . We thought would really nice use kind partitioning . We designed service Azure SQL Database backend . Our tables look like ( relevant part left brevity ) : TABLE dbo.Orders ( CustomerId INT NOT NULL DEFAULT ( FEDERATION_FILTERING_VALUE ( 'FEDERATION_BY_CUSTOMER ' ) ) , OrderId INT NOT NULL , ... . , CONSTRAINT PK_Orders PRIMARY KEY CLUSTERED ( CustomerId , OrderId ) ) FEDERATED ON ( FEDERATION_BY_CUSTOMER = CustomerId ) ; Now allowed us crazy things . Our entry points SQL related stuff always contains following command first : USE FEDERATION GroupFederation ( FEDERATION_BY_CUSTOMER = 1 ) WITH RESET , FILTERING = ON In case statement : SELECT * FROM Orders INSERT INTO Orders ( OrderId ) VALUES ( 10 ) ; work without problem , working data given customer . The CustomerId COLUMN always inferred system function FEDERATION_FILTERING_VALUE ; Now could customer single database without problem , would isolated . If sometime future , one got big , could SPLIT federation particular customer ID n't change anything code support . Heck , could customer separated federation database service using would n't know single thing . We happy solution I thought I clever coming . Not recently Microsoft announced deprecating azure federations feature new azure database editions coming . Read [ ] ( http : //msdn.microsoft.com/library/azure/dn741330.aspx ) [ ] ( http : //blogs.msdn.com/b/windowsazure/archive/2014/04/24/azure-sql- database-introduces-new-service-tiers.aspx ? CommentPosted=true # commentmessage ) . I hope see problem . What think alternatives ? Do use Azure Federations going transition ? Thank .","Jon , See session - referenced sample library available soon : SQL Database Sharding Patterns : < http : //channel9.msdn.com/Shows/Data- Exposed/SqlDbShardingIntro > -Simon ."
Stackoverflow,"The schema follows : CREATE TABLE [ Structure ] ( [ StructureId ] [ uniqueidentifier ] NOT NULL , [ SequenceNumber ] [ int ] NOT NULL , -- order siblings , unique per parent [ ParentStructureId ] [ uniqueidentifier ] NULL , CONSTRAINT [ Structure_PK ] PRIMARY KEY CLUSTERED ( [ StructureId ] ASC ) ) ON [ PRIMARY ] ALTER TABLE [ Structure ] WITH CHECK ADD CONSTRAINT [ Structure_FK1 ] FOREIGN KEY ( [ ParentStructureId ] ) REFERENCES [ Structure ] ( [ StructureId ] ) Currently , I get logical data follow CTE , I would like print directly depth first fashion . WITH SCTE ( StructureId , Level , Seq , ParentId ) AS ( SELECT StructureId , 0 , SequenceNumber , [ ParentStructureId ] FROM Structure WHERE [ ParentStructureId ] IS NULL AND StructureId = 'F6C5F016-1270-47C1-972F-349C32DFC92A' UNION ALL SELECT Structure.StructureId , Level + 1 , SequenceNumber , ParentStructureId FROM Structure INNER JOIN SCTE ON SCTE.StructureId = Structure.ParentStructureId ) SELECT * FROM SCTE ORDER BY Level , ParentId , Seq The output follows ( truncated ) : StructureId Level Seq ParentId F6C5F016-1270-47C1-972F-349C32DFC92A 0 0 NULL D2E34429-401A-4A49-9E18-E81CCA0FB417 1 0 F6C5F016-1270-47C1-972F-349C32DFC92A 0CC5E16C-9194-40CA-9F72-1CED2972D7CA 1 1 F6C5F016-1270-47C1-972F-349C32DFC92A 1ECD1D30-EB85-42B0-969F-75794343E3B4 1 2 F6C5F016-1270-47C1-972F-349C32DFC92A EEC3A981-B790-4600-8CD1-F15972CD9230 2 0 0CC5E16C-9194-40CA-9F72-1CED2972D7CA 4406F639-2F58-4918-A9EF-A4B0F379BEA0 2 1 0CC5E16C-9194-40CA-9F72-1CED2972D7CA FCAF7870-C606-4AA6-85EE-57B90B1B0CC3 2 2 0CC5E16C-9194-40CA-9F72-1CED2972D7CA 855DF5FB-1593-4E5B-8EF9-3770B45F89D6 2 3 0CC5E16C-9194-40CA-9F72-1CED2972D7CA 3D16DF32-C04F-49B4-B0D9-5BDC9104F810 2 4 0CC5E16C-9194-40CA-9F72-1CED2972D7CA A1084D00-0198-47D9-87E0-BB8234233F14 2 5 0CC5E16C-9194-40CA-9F72-1CED2972D7CA CE443C0D-376F-46EC-9914-32C6B7200DB1 2 6 0CC5E16C-9194-40CA-9F72-1CED2972D7CA 0DEA587D-4FCF-414C-AD71-FB00829F8082 2 7 0CC5E16C-9194-40CA-9F72-1CED2972D7CA CC9FC8D3-254A-486B-8DC4-07E57627476C 2 0 1ECD1D30-EB85-42B0-969F-75794343E3B4 215565CC-501F-4850-B8AE-5466DA5E6854 2 1 1ECD1D30-EB85-42B0-969F-75794343E3B4 D4E6C8E5-5ADD-4AD1-B59B-1A672F66888A 2 2 1ECD1D30-EB85-42B0-969F-75794343E3B4 796C65BF-4714-4DBF-A97A-2150DBE3098C 2 3 1ECD1D30-EB85-42B0-969F-75794343E3B4 B39DEB9C-BE42-43B4-9C38-968399D7D1E2 2 4 1ECD1D30-EB85-42B0-969F-75794343E3B4 6C2F70C6-1DA0-4E1A-BBC1-D7FCAFE6AFEE 2 0 D2E34429-401A-4A49-9E18-E81CCA0FB417 75D7B43B-C971-46B4-BC42-58C3605ADD79 2 1 D2E34429-401A-4A49-9E18-E81CCA0FB417 0B5AAAA0-A69F-431E-86BA-148444D7B1E6 2 2 D2E34429-401A-4A49-9E18-E81CCA0FB417 CB3CF66B-D83A-45E2-953A-6F0CEE094F5B 2 3 D2E34429-401A-4A49-9E18-E81CCA0FB417 1D5F69C3-F036-4667-BD75-A0DC1506DB6D 2 4 D2E34429-401A-4A49-9E18-E81CCA0FB417 71B894F7-B9FC-44DE-AEDB-E6FA026A6082 2 5 D2E34429-401A-4A49-9E18-E81CCA0FB417 F1DFA1E1-013B-449C-9D9D-14C64E75D418 2 6 D2E34429-401A-4A49-9E18-E81CCA0FB417 As see , result 'breadth first ' makes printing tree kinda impossible . Is way ( probably trivial way , SQL skills extremely poor ) get resultant list 'tree printing friendly ' format ? I know I could dump results program code output , exercise I would prefer SQL . Thanks","Edited comment . You could add path node , order : declare @ table ( id int , parent int ) insert @ ( id , parent ) values ( 1 , null ) , ( 2,1 ) , ( 3,2 ) , ( 4,3 ) , ( 5 , null ) , ( 6,5 ) ; cte ( select id , parent , cast ( RIGHT ( REPLICATE ( ' 0',12 ) + CONVERT ( varchar ( 12 ) , id ) ,12 ) varchar ( max ) ) Path @ parent null union select child.id , child.parent , parent.Path + RIGHT ( REPLICATE ( ' 0',12 ) + CONVERT ( varchar ( 12 ) , child.id ) ,12 ) Path @ child join cte parent parent.id = child.parent ) select * cte order Path This prints root first , followed leaves order . If id larger 12 digits , increase number ` char ( x ) ` casts ."
Stackoverflow,"We 're using Sql Server 2012 SSDT removed deploy option Visual Studio database projects ( sql projects ) . We 'd like automate Publish step deploy , 's clear . thA couple questions : 1 . I 've added .publish.xml project ( first manual publish , checking add project ) . Even , setting default , I double click , builds , always pops settings window , I need click `` Publish '' button continue . Is setting would skip prompt use current values ? 2 . It seems publish generates version sql output . How I suppress this- i.e . overwrite base file time ? 3 . And lastly , pointers updating build use new project type publish command automated builds would appreciated .","**How restore Deploy option : ** _ ( Visual Studio 2010/2012 -- longer supported Visual Studio 2013 ) _ The Deploy option still present reason 's available menus . ( Cursed Visual Studio team ! ) I 've worked around adding Deploy option one toolbars follows : 1 . Click arrow right-hand side toolbar . 2 . Click `` Add Remove Buttons '' , Customize . 3 . In Customize dialog click Add Command . 4 . Select `` Build '' category , select `` Deploy Selection '' command . 5 . After saving selection `` Deploy [ project name ] '' option appear toolbar . _You 'll need select project Solution Explorer button become enabled._ Note deployment settings different publish settings . The deployment settings configured project 's properties Debug tab . * * * To answer questions Publish option : **1 ) How use specific publish file default avoid annoying prompt** I n't think 's way around . **2 ) How publish entire database , changes** Open .publish.xml file text editor add ` < AlwaysCreateNewDatabase > true < /AlwaysCreateNewDatabase > ` . For example : < ? xml version= '' 1.0 '' encoding= '' utf-8 '' ? > < Project ToolsVersion= '' 4.0 '' xmlns= '' http : //schemas.microsoft.com/developer/msbuild/2003 '' > < PropertyGroup > < TargetDatabaseName > MyDatabase < /TargetDatabaseName > < DeployScriptFileName > MyDatabaseProject.sql < /DeployScriptFileName > < TargetConnectionString > Data Source=localhost\SQL2012 ; Integrated Security=True ; Pooling=False < /TargetConnectionString > < PublishDependentProjects > False < /PublishDependentProjects > < ProfileVersionNumber > 1 < /ProfileVersionNumber > < AlwaysCreateNewDatabase > true < /AlwaysCreateNewDatabase > < /PropertyGroup > < /Project > **3 ) Command-line syntax automated builds** First build project msbuild normally would .dacpac file created bin . Then use ` sqlpackage.exe ` publish using .publish.xml file : ` C : \Program Files\Microsoft Visual Studio 10.0\Microsoft SQL Server Data Tools\sqlpackage.exe /Action : Publish /SourceFile : C : \ [ path project ] \bin\Debug\MyDatabaseProject.dacpac /Profile : C : \ [ path project ] \MyDatabaseProject.publish.xml ` Note path sqlpackage.exe may different ."
Stackoverflow,"So I 'm trying run lambda amazon narrowed error finally testing lambda amazons testing console . The error I got . { `` errorMessage '' : `` Please install mysql2 package manually '' , `` errorType '' : `` Error '' , `` stackTrace '' : [ `` new MysqlDialect ( /var/task/node_modules/sequelize/lib/dialects/mysql/index.js:14:30 ) '' , `` new Sequelize ( /var/task/node_modules/sequelize/lib/sequelize.js:234:20 ) '' , `` Object.exports.getSequelizeConnection ( /var/task/src/twilio/twilio.js:858:20 ) '' , `` Object. < anonymous > ( /var/task/src/twilio/twilio.js:679:25 ) '' , `` __webpack_require__ ( /var/task/src/twilio/twilio.js:20:30 ) '' , `` /var/task/src/twilio/twilio.js:63:18 '' , `` Object. < anonymous > ( /var/task/src/twilio/twilio.js:66:10 ) '' , `` Module._compile ( module.js:570:32 ) '' , `` Object.Module._extensions..js ( module.js:579:10 ) '' , `` Module.load ( module.js:487:32 ) '' , `` tryModuleLoad ( module.js:446:12 ) '' , `` Function.Module._load ( module.js:438:3 ) '' , `` Module.require ( module.js:497:17 ) '' , `` require ( internal/module.js:20:19 ) '' ] } Easy enough , I install mysql2 . So I added package.json file . { `` name '' : `` test-api '' , `` version '' : `` 1.0.0 '' , `` description '' : `` '' , `` main '' : `` handler.js '' , `` scripts '' : { `` test '' : `` echo \ '' Error : test specified\ '' & & exit 0 '' } , `` keywords '' : [ ] , `` author '' : `` '' , `` license '' : `` ISC '' , `` devDependencies '' : { `` aws-sdk '' : `` ^2.153.0 '' , `` babel-core '' : `` ^6.26.0 '' , `` babel-loader '' : `` ^7.1.2 '' , `` babel-plugin-transform-runtime '' : `` ^6.23.0 '' , `` babel-preset-es2015 '' : `` ^6.24.1 '' , `` babel-preset-stage-3 '' : `` ^6.24.1 '' , `` serverless-domain-manager '' : `` ^1.1.20 '' , `` serverless-dynamodb-autoscaling '' : `` ^0.6.2 '' , `` serverless-webpack '' : `` ^4.0.0 '' , `` webpack '' : `` ^3.8.1 '' , `` webpack-node-externals '' : `` ^1.6.0 '' } , `` dependencies '' : { `` babel-runtime '' : `` ^6.26.0 '' , `` mailgun-js '' : `` ^0.13.1 '' , `` minimist '' : `` ^1.2.0 '' , `` mysql '' : `` ^2.15.0 '' , `` mysql2 '' : `` ^1.5.1 '' , `` qs '' : `` ^6.5.1 '' , `` sequelize '' : `` ^4.31.2 '' , `` serverless '' : `` ^1.26.0 '' , `` serverless-plugin-scripts '' : `` ^1.0.2 '' , `` twilio '' : `` ^3.10.0 '' , `` uuid '' : `` ^3.1.0 '' } } I noticed I sls deploy however , seems packaging modules ? Serverless : Package lock found - Using locked versions Serverless : Packing external modules : babel-runtime @ ^6.26.0 , twilio @ ^3.10.0 , qs @ ^6.5.1 , mailgun-js @ ^0.13.1 , sequelize @ ^4.31.2 , minimi st @ ^1.2.0 , uuid @ ^3.1.0 Serverless : Packaging service ... Serverless : Uploading CloudFormation file S3 ... Serverless : Uploading artifacts ... Serverless : Validating template ... Serverless : Updating Stack ... Serverless : Checking Stack update progress ... ... ... ... ... ... ... ... ... ... ... .. Serverless : Stack update finished ... I think 's working . **In short , I get mysql2 library packaged correctly serverless lambda function work sequelize library ? ** Please note **when I test locally code works fine . ** My serverless file service : testapi # Use serverless-webpack plugin transpile ES6/ES7 plugins : - serverless-webpack - serverless-plugin-scripts # - serverless-domain-manager custom : # Define Stage default Staging . stage : $ { opt : stage , self : provider.stage } webpackIncludeModules : true # Define Databases Here databaseName : `` $ { self : service } - $ { self : custom.stage } '' # Define Bucket Names Here uploadBucket : `` $ { self : service } -uploads- $ { self : custom.stage } '' # Custom Script setup scripts : hooks : # Script run schema changes database neccesary update according stage . 'deploy : finalize ' : node database-schema-update.js -- stage $ { self : custom.stage } # Domain Setup # customDomain : # basePath : `` / '' # domainName : `` api- $ { self : custom.stage } .test.com '' # stage : `` $ { self : custom.stage } '' # certificateName : `` *.test.com '' # createRoute53Record : true provider : name : aws runtime : nodejs6.10 stage : staging region : us-east-1 environment : DOMAIN_NAME : `` api- $ { self : custom.stage } .test.com '' DATABASE_NAME : $ { self : custom.databaseName } DATABASE_USERNAME : $ { env : RDS_USERNAME } DATABASE_PASSWORD : $ { env : RDS_PASSWORD } UPLOAD_BUCKET : $ { self : custom.uploadBucket } TWILIO_ACCOUNT_SID : `` '' TWILIO_AUTH_TOKEN : `` '' USER_POOL_ID : `` '' APP_CLIENT_ID : `` '' REGION : `` us-east-1 '' IDENTITY_POOL_ID : `` '' RACKSPACE_API_KEY : `` '' # Below controls permissions lambda functions . iamRoleStatements : - Effect : Allow Action : - dynamodb : DescribeTable - dynamodb : UpdateTable - dynamodb : Query - dynamodb : Scan - dynamodb : GetItem - dynamodb : PutItem - dynamodb : UpdateItem - dynamodb : DeleteItem Resource : `` arn : aws : dynamodb : us-east-1 : * : * '' functions : create_visit : handler : src/visits/create.main events : - http : path : visits method : post cors : true authorizer : aws_iam get_visit : handler : src/visits/get.main events : - http : path : visits/ { id } method : get cors : true authorizer : aws_iam list_visit : handler : src/visits/list.main events : - http : path : visits method : get cors : true authorizer : aws_iam update_visit : handler : src/visits/update.main events : - http : path : visits/ { id } method : put cors : true authorizer : aws_iam delete_visit : handler : src/visits/delete.main events : - http : path : visits/ { id } method : delete cors : true authorizer : aws_iam twilio_send_text_message : handler : src/twilio/twilio.send_text_message events : - http : path : twilio/sendtextmessage method : post cors : true authorizer : aws_iam # This function handles incoming calls route . twilio_incoming_call : handler : src/twilio/twilio.incoming_calls events : - http : path : twilio/calls method : post twilio_failure : handler : src/twilio/twilio.twilio_failure events : - http : path : twilio/failure method : post twilio_statuschange : handler : src/twilio/twilio.statuschange events : - http : path : twilio/statuschange method : post twilio_incoming_message : handler : src/twilio/twilio.incoming_message events : - http : path : twilio/messages method : post twilio_whisper : handler : src/twilio/twilio.whisper events : - http : path : twilio/whisper method : post - http : path : twilio/whisper method : get twilio_start_call : handler : src/twilio/twilio.start_call events : - http : path : twilio/startcall method : post - http : path : twilio/startcall method : get resources : Resources : uploadBucket : Type : AWS : :S3 : :Bucket Properties : BucketName : $ { self : custom.uploadBucket } RDSDatabase : Type : AWS : :RDS : :DBInstance Properties : Engine : mysql MasterUsername : $ { env : RDS_USERNAME } MasterUserPassword : $ { env : RDS_PASSWORD } DBInstanceClass : db.t2.micro AllocatedStorage : ' 5' PubliclyAccessible : true # TODO : The Value Stage also available TAG automatically I may use replace manually put here.. Tags : - Key : `` Name '' Value : $ { self : custom.databaseName } DeletionPolicy : Snapshot DNSRecordSet : Type : AWS : :Route53 : :RecordSet Properties : HostedZoneName : test.com . Name : database- $ { self : custom.stage } .test.com Type : CNAME TTL : '300' ResourceRecords : - { `` Fn : :GetAtt '' : [ `` RDSDatabase '' , '' Endpoint.Address '' ] } DependsOn : RDSDatabase UPDATE : : So I confirmed running sls package -- stage dev seems create zip folder would eventually upload AWS . **This confirms serverless creating package correctly mysql2 reference** reason ? **Why ? ** [ ! [ enter image description ] ( https : //i.stack.imgur.com/mVNYs.png ) ] ( https : //i.stack.imgur.com/mVNYs.png ) **webpack config file requested** const slsw = require ( `` serverless-webpack '' ) ; const nodeExternals = require ( `` webpack-node-externals '' ) ; module.exports = { entry : slsw.lib.entries , target : `` node '' , // Since 'aws-sdk ' compatible webpack , // exclude node dependencies externals : [ nodeExternals ( ) ] , // Run babel .js files skip node_modules module : { rules : [ { test : /\.js $ / , loader : `` babel-loader '' , include : __dirname , exclude : /node_modules/ } ] } } ;","Turns disabling minification Webpack fixed issue . This done updating ` webpack.config.js ` include optimization : { // We want minimize code . minimize : false } ,"
Stackoverflow,"Do people use .NET 's SqlMembershipProvider , SqlRoleProvider , SqlProfileProvider developing site membership capabilities ? Or many people make providers , even membership systems entirely ? What limitations SQL providers would make roll ? Is easy extend SQL providers provide additional functionality ? **For Reference** [ Per Scott Gu's Blog ] ( http : //weblogs.asp.net/scottgu/archive/2006/04/13/442772.aspx ) , [ Microsoft provides source code SqlMembershipProvider ] ( http : //download.microsoft.com/download/a/b/3/ab3c284b-dc9a-473d-b7e3-33bacfcc8e98/ProviderToolkitSamples.msi ) customize , instead starting scratch . Just FYI .",_ [ blergh ] ( http : //www.urbandictionary.com/define.php ? term=blergh ) _ Googling tags Stack Overflow provided I came across site : < http : //www.lhotka.net/weblog/CallingRolesGetRolesForUserInAWCFService.aspx > In short : apparently something broke .net 3.5 .net 4 . To solve issue call : string [ ] roles = Roles.Provider.GetRolesForUser ( ServiceSecurityContext.Current.PrimaryIdentity.Name ) ; instead string [ ] roles = Roles.GetRolesForUser ( ServiceSecurityContext.Current.PrimaryIdentity.Name ) ; The difference ` .Provider ` added middle . After adding worked fine .
Stackoverflow,I 'm trying get multiple inserts work single script using SQL Compact 4.0 Toolbox luck . I keep getting parsing error . I 've even tried adding GO ; statement like so.. INSERT INTO ... ; GO ; INSERT INTO ... ; luck.. I luck ? Do I execute statement one time ?,"Update latest release version ( 2.3 ) . There bug multiple statement execution , fixed latest release version . Seperate statement GO separate line , semicolon ... Like INSERT statements created tool : INSERT INTO [ Shippers ] ( [ Shipper ID ] , [ Company Name ] ) VALUES ( 1 , N'Speedy Express ' ) ; GO INSERT INTO [ Shippers ] ( [ Shipper ID ] , [ Company Name ] ) VALUES ( 2 , N'United Package ' ) ; GO INSERT INTO [ Shippers ] ( [ Shipper ID ] , [ Company Name ] ) VALUES ( 3 , N'Federal Shipping ' ) ; GO"
Stackoverflow,"I using SqlCommandProvider I need get data id id collection let ids= [ `` B058A99-C4B2-4CC3-BA9F-034B1F1ECCBD '' ; '' A09C01C-D51B-41C1-B44C-0995DD285088 '' ] [ < Literal > ] let qLogo = '' '' '' SELECT Id , LogoUrl FROM Hotels WHERE Id IN ( @ IDS ) '' '' '' let queryLogo = new SqlCommandProvider < qLogo , constring > ( ) queryLogo .Execute ( ids ) //i need able pass collection `","This kind exception may get n't correct ` bindingRedirect ` ` FSharp.Core.dll ` . Check [ article Mark Seemann ] ( http : //blog.ploeh.dk/2014/01/30/how-to-use-fsharpcore-430-when-all- you-have-is-431/ ) . In principle , I think adding ` app.config ` ` bindingRedirect ` application solve problem . This typically happens using library compiled older version FSharp.Core application uses newer version FSharp.Core . The .NET runtime loads new version FSharp.Core know types older version ( like ` FSharpFunc ` ) mapped corresponding types new version - get MethodMissing , .NET thinks ` FSharpFunc ` different type loaded one . ( Though things get bit complicated type providers . )"
Stackoverflow,"I new project needs SQL Server unit test , CI/CD VSTS . Below features required * SQL server unit test stored procedure , initial target tables setup test clean * Unit test sql * CI/CD VSTS Git * Easy setup easy use I looked SSDT 2017 , seems good . But seems lacks feature common setup script shared easily test Pre-Test step . It might lack features available daily usage . But I might wrong . Which tool fits better general sql server unit testing 2017 ? [ SQL Server Data Tools Visual Studio ] ( https : //www.visualstudio.com/vs/ssdt/ ) [ TSQLT ] ( http : //tsqlt.org/ )","One reasons n't unit testing solutions SQL development proper unit testing inherently harder databases people n't . This databases maintain state also referential integrity . Imagine writing unit test stored procedure ( ` order_detail_update_status ` ) updates status flag ` order_detail ` table . The order_detail table dependency ` order_header ` ` product ` tables , order_header turn foreign keys ` customer ` ` employee ` whilst product table may depend ` product_category ` , ` product_type ` ` supplier ` . That least seven tables ( probably ) need populated valid data write one test one tables nothing code test . So looking unit testing solution exactly - ability test discrete units code - minimum set-up . So ideally , would able set required test data ` order_detail ` ignore rest tables - I aware one testing framework allows . Additionally , unit tests minimal reasons fail , example , ` order_detail_update_status ` updates single row order_detail table . If new null column added customer table , handled test set-up scenario test could fail totally unrelated reason . This makes brittle tests , pressure tight delivery deadlines , developers quickly give writing maintaining tests . A suite unit tests runnable order , interdependencies good test framework support along set-up , tear support mocking objects ( may may part framework ) . In scenario , ability mock ` order_detail ` table test module touches order_detail table one important features n't want spend huge amounts time fixing tests failing `` good '' reason . So terms requirements , points , one framework I aware - [ tSQLt ] ( http : //tsqlt.org ) . This based real-world experience - 6,000 tSQLt unit tests last project . It includes following feautures : * Unit test stored procedures , functions views * Mock tables , views functions * Mock ( spy ) stored procedures - either isolation , replay pre-defined outcomes * Suite set-up * Automatic tear-down ( every test runs 's translation ) * Unit tests completely isolated executed order It works well VSTS CI/CD , unit tests written T-SQL , easy use . The best way use tSQLt Visual Studio make use composite projects - application database objects modules maintained one project whilst tSQLt framework unit tests part second project . There good aticle get started [ ] ( https : //kzhendev.wordpress.com/2014/01/08/setting-up-ssdt-database- projects-and-tsqlt/ ) . I wrote detailed article benefits tSQLt [ Simple- Talk ] ( https : //www.red-gate.com/simple-talk/sql/t-sql-programming/test-driven- database-development-why-tsqlt/ ) year back might also helpful"
Stackoverflow,"I result set I pull large database : $ result = mysql_query ( $ sql ) ; I loop recordset pull specific bits data get averages using ` ( $ row = mysql_fetch_array ( $ result ) ) ` . Later page , I want loop recordset output everything - I used recordset earlier , second loop returns nothing . I finally hacked around looping second identical recordset ( ` $ result2 = mysql_query ( $ sql ) ; ` ) , I hate make SQL call twice . Any way I loop dataset multiple times ?","Use : mysql_data_seek ( $ result , 0 ) ; You get `` free '' , since 's already buffered . As separate note , explicitly unbuffered query [ ` mysql_unbuffered_query ` ] ( http : //www.php.net/manual/en/function.mysql- unbuffered-query.php ) ."
Stackoverflow,"I several jobs several packages . In SQL Server 2005 used use DTS Packages , defunct ( I know I re-enable , 's I 'm ) . I receive following error running one packages : > Message : SSIS Warning Code DTS_W_MAXIMUMERRORCOUNTREACHED . The Execution method succeeded , number errors raised ( 1 ) reached maximum allowed ( 1 ) ; resulting failure . This occurs number errors reaches number specified MaximumErrorCount . Change MaximumErrorCount fix errors . Obviously , result earlier error . I fix error , I want increase ` MaximumErrorCount ` . But even though numerous posts internet explaining select Properties package ( job ? ) , package n't _Properties_ anymore ( I found _Integration Services_ 're listed _DTS Packages_ odd , considering wizard created package DTS supported 2008 ? ) , I 've clue look package , really . And job _Properties_ , nowhere setting _Maximum error count_ . Can someone eyes see I see ? Where I increase MaximumErrorCount requested error message ?","If I open package BIDS ( `` Business Intelligence Development Studio '' , tool use design packages ) , select item , I `` Properties '' pane bottom right containing - among others , ` MaximumErrorCount ` property . If see , maybe minimized open ( look tabs right ) . If find way , try menu : View/Properties Window . Or try F4 key ."
Stackoverflow,"Is way ABAP 's OpenSQL simplify select columns ` JOIN ` I want grab fields one table selected fields table ( ) ? For instance , mysql [ simply ] ( https : //stackoverflow.com/a/14000583/1103571 ) : SELECT tb1 . * , tb2.b , tb2.d FROM tableA tb1 INNER JOIN tableB tb2 ON tb1.x = tb2.a However , OpenSQL seem allow selecting ` tb1~* , tb2~b , tb2~d ` I resort : SELECT tb1.x , tb1.y , tb1.z , tb2.b , tb2.d FROM tableA tb1 INNER JOIN tableB tb2 ON tb1.x = tb2.a For large tables , especially standard tables , becomes unwieldy , difficult read annoying maintain . **Is better way select fields tb1 fields tb2 ? **","The system variable SY-DBCNT give number rows selected , select ends . The alternative SELECT-ENDSELECT select rows SELECT INTO TABLE internal table ( provided selecting much ! ) . For example : data : lt_t000 type table t000 . select * t000 table lt_t000 . This select everything table one go internal table . So could declare internal table fields currently INTO clause specify INTO TABLE internal table . After SELECT executes , SY-DBCNT contain number selected rows . Here complete example , built around SELECT statement question , I checked sanity , I hope works ! tables : spfli . select-options : s_carrid spfli-carrid . * Definition line/structure data : begin ls_dat , carrid type s_carr_id , carrname type s_carrname , planetype type s_planetye , fldate type s_date , price type s_price , cityfrom type s_from_cit , cityto type s_to_city , end ls_dat . * Definition table : data : lt_dat like table ls_dat . * Select data select spfli~carrid scarr~carrname sflight~planetype sflight~fldate sflight~price spfli~cityfrom spfli~cityto table lt_dat spfli inner join sflight spfli~carrid = sflight~carrid spfli~connid = sflight~connid inner join scarr scarr~carrid = spfli~carrid spfli~carrid = s_carrid-low . * Output data write : 'Total records selected ' , sy-dbcnt . loop lt_dat ls_dat . write : / ls_dat-carrid , ls_dat-carrname , ls_dat-planetype , ls_dat-fldate , ls_dat-price , ls_dat-cityfrom , ls_dat-cityto . endloop . Note : Report ( type 1 ) programs still support notion declaring internal tables header lines backward compatibility , encouraged ! Hope works !"
Stackoverflow,"**Sql : ** SELECT date , total_usage_T1 TotalUsageValue , 'T1 ' UsageType FROM TblSayacOkumalari UNION ALL SELECT date , total_usage_T2 TotalUsageValue , 'T2 ' UsageType FROM TblSayacOkumalari And I try convert linq IEnumerable < TblSayacOkumalari > sayac_okumalari = entity.TblSayacOkumalari .Select ( x = > new { x.date , x.total_usage_T1 } ) .Union ( entity.TblSayacOkumalari.Select ( x = > new { x.date , x.total_usage_T2 } ) ) ; But I dont know convert ` 'T1 ' UsageType ` linq . Also union using incorrect . My table fields like : | date | total_usage_T1 | total_usage_T2 | | 2010 | 30 | 40 | | 2011 | 40 | 45 | | 2012 | 35 | 50 | I want like | date | TotalUsageValue | UsageType | | 2010 | 30 | T1 | | 2011 | 40 | T1 | | 2012 | 35 | T1 | | 2010 | 40 | T2 | | 2011 | 45 | T2 | | 2012 | 50 | T2 | I tried hard , could . Please help .","**EDIT** Def . MSDN Enumerable.Concat - Concatenates two sequences . Enumerable.Union - Produces set union two sequences using default equality comparer . My post : [ **Concat ( ) vs Union ( ) ** ] ( http : //pranayamr.blogspot.ca/2012/06/concat-vs-union.html ) IEnumerable < TblSayacOkumalari > sayac_okumalari = entity.TblSayacOkumalari .Select ( x = > new { date= x.date , TotalUsageValue = x.total_usage_T1 , UsageType = `` T1 '' } ) .Concat ( entity.TblSayacOkumalari .Select ( x = > new { date= x.date , TotalUsageValue = x.total_usage_T2 , UsageType = `` T2 '' } ) ) ; usage type juse need add ` UsageType = `` T2 '' ` new anonymous type task * * * Than go Concat method rather Union method .. Example int [ ] ints1 = { 1 , 2 , 3 } ; int [ ] ints2 = { 3 , 4 , 5 } ; IEnumerable < INT > union = ints1.Union ( ints2 ) ; Console.WriteLine ( `` Union '' ) ; foreach ( int num union ) { Console.Write ( `` { 0 } `` , num ) ; } Console.WriteLine ( ) ; IEnumerable < INT > concat = ints1.Concat ( ints2 ) ; Console.WriteLine ( `` Concat '' ) ; foreach ( int num concat ) { Console.Write ( `` { 0 } `` , num ) ; } output ! [ enter image description ] ( https : //i.stack.imgur.com/XXGDk.jpg ) **Fact Union Concat** The output shows Concat ( ) method combine two enumerable collection single one n't perform operation/ process element return single enumerable collection element two enumerable collections . Union ( ) method return enumerable collection eliminating duplicate i.e return single element element exists enumerable collection union performed . **_Important point Note_** * By fact say Concat ( ) faster Union ( ) n't processing . * But combining two collection using Concat ( ) single collection many number duplicate element want perform operation created collection takes longer time collection created using Union ( ) method , Union ( ) eliminate duplicate create collection less elements ."
Stackoverflow,"Hi I 'm logging slow queries 're performance issues I read mysqldumpslow thought would good way sort queries . At command prompt I type ` mysqldumpslow ` I get : > 'mysqldumpslow ' recognized internal external command , operable program batch file . I 'm using MySQL version 5.0.79 Windows Vista Note : c : \Program Files\MySQL\MySQL Server 5.0\bin path I searched drive 'mysqldumpslow ' find . What I wrong ? Note : MySql 5.0.x support mysqldumpslow command follow [ link ] ( http : //dev.mysql.com/doc/refman/5.0/en/mysqldumpslow.html ) manual","_edit : oops , I read manual wrong , gave wrong information : - ( sorry : - ( let 's give another try ... _ I 've installed MySQL windows , try using ` mysqldumpslow ` , I n't ` mysqldumpslow ` installed either : - ( So , alone , n't seem problem install _ ( I 've tried 5.1.x , highlighted , 5.0.x ) _ Looking `` ` mysqldumpslow ` `` I Linux , appears Perl script ; Perl often installed Windows machine . Maybe would hint solution ... Well , bit testing , installing MySQL , seems **you select `` Developpers Components > Scripts , examples '' ** , installed default ( least windows ) -- need reinstall everything : '' modify '' installation , add option . Then , `` script '' directory next `` bin '' one . For instance , install , something like `` c : \Program Files\MySQL\MySQL Server 5.0\scripts '' . In directory , scripts ; one **mysqldumpslow.pl** ; looking ; - ) Now , `` '' get Perl installed running machine ( sorry , I 've never installed Perl windows ; find informations [ ] ( http : //www.perl.com/download.csp # win32 ) ) Hope helps better I posted !"
Stackoverflow,I 350MB table 's fairly wide two varchar ( 2000 ) columns . Via SSIS data flow takes 60 minutes load via OLEDB `` fast load '' destination Azure SQL DW . I changed destination data flow Azure Blob Destination ( [ SSIS Azure feature pack ] ( https : //www.microsoft.com/en-us/download/details.aspx ? id=47366 ) ) data flow completed 1.5 minutes ( Polybase new flat file takes 2 minutes ) . For another source I existing 1GB flat file . SSIS data flow OLEDB destination Azure SQL DW takes 90 minutes . Copy file blob storage Polybase load takes 5 minutes . SSIS SSIS 2014 's running Azure VM region Azure SQL DW . I know bulk load much slower Polybase since bulk load funnels control node Polybase parallelized compute nodes . But bulk load numbers extremely slow . What optimal settings SSIS data flow destination order load Azure SQL DW stage table fast possible via bulk load ? Particularly I 'm interested optimal value following settings addition settings I 'm considering : * Stage table geometry = HEAP ( fastest I believe ) * Data flow settings : * DefaultBufferMaxRows = ? * DefaultBufferSize = ? * OLEDB destination settings * Data access mode = Table view - fast load * Keep Identity = unchecked * Keep Nulls = ? * Table Lock = ? * Check constraints = ? * Rows per batch = ? * Maximum insert commit size = ?,"I 've experienced . Your connection n't actually recognised SQL DW connection . I bet query window .sql file , .dsql needs . Go back Azure portal use link connect using SSDT . You get connection SQL Server Explorer pane looks different , start New Query based , get .dsql window , .sql one ."
Stackoverflow,"I 'm excited several recently-added Postgres features , foreign data wrappers . I 'm aware RDBMS feature , I try make case main client begin preferring Postgres current cocktail RDBMSs , include case database , I 'd like verify . I 've unable find evidence database supporting SQL/MED , things like short note stating [ Oracle support SQL/MED ] ( http : //docs.oracle.com/cd/E11882_01/server.112/e26088/ap_standard_sql007.htm # SQLRF55527 ) . The main thing gives doubt statement < http : //wiki.postgresql.org/wiki/SQL/MED > : > SQL/MED Management External Data , part SQL standard deals database management system integrate data stored outside database . If FDWs based SQL/MED , SQL/MED open standard , seems likely RDBMSs implemented . # TL ; DR : Does database besides Postgres support SQL/MED ?","* [ IBM DB2 ] ( http : //www.ibm.com/developerworks/data/library/techarticle/0203haas/0203haas.html # iso ) claims compliance SQL/MED ( including full FDW API ) ; * [ MySQL ] ( https : //dev.mysql.com/doc/refman/5.1/en/federated-storage-engine.html ) 's FEDERATED storage engine [ connect another MySQL database , **_NOT_** RDBMSs ] ( https : //dev.mysql.com/doc/refman/5.0/en/federated-limitations.html ) ; * [ MariaDB ] ( https : //mariadb.com/kb/en/introduction-to-the-connect-engine/ ) 's CONNECT engine allows access various file formats ( CSV , XML , Excel , etc ) , gives access `` '' ODBC data sources ( Oracle , DB2 , SQLServer , etc ) access data storage engines MyIsam InnoDB . * [ Farrago ] ( http : //farrago.sourceforge.net/design/sqlmed.html ) ; * [ PostgreSQL ] ( http : //www.postgresql.org/docs/9.3/static/fdwhandler.html ) implements parts ( notably implement routine mappings , simplified FDW API ) . It usable readeable since PG 9.1 writeable since 9.3 , prior [ DBI-Link ] ( http : //pgfoundry.org/projects/dbi-link/ ) . PostgreSQL communities plenty nice [ FDW ] ( http : //wiki.postgresql.org/wiki/Foreign_data_wrappers ) like noSQL FDW ( couchdb_fdw , mongo_fdw , redis_fdw ) , Multicorn ( using Python output instead C wrapper per se ) , nuts [ PGStrom ] ( http : //wiki.postgresql.org/wiki/PGStrom ) ( uses GPU operations ! )"
Stackoverflow,"I following tables : Employees -- -- -- -- -- -- - ClockNo int CostCentre varchar Department int Departments -- -- -- -- -- -- - DepartmentCode int CostCentreCode varchar Parent int Departments departments parents meaning infinite hierarchy . All departments belong cost centre always ` CostCentreCode ` . If ` parent = 0 ` top level department Employees _must_ ` CostCentre ` value may ` Department ` 0 meaning department **What I want try generate query give four levels hierarchy . Like : ** EmployeesLevels -- -- -- -- -- -- -- -- - ClockNo CostCentre DeptLevel1 DeptLevel2 DeptLevel3 DeptLevel4 I 've managed get something display department structure 's , I ca n't work link employees without creating duplicate employee rows : SELECT d1.Description AS lev1 , d2.Description lev2 , d3.Description lev3 , d4.Description lev4 FROM departments AS d1 LEFT JOIN departments AS d2 ON d2.parent = d1.departmentcode LEFT JOIN departments AS d3 ON d3.parent = d2.departmentcode LEFT JOIN departments AS d4 ON d4.parent = d3.departmentcode WHERE d1.parent=0 ; * * * SQL To create Structure sample data : CREATE TABLE Employees ( ClockNo integer NOT NULL PRIMARY KEY , CostCentre varchar ( 20 ) NOT NULL , Department integer NOT NULL ) ; CREATE TABLE Departments ( DepartmentCode integer NOT NULL PRIMARY KEY , CostCentreCode varchar ( 20 ) NOT NULL , Parent integer NOT NULL ) ; CREATE INDEX idx0 ON Employees ( ClockNo ) ; CREATE INDEX idx1 ON Employees ( CostCentre , ClockNo ) ; CREATE INDEX idx2 ON Employees ( CostCentre ) ; CREATE INDEX idx0 ON Departments ( DepartmentCode ) ; CREATE INDEX idx1 ON Departments ( CostCentreCode , DepartmentCode ) ; INSERT INTO Employees VALUES ( 1 , 'AAA ' , 0 ) ; INSERT INTO Employees VALUES ( 2 , 'AAA ' , 3 ) ; INSERT INTO Employees VALUES ( 3 , 'BBB ' , 0 ) ; INSERT INTO Employees VALUES ( 4 , 'BBB ' , 4 ) ; INSERT INTO Employees VALUES ( 5 , 'CCC ' , 0 ) ; INSERT INTO Employees VALUES ( 6 , 'AAA ' , 1 ) ; INSERT INTO Employees VALUES ( 7 , 'AAA ' , 5 ) ; INSERT INTO Employees VALUES ( 8 , 'AAA ' , 15 ) ; INSERT INTO Departments VALUES ( 1 , 'AAA ' , 0 ) ; INSERT INTO Departments VALUES ( 2 , 'AAA ' , 1 ) ; INSERT INTO Departments VALUES ( 3 , 'AAA ' , 1 ) ; INSERT INTO Departments VALUES ( 4 , 'BBB ' , 0 ) ; INSERT INTO Departments VALUES ( 5 , 'AAA ' , 3 ) ; INSERT INTO Departments VALUES ( 12 , 'AAA ' , 5 ) ; INSERT INTO Departments VALUES ( 15 , 'AAA ' , 12 ) ; This gives following structure ( employee clock numbers square brackets ) : Root | | -- -AAA [ 1 ] | \ -- -1 [ 6 ] | | -- -2 | \ -- -3 [ 2 ] | \ -- -5 [ 7 ] | \ -- -12 | \ -- -15 [ 8 ] | | -- -BBB [ 3 ] | \ -- -4 [ 4 ] | \ -- -CCC [ 5 ] **The query return following : ** ClockNo CostCentre Level1 Level2 Level3 Level4 1 AAA 2 AAA 1 3 3 BBB 4 BBB 4 5 CCC 6 AAA 1 7 AAA 1 3 5 8 AAA 1 3 5 12 * ` * ` In case Employee 8 , level5 . Ideally I would like show levels level4 , I happy show CostCentre case","Just use `` SUBSTRING '' function : SELECT * FROM `` BOM_SUB_LEVEL '' SUBSTRING ( TOP_CODE , 11 , 1 ) = `` _ '' Marc"
Stackoverflow,"In SQL Server database , one use table variables like : declare @ table table ( int ) In Azure Data Warehouse , throws error . > Parse error line : 1 , column : 19 : Incorrect syntax near 'table' In Azure Data Warehouse , use temporary tables : create table # table ( int ) inside functions . > Msg 2772 , Level 16 , State 1 , Line 6 Can access temporary tables within function . [ This document ] ( https : //docs.microsoft.com/en-us/sql/relational-databases/in- memory-oltp/faster-temp-table-and-table-variable-by-using-memory-optimization ) Microsoft says , > ◦Must declared two steps ( rather inline ) : ◾CREATE TYPE my_type AS TABLE ... ; , ◾DECLARE @ mytablevariable my_type ; . But I try : create type table ( int ) ; drop type ; I get : > Msg 103010 , Level 16 , State 1 , Line 1 Parse error line : 1 , column : 8 : Incorrect syntax near 'type ' . My objective function Azure Data Warehouse uses temporary table . Is achievable ? **Edit Start Here** Note I looking ways create one specific function . I actually done moved . I 'm veteran programmer Azure Data Warehouse rookie . I want know 's possible incorporate concept temporary tables Azure Data Warehouse function .","Drop External Table External File Format . Then recreate External File Format ` FIRST_ROW=2 ` skip one row mentioned [ documentation ] ( https : //msdn.microsoft.com/library/dn935026.aspx ) : CREATE EXTERNAL FILE FORMAT TextFileFormat WITH ( FORMAT_TYPE = DELIMITEDTEXT , FORMAT_OPTIONS ( FIELD_TERMINATOR = '|' , STRING_DELIMITER = `` , DATE_FORMAT = 'yyyy-MM-dd HH : mm : ss.fff' , USE_TYPE_DEFAULT = FALSE , FIRST_ROW = 2 ) ) ;"
Stackoverflow,"I table 3 fields , I want rank column based user_id game_id . Here SQL Fiddle : < http : //sqlfiddle.com/ # ! 9/883e9d/1 > table already I : user_id | game_id | game_detial_sum | -- -- -- -- | -- -- -- -- -| -- -- -- -- -- -- -- -- -- -- | 6 | 10 | 1000 | 6 | 11 | 260 | 7 | 10 | 1200 | 7 | 11 | 500 | 7 | 12 | 360 | 7 | 13 | 50 | expected output : user_id | game_id | game_detial_sum | user_game_rank | -- -- -- -- | -- -- -- -- -| -- -- -- -- -- -- -- -- -- -- | -- -- -- -- -- -- -- -- -- | 6 | 10 | 1000 | 1 | 6 | 11 | 260 | 2 | 7 | 10 | 1200 | 1 | 7 | 11 | 500 | 2 | 7 | 12 | 360 | 3 | 7 | 13 | 50 | 4 | My efforts far : SET @ : = 0 ; SELECT user_id , game_id , game_detail , CASE WHEN user_id = user_id THEN ( @ : = @ s+1 ) ELSE @ = 0 END As user_game_rank FROM game_logs **Edit : ** ( From OP [ Comments ] ( https : //stackoverflow.com/questions/53465111/set-rank-based-on- multiple-columns/53465139 # comment93801173_53465111 ) ) : Ordering based descending order ` game_detail ` > order game_detail","In [ Derived Table ] ( https : //dev.mysql.com/doc/refman/8.0/en/derived- tables.html ) ( subquery inside ` FROM ` clause ) , order data rows ` user_id ` values come together , sorting based ` game_detail ` Descending order . Now , use result-set use conditional ` CASE..WHEN ` expressions evaluate row numbering . It like Looping technique ( use application code , eg : PHP ) . We would store previous row values User-defined variables , check current row 's value ( ) previous row . Eventually , assign row number accordingly . **Edit : ** Based MySQL [ docs ] ( https : //dev.mysql.com/doc/refman/8.0/en/user- variables.html ) @ Gordon Linoff 's observation : > The order evaluation expressions involving user variables undefined . For example , guarantee SELECT @ , @ : = @ a+1 evaluates @ first performs assignment . We need evaluate row number assign ` user_id ` value ` @ u ` variable within expression . SET @ r : = 0 , @ u : = 0 ; SELECT @ r : = CASE WHEN @ u = dt.user_id THEN @ r + 1 WHEN @ u : = dt.user_id /* Notice : = instead = */ THEN 1 END AS user_game_rank , dt.user_id , dt.game_detail , dt.game_id FROM ( SELECT user_id , game_id , game_detail FROM game_logs ORDER BY user_id , game_detail DESC ) AS dt **Result** | user_game_rank | user_id | game_detail | game_id | | -- -- -- -- -- -- -- | -- -- -- - | -- -- -- -- -- - | -- -- -- - | | 1 | 6 | 260 | 11 | | 2 | 6 | 100 | 10 | | 1 | 7 | 1200 | 10 | | 2 | 7 | 500 | 11 | | 3 | 7 | 260 | 12 | | 4 | 7 | 50 | 13 | [ **View DB Fiddle** ] ( https : //www.db-fiddle.com/f/hoWE58V5RS4nRSdHsXoPrH/3 ) * * * An interesting note MySQL [ Docs ] ( https : //dev.mysql.com/doc/refman/8.0/en/user-variables.html ) , I discovered recently : > Previous releases MySQL made possible assign value user variable statements SET . This functionality supported MySQL 8.0 backward compatibility subject removal future release MySQL . Also , thanks fellow SO member , came across blog MySQL Team : < https : //mysqlserverteam.com/row-numbering-ranking-how-to-use-less-user- variables-in-mysql-queries/ > General observation using ` ORDER BY ` evaluation user variables query block , ensure values correct always . As , MySQL optimizer _may_ come place change _presumed_ order evaluation . Best approach problem would upgrade MySQL 8+ utilize [ ` Row_Number ( ) ` ] ( https : //dev.mysql.com/doc/refman/8.0/en/window-function- descriptions.html # function_row-number ) functionality : **Schema ( MySQL v8.0 ) ** SELECT user_id , game_id , game_detail , ROW_NUMBER ( ) OVER ( PARTITION BY user_id ORDER BY game_detail DESC ) AS user_game_rank FROM game_logs ORDER BY user_id , user_game_rank ; **Result** | user_id | game_id | game_detail | user_game_rank | | -- -- -- - | -- -- -- - | -- -- -- -- -- - | -- -- -- -- -- -- -- | | 6 | 11 | 260 | 1 | | 6 | 10 | 100 | 2 | | 7 | 10 | 1200 | 1 | | 7 | 11 | 500 | 2 | | 7 | 12 | 260 | 3 | | 7 | 13 | 50 | 4 | [ **View DB Fiddle** ] ( https : //www.db-fiddle.com/f/hoWE58V5RS4nRSdHsXoPrH/1 )"
Stackoverflow,"Created stream following field CREATE STREAM pageviews_original_string ( view_time string , user_id varchar , pageid varchar ) WITH ( kafka_topic='pageviews ' , value_format='DELIMITED ' , KEY='pageid ' ) ; Changed pageid uppercase along following values . create stream up_case AS SELECT UCASE ( pageid ) , user_id FROM PAGEVIEWS_ORIGINAL_STRING user_id = 'User_9 ' ; outcome PAGE_26 | User_9 PAGE_67 | User_9 PAGE_39 | User_9 PAGE_80 | User_9 PAGE_40 | User_9 PAGE_92 | User_9 Now want condition satisfied data modified extracted along remaining field values something like ****PAGE_26 | User_9 PAGE_67 | User_9 PAGE_39 | User_9 PAGE_80 | User_9 PAGE_40 | User_9 PAGE_92 | User_9**** Page_66 | User_7 Page_25 | User_2 Page_41 | User_3 Page_34 | User_1 Page_28 | User_2 Page_55 | User_5 Page_77 | User_5 Page_32 | User_8 Page_60 | User_4 please help solving use case","You counts count key query . You two queries , one counting value given column another counting values given column . Let 's assume stream two columns , col1 col2 . To count value col1 infinite window size use following query : SELECT col1 , count ( * ) FROM mystream1 GROUP BY col1 ; To count rows need write two queries since KSQL always needs GROUP BY clause aggregation . First create new column constant value count values new column since constant , count represent count rows . Here example : CREATE STREAM mystream2 AS SELECT 1 AS col3 FROM mystream1 ; SELECT col3 , count ( * ) FROM mystream2 GROUP BY col3 ;"
Stackoverflow,Fabric : 10.101.90.5 Master : 10.101.90.6 Slave : 10.101.90.7 I add slave group I get error : mysqlfabric group add mygroup 10.101.90.7 Password admin : Fabric UUID : 5calable-a007-feed-food-cab3fe13249e Time-to-live : 1 ServerError : Error accessing server ( 10.101.90.7 ) : 2003 : Can connect MySQL server '10.101.90.7:3306 ' ( 113 No route host ) Does anyone know happened ? How I solve ?,"Just noticed non-fabric driver also jar , setting ENVIRONMENT.DRIVER modifications Environment.URL fixed . Configuration cfg = new Configuration ( ) .setProperty ( Environment.URL , '' jdbc : mysql : //localhost/songkong '' ) .setProperty ( Environment.DRIVER , '' com.mysql.jdbc.Driver '' ) .setProperty ( Environment.DIALECT , `` org.hibernate.dialect.MySQL5Dialect '' ) .setProperty ( `` hibernate.show_sql '' , `` true '' ) .setProperty ( `` hibernate.connection.username '' , '' dbuser '' ) .setProperty ( `` hibernate.connection.password '' , `` dbpassword '' ) ; ;"
Stackoverflow,"I database including tables , I want delete data tables includes `` Auto Increment '' field , using query : delete test.table1 ; I got error : Error Code : 1030Got error -1 storage engine Why happens ? What I ?",Try change ` innodb_force_recovery ` value ( ` /etc/my.cnf ` ) . **Error -1** says NOTHING . Without tables creation code ( ` SHOW CREATE TABLE table_name ` ) say exactly problem .
Stackoverflow,"I trying update ` Time_Stamp ` field table , ` simple_pack_data ` , match values similarly titled field ` temp_data ` table . The tables fields called ` Test_Number ` ` Time_Marker ` , I'm using ` INNER JOIN ` tables . ` Time_Marker ` like reading count , ` Time_Stamp ` actual time start test . I want update ` Time_Stamp ` one test time , code I trying : UPDATE simple_pack_data INNER JOIN ( SELECT * FROM temp_data WHERE t.Test = `` 3 '' ) AS tmp ON s.Test_Number = tmp.Test_Number AND s.Time_Marker = tmp.Time_Marker SET s.Time_Stamp = tmp.Time_Stamp WHERE s.Test_Number = `` 3 '' ; When I run takes 50 seconds I get 1205 error . If I run similarly structured select statement : SELECT * FROM simple_pack_data INNER JOIN ( SELECT * FROM temp_data WHERE t.Test = `` 3 '' ) AS tmp ON s.Test_Number = tmp.Test AND s.Time_Marker = tmp.Time_Marker WHERE s.Test_Number = `` 3 '' ; It takes much less second I know join working fine . Is update really taking long ? If , way change timeout value get ?","This error entirely MySQL . The best solution get MySQL , lacking ability , [ performance blog post ] ( http : //mysqlperformanceblog.com/2012/03/27/innodbs-gap-locks ) helped get around past . MySQL lot little gotchas . It 's like working Access , half time program going wrong thing raise error ."
Stackoverflow,"We trying use FOR JSON Path SQL Server 2016 forming Nested Array SQL Query . SQL Query : SELECT A , B.name [ child.name ] , B.date [ child.date ] Table 1 join Table 2 Table 1.ID=Table 2.ID FOR JSON PATH Desired Output : [ { A : '' text '' , `` child : '' [ { `` name '' : '' value '' , `` date '' : '' value '' } , { `` name '' : '' value '' , `` date '' : '' value '' } ] } ] However getting : [ { A : '' text '' , `` child : '' { `` name '' : '' value '' , `` date '' : '' value '' } } , { A : '' text '' , `` child '' : { `` name '' : '' value '' , `` date '' : '' value '' } } ] How use FOR JSON PATH form nested child array .","With SQL Server 2016 , done using built-in functions manipulate JSON data . The following function return modified JSON data : JSON_MODIFY ( JsonColumn , ' $ .Info2 ' , 'Value2 ' ) The expression used normal ` UPDATE ` statement : UPDATE Table1 SET JsonColumn = JSON_MODIFY ( JsonColumn , ' $ .Info2 ' , 'Value2 ' ) The ` NULL ` values ` JsonColumn ` updated ` { `` Info2 '' : '' Value2 '' } ` . If ` JsonColumn ` contains another value ` Info2 ` key , overwritten ."
Stackoverflow,"I feel like dork asking , I 'm getting help Google , I paged SO 's results simple search SMO n't see either . The short version I 'm starting play around T4 . I 'm expanding [ Oleg Sych 's initial tutorial ] ( http : //www.olegsych.com/2008/09/t4-tutorial- creatating-your-first-code-generator/ ) provide enumeration tables create ( IMHO rather silly ) delete proc . This experiment , utter uselessness n't bother . : ) My expansion Oleg 's tutorial looks like : < # @ template language= '' C # '' hostspecific= '' true '' # > < # @ output extension= '' SQL '' # > < # @ assembly name= '' Microsoft.SqlServer.ConnectionInfo '' # > < # @ assembly name= '' Microsoft.SqlServer.Smo '' # > < # @ import namespace= '' Microsoft.SqlServer.Management.Smo '' # > < # @ include file= '' T4Toolbox.tt '' # > < # // Config variables string serverName = `` dbserver\\dbinstance '' ; string dbName = `` dbname '' ; # > USE < # = dbName # > < # // Iterate tables generate procs Server server = new Server ( serverName ) ; Database database = new Database ( server , dbName ) ; WriteLine ( `` /* Number tables : `` + database.Tables.Count.ToString ( ) + `` */ '' ) ; foreach ( Table table database.Tables ) { table.Refresh ( ) ; # > CREATE PROCEDURE < # = table.Name # > _Delete < # PushIndent ( `` `` ) ; foreach ( Column column table.Columns ) { ( column.InPrimaryKey ) WriteLine ( `` @ '' + column.Name + `` `` + column.DataType.Name ) ; } PopIndent ( ) ; # > AS DELETE FROM < # = table.Name # > WHERE < # PushIndent ( `` `` ) ; foreach ( Column column table.Columns ) { ( column.InPrimaryKey ) WriteLine ( column.Name + `` = @ '' + column.Name ) ; } PopIndent ( ) ; WriteLine ( `` GO '' ) ; } # > The issue tables returned ` Tables ` collection . This validated table count SQL comment I 'm generating , outputs ` 0 ` . As written , code generates following : USE dbname /* Number tables : 0 */ However , I remove loop manually supply valid table name exists database , generates ( silly ) proc -- table . The tables separated schema , would matter ? Also , going SQL2005 instance -- would potentially cause issues ? Finally , I 'm also finding I ca n't enumerate synonyms via Synonyms collection . ( I thought I 'd clever go route since tables schema , synonyms defined . But ... dice . ) Again , reiterate , code naturally production , even production worthy . I 'm trying learn T4 SMO , hit roadblock trying something I 'd thought would ridiculously simple . : )","SMO retrieve metadata automatically simply create new instance Database class . Retrieving metadata take , especially cold environment . Call database.Refresh ( ) loop ."
Stackoverflow,"Can I ask difference ` xp_sendmail ` ` sp_send_dbmail ` proc ? They send e-mail message , may include query result set attachment , specified recipients ... .. What difference ?","` xp_sendmail ` requires MAPI client installed , Outlook , server . This option SQL Server 2000 . ` sp_send_dbmail ` simple SMTP solution , added SQL Server 2005+ ` sp_send_dbmail ` far better ."
Stackoverflow,"I system multiple satellites create financial transactions need sync core server . The satellites remote servers run Rails apps local Postgres database . The core another Rails app Postgres database . The satellites core pretty much schema ( identical ) . Everything containerized ( apps database ) . Very rarely , core server update data satellites need . Currently I one satellite , number grow couple ( I ’ think 100 distant future ) . There problem sequence contention core satellites . The core never update transaction satellites satellites update transaction satellites . Even better , financial transactions uuid primary key . Since multi-master sync problem , I naturally came across BDR . I following questions : 1 . Is BDR production ready stable ? I ’ reading several competing technologies ( like Bucardo Londiste ) . Will really part Postgres 9.6 ? 2 . Can BDR handle disconnected model ? I ’ think often , satellites could disconnected hours . 3 . Can BDR selective syncs ? For example , I ’ want certain tables sync-ed . 4 . Could BDR handle 100 satellites ?","> Is BDR production ready stable ? Yes , BDR 1.0 BDR-Postgres 9.4 production-ready stable . But I would say [ since I work 2ndQuadrant , develop BDR ] ( http : //2ndquadrant.com/bdr ) . It _not_ drop-in replacement standalone PostgreSQL use without application changes though . See overview section manual . > I ’ reading several competing technologies ( like Bucardo Londiste ) . They 're different . Different trade-offs . There 's discussion BDR manual , course , take grain salt since hardly claim unbiased . > Will really part Postgres 9.6 ? No , definitely . Where seen claim ? There future ( yet ) _extension_ released _add_ BDR PostgreSQL 9.6 's ready . But wo n't _part of_ PostgreSQL 9.6 , 'll something install top . > Can BDR handle disconnected model ? I ’ think often , satellites could disconnected hours . Yes , handles temporary partitions network outages well , caveats around global sequences . See manual details . > Can BDR selective syncs ? Yes . See manual replication sets . Table structure always replicated . So initial table contents moment . But table changes replicated selectively , table-by-table . > For example , I ’ want certain tables sync-ed . Sure . > Could BDR handle 100 satellites ? Not well . It 's mesh topology would expect every satellite talk every satellite . Also , 'd 198 backends ( 99 walsenders + 99 apply workers ) per node . Not pretty . You really want star-and-hub model satellite talks hub . That 's supported BDR 1.0 , targeted support BDR 2.0 . I think better use case [ pglogical ] ( http : //2ndquadrant.com/ ) Londiste . I ca n't really go detail , since overlaps commercial consulting services I involved . The team I work designs things like customers [ professional service ] ( https : //www.postgresql.org/support/professional_support/ ) ."
Stackoverflow,"Familiar question , Vertica . I 'd like return top 5 geo_country rows based sum ( imps ) tag_id . This query I started : SELECT tag_id , geo_country , SUM ( imps ) AS imps , RANK ( ) OVER ( PARTITION BY tag_id ORDER BY SUM ( imps ) DESC ) AS rank FROM table1 WHERE tag_id IN ( 2013150,1981153 ) AND ymd > CURRENT_DATE - 3 GROUP BY 1 , 2 LIMIT 10 ; This actually returns rows first tag WHERE clause ( 2013150 ) . I know tag sum ( imps ) values high enough include results . Also , I implement Top N part ? I tried adding LIMIT clause within OVER function , n't look like accepted parameter .",In ` vsql ` type : \timing hit Enter . You 'll like 'll see : - ) Repeating turn .
Stackoverflow,"I read MOS Doc ID 1945619.1 starting 12.1.3 Oracle HTTP Server ( OHS ) , mod_plsql feature deprecated included 12.2 Oracle HTTP Server . For future , Oracle recommends moving Oracle REST Data Services ( formerly known Oracle APEX Listener ) alternative mod_plsql . Our shop lot mod_plsql applications ( i.e . applications written usinjg HTP/HTF packages ) production . Since I n't know anything Oracle REST Data Services I 'm asking migrate old applications new product without changing code . Thank . Kind regards , Cristian","If n't access Apache config , probably put following code top Oracle procedure : owa_util.get_cgi_env ( 'REQUEST_METHOD ' ) ! = 'POST ' raise_application_error ( -20001 , 'Only POST request method allowed . ' ) ; end ;"
Stackoverflow,"@ Entity public class FruitStore { @ Id private Long storeId ; @ ElementCollection private Set < Fruit > fruits ; } Of course , ` Fruit ` class marked ` @ Embeddable ` . In database ( postgresql exact , although n't matter ) , table created called ` fruitstore_fruits ` . It grows huge , queries become slow . I manually modified database , ` fruitstore_fruits ` table indexes ` FruitStore ` ` id ` column . Happily , dramatically improves performance . I want done automatically . The question , I annotate code get Hibernate automatically index fruitstore_fruits ` FruitStore ` ` id ` column ? **EDIT : ** [ This Hibernate bug ] ( https : //hibernate.atlassian.net/browse/HHH-4263 ) removed lots hope . I think I want simply supported right . Which kinda sad , feature n't exotic ( indexing element collection foreign column ) . However , I 'd _love_ proven wrong .","There 's difference Hibernate configuration file ( hibernate.cfg.xml ) Hibernate mapping files ( *.hbm ) . If heard generated code concerns last ones , first one included mapping files . There 's two three different approaches development . First create Java classes mappings generate database schema , Second create database schema generate Java classes ( reverse engineering ) , Third create classes mapping existed schema . Whichever approach use , ` hibernate.cfg.xml ` create manually 's created IDE . Some frameworks like Spring provide 's configuration Hibernate , thus completely ignoring ` hibernate.cfg.xml ` . There 's nothing magical creating ` hibernate.cfg.xml ` file . You start < ? xml version= ' 1.0 ' encoding='UTF-8 ' ? > < ! DOCTYPE hibernate-configuration PUBLIC `` -//Hibernate/Hibernate Configuration DTD 3.0//EN '' `` http : //hibernate.sourceforge.net/hibernate-configuration-3.0.dtd '' > < hibernate-configuration > < session-factory > < ! -- place properties mapping -- > < /session-factory > < /hibernate-configuration > And right , ` src ` folder project . At runtime ` .class ` files generated , i.e . classpath ."
Stackoverflow,"I 'm trying add Azure SQL Server connection string ` app.config ` file , red underlines connection string I try copy paste Azure . I 'm using Windows Forms Visual Studio . Here connection string : < ? xml version= '' 1.0 '' encoding= '' utf-8 '' ? > < configuration > < connectionStrings > < add name= '' AddSales '' Server= '' tcp : Sales99.database.windows.net,1433 ; Initial '' Catalog= '' Sales ; Persist '' Security= '' '' Info= '' False ; User '' ID= '' '' { your_username } ; Password= '' '' { your_password } ; MultipleActiveResultSets= '' '' False ; Encrypt= '' '' True ; TrustServerCertificate= '' False ; Connection '' Timeout= '' 30 '' providerName= '' System.Data.SqlClient '' / > < /connectionStrings > < /configuration > Is way fix issue ? Please advise .","Microsoft reached provided sample resource template accomplish : { `` $ schema '' : `` http : //schema.management.azure.com/schemas/2014-04-01-preview/deploymentTemplate.json # '' , `` contentVersion '' : `` 1.0.0.0 '' , `` parameters '' : { `` SQL Administrator Login '' : { `` type '' : `` String '' } , `` SQL Administrator Password '' : { `` type '' : `` SecureString '' } , `` AAD Admin Login '' : { `` type '' : `` String '' } , `` AAD Admin ObjectID '' : { `` type '' : `` String '' } , `` AAD TenantId '' : { `` type '' : `` String '' } , `` Location ( Region ) '' : { `` type '' : `` String '' } , `` Server Name '' : { `` type '' : `` String '' } } , `` variables '' : { } , `` resources '' : [ { `` type '' : `` Microsoft.Sql/servers '' , `` name '' : `` [ parameters ( 'Server Name ' ) ] '' , `` apiVersion '' : `` 2014-04-01-preview '' , `` location '' : `` [ parameters ( 'Location ( Region ) ' ) ] '' , `` properties '' : { `` administratorLogin '' : `` [ parameters ( 'SQL Administrator Login ' ) ] '' , `` administratorLoginPassword '' : `` [ parameters ( 'SQL Administrator Password ' ) ] '' , `` version '' : `` 12.0 '' } , `` resources '' : [ { `` type '' : `` firewallrules '' , `` name '' : `` AllowAllWindowsAzureIps '' , `` apiVersion '' : `` 2014-04-01-preview '' , `` location '' : `` [ parameters ( 'Location ( Region ) ' ) ] '' , `` properties '' : { `` endIpAddress '' : `` 0.0.0.0 '' , `` startIpAddress '' : `` 0.0.0.0 '' } , `` dependsOn '' : [ `` [ concat ( 'Microsoft.Sql/servers/ ' , parameters ( 'Server Name ' ) ) ] '' ] } , { `` type '' : `` administrators '' , `` name '' : `` activeDirectory '' , `` apiVersion '' : `` 2014-04-01-preview '' , `` location '' : `` [ parameters ( 'Location ( Region ) ' ) ] '' , `` properties '' : { `` administratorType '' : `` ActiveDirectory '' , `` login '' : `` [ parameters ( 'AAD Admin Login ' ) ] '' , `` sid '' : `` [ parameters ( 'AAD Admin ObjectID ' ) ] '' , `` tenantId '' : `` [ parameters ( 'AAD TenantID ' ) ] '' } , `` dependsOn '' : [ `` [ concat ( 'Microsoft.Sql/servers/ ' , parameters ( 'Server Name ' ) ) ] '' ] } ] } ] }"
Stackoverflow,"I 'm using Hugsql Clojure access Postgresql db . Several database tables optional columns - simple example consider '' users '' table various address columns - address1 , address2 , city , etc . When I write Hugsql query specification `` update '' I n't know values present map I pass . So I write query : -- : name update-user ! : ! : n UPDATE users set firstname = : firstname , address1 = : address1 id = : id pass user map ( update-user ! { : id `` testuser '' : firstname `` Bartholamew '' } ) exception thrown . I 'd expect create something like UPDATE users SET firstname='Bartholamew ' , address1=NULL id='testuser' I 've looked Hugsql source - calls ( validate-parameters ) function throws exception I ca n't see way around . I 'm sure I 'm missing something obvious : n't seem like unusual requirement , I sure n't want write distinct SQL query spec every possible combination optional columns . Is way handle missing parameters I 'm missing ? Am I abusing database optional columns ?","If 're using Postgres 9.5 newer ( I assume , since released back January 2016 ) , 's useful ` ON CONFLICT ` cluase use : INSERT INTO mytable ( id , col1 , col2 ) VALUES ( 123 , 'some_value ' , 'some_other_value ' ) ON CONFLICT ( id ) DO NOTHING"
Stackoverflow,"I 'm compiling project XCode MySQL++ included linked . For reason , I keep getting following compiler error : 'assert ’ declared scope originating cpool.h , header file 's part MySQL++ . Does anyone know triggered ? EDIT : For reference , MySQL++ installed via Macports .","While stacker 's answer work , MySQL++ wraps function [ SimpleResult : :insert_id ( ) ] ( http : //tangentsoft.net/mysql++/doc/html/refman/classmysqlpp_1_1SimpleResult.html # c7493c4b03818e8a88b69abc987286b6 ) . Example : Query q = conn.query ( ) ; q.insert ( something ) ; ( SimpleResult res = q.execute ( ) ) { cout < < `` Auto-increment value : `` < < res.insert_id ( ) < < endl ; }"
Stackoverflow,"I need use foreign keys update cascade , etc . ALTER TABLE topics ADD FOREIGN KEY ( topic_by ) REFERENCES users ( user_id ) ON DELETE RESTRICT ON UPDATE CASCADE ; I able make foreign keys SQL Buddy . Any way ?","I issue I solve . You must folder named `` exports '' '' export '' ( read somewhere ) , I use first one name . Create folder sqlbuddy folder /../sqlbuddy/calvinlough-sqlbuddy-1b38cf4/exports/ path maybe different case , principle . After must give folder 777 permission . chmod -R 777 /../sqlbuddy/calvinlough-sqlbuddy-1b38cf4/exports/ Ok , done . Sqlbuddy export sql base file . This may secure , I advise remove .sql file server every export . Hope help , help ."
Stackoverflow,"This follow-up [ question ] ( https : //stackoverflow.com/questions/49752388/editable-qtableview-of- complex-sql-query ) . In , created editable subclass QSqlQueryModel , use complex queries . Now I need add functionality like QTableModel 's setEditStrategy I cache changes accept revert using buttons . PyQt apparently n't allow multiple inheritance I find sufficient documentation re-implement method custom model , therefor 's question : **How I re-implement QSqlTableModel.setEditStragety ( something like ) including RevertAll ( ) SubmitAll ( ) editable QSqlQueryModel ? ** Here 's CVME : ( I out-commented parts Example class I would like get working ) import sys PyQt5.QtCore import Qt PyQt5.QtSql import QSqlDatabase , QSqlQuery , QSqlQueryModel , QSqlTableModel PyQt5.QtWidgets import QApplication , QTableView , QWidget , QGridLayout PyQt5.Qt import QPushButton db_file = `` test.db '' def create_connection ( file_path ) : db = QSqlDatabase.addDatabase ( `` QSQLITE '' ) db.setDatabaseName ( file_path ) db.open ( ) : print ( `` Can establish database connection { } ! `` .format ( file_path ) ) return False return True def fill_tables ( ) : q = QSqlQuery ( ) q.exec_ ( `` DROP TABLE IF EXISTS Manufacturers ; '' ) q.exec_ ( `` CREATE TABLE Manufacturers ( Company TEXT , Country TEXT ) ; '' ) q.exec_ ( `` INSERT INTO Manufacturers VALUES ( 'VW ' , 'Germany ' ) ; '' ) q.exec_ ( `` INSERT INTO Manufacturers VALUES ( 'Honda ' , 'Japan ' ) ; '' ) q.exec_ ( `` DROP TABLE IF EXISTS Cars ; '' ) q.exec_ ( `` CREATE TABLE Cars ( Company TEXT , Model TEXT , Year INT ) ; '' ) q.exec_ ( `` INSERT INTO Cars VALUES ( 'Honda ' , 'Civic ' , 2009 ) ; '' ) q.exec_ ( `` INSERT INTO Cars VALUES ( 'VW ' , 'Golf ' , 2013 ) ; '' ) q.exec_ ( `` INSERT INTO Cars VALUES ( 'VW ' , 'Polo ' , 1999 ) ; '' ) class SqlQueryModel_editable ( QSqlQueryModel ) : `` '' '' subclass QSqlQueryModel individual columns defined editable `` '' '' def __init__ ( self , editables ) : `` '' '' editables dict format : { INT editable_column_nr : ( STR update query performed changes made column INT model 's column number filter-column ( used where-clause ) , ) } `` '' '' super ( ) .__init__ ( ) self.editables = editables def flags ( self , index ) : fl = QSqlQueryModel.flags ( self , index ) index.column ( ) self.editables : fl |= Qt.ItemIsEditable return fl def setData ( self , index , value , role=Qt.EditRole ) : role == Qt.EditRole : mycolumn = index.column ( ) mycolumn self.editables : ( query , filter_col ) = self.editables [ mycolumn ] filter_value = self.index ( index.row ( ) , filter_col ) .data ( ) q = QSqlQuery ( query.format ( value , filter_value ) ) result = q.exec_ ( ) result : self.query ( ) .exec_ ( ) else : print ( self.query ( ) .lastError ( ) .text ( ) ) return result return QSqlQueryModel.setData ( self , index , value , role ) def setFilter ( self , myfilter ) : text = ( self.query ( ) .lastQuery ( ) + `` WHERE `` + myfilter ) self.setQuery ( text ) class Example ( QWidget ) : def __init__ ( self ) : super ( ) .__init__ ( ) self.resize ( 400 , 150 ) self.createModel ( ) self.initUI ( ) def createModel ( self ) : editables = { 1 : ( `` UPDATE Manufacturers SET Country = ' { } ' WHERE Company = ' { } ' '' , 2 ) } self.model = SqlQueryModel_editable ( editables ) query = `` ' SELECT ( comp.company || `` `` || cars.model ) Car , comp.Country , cars.company , ( CASE WHEN cars.Year > 2000 THEN 'yes ' ELSE 'no ' END ) this_century manufacturers comp left join cars comp.company = cars.company `` ' q = QSqlQuery ( query ) self.model.setQuery ( q ) self.model.setFilter ( `` cars.Company = 'VW ' '' ) # self.model.setEditStrategy ( QSqlTableModel.OnManualSubmit ) def initUI ( self ) : self.layout = QGridLayout ( ) self.setLayout ( self.layout ) self.view = QTableView ( ) self.view.setModel ( self.model ) self.view.hideColumn ( 2 ) self.layout.addWidget ( self.view,0,0,1,2 ) self.accept_btn = QPushButton ( `` Accept Changes '' ) # self.accept_btn.clicked.connect ( self.model.submitAll ) self.layout.addWidget ( self.accept_btn , 1,0 ) self.reject_btn = QPushButton ( `` Reject Changes '' ) # self.reject_btn.clicked.connect ( self.model.revertAll ) self.layout.addWidget ( self.reject_btn , 1,1 ) __name__ == '__main__ ' : app = QApplication ( sys.argv ) create_connection ( db_file ) : sys.exit ( -1 ) fill_tables ( ) ex = Example ( ) ex.show ( ) sys.exit ( app.exec_ ( ) ) **Edit clarify : ** I need editable QSqlQueryModel , I use ` submitAll ( ) ` ` revertAll ( ) ` , changes model 's data accepted Accept-button clicked , reverted using `` Reject '' button .","Using information _A generic approach_ [ _How Use QSqlQueryModel QML_ ] ( https : //wiki.qt.io/How_to_Use_a_QSqlQueryModel_in_QML ) build general model , make easy use QML create property pass query . **sqlquerymodel.h** # ifndef SQLQUERYMODEL_H # define SQLQUERYMODEL_H # include < QSqlQuery > # include < QSqlQueryModel > # include < QSqlRecord > class SqlQueryModel : public QSqlQueryModel { Q_OBJECT Q_PROPERTY ( QString query READ queryStr WRITE setQueryStr NOTIFY queryStrChanged ) Q_PROPERTY ( QStringList userRoleNames READ userRoleNames CONSTANT ) public : using QSqlQueryModel : :QSqlQueryModel ; QHash < int , QByteArray > roleNames ( ) const { QHash < int , QByteArray > roles ; ( int = 0 ; < record ( ) .count ( ) ; ++ ) { roles.insert ( Qt : :UserRole + + 1 , record ( ) .fieldName ( ) .toUtf8 ( ) ) ; } return roles ; } QVariant data ( const QModelIndex & index , int role ) const { QVariant value ; ( index.isValid ( ) ) { ( role < Qt : :UserRole ) { value = QSqlQueryModel : :data ( index , role ) ; } else { int columnIdx = role - Qt : :UserRole - 1 ; QModelIndex modelIndex = this- > index ( index.row ( ) , columnIdx ) ; value = QSqlQueryModel : :data ( modelIndex , Qt : :DisplayRole ) ; } } return value ; } QString queryStr ( ) const { return query ( ) .lastQuery ( ) ; } void setQueryStr ( const QString & query ) { ( queryStr ( ) == query ) return ; setQuery ( query ) ; emit queryStrChanged ( ) ; } QStringList userRoleNames ( ) const { QStringList names ; ( int = 0 ; < record ( ) .count ( ) ; ++ ) { names < < record ( ) .fieldName ( ) .toUtf8 ( ) ; } return names ; } signals : void queryStrChanged ( ) ; } ; # endif // SQLQUERYMODEL_H **main.cpp** # include `` sqlquerymodel.h '' # include < QGuiApplication > # include < QQmlApplicationEngine > # include < QDebug > # include < QSqlError > static bool createConnection ( ) { QSqlDatabase db = QSqlDatabase : :addDatabase ( `` QSQLITE '' ) ; db.setDatabaseName ( `` : memory : '' ) ; ( ! db.open ( ) ) { qDebug ( ) < < `` Can open database\n '' `` Unable establish database connection.\n '' `` This example needs SQLite support . Please read `` `` Qt SQL driver documentation information `` `` build it.\n\n '' `` Click Cancel exit . `` ; return false ; } QSqlQuery query ; ( ! query.exec ( `` CREATE TABLE COMPANY ( `` `` ID INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL , '' `` NAME TEXT NOT NULL , '' `` AGE INT NOT NULL , '' `` SALARY REAL '' `` ) '' ) ) { qDebug ( ) < < query.lastError ( ) .text ( ) ; } ( int i=0 ; < 10 ; i++ ) { query.prepare ( `` insert COMPANY ( NAME , AGE , SALARY ) values ( : name , : age , : salary ) '' ) ; query.bindValue ( `` : name '' , QString ( `` name- % 1 '' ) .arg ( ) ) ; query.bindValue ( `` : age '' , ( i+1 ) *1000 ) ; query.bindValue ( `` : salary '' , ( 11-i ) *11.5 ) ; ( ! query.exec ( ) ) { qDebug ( ) < < query.lastError ( ) .text ( ) ; } } return true ; } int main ( int argc , char *argv [ ] ) { qmlRegisterType < SqlQueryModel > ( `` Foo '' , 1 , 0 , `` SqlQueryModel '' ) ; QCoreApplication : :setAttribute ( Qt : :AA_EnableHighDpiScaling ) ; QGuiApplication app ( argc , argv ) ; ( ! createConnection ( ) ) return -1 ; QQmlApplicationEngine engine ; engine.load ( QUrl ( QStringLiteral ( `` qrc : /main.qml '' ) ) ) ; ( engine.rootObjects ( ) .isEmpty ( ) ) return -1 ; return app.exec ( ) ; } **main.qml** import QtQuick 2.9 import QtQuick.Window 2.2 import QtQuick.Controls 1.4 import Foo 1.0 Window { visible : true width : 640 height : 480 title : qsTr ( `` SqlQueryModel '' ) SqlQueryModel { id : sqlmodel query : `` select * COMPANY '' } Component { id : columnComponent TableViewColumn { width : 100 } } TableView { id : view anchors.fill : parent resources : { var roleList = sqlmodel.userRoleNames var temp = [ ] ( var roleList ) { var role = roleList [ ] temp.push ( columnComponent.createObject ( view , { `` role '' : role , `` title '' : role } ) ) } return temp } model : sqlmodel } } [ ! [ enter image description ] ( https : //i.stack.imgur.com/t2eYk.png ) ] ( https : //i.stack.imgur.com/t2eYk.png )"
Stackoverflow,"I SQL Server installed Linux . It installed Microsoft 's repos described : < https : //docs.microsoft.com/en-us/sql/linux/quickstart- install-connect-ubuntu > In MySql I used write ` EXPLAIN ` front query see execution plan . In SQL Server n't seem work . But I n't studio program installed , SQL Server ` sqlcmd ` tool . How I see execution plan query SQL Server Linux ?","Microsoft released new tool called [ SQL Operations studio ] ( https : //github.com/Microsoft/sqlopsstudio ) , similar SSMS , available Windows , Linux , Macos . **location download : ** < https : //docs.microsoft.com/en-us/sql/sql-operations-studio/download > Below screenshot looks like [ ! [ enter image description ] ( https : //i.stack.imgur.com/THna9.jpg ) ] ( https : //i.stack.imgur.com/THna9.jpg ) **To view actual execution plan using sqlopsstudio ( steps platforms ) ** * Press ` CTRL+SHIFT+P ` * Type run query actual execution plan shown select highlighted , get actual execution plan [ ! [ enter image description ] ( https : //i.stack.imgur.com/NAlW1.gif ) ] ( https : //i.stack.imgur.com/NAlW1.gif ) **To view estimated execution plan : ** Just press ICON shown [ ! [ enter image description ] ( https : //i.stack.imgur.com/Q3HfU.gif ) ] ( https : //i.stack.imgur.com/Q3HfU.gif ) **You also use keybinding view actual execution plan .Below steps** 1.Press ` CTRL+SHIFT+P ` 2.Type keyboard shortcuts 3.In search plan type actual shown [ ! [ enter image description ] ( https : //i.stack.imgur.com/WKSW8.gif ) ] ( https : //i.stack.imgur.com/WKSW8.gif ) 4.Right click actual query plan shortcut say ` add key binding ` key choice ( ` CTRL+M ` [ ! [ enter image description ] ( https : //i.stack.imgur.com/5C3aH.gif ) ] ( https : //i.stack.imgur.com/5C3aH.gif ) **Below Part answer written time SQLOPS studio available.This ben helpfull one n't SQLopsstudio : ** Currently viewing execution plan supported Windows , using SSMS third party tool like SQLSentry.. There feature request tracked : [ Return ShowPlan data Text XML Query Execution ] ( https : //github.com/Microsoft/vscode- mssql/issues/509 ) one option connect using VSCODE linux set ` show plan xml ` shown screenshot below..this provides xml execution plan SET showplan_xml ON ; [ ! [ enter image description ] ( https : //i.stack.imgur.com/bkvJX.png ) ] ( https : //i.stack.imgur.com/bkvJX.png ) take xml upload [ Paste The Plan ] ( https : //www.brentozar.com/pastetheplan/ ) website view plans Below screenshot XML [ ! [ enter image description ] ( https : //i.stack.imgur.com/XVFqj.png ) ] ( https : //i.stack.imgur.com/XVFqj.png ) also view SQLSENTRY plan explorer well ( Windows ) indepth analysis [ ! [ enter image description ] ( https : //i.stack.imgur.com/IZ1Dk.png ) ] ( https : //i.stack.imgur.com/IZ1Dk.png )"
Stackoverflow,"How I tell SQLAlchemy automatically reflect basic Foreign Key references references ORM objects integer fields ? In [ SQLAlchemy ] ( http : //www.sqlalchemy.org/docs/orm/tutorial.html # building-a- relationship ) [ SqlSoup ] ( http : //www.sqlalchemy.org/docs/orm/extensions/sqlsoup.html # relationships ) , table columns reflected automatically relations defined manually : class User ( Base ) : __table__ = metadata.tables [ 'users ' ] loan = relation ( Loans ) ... You define relationships SqlSoup classes : > > > db.users.relate ( 'loans ' , db.loans )","In python , pass dictionary _keyword arguments_ function using ` ** ` argument packing syntax . If dictionary ` arguments ` defined : arguments = { 'blah_field ' : 'blah_value ' } You call ` db.blah_table.insert ` dictionary keyword arguments like : db.blah_table.insert ( **arguments ) Under hood equivalent : db.blah_table.insert ( blah_field=blah_value ) As aside , ` ** ` unpack named arguments dictionary , single ` * ` used unpack positional arguments list tuple ."
Stackoverflow,"I SQL Server installed Linux . It installed Microsoft 's repos described : < https : //docs.microsoft.com/en-us/sql/linux/quickstart- install-connect-ubuntu > In MySql I used write ` EXPLAIN ` front query see execution plan . In SQL Server n't seem work . But I n't studio program installed , SQL Server ` sqlcmd ` tool . How I see execution plan query SQL Server Linux ?","Microsoft released new tool called [ SQL Operations studio ] ( https : //github.com/Microsoft/sqlopsstudio ) , similar SSMS , available Windows , Linux , Macos . **location download : ** < https : //docs.microsoft.com/en-us/sql/sql-operations-studio/download > Below screenshot looks like [ ! [ enter image description ] ( https : //i.stack.imgur.com/THna9.jpg ) ] ( https : //i.stack.imgur.com/THna9.jpg ) **To view actual execution plan using sqlopsstudio ( steps platforms ) ** * Press ` CTRL+SHIFT+P ` * Type run query actual execution plan shown select highlighted , get actual execution plan [ ! [ enter image description ] ( https : //i.stack.imgur.com/NAlW1.gif ) ] ( https : //i.stack.imgur.com/NAlW1.gif ) **To view estimated execution plan : ** Just press ICON shown [ ! [ enter image description ] ( https : //i.stack.imgur.com/Q3HfU.gif ) ] ( https : //i.stack.imgur.com/Q3HfU.gif ) **You also use keybinding view actual execution plan .Below steps** 1.Press ` CTRL+SHIFT+P ` 2.Type keyboard shortcuts 3.In search plan type actual shown [ ! [ enter image description ] ( https : //i.stack.imgur.com/WKSW8.gif ) ] ( https : //i.stack.imgur.com/WKSW8.gif ) 4.Right click actual query plan shortcut say ` add key binding ` key choice ( ` CTRL+M ` [ ! [ enter image description ] ( https : //i.stack.imgur.com/5C3aH.gif ) ] ( https : //i.stack.imgur.com/5C3aH.gif ) **Below Part answer written time SQLOPS studio available.This ben helpfull one n't SQLopsstudio : ** Currently viewing execution plan supported Windows , using SSMS third party tool like SQLSentry.. There feature request tracked : [ Return ShowPlan data Text XML Query Execution ] ( https : //github.com/Microsoft/vscode- mssql/issues/509 ) one option connect using VSCODE linux set ` show plan xml ` shown screenshot below..this provides xml execution plan SET showplan_xml ON ; [ ! [ enter image description ] ( https : //i.stack.imgur.com/bkvJX.png ) ] ( https : //i.stack.imgur.com/bkvJX.png ) take xml upload [ Paste The Plan ] ( https : //www.brentozar.com/pastetheplan/ ) website view plans Below screenshot XML [ ! [ enter image description ] ( https : //i.stack.imgur.com/XVFqj.png ) ] ( https : //i.stack.imgur.com/XVFqj.png ) also view SQLSENTRY plan explorer well ( Windows ) indepth analysis [ ! [ enter image description ] ( https : //i.stack.imgur.com/IZ1Dk.png ) ] ( https : //i.stack.imgur.com/IZ1Dk.png )"
Stackoverflow,"Usually Java I execute SELECT statement check size ResultSet . If zero I issue INSERT otherwise UPDATE . Since Groovy provides syntactic sugar top JDBC , I 'm wondering provides way ease process ? Is easier way save update record ? Note : I know Hibernate offers , I 'd rather stick Groovy API .",The difference Groovy Sql class explicitly works GStrings ensure parameters properly quoted ( [ explained documentation ] ( http : //groovy.codehaus.org/api/groovy/sql/Sql.html ) ) . So converts first example truncate 'my_table' Which wrong ( error explains ) You also use : sql.execute `` truncate $ { Sql.expand ( tableName ) } ''
Stackoverflow,"My problem probably straight forward , I ca n't figure what's happening behind scenes . I 'm looping series domains database table , calling , grabbing SSL certificate storing information back database . For part , 's working - except loop exits calls n't get completed stop dead . Database retrieval begins check : function queryRows ( ) { complete = false ; var query = c.query ( `` SELECT * FROM domains LIMIT 100 OFFSET `` + offset ) ; query.on ( 'result ' , function ( res ) { res.on ( 'data ' , function ( row ) { checkUrl ( row ) } ) .on ( 'end ' , function ( ) { complete = true ; } ) ; } ) .on ( 'end ' , function ( ) { console.log ( complete ) ; offset += 100 ; ( offset < = ( parseInt ( rows ) + 400 ) ) { queryRows ( ) ; } else { console.log ( `` Done , waiting '' ) ; setTimeoutPromise ( 600000 , 'foobar ' ) .then ( ( value ) = > { console.log ( `` restarting '' ) offset = 0 ; getTotal ( ) ; } ) ; } } ) ; } And code checks SSL : function checkSSL ( id , domain ) { complete = false var options = { host : domain , rejectUnauthorized : false } ; callback = function ( response ) { var str = `` ; try { ( domain == `` arstechnica.com '' ) { console.log ( `` Found ars - savingCertificate '' ) ; } cert = response.connection.getPeerCertificate ( true ) ; complete = hasSSL ( cert , domain , id ) ; // updateDomainRecord ( cert , domain , id ) } catch ( error ) { console.log ( error ) ; complete = true ; noSSLRecord ( domain , id ) ; } } const req = https.request ( options , callback ) ; req.on ( 'error ' , ( e ) = > { // console.error ( e ) ; } ) ; } It 's worth noting I put console.log https.request , I see console . However logs within callback fail trigger ( callback never fires ) . Again , time callback . It near end database loop appears stop working . Any advice would appreciated !","Looks like request never sent , ` callback ` never get fired . Make sure request actually sent , add one line end : req.end ( ) ;"
Stackoverflow,"( Apologies necessary -- first Stack Overflow question . I 'll happy modify anyone suggestions . I looked answer I'm afraid grasp terminology n't good enough make complete search . ) I 'm accustomed using mysql_fetch_array get records database . When getting records way , mysql_num_rows gives count rows . On current project , however , I 'm using mysql_fetch_object . mysql_num_rows seem work function , I 'count ' results query I get expected answer : 1 ( one object ) . Is way 'see ' object count elements inside ?","The function ` mysql_num_rows ` works result resource , object row . # # # Example $ link = mysql_connect ( `` localhost '' , `` mysql_user '' , `` mysql_password '' ) ; mysql_select_db ( `` database '' , $ link ) ; $ sql = `` SELECT id , name FROM myTable '' ; $ result = mysql_query ( $ sql , $ link ) ; $ rowCount = mysql_num_rows ( $ result ) ; ( $ row = mysql_fetch_object ) { echo `` id : `` . $ row- > id . '' name : `` . $ row- > name . `` < BR > '' ; } echo `` total : `` . $ rowCount ;"
Stackoverflow,"I want validate System.DateTime value I add parameter SqlCommand instance . The MSDN documentation SqlDbType enumeration says : **Date time data ranging value January 1 , 1753 December 31 , 9999 accuracy 3.33 milliseconds . ** To validate value , I 'm using public readonly DateTime SqlDateTimeMin = new DateTime ( 1753 , 1 , 1 ) ; public readonly DateTime SqlDateTimeMax = new DateTime ( 9999 , 12 , 31 ) ; ( value < SqlDateTimeMin || value > SqlDateTimeMax ) // Validation check fails else // Validation check succeeds Is best way ? Is alternative hard coding min max values ?",What [ SqlDateTime.MinValue ] ( http : //msdn.microsoft.com/en- us/library/system.data.sqltypes.sqldatetime.minvalue.aspx ) [ SqlDateTime.MaxValue ] ( http : //msdn.microsoft.com/en- us/library/system.data.sqltypes.sqldatetime.maxvalue.aspx ) ? Note : _SQL_ type min/max .net types like previous 2 answers : - )
Stackoverflow,"**TLDR : ** > The following code run different databases , Oracle : ` select sysdate dual ` SQLite ` select datetime ( 'now ' ) ` > > When ` Session.CreateSQLQuery ( cmd ) .UniqueResult < DateTime > ( ) ` result DateTime working Oracle string working SQLite . > > It feels like bug SQLite driver hack check returned type DateTime.Parse ( ) string . I could ways NHibernate return correct type ? I trying fetch current database time database . It works fine using Oracle I try SQLite ( unit tests ) breaks date returned DateTime string . I 've seen solutions using custom IUserType I see I use case . Any suggestions ? using System ; using System.Collections.Generic ; using NHibernate ; using NHibernate.Criterion ; using NHibernate.Dialect.Function ; namespace My.Common.Types { public class MyNHibernateDialectException : Exception { public MyNHibernateDialectException ( string message ) : base ( message ) { } } /// < summary > /// Define custom functions name . It important adding new custom sql function , function work /// dialects supported . /// < /summary > public static class MyDatabaseDialects { public enum Query { SysDate } /// < summary > /// Dialect implementations use function verify implement functions . /// < /summary > /// < param name= '' dialect '' > < /param > public static void VerifyRegistrations ( NHibernate.Dialect.Dialect dialect ) { // Verify required function foreach ( var func Enum.GetValues ( typeof ( Function ) ) ) { var enumName = func.ToString ( ) ; ( ! dialect.Functions.ContainsKey ( enumName ) ) { throw new MyNHibernateDialectException ( string.Format ( `` The custom function ' { 0 } ' defined . Did forget factory ' { 1 } ' ? `` , enumName , dialect ) ) ; } } } } /// < summary > /// An interface reveal advanced functionality database specific /// < /summary > public interface IDialectExtensions { /// < summary > /// Fetch query specfic current database . /// < /summary > ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query ) ; /// < summary > /// Fetch parameterized query specfic current database . /// < /summary > ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query , params object [ ] queryParams ) ; } /// < summary > /// Class store database specific objects except functions ( supported NHibernate ) . /// < /summary > class DialectExtension { private readonly Dictionary < MyDatabaseDialects.Query , string > queryDictionary = new Dictionary < MyDatabaseDialects.Query , string > ( ) ; public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query ) { return this.GetQuery ( session , query , null ) ; } public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query , params object [ ] queryParams ) { var cmd = ( queryParams == null ) ? queryDictionary [ query ] : string.Format ( queryDictionary [ query ] , queryParams ) ; return session.Session.CreateSQLQuery ( cmd ) ; } public void RegisterQuery ( MyDatabaseDialects.Query query , string hqlString ) { queryDictionary.Add ( query , hqlString ) ; } public void VerifyQueryRegistrations ( ) { foreach ( var query Enum.GetValues ( typeof ( MyDatabaseDialects.Query ) ) ) { ( ! queryDictionary.ContainsKey ( ( MyDatabaseDialects.Query ) query ) ) { throw new MyNHibernateDialectException ( string.Format ( `` The custom query ' { 0 } ' defined . `` , query.ToString ( ) ) ) ; } } } } public class MyOracle10gDialect : NHibernate.Dialect.Oracle10gDialect , IDialectExtensions { private readonly DialectExtension dialectExtension = new DialectExtension ( ) ; public MyOracle10gDialect ( ) { # region Dialect extensions dialectExtension.RegisterQuery ( MyDatabaseDialects.Query.SysDate , @ '' select sysdate dual '' ) ; dialectExtension.VerifyQueryRegistrations ( ) ; # endregion Dialect extensions } public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query ) { return dialectExtension.GetQuery ( session , query ) ; } public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query , params object [ ] queryParams ) { return dialectExtension.GetQuery ( session , query , queryParams ) ; } } public class MySqliteDialect : NHibernate.Dialect.SQLiteDialect , IDialectExtensions { private readonly DialectExtension dialectExtension = new DialectExtension ( ) ; public MySqliteDialect ( ) { # region Dialect extensions dialectExtension.RegisterQuery ( MyDatabaseDialects.Query.SysDate , @ '' select datetime ( 'now ' ) '' ) ; dialectExtension.VerifyQueryRegistrations ( ) ; # endregion Dialect extensions } public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query ) { return dialectExtension.GetQuery ( session , query ) ; } public ISQLQuery GetQuery ( ISession session , MyDatabaseDialects.Query query , params object [ ] queryParams ) { return dialectExtension.GetQuery ( session , query , queryParams ) ; } } } And I use code like : /// < summary > /// Fetches DialectExtensions object allowing us advanced functionality database specific /// < /summary > public static IDialectExtensions GetDialectExtensions ( IOperationContext operationContext ) { return Session.GetSessionImplementation ( ) .Factory.Dialect IDialectExtensions ; } /// < summary > /// Get database time executing raw SQL statement . /// < /summary > public static DateTime ? GetDatabaseTime ( ) { DateTime ? result = null ; try { result = GetDialectExtensions ( ) .GetQuery ( Session , MyDatabaseDialects.Query.SysDate ) .UniqueResult < DateTime > ( ) ; } catch { // SQLite throw exception result returned string instead DateTime } return result ; }","You need add ` GROUP BY ` SELECT fname , SUM ( salary ) FROM employee GROUP BY fname Most RDBMSs would reject original query invalid . Or response comment get ` SUM ` whole table additional column ` OVER ` clause supported use SELECT fname , SUM ( salary ) OVER ( ) FROM employee And n't use non correlated sub query . SELECT fname , ( SELECT SUM ( salary ) FROM employee ) FROM employee"
Stackoverflow,"I 'm looking [ tutorial Spatialite-Android ] ( https : //www.gaia- gis.it/fossil/libspatialite/wiki ? name=spatialite-android-tutorial ) , I'm noticing following code samples : String query = `` SELECT AsText ( Transform ( MakePoint ( `` + TEST_LON + `` , `` + TEST_LAT + `` , 4326 ) , 32632 ) ) ; '' ; sb.append ( `` Execute query : `` ) .append ( query ) .append ( `` \n '' ) ; try { Stmt stmt = db.prepare ( query ) ; ( stmt.step ( ) ) { String pointStr = stmt.column_string ( 0 ) ; sb.append ( `` \t '' ) .append ( TEST_LON + `` / '' + TEST_LAT + `` /EPSG:4326 '' ) .append ( `` = `` ) // .append ( pointStr + `` /EPSG:32632 '' ) .append ( `` ... \n '' ) ; } stmt.close ( ) ; } catch ( Exception e ) { e.printStackTrace ( ) ; sb.append ( ERROR ) .append ( e.getLocalizedMessage ( ) ) .append ( `` \n '' ) ; } In particular , I noticed poor practice done simply stringing together SQL query , instead proper method , used Android SQLite library . Is way I make Spatialite use true prepared statements ? Just clear , I 'm looking something like , using standard SQLite database Android : String query= '' SELECT * FROM table WHERE _id= ? `` ; Cursor data=db.rawQuery ( query , new String [ ] { id } ) ;","There tricks . They use exec ( ) call , 3 arguments version . The statement [ source code ] ( https : //github.com/mrenouf/android- spatialite/blob/master/src/jsqlite/Database.java ) : public void exec ( String sql , jsqlite.Callback cb , String args [ ] ) A jsqlite.Callback interface , several . But best way seems using ` db.get_table ( query , args ) ` function . ` % q ` effective replacement ` ? ` Android SQLite representation . Here 's transformation given code : String query = `` SELECT AsText ( Transform ( MakePoint ( % q , % q , 4326 ) , 32632 ) ) ; '' ; TableResult result=db.get_table ( query , new String [ ] { `` '' +TEST_LONG , '' '' +TEST_LAT } ) ; From , get results TableResult . There n't method call get results , actually grab publicly declared variable manually parse . Here 's example done . TableResult result=db.get_table ( query , new String [ ] { `` '' +lng , '' '' +lat } ) ; Vector < String [ ] > rows=result.rows ; ( String [ ] row : rows ) { ( String val : row ) { Log.v ( TAG , val ) ; } } If n't select , try something like : TableResult result=new TableResult ( ) ; db.exec ( `` ATTACH DATABASE % q AS newDb '' , result , new String [ ] { path } ) ; I assume pattern work INSERTS like"
Stackoverflow,I using SSMS v17.6 SQL Server Express v14.0.1000.169 . When I run following ` DELETE ` statement : delete foo go I get error : > Incorrect syntax near 'go ' . But I execute similar ` SELECT ` statement : select * foo go Then error . It seems error happens statement n't return results . I checked query execution settings SSMS batch separator set go . It seems SSMS sending server n't . **I seen multiple machines . **,"If using newer version SQL Server , look [ temporal tables ] ( https : //docs.microsoft.com/en-us/sql/relational- databases/tables/temporal-tables ? view=sql-server-2017 ) might best option . If need support older versions , preferred method history table new PK column , change flag ( I , U , D ) , date modified , user made change , columns primary table . I index column related PK non-history table . Triggers impact performance much n't put logic . Example ( pseudocode ) : Table : Car Column : CarID INT IDENTITY ( 1,1 ) Primary Key Column : Name varchar Table : Car_hist Column : Car_histID INT IDENTITY ( 1,1 ) Primary Key Column : Change char ( 1 ) Column : DateOfChange DateTime2 Column : ChangedByUser ( varchar int ) Column : CarID < -add unique non-clustered index Column : Name varchar You write generator SQL generates script create history table , indexes , etc . It helps consistent table design practice . Now reason : I rarely query history tables , I , almost always single record see happened changed . This method allows select history parent table 's PK value quickly read historical change log easily ( changed ) . I n't see design . If really slick , find write grid diffs rows quickly see changed ."
Stackoverflow,"In order host Python/Django app Heroku , I 'm trying convert db MySQL Postgres following instructions < https : //realpython.com/blog/python/migrating-your-django-project-to-heroku/ > . I 'm currently running OSX 10.9 , using tool mysql2pgsql make transfer . When I try run command `` ` py-mysql2pgsql -v -f mysql2pgsql.yml ` `` actually transfer db , copies first three tables , hits snag auth_user , returning error `` ` raise Exception ( 'unknown % ' % column [ 'type ' ] ) Exception : unknown datetime ( 6 ) ` `` . This seems strange , auth_user generated one Django 's default installed apps , I would n't expect cause errors . Any idea could causing error I differently ? Thanks .","directory 'Lib\site- packages\py_mysql2pgsql-0.1.6-py2.7.egg\mysql2pgsql\lib ' edit like 76 postgres_writer.py file elif column [ 'type ' ] == 'datetime ' : elif column [ 'type ' ] == 'datetime ' column [ 'type ' ] .startswith ( 'datetime ( ' ) : I facing problem , solution worked ."
Stackoverflow,"This first day ` tsqlt ` expect vague statements . I trying test storedProcedure ` Try Catch Block ` actual statements test insert update command . I want test case ErrorRaised catch block performs expected tasks . Can please guide I Raise Error stored procedure test anything mock/fake inside . Hope question understandable , happy clarify needed .","So I understand question correctly , trying test catch block works ? The way depend happens within catch block . Imagine scenario : create table mySimpleTable ( Id int null primary key , StringVar varchar ( 8 ) null , IntVar tinyint null ) go We stored procedure inserts data table thus . This based template I use many procedures . It starts validating input ( ) , work needs . Naming step particularly useful understand error occurred complex , multi-step procedures . The catch block uses Log4TSql logging framework read [ blog ] ( http : //datacentricity.net/2011/11/t-sql-tuesday-024-exception-handling- made-simple/ `` blog '' ) download [ SourceForge ] ( http : //sourceforge.net/projects/log4tsql/ `` SourceForge '' ) . The pattern I follow capture information exception procedure time error within catch block ensure error still thrown end procedure . You could also choose call raiserror ( also ` throw ` SQL2012 ) within catch block . Either way , I believe procedure hits exception always notified chain ( i.e . never hidden ) . create procedure mySimpleTableInsert ( @ Id int , @ StringVar varchar ( 16 ) = null , @ IntVar int = null ) begin -- ! Standard/ExceptionHandler variables declare @ _FunctionName nvarchar ( 255 ) = quotename ( object_schema_name ( @ @ procid ) ) + ' . ' + quotename ( object_name ( @ @ procid ) ) ; declare @ _Error int = 0 ; declare @ _ReturnValue int ; declare @ _RowCount int = 0 ; declare @ _Step varchar ( 128 ) ; declare @ _Message nvarchar ( 1000 ) ; declare @ _ErrorContext nvarchar ( 512 ) ; begin try set @ _Step = 'Validate Inputs' @ Id null raiserror ( ' @ Id invalid : % ' , 16 , 1 , @ Id ) ; set @ _Step = 'Add Row' insert dbo.mySimpleTable ( Id , StringVar , IntVar ) values ( @ Id , @ StringVar , @ IntVar ) end try begin catch set @ _ErrorContext = 'Failed add row mySimpleTable step : ' + coalesce ( ' [ ' + @ _Step + ' ] ' , 'NULL ' ) exec log4.ExceptionHandler @ ErrorContext = @ _ErrorContext , @ ErrorProcedure = @ _FunctionName , @ ErrorNumber = @ _Error , @ ReturnMessage = @ _Message ; end catch -- ! Finally , throw exception detected caller @ _Error > 0 raiserror ( @ _Message , 16 , 99 ) ; set nocount ; -- ! Return value @ @ ERROR ( zero success ) return ( @ _Error ) ; end go Let 's start creating new schema ( class ) hold tests . exec tSQLt.NewTestClass 'mySimpleTableInsertTests ' ; go Our first test simplest checks even exception caught catch block , error still returned procedure . In test simply use ` exec tSQLt.ExpectException ` check error raised @ Id supplied NULL ( fails input validation checks ) create procedure [ mySimpleTableInsertTests ] . [ test throws error catch block ] begin exec tSQLt.ExpectException @ ExpectedErrorNumber = 50000 ; -- ! Act exec dbo.mySimpleTableInsert @ Id = null end ; go Our second test little complex makes use ` tsqlt.SpyProcedure ` `` mock '' ExceptionHandler would otherwise record exception . Under hood , mock procedure way , tSQLt creates table named procedure spied replaces spied procedure one writes input parameter values table . This rolled back end test . This allows us check ExceptionHandler called values passed . In test check ExceptionHander called mySimpleTableInsert result input validation error . create procedure [ mySimpleTableInsertTests ] . [ test calls ExceptionHandler error ] begin -- ! Set Error returned ExceptionHandler zero sproc test n't throw error exec tsqlt.SpyProcedure 'log4.ExceptionHandler ' , 'set @ ErrorNumber = 0 ; ' ; select cast ( 'Failed add row mySimpleTable step : [ Validate inputs ] ' varchar ( max ) ) [ ErrorContext ] , ' [ dbo ] . [ mySimpleTableInsert ] ' [ ErrorProcedure ] # expected -- ! Act exec dbo.mySimpleTableInsert @ Id = null -- ! Assert select ErrorContext , ErrorProcedure # actual log4.ExceptionHandler_SpyProcedureLog ; -- ! Assert exec tSQLt.AssertEqualsTable ' # expected ' , ' # actual ' ; end ; go Finally following ( somewhat contrived ) examples use pattern check error caught thrown value @ IntVar big table : create procedure [ mySimpleTableInsertTests ] . [ test calls ExceptionHandler invalid IntVar input ] begin -- ! Set Error returned ExceptionHandler zero sproc test n't throw error exec tsqlt.SpyProcedure 'log4.ExceptionHandler ' , 'set @ ErrorNumber = 0 ; ' ; select cast ( 'Failed add row mySimpleTable step : [ Add Row ] ' varchar ( max ) ) [ ErrorContext ] , ' [ dbo ] . [ mySimpleTableInsert ] ' [ ErrorProcedure ] # expected -- ! Act exec dbo.mySimpleTableInsert @ Id = 1 , @ IntVar = 500 -- ! Assert select ErrorContext , ErrorProcedure # actual log4.ExceptionHandler_SpyProcedureLog ; -- ! Assert exec tSQLt.AssertEqualsTable ' # expected ' , ' # actual ' ; end ; go create procedure [ mySimpleTableInsertTests ] . [ test throws error invalid IntVar input ] begin exec tSQLt.ExpectException @ ExpectedErrorNumber = 50000 ; -- ! Act exec dbo.mySimpleTableInsert @ Id = 1 , @ IntVar = 500 end ; go If n't answer question , perhaps could post example trying achieve ."
Stackoverflow,"I database table 6 columns . The primary key composite key made 5 6 columns I trying use ` SqlClient.SqlCommandBuilder.GetDeleteCommand ` delete row . However I getting following error : > `` System.InvalidOperationException : Dynamic SQL generation DeleteCommand supported SelectCommand return key column information . '' The ` SelectCommmand ` contains columns table : SELECT TABLENAME.COL1 , TABLENAME.COL2 , TABLENAME.COL3 , TABLENAME.COL4 , TABLENAME.COL5 , TABLENAME.COL6 FROM TABLENAME Could problem composite key ?","Your information supplied useless . But I explain meaning error . Every update command written ADO.Net form : Update col1 , col2 col1=col1value AND col2=col2value ADO.Net keeps value column selected database . When performs update condition none columns changed commit . The reason see error database row changed performing select calling ` da2.UpdateChanges ( ds2 ) ` . If look logic perhaps selected value row two separate datasets ( two different threads ) performed update performing select ."
Stackoverflow,"I 'm supporting IIS web application constructs sends SELECT statements SQL Server . Sometimes statements efficient quite large tables take three four minutes complete run SQL Management Studio . When statements sent application , following time-out reported : > ERROR [ HYT00 ] [ Microsoft ] [ ODBC SQL Server Driver ] Timeout expired SQL : SELECT ... large statement ... It 's possible ( immediately ) improve SQL statements sent I need temporarily increase whatever time-outs hit . But I seem find time-out corresponds error message . I hoping someone tell time-out refers viewed/changed ?","Note set protocol used connection string , need configure machine . I would recommend NOT changing machine configuration using cliconfg , since impacts applications running machine . So 2 ways set network protocol . 1 . Use protocol prefix : ` Server=tcp : myserver ` ` Server=np : myserver ` ` tcp : ` prefix means use tcp protocol . ` np : ` prefix means use named pipes protocol . Just stick front server name connecting . 2 . Second ways set Network keyword connection string : ` Network=dbmssocn ` ` Network=dbnmpntw ` I prefer protocol prefix I never remember network type abbreviations ."
Stackoverflow,"I got started looking Lua easy way access SQLite DLL , I ran error trying use DB-agnostic LuaSQL module : require `` luasql.sqlite '' module `` luasql.sqlite '' print ( `` Content-type : Text/html\n '' ) print ( `` Hello ! '' ) Note I 'm trying start basic setup , following files work directory , sqlite.dll actually renamed sqlite3.dll [ LuaForge ] ( http : //luaforge.net/frs/ ? group_id=12 ) site : Directory C : \Temp < DIR > luasql lua5.1.exe lua5.1.dll hello.lua Directory C : \Temp\luasql sqlite.dll Am I missing binaries would explain error ? Thank . * * * Edit : I renamed DLL original sqlite3.dll updated source reflect ( originally renamed 's called sample I found ) . At point , 's code looks like ... require `` luasql.sqlite3 '' -- attempt call field 'sqlite ' ( nil value ) env = luasql.sqlite ( ) env : close ( ) ... error message I 'm getting : C : \ > lua5.1.exe hello.lua lua5.1.exe : hello.lua:4 : attempt call field 'sqlite ' ( nil value ) * * * Edit : Found : env = luasql.sqlite3 ( ) instead env = luasql.sqlite ( ) . For newbies like , 's complete example latest [ SQLite LuaSQL driver ] ( http : //www.keplerproject.org/luasql/ ) : require `` luasql.sqlite3 '' env = luasql.sqlite3 ( ) conn = env : connect ( `` test.sqlite '' ) assert ( conn : execute ( `` create table exists tbl1 ( one varchar ( 10 ) , two smallint ) '' ) ) assert ( conn : execute ( `` insert tbl1 values ( 'hello ! ',10 ) '' ) ) assert ( conn : execute ( `` insert tbl1 values ( 'goodbye',20 ) '' ) ) cursor = assert ( conn : execute ( `` select * tbl1 '' ) ) row = { } cursor : fetch ( row ) print ( table.concat ( row , '| ' ) ) end cursor : close ( ) conn : close ( ) env : close ( ) Thank .",You could try luasql = require `` luasql.postgres '' env = assert ( luasql.postgres ( ) )
Stackoverflow,I 'm using SQL Server CE database . Can I create view SQL Server CE 3.5 ? I tried create saying create view statement supported . In application I table called ` Alarm ` 12 columns . But I 'm always accessing three columns . So I want create view three columns . Will improve performance ?,"Instead ` true ` ` false ` use ` 1 ` ` 0 ` . Eg : insert EMP ( ROW_ID , NAME , TEST ) values ( '123 ' , 'XYZ',1 ) ; This [ SQL Server 2005 ` bit ` ] ( http : //msdn.microsoft.com/en- us/library/ms177603 % 28v=sql.90 % 29.aspx ) : > The string values TRUE FALSE converted bit values : TRUE converted 1 FALSE converted 0 . You try . If applies CE following code ( ` 'TRUE ' ` string ) might work well : insert EMP ( ROW_ID , NAME , TEST ) values ( '123 ' , 'XYZ ' , 'TRUE ' ) ;"
Stackoverflow,"I using platform ( perfectforms ) requires use stored procedures queries , never used stored procedures , I can't figure I 'm wrong . The following statement executes without error : DELIMITER // DROP PROCEDURE IF EXISTS test_db.test_proc// CREATE PROCEDURE test_db.test_proc ( ) SELECT 'foo ' ; // DELIMITER ; But I try call using : CALL test_proc ( ) ; I get following error : # 1312 - PROCEDURE test_db.test_proc ca n't return result set given context I executing statements within phpmyadmin 3.2.4 , PHP Version 5.2.12 mysql server version 5.0.89-community . When I write stored procedure returns parameter , select , things work fine ( e.g . ) : DELIMITER // DROP PROCEDURE IF EXISTS test_db.get_sum// CREATE PROCEDURE test_db.get_sum ( total int ) BEGIN SELECT SUM ( field1 ) INTO total FROM test_db.test_table ; END // DELIMITER ; works fine , I call : CALL get_sum ( @ ) ; SELECT @ ; I get sum problem . Ultimately , I need fancy SELECT statement wrapped stored procedure , I call , return multiple rows multiple fields . For I 'm trying get _any_ select working . Any help greatly appreciated .","Figured . This bug PHP ( though used ) - 's bug versions phpmyadmin . The bug intermittently reappears fixed various subversions ( see ) : # 1312 - PROCEDURE [ name ] ca n't return result set given context This behavior appears limited **_SELECT statements within stored procedures inside phpmyadmin_** . Using client like MySQL Workbench works around problem ( could upgrade phpmyadmin , 's pain 're shared server like I ) . Anyway , thanks everyone help ."
Stackoverflow,With ` date ` field I [ ] ( https : //stackoverflow.com/a/12402084/96588 ) : ORDER BY ABS ( expiry - CURRENT_DATE ) With ` timestamp ` field I get following error : > function abs ( interval ) exist,"Use [ ( ) CURRENT_TIMESTAMP ] ( http : //www.postgresql.org/docs/current/interactive/functions- datetime.html # FUNCTIONS-DATETIME-CURRENT ) purpose . The reason different outcome queries : When subtract two values type ` date ` , result ** ` integer ` ** ` abs ( ) ` applicable . When subtract two values type ` timestamp ` ( one ` timestamp ` ) , result ** ` interval ` ** , ` abs ( ) ` applicable . You could substitute ` CASE ` expression : ORDER BY CASE WHEN expiry > ( ) THEN expiry - ( ) ELSE ( ) - expiry END Or [ ` extract ( ) ` ] ( http : //www.postgresql.org/docs/current/interactive/functions- datetime.html # FUNCTIONS-DATETIME-EXTRACT ) unix ` epoch ` resulting ` interval ` like @ Craig already demonstrated . I quote : `` interval values , total number seconds interval '' . Then use ` abs ( ) ` : ORDER BY abs ( extract ( epoch ( expiry - ( ) ) ) ) ; [ ` age ( ) ` ] ( http : //www.postgresql.org/docs/9.1/interactive/functions- datetime.html # FUNCTIONS-DATETIME-TABLE ) would add human readable representation interval summing days months years bigger intervals . But 's beside point : value used sorting . As column type timestamp , use ` CURRENT_TIMESTAMP ` ( ` ( ) ` ) instead ` CURRENT_DATE ` , get inaccurate results ( even incorrect `` today '' ) ."
Stackoverflow,"The error *** Exception : Incompatible { errSQLType = `` int8 '' , errHaskellType = `` Int '' , errMessage = `` types incompatible '' } It looks like value returned ` count ( * ) ` query must converted ` Integer ` rather ` Int ` . If I change specific variables type Integer , queries work . But error n't raised another machine exact code . The first machine 32 bit one 64-bit . That 's difference I could discern . Does anyone insight going ?","The module ` Database.PostgreSQL.Simple ` exports type classes ` ToRow ` ` FromRow ` , without methods . For methods need import modules ` Database.PostgreSQL.Simple.ToRow ` ` Database.PostgreSQL.Simple.FromRow ` , done sample linked ."
Stackoverflow,I using MySQL v5.1.36 I trying create stored function using code . DELIMITER // CREATE FUNCTION ` modx ` .getSTID ( x VARCHAR ( 255 ) ) RETURNS INT DETERMINISTIC BEGIN DECLARE INT ; SELECT id INTO FROM ` modx ` .coverage_state WHERE ` coverage_state ` .name = x ; RETURN ; END// When entered MySQL Console I get response . mysql > DELIMITER // mysql > CREATE FUNCTION ` modx ` .getSTID ( x VARCHAR ( 255 ) ) RETURNS INT DETERMINISTIC - > BEGIN - > DECLARE INT ; ERROR 1064 ( 42000 ) : You error SQL syntax ; check manual corresponds MySQL server version right syntax use near `` line 3 mysql > SELECT id INTO - > FROM ` modx ` .coverage_state - > WHERE ` coverage_state ` .name = x ; ERROR 1327 ( 42000 ) : Undeclared variable : mysql > RETURN ; ERROR 1064 ( 42000 ) : You error SQL syntax ; check manual corresponds MySQL server version right syntax use near 'RETUR N ' line 1 mysql > END// From I find online syntax correct . What I wrong ?,"When creating function/procedure mysql console , first command ` DELIMITER // ` . Otherwise , uses default delimiter ( ` ; ` ) ,"
Stackoverflow,"Trying install oursql driver python3x sqlalchemy0.8 ubuntu 12.10 . It fails following error . sudo pip-3.2 install oursql Downloading/unpacking oursql Running setup.py egg_info package oursql Traceback ( recent call last ) : File `` < string > '' , line 16 , < module > File `` /tmp/pip-build/oursql/setup.py '' , line 53 print `` cython found , using previously-cython 'd .c file . '' ^ SyntaxError : invalid syntax Complete output command python setup.py egg_info : Traceback ( recent call last ) : File `` < string > '' , line 16 , < module > File `` /tmp/pip-build/oursql/setup.py '' , line 53 print `` cython found , using previously-cython 'd .c file . '' ^ SyntaxError : invalid syntax When I try install **cython** I seem already : sudo pip-3.2 install cython Requirement already satisfied ( use -- upgrade upgrade ) : cython /usr/local/lib/python3.2/dist-packages Cleaning . What I make run ?","The difference MySQLdb hackery query oursql ... Taking : cursor.executemany ( `` INSERT INTO sometable VALUES ( % , % , % ) '' , [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ) MySQLdb translates running : cursor.execute ( `` INSERT INTO sometable VALUES ( 1,2,3 ) , ( 4,5,6 ) , ( 7,8,9 ) '' ) But : cursor.executemany ( `` INSERT INTO sometable VALUES ( ? , ? , ? ) '' , [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] ) In oursql , gets translated something like pseudocode : stmt = prepare ( `` INSERT INTO sometable VALUES ( ? , ? , ? ) '' ) params [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] : stmt.execute ( *params ) So want emulate mysqldb benefit prepared statements goodness oursql , need : itertools import chain data = [ [ 1,2,3 ] , [ 4,5,6 ] , [ 7,8,9 ] ] one_val = `` ( { } ) '' .format ( ' , '.join ( `` ? '' data [ 0 ] ) ) vals_clause = ' , '.join ( one_val data ) cursor.execute ( `` INSERT INTO sometable VALUES { } '' .format ( vals_clause ) , chain.from_iterable ( data ) ) I bet oursql faster : - ) Also , think ugly , right . But remember MySQL db something uglier internally - using regular expressions parse INSERT statement break parameterized part THEN I suggested oursql ."
Stackoverflow,I web app azure let user input data . I want save sql db I created azure . Is way check/view entered data . I referring something like `` mysql workbench mysql '',You may also view data browser using [ Azure Portal ] ( https : //portal.azure.com/ ) Select DB `` Query editor '' menu start querying . [ ! [ ... ] ( https : //i.stack.imgur.com/fWOY5.png ) ] ( https : //i.stack.imgur.com/fWOY5.png )
Stackoverflow,"D : \Python27 > python sqlmap\sqlmap.py -u www.mail.ru -- tor sqlmap/1.0-dev - automatic SQL injection database takeover tool http : //sqlmap.org [ ! ] legal disclaimer : Usage sqlmap attacking targets without prior mutual consent illegal . It end user 's responsibility obey applicable local , state federal laws . Developers assume liability respon sible misuse damage caused program [ * ] starting 22:28:49 [ 22:28:49 ] [ WARNING ] increasing default value option ' -- time-sec ' 10 beca use switch ' -- tor ' provided [ 22:28:49 ] [ INFO ] setting Tor HTTP proxy settings [ 22:28:52 ] [ CRITICAL ] ca n't establish connection Tor proxy . Please make sure Vidalia , Privoxy Polipo bundle installed ab le successfully use switch ' -- tor ' ( e.g . https : //www.torproject.org/projects/ vidalia.html.en ) [ * ] shutting 22:28:52 [ CRITICAL ] ca n't establish connection Tor proxy . Vidalia Tor already instaled runing . How use TOR [ sqlmap ] ( http : //sqlmap.org/ ) ?",p option used following way ` -u `` http : //localhost/vuln/test.php ? feature=music & song=1 '' -p song `
Stackoverflow,I usually use linux mysql something called mysqltuner.pl great mysql server windows 2008 r2 server wondering know something simular mysqltuner use . I seen project page windows version works winXP . Any pointers anyone ported version would great . many thanks,"When come optimization , I believe `` n't fix n't broke '' . said heres excellent post regarding temp tables disk < https : //dba.stackexchange.com/questions/17677/why-is-mysql-is-creating-so- many-temporary-tables-on-disk > sure settings huge sign saying `` Do touch '' . Maybe change question bit , tell everyone WHY would want optimize DB . Is something ran extremly slow ? Is DB taking much resource server ? Optimization without specific goal quite dangerous , optimization means tweaking system way best suits way use , implies lot time trading preformance area n't need , case idea n't need ."
Stackoverflow,"I 've set Postgres 9.6 checked large table random integers parallel queries working . However , simple XPath query XML column another table always sequential . Both XPath functions marked parallel safe Postgres . I tried alter XPath cost , expected cost skyrocketed , n't change anything . What I missing ? Example table DDL : ` CREATE TABLE `` test_table '' ( `` xml '' XML ) ; ` Example query : ` SELECT xpath ( '/a ' , `` xml '' ) FROM `` test_table '' ; ` Example data : ` < > < /a > ` . Note real data contains XMLs 10-1000kB size . > select pg_size_pretty ( pg_total_relation_size ( 'test_table ' ) ) ; 28 MB > explain ( analyze , verbose , buffers ) select xpath ( '/a ' , `` xml '' ) test_table ; Seq Scan public.test_table ( cost=0.00..64042.60 rows=2560 width=32 ) ( actual time=1.420..4527.061 rows=2560 loops=1 ) Output : xpath ( '/a ' : :text , xml , ' { } ' : :text [ ] ) Buffers : shared hit=10588 Planning time : 0.058 ms Execution time : 4529.503 ms","The relevant point likely distinction `` relation size '' '' total relation size '' : CREATE TABLE test_table AS SELECT ( ' < > ' || repeat ( ' x ' , 1000000 ) || ' < /a > ' ) : :xml AS `` xml '' FROM generate_series ( 1 , 2560 ) ; SELECT pg_size_pretty ( pg_relation_size ( 'test_table ' ) ) AS relation_size , pg_size_pretty ( pg_total_relation_size ( 'test_table ' ) ) AS total_relation_size ; relation_size | total_relation_size -- -- -- -- -- -- -- -+ -- -- -- -- -- -- -- -- -- -- - 136 kB | 30 MB Large column values like stored within main relation , instead pushed associated [ TOAST table ] ( https : //www.postgresql.org/docs/current/static/storage-toast.html ) . This external storage count towards ` pg_relation_size ( ) ` , optimiser appears comparing [ ` min_parallel_relation_size ` ] ( https : //www.postgresql.org/docs/current/static/runtime- config-query.html # GUC-MIN-PARALLEL-RELATION-SIZE ) evaluating parallel plan : SET parallel_setup_cost = 0 ; SET parallel_tuple_cost = 0 ; SET min_parallel_relation_size = '144kB ' ; EXPLAIN SELECT xpath ( '/a ' , `` xml '' ) FROM test_table ; QUERY PLAN -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - Seq Scan test_table ( cost=0.00..49.00 rows=2560 width=32 ) SET min_parallel_relation_size = '136kB ' ; EXPLAIN SELECT xpath ( '/a ' , `` xml '' ) FROM test_table ; QUERY PLAN -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Gather ( cost=0.00..38.46 rows=2560 width=32 ) Workers Planned : 1 - > Parallel Seq Scan test_table ( cost=0.00..35.82 rows=1506 width=32 )"
Stackoverflow,Right . So I 've created stored procedure MySQL DB happens use SUBSTRING . Running procedure via query gives : > SQL Error 1630 : Function mydatabase.SUBSTRING exist Beg pardon ?,"Is space method call ` Substring ` first parenthesis ? It appears Line 40 : IF > 1 AND j > 1 AND ( s1_char = SUBSTRING ( s2 , j - 1 , 1 ) ) i.e . Ensure select substring ( CustomerName , 1 , 4 ) AS CustName MyTable ; instead : select substring ( CustomerName , 1 , 4 ) AS CustName MyTable ;"
Stackoverflow,"I using SQL Server project manager structure database . First I created project , imported database . Then , I need make schema change , say change field name , I SQL Server Project , publish actual database using Schema Compare Option . I would like take step hold basic data database needs . Say example ` OrderType ` table 2 records `` PHONE '' , `` WEB '' This data needed new instances database . Is way keep SQL Server project well n't get lost ? It seems way , keep actual copy master database metadata , use data-compare . But would great data could get published time schema resulting database complete .","There two ways preserve static data publish database . 1 . Have `` reference '' database static data populated . At time publishing new instance , SQL Server Data tools `` Data Compare '' tool allows compare live databases , creates custom script update one database data . 2 . Create scripts contain insert statements , run scripts publish time . SQL Server Data tools two tools assist . . Open data table ( right click SQL Server object explorer , select '' View Data '' ) , click `` Script '' button top . It create insert script rows table . [ More Comparing Data MSDN ] ( https : //msdn.microsoft.com/en-us/library/dn266029\ ( v=vs.103\ ) .aspx ) b . Then take created script , add Database Project '' Post Deployment '' script . When create publish script database , Post Deployment scripts project automatically included master script . [ More post deployment scripts MSDN ] ( https : //msdn.microsoft.com/en-us/library/aa833410\ ( v=vs.100\ ) .aspx )"
Stackoverflow,"The following command fails mysqldbcompare -- server1=un : pw @ server1 -- server2=un : pw @ server2 -- difftype=sql store-staging : store-beta following error : > mysqldbcompare : error : Can parse specified database ( ) : 'store- staging : store-beta ' . Please verify database ( ) specified valid format ( i.e. , db1 [ : db2 ] ) backtick quotes properly used required . The use backticks required non alphanumeric characters used database names . Parsing specified database results db1 = 'store ' db2 = 'store ' . My question I 'escape ' schemas run part command ? I tried following : 'store-staging : store-beta' `` store-staging : store-beta '' ` store-staging : store-beta ` 'store-staging ' : 'store-beta' `` store-staging '' : '' store-beta '' ` store-staging ` : ` store-beta ` fail .","It probable backticks intend ` mysqldbcompare ` actually interpreted shell ` mysqldbcompare ` actually sees . Try including backticks inside quotes ensure get properly passed , command looks something like : ` mysqldbcompare -- server1=un : pw @ server1 -- server2=un : pw @ server2 -- difftype= ' ` sql store-staging ` : ` store-beta ` ' `"
Stackoverflow,"Does anyone know canned long time ago ? Seemed like bad idea . My naive guess ORM tools n't need SQLJ also plugged gap . Anyway , still curious .","This n't authoritative answer , I see reasons : * In early 2000 's , powerful IDEs ( Eclipse , Netbeans , JBuilder ) made way Java ecosystem . SQLJ n't work well IDEs * In mid 2000 's , everyone thought SQL dead ORMs started hide SQL away * Today , typesafe alternatives like [ jOOQ ] ( http : //www.jooq.org ) ( LINQ .NET . Disclaimer : I work company behind jOOQ ) . Besides , two major flaws SQLJ : * SQLJ simplified embedding SQL , never brought additional compile-time safety , type checks syntax checks . * SQLJ good static SQL , dynamic SQL ( dynamic predicate table source composition ) achieved easily ."
Stackoverflow,"I using ` encrypted-core-data ` encrypts data persisted , previously simple ` CoreData ` using . ` persistentStoreCoordinator ` creation code follows . - ( NSPersistentStoreCoordinator * ) persistentStoreCoordinator { ( _persistentStoreCoordinator ! = nil ) { return _persistentStoreCoordinator ; } NSURL *oldStoreURL = [ [ self applicationDocumentsDirectory ] URLByAppendingPathComponent : @ '' VistaJetApp.sqlite '' ] ; NSURL *newStoreURL = [ [ self applicationDocumentsDirectory ] URLByAppendingPathComponent : @ '' VistaJet.sqlite '' ] ; NSError *error = nil ; NSString *currentPassword = [ [ VJAesCryptoWrapper getInstance ] getCurrentPassword ] ; NSDictionary *options = [ self getEncryptedStoreOptionsWithPassword : currentPassword andDatabaseStore : newStoreURL ] ; _persistentStoreCoordinator = [ [ NSPersistentStoreCoordinator alloc ] initWithManagedObjectModel : [ self managedObjectModel ] ] ; //if old store exists , means fresh installation ( [ [ NSFileManager defaultManager ] fileExistsAtPath : oldStoreURL.path ] == NO ) { ( ! [ _persistentStoreCoordinator addPersistentStoreWithType : EncryptedStoreType configuration : nil URL : newStoreURL options : options error : & error ] ) { } } else { ( ! [ _persistentStoreCoordinator addPersistentStoreWithType : NSSQLiteStoreType configuration : nil URL : oldStoreURL options : @ { NSMigratePersistentStoresAutomaticallyOption : @ YES , NSInferMappingModelAutomaticallyOption : @ YES } error : & error ] ) { } NSPersistentStore *oldUnsecureStore = [ _persistentStoreCoordinator persistentStoreForURL : oldStoreURL ] ; [ ConsoleLogger logText : [ NSString stringWithFormat : @ '' Migration started '' ] ] ; //start migration ( ! [ _persistentStoreCoordinator migratePersistentStore : oldUnsecureStore toURL : newStoreURL options : options withType : EncryptedStoreType error : & error ] ) { } else { [ [ NSFileManager defaultManager ] removeItemAtURL : oldStoreURL error : nil ] ; } } return _persistentStoreCoordinator ; } Creation options dictionary - ( NSDictionary* ) getEncryptedStoreOptionsWithPassword : ( NSString* ) password andDatabaseStore : ( NSURL* ) storeUrl { return @ { EncryptedStorePassphraseKey : password , EncryptedStoreDatabaseLocation : storeUrl , NSMigratePersistentStoresAutomaticallyOption : @ YES , NSInferMappingModelAutomaticallyOption : @ YES } ; } I saving password keychain using ` KeychainItemWrapper ` code crashing exactly ` getEncryptedStoreOptionsWithPassword : currentPassword ` method . App live I able reproduce crash , crashlytics showing many crashes [ crashlytics crash logs image ] ( https : //drive.google.com/file/d/1HOR95UtUKU2Jm-e0s1mMLLQtdXjBAVoB/view ? usp=sharing ) Also using ` AESCrypt ` encrypt password saving keychain using ` KeychainItemWrapper ` . **Observation : ** The crash crashlytics displaying appear upload build test flight using distribution profile . crash happening 100 % iOS 11 reported crashlytics","You ca n't use single persistent store coordinator without merging models . However , n't edit data models -- merge run time . ` NSManagedObjectModel ` offers couple different ways merge multiple models single unified model . If load model independently merge code , get single ` NSManagedObjectModel ` representing combined model model file . You could use combined model single persistent store coordinator . If 're still using multiple model files , add one separately . This raises complication though -- Core Data know model file use create new model object instance ? You would use ` assignObject : toPersistentStore : ` method ` NSManagedObjectContext ` tell one use . Every time create new instance , well . This means need keep references ` NSPersistentStore ` instances file , know use every case . I add I used encrypted Core Data I n't know solve real problem . This approach allow multiple model files multiple persistent stores single coordinator , though ."
Stackoverflow,"In application , user has_many tickets . Unfortunately , tickets table user_id : user_login ( legacy database ) . I going change someday , change would many implications . So I build `` user has_many : tickets '' association **login** column ? I tried following finder_sql , work . class User < ActiveRecord : :Base has_many : tickets , : finder_sql = > 'select t.* tickets t.user_login= # { login } ' ... end I get weird error : ArgumentError : /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:402 : ` to_constant_name ' : Anonymous modules name referenced /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/base.rb:2355 : ` interpolate_sql' /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:214 : ` qualified_name_for' /var/lib/gems/1.8/gems/activesupport-2.0.2/lib/active_support/dependencies.rb:477 : ` const_missing' ( eval ) :1 : ` interpolate_sql' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/association_proxy.rb:95 : ` send' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/association_proxy.rb:95 : ` interpolate_sql' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/has_many_association.rb:143 : ` construct_sql' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations/has_many_association.rb:6 : ` initialize' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations.rb:1032 : ` new' /var/lib/gems/1.8/gems/activerecord-2.0.2/lib/active_record/associations.rb:1032 : ` tickets' ( irb ) :1 I also tried finder_sql ( double quotes around login ) : : finder_sql = > 'select t.* tickets t.user_login= '' # { login } '' ' But fails way ( anyway , worked would vulnerable sql injection ) . In test database , I added user_id column tickets table , tried finder_sql : : finder_sql = > 'select t.* tickets t.user_login= # { id } ' Now works fine . So apparently , problem fact users column I trying use string , id . I searched net quite time ... could find clue . I would love able pass parameter finder_sql , write things like : has_many : tickets_since_subscription , : finder_sql = > [ 'select t.* tickets t.user_login= ? '+ ' t.created_at > = ? ' , ' # { login } ' , ' # { subscription_date } ' ] Edit : I use : foreign_key parameter has_many association users table _does_ id primary key column , used elsewhere application . Edit # 2 : apparently I read documentation thoroughly enough : has_many association take : primary_key parameter , specify column local primary key ( default id ) . Thank Daniel opening eyes ! I guess answers original question : has_many tickets , : primary_key= '' login '' , : foreign_key= '' user_login '' But I would still love know I make has_many : tickets_since_subscription association work .","I think 're actually looking : has_many : posts , : finder_sql = > proc { `` SELECT p.* posts p join topics p.topic_id = t.id t.id= # { id } '' } As Rails 3.1 use proc instead string use fields like ` # { id } ` . See issue : < https : //github.com/rails/rails/issues/3920 >"
Stackoverflow,"Our application uses SQL Server LocalDb 2014 database engine . The connection string use `` Data Source= ( localdb ) \MSSQLLOCALDB ; Initial Catalog=OurDatabase ; MultipleActiveResultSets=True ; Integrated Security=True ; AttachDBFilename=|DataDirectory|OurDatabase.mdf '' Now , one computers , VS 2015SP3 latest version SQL Server objects installed , application starts using SQL Server LocalDb 2016 . This undesirable exchange back-ups database files regularly computers back-ups made LocalDb 2016 format read computers LocalDb 2016 . The problem connection string specify version LocalDb used . It way force LocalDb 2014 ( 2016 , decide upgrade ? )","The fix indeed update version ` Microsoft.SqlServer.Dac ` assemblies using - I 'd discovered I try shortly seeing @ kevin-cunnane 's suggestion . There factors made less obvious , SO : 1 . Dac 's error message `` Unable connect target server '' n't indicate version incompatibility way . However poking around internet ( eg [ DACPAC wo n't deploy 'ca n't connect server ' ? ] ( https : //stackoverflow.com/questions/35952040/dacpac-wont-deploy-because-cant-connect-to-server ) ) seems error message mean version incompatibility addition incorrect connection string , firewall issue , etc . 2 . There several NuGet packages posted contain ` Microsoft.SqlServer.Dac ` related assemblies . A maintained Microsoft , including one I using ( [ Microsoft.SqlServer.Dac ] ( https : //www.nuget.org/packages/Microsoft.SqlServer.Dac ) ) . The official Microsoft release available NuGet.org June 2016 , n't obvious NuGet id ( [ Microsoft.SqlServer.DacFx.x64 ] ( https : //www.nuget.org/packages/Microsoft.SqlServer.DacFx.x64/ ) ) . So running ` update-package Microsoft.SqlServer.Dac ` desired effect . 3 . The `` official '' NuGet package n't listed anywhere MSDN + DAC pages - 'd think would mentioned : < https : //msdn.microsoft.com/en-us/library/dn702988 % 28v=sql.120 % 29.aspx > \- 's . 4 . Visual Studio 2016 installs SQL LocalDB 2016 , _does_ include correct Dac assemblies ( ` C : \Program Files ( x86 ) \Microsoft Visual Studio 14.0\Common7\IDE\Extensions\Microsoft\SQLDB\DAC\130\Microsoft.SqlServer.Dac.dll ` ) , 're installed GAC otherwise easily findable . The fix worked # Remove old NuGet dependencies uninstall-package Microsoft.SqlServer.Dac # Install new Dac NuGet package Install-Package Microsoft.SqlServer.DacFx.x64 Requests Dac team , happen see : * Please link correct NuGet package MSDN documentation * Please improve error message indicate newer client software needed * Please request NuGet package maintainers note presence official NuGet package , provide upgrade references official NuGet package , b/c presence multiple packages likely cause angst . ( Btw , despite difficulties , Dac/SSDT _AWESOME_ . I n't seen comparable dev tooling competitive relational databases . )"
Stackoverflow,"Some background : I 've encountering memory exception within SSMS 2012 since released coupled Red Gate 's SQLPrompt ( exception never happens SSMS 2008R2 laptop ) . I originally encountering exceptions daily ( SSMS2012 SQLPrompt ) forces close reopen SSMS ( along everything I working ) . Months ago I stumbled upon support thread pointed possible add- 's cause I **uninstalled** add-on ( SQL Prompt , uninstall rest developer 's bundle ) went many months _without_ single exception much less System.OutOfMemoryException exception . Once SP1 released 2012 I applied reinstalled SQL Prompt ( latest version ) see maybe issue addressed within 7 hours development time I greeted infamous System.OutOfMemoryException . Through I 've opening tickets Red Gate submitting debug logs see exception since memory exception explicitly list SQLPrompt wo n't escalate issue development team . Prior specific exception SQL Prompt however throwing numerous exceptions ( listed ) within SSMS 2012 IDE ( Visual Studio 2010 ) . I believe memory exception symptomatic issue SQL Prompt manages cached data eats available memory SSMS eventually throws exception result . I 've learn defer issue reproduce directly related two variables : 1 . Connecting working multiple instances SSMS ( Object Explorer Query windows ) . I.e . Connected 7 instances netted exception within 2-3 hours . 2 . Returning result sets multiple instances . This includes queries used SSMS return information IDE results returned individual query windows . The instances I 'm connected fast exception raised leads SQL Prompt 's caching object information per instances . Once memory exception raised situation degrades SSMS crashes completely ( unless I close first ) . What I collect more/better information submit Red Gate correct issue . This I need help . Laptop : HP Elite book 8440 RAM : 6GB Current OS : Win 7 Enterprise Ed Sp1 Here exceptions caused SQL Prompt : System.ArgumentOutOfRangeException `` Specified argument range valid values . '' Microsoft.VisualStudio.Text.Implementation.BinaryStringRebuilder.GetLineNumberFromPosition ( N/A , N/A ) Microsoft.VisualStudio.Text.Implementation.TextSnapshot.GetLineFromPosition ( Microsoft.VisualStudio.Text.Implementation.TextSnapshot , N/A ) Microsoft.VisualStudio.Editor.Implementation.VsTextBufferAdapter.GetLineIndexOfPosition ( N/A , System.Int32 , System.Int32 & , System.Int32 & ) RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider.PositionFromIndex ( RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider , System.Int32 ) RedGate.SqlPrompt.Metadata.Script.ScriptProviderBase.GetText ( RedGate.SQLPrompt.CommonVS.Editor.VSScriptProvider , System.Int32 , System.Int32 ) RedGate.SqlPrompt.Engine.NewEngine.SqlPromptEngine.GetCandidates ( RedGate.SqlPrompt.Engine.NewEngine.SqlPromptEngine , System.Int32 ) RedGate.SqlPrompt.Engine.PromptEngineEmulator.get_GetSuggestions ( RedGate.SqlPrompt.Engine.PromptEngineEmulator ) RedGate.SqlPrompt.Engine.AutoCompleter.m_FilterChanged ( RedGate.SqlPrompt.Engine.AutoCompleter , RedGate.SqlPrompt.Engine.PromptEngineEmulator , System.EventArgs ) RedGate.SqlPrompt.Engine.PromptEngineEmulator.OnFilterChanged ( RedGate.SqlPrompt.Engine.PromptEngineEmulator ) RedGate.SqlPrompt.Engine.PromptEngineEmulator.set_Index ( RedGate.SqlPrompt.Engine.PromptEngineEmulator , System.Int32 ) RedGate.SqlPrompt.Engine.PromptEngineEmulator.set_CaretPosition ( RedGate.SqlPrompt.Engine.PromptEngineEmulator , N/A ) RedGate.SQLPrompt.CommonUI.Editor.EditorWindowBase.SetEngineCaretPosition ( RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow , N/A ) RedGate.SQLPrompt.CommonUI.Editor.EditorWindowBase.UpdateUIPrompts ( RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow ) RedGate.SQLPrompt.CommonVS.Editor.VSEditorWindow.OnTextViewCommandExec ( RedGate.SQLPrompt.SSMSUI.SSMSEditorWindow , RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor , RedGate.SQLPrompt.CommonVS.Editor.CommandExecEventArgs ) RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.AfterCommandExecute ( RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor , RedGate.SQLPrompt.CommonVS.Editor.CommandExecEventArgs ) RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.. ( RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor . ) RedGate.SQLPrompt.CommonUI.Utils.ErrorDialog.Do ( System.Action ) RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor . ( RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor , System.Guid & , System.Uint32 , System.Uint32 , System.IntPtr , System.IntPtr ) RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor.. ( RedGate.SQLPrompt.CommonVS.Editor.TextViewMonitor. , System.Guid & , System.Uint32 , System.Uint32 , System.IntPtr , System.IntPtr ) Microsoft.VisualStudio.Editor.Implementation.CommandChainNode.Exec ( N/A , N/A , N/A , N/A , N/A , N/A ) System.ArgumentException 00:05:14.7510000 `` The parameter incorrect . ( Exception HRESULT : 0x80070057 ( E_INVALIDARG ) ) '' # mMc. # JQub. # OQub ( # mMc. # JQub , N/A , System.Uint32 , # mMc. # k3ub & ) # mMc. # JQub. # z26. # 8Di ( # mMc. # JQub. # z26 ) RedGate.SQLSourceControl.Engine.SmartAssembly.ExceptionReporting.ErrorReporterBase.Do ( RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog , System.Action , System.Predicate ` 1 < System.Exception > , System.Boolean ) RedGate.SQLSourceControl.Engine.SmartAssembly.ExceptionReporting.ErrorReporterBase.Do ( RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog , System.Action ) RedGate.SQLSourceControl.CommonUI.Forms.ErrorDialog.Do ( System.Action ) And Memory Exception : ! [ Exception thrown result set ] ( https : //i.stack.imgur.com/nbOlu.png ) System.OutOfMemoryException < null > System.Text.StringBuilder.set_Capacity ( System.Text.StringBuilder , N/A ) Microsoft.SqlServer.Management.QueryExecution.QEDiskStorageView.set_MaxNumBytesToDisplay ( N/A , N/A ) Microsoft.SqlServer.Management.QueryExecution.QEDiskDataStorage.GetStorageView ( N/A ) Microsoft.SqlServer.Management.QueryExecution.QEResultSet.StartRetrievingData ( Microsoft.SqlServer.Management.QueryExecution.QEResultSet , System.Int32 , N/A ) Microsoft.SqlServer.Management.QueryExecution.ResultSetAndGridContainer.StartRetrievingData ( N/A , N/A , N/A ) Microsoft.SqlServer.Management.QueryExecution.ResultsToGridBatchConsumer.OnNewResultSet ( Microsoft.SqlServer.Management.QueryExecution.ResultsToGridBatchConsumer , N/A , N/A ) Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.ProcessResultSet ( Microsoft.SqlServer.Management.QueryExecution.QESQLBatch , N/A ) Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.DoBatchExecution ( Microsoft.SqlServer.Management.QueryExecution.QESQLBatch , System.Data.SqlClient.SqlConnection , N/A ) Microsoft.SqlServer.Management.QueryExecution.QESQLBatch.Execute ( N/A , N/A , N/A ) Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.DoBatchExecution ( Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec , Microsoft.SqlServer.Management.QueryExecution.QESQLBatch ) Microsoft.SqlServer.Management.QueryExecution.QESQLExec.ExecuteBatchCommon ( Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec , N/A , N/A , System.Boolean & ) Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.ExecuteBatchHelper ( N/A , N/A , N/A , N/A ) Microsoft.SqlServer.Management.QueryExecution.QEOLESQLExec.ProcessBatch ( N/A , N/A , N/A ) .BatchParser.ThunkCommandExecuter.ProcessBatch ( N/A , N/A , N/A ) Again , clear , I n't know exceptions related memory exception **only** happens SQL Prompt installed . Thanks assistance !",Just go : Tools - > Options - > Text Editor - > SQL Server Tools - > IntelliSense Uncheck `` Enable IntelliSense '' I hope solve issue .
Stackoverflow,"I beg n't ask I using SQL Server 6.5 There SELECT TOP command SQL Server 6.5 , guess , I need : ) I need perform something like Select top 1 * persons name ='Mike' order id desc I 've tried something SET ROWCOUNT 1 , case use order . I end Select top 1 * persons id = ( select max ( id ) persons name ='Mike ' ) There must better way ! Any suggestions ? Thanx !","Try selecting temporary table , ordered ID , SET ROWCOUNT 1 select * temporary table . ( This work top N SET ROWCOUNT N , existing solution work top 1 . )"
Stackoverflow,"I would like identify returning customers Oracle ( 11g ) table like : CustID | Date -- -- -- -| -- -- -- -- -- XC321 | 2016-04-28 AV626 | 2016-05-18 DX970 | 2016-06-23 XC321 | 2016-05-28 XC321 | 2016-06-02 So I see customers returned within various windows , example within 10 , 20 , 30 , 40 50 days . For example : CustID | 10_day | 20_day | 30_day | 40_day | 50_day -- -- -- -| -- -- -- -- | -- -- -- -- | -- -- -- -- | -- -- -- -- | -- -- -- -- XC321 | | | 1 | | XC321 | | | | 1 | I would even accept result like : CustID | Date | days_from_last_visit -- -- -- -| -- -- -- -- -- -- | -- -- -- -- -- -- -- -- -- -- - XC321 | 2016-05-28 | 30 XC321 | 2016-06-02 | 5 I guess would use partition windowing clause unbounded following preceding clauses ... I find suitable examples . Any ideas ... ? Thanks","No need window functions , simply conditional aggregation using ` CASE EXPRESSION ` : SELECT t.custID , COUNT ( CASE WHEN ( last_visit- t.date ) < = 10 THEN 1 END ) 10_day , COUNT ( CASE WHEN ( last_visit- t.date ) 11 20 THEN 1 END ) 20_day , COUNT ( CASE WHEN ( last_visit- t.date ) 21 30 THEN 1 END ) 30_day , ... .. FROM ( SELECT s.custID , LEAD ( s.date ) OVER ( PARTITION BY s.custID ORDER BY s.date DESC ) last_visit FROM YourTable ) GROUP BY t.custID"
Stackoverflow,"I wrote child mysqli query method returns child mysqli_result . This result child additional methods unique app . public MySQL extends mysqli { public function query ( $ query ) { ( $ this- > real_query ( $ query ) ) { ( $ this- > field_count > 0 ) { return new MySQL_Result ( $ ) ; } return true ; } return false ; } } class MySQL_Result extends mysqli_result { public function fetch_objects ( ) { $ rows = array ( ) ; ( $ row = $ this- > fetch_object ( ) ) $ rows [ $ row- > id ] = $ row ; return $ rows ; } } What I ca n't figure whether ` fetch_object ( ) ` uses buffered unbuffered SQL data . The constructor ` mysqli_result ` n't visible ` mysqli.php ` , I can't see calling ` $ mysqli- > store_result ( ) ` ` $ mysqli- > use_result ( ) ` . I tried adding methods ` MySQL ` neither function echoed . public function store_result ( $ option= ' ' ) { echo `` STORE RESULT < br/ > '' ; } public function use_result ( $ option= ' ' ) { echo `` USE RESULT < br/ > '' ; } Does mean ` mysqli_result ` constructor n't call either ? If , access SQL data ` fetch_object ` called ? ... ... ... ... ... . I want buffered SQL data . If I ca n't figure child constructor , I may replace result child class decorator calls ` $ mysqli- > store_result ( ) ` . I 'm new PHP / OOP would much appreciate feedback . Thank .",Missing ` ; ` ` header ( 'location : home.php ' ) ` It : ( $ check_user > 0 ) { header ( 'location : home.php ' ) ; } else { echo `` wrong username password '' ; } **Upadated : ** Another Problem ` SELECT > ` ` SELECT * ` **Your Code : ** $ select_user= `` SELECT > FROM example WHERE Email= ' $ email ' AND Passwords= ' $ password ' '' ; **It : ** $ select_user= `` SELECT * FROM example WHERE Email= ' $ email ' AND Passwords= ' $ password ' '' ; **Comments Problem : ** You used ` mysqli_master_query ( ) ` This function currently documented ; argument list available . Please see website ** [ doc ] ( http : //php.net/manual/en/function.mysqli-master-query.php ) . **
Stackoverflow,"I 'm trying join across number tables ( three plus join table middle ) . I think korma lazily evaluating last join . What I 'm trying add condition restricts results first table join , I 'm interested fields last table join . So example say I 've got ` clubs ` , ` people ` ` hobbies ` tables , ` people-to-hobbies ` join table last two . Each club many people , person many hobbies . I 'm trying get full details hobbies people belong specific club , I n't want fields ` club ` table . The join table means korma create two queries , one get people specific club , another retrieve hobbies person via ` people-to-hobbies ` join table . My korma query looks something like : ( select clubs ( people ( hobbies ( fields : hobby-name : id ) ( { : clubs.name `` korma coders '' } ) ) ) ) The problem I n't specified fields I want ` clubs ` ` people ` , default select ` * ` . How I include fields tables ? Is possible , fact ` hobbies ` lazily loaded mean korma return results first query ( gets filtered list people ) , I come interrogate later hobbies , ids needs run second query ?","The result get depends default timezone JVM . You fix via whatever mechanism host operating system gives you.But experience 's generally better force JVM known value explicitly . This achieving property command line , leiningen ` project.clj ` : jvm-opts [ `` -Duser.timezone=UTC '' ]"
Stackoverflow,"In Android Pie sqlite Write-Ahead logging ( WAL ) enabled default . This causing errors existing code Pie devices . I unable turn WAL successfully using ` SQLiteDatabase.disableWriteAheadLogging ( ) ` ` PRAGMA journal_mode ` due way I access database . I would like disable WAL completely Android setting called _db_compatibility_wal_supported_ : [ Compatibility WAL ( Write-Ahead Logging ) Apps ] ( https : //source.android.com/devices/tech/perf/compatibility- wal # disabling_compatibility_wal ) Does anyone know configure ? I n't know file altered programmatically startup changed manually . * * * # Further Details problem I sqlite database ( 20mb+ / 250k records ) app . This db generated using plain java server . It contains food database user app add database ( server updated ) . This stored assets folder android . During first installation database copied assets app folder written , using effectively method : [ Copy SQLite database assets folder ] ( https : //stackoverflow.com/questions/16354154/copy-sqlite-database- from-assets-folder # answer-16354263 ) Unfortunately , I start writing database using SqlDroid wal enabled tables original db vanished newly created tables remain . The size database however still 20mb+ . All database errors due missing tables . The table copying writing method works perfectly versions Android prior Pie .","@ Rockvole please share error facing , help us find appropriate solution . Mean , understand want close **WAL** android pie using `` SQLDroid '' lib create Sqlite DB . This lib internally using `` **SQLiteDatabase** `` store data locally , I think need call `` **SQLiteDatabase.disableWriteAheadLogging ( ) ** `` `` **SQLiteDatabase** `` class DB instance created package name `` ** _package org.sqldroid ; _** `` **_Get internal SQLiteDatabase instance_** call disableWriteAheadLogging ( ) . Second solution create `` config.xml '' inside values folder wirte ` `` < bool name= '' db_compatibility_wal_supported '' > false < /bool > '' ` run check work ."
Stackoverflow,"I data set I want parse see multi-touch attribution . The data set made leads responded marketing campaign marketing source . Each lead respond multiple campaigns I want get first marketing source last marketing source table . I thinking I could create two tables use select statement . The first table would attempt create table recent marketing source every person ( using email unique ID ) . create table temp.multitouch1 ( select distinct ( email ) email , date , market_source last_source sf.campaignmember date > = ' 1/1/2016 ' ORDER BY DATE DESC ) ; Then I would create table deduped emails time first source . create table temp.multitouch2 ( select distinct ( email ) email , date , market_source first_source sf.campaignmember date > = ' 1/1/2016 ' ORDER BY DATE ASC ) ; Finally I wanted simply select email join first last market sources column . select a.email , a.last_source , b.first_source , a.date temp.multitouch1 left join temp.multitouch b b.email = a.email Since distinct n't work redshift 's postgresql version I hoping someone idea solve issue another way . EDIT 2/22 : For context I 'm dealing people campaigns they've responded . Each record `` campaign response '' every person one campaign response multiple sources . I 'm trying make select statement would dedupe person columns first campaign/marketing source 've responded last campaign/marketing source 've responded respectively . EDIT 2/24 : Ideal output table 4 columns : email , last_source , first_source , date . The first last source columns would people 1 campaign member record different everyone 1 campaign member record .","As long table rows required series numbers , worked past : select ( row_number ( ) ( order 1 ) ) - 1 hour large_table limit 24 ; Which returns numbers ` 0-23 ` ."
Stackoverflow,"Are methods page queries FileNet ? I grid control paging need get count elements total query possibility get example page 3 results ( page size ) . I found TOP operator , SKIP COUNT ?","There way accomplish exactly want . The main reason expensive Content Engine count rows returned query ( assuming thousands ) . There quite involved security restrictions potentially applied returned object . This effectively exclude objects query results , affecting result count . Evaluating effective security permissions multiple objects would kill performance , thus avoided . That n't ` COUNT ` aggregate functions query language . If restrict amount records displayed grid reasonable number , ` COUNT_LIMIT ` might work — see [ Query Options ] ( http : //www-01.ibm.com/support/knowledgecenter/SSNW2F_5.2.0/com.ibm.p8.ce.dev.ce.doc/query_sql_syntax_rel_queries.htm % 23query_sql_syntax_rel_queries__Query_Options ? lang=en ) [ getTotalCount ( ) ] ( http : //www-01.ibm.com/support/knowledgecenter/SSNW2F_5.2.0/com.ibm.p8.ce.dev.java.doc/com/filenet/api/collection/PageIterator.html ? lang=en- us # getTotalCount\ ( \ ) ) . However , require looping required page inherent performance drawback ."
Stackoverflow,"As far I know package body replaced recompiled without affecting specification . A package specification declares procedures functions , defines , reference objects , make package specification INVALID . I know package specification reference objects uses stand- alone subprograms packages define 's variables . In case changing referenced objects may cause specification invalidation . Is way Oracle package specification depend ( reference ) objects become INVALID whether referenced objects chаnge another way ?","Procedures'/Functions ' declarations must equal ` package ` ` body ` . Try avoid _magic numbers_ like ` p_amount NUMBER ( 10 ) ` : meaning ` 10 ` ? But use ` p_amount receipt.Amount % Type ` clealy type ` receipt.Amount ` field . CREATE OR REPLACE PACKAGE form_pkg AS -- interface : procedure 3 arguments PROCEDURE Insert_receipts ( p_receipt receipt.Receipt_Number % Type , p_transaction receipt.Transaction_ID % Type , p_amount receipt.Amount % Type ) ; ... END form_pkg ; / CREATE OR REPLACE PACKAGE BODY form_pkg AS -- package body -- implementation -- The three arguments ( seq_value ! ) PROCEDURE Insert_receipts ( p_receipt receipt.Receipt_Number % Type , p_transaction receipt.Transaction_ID % Type , p_amount receipt.Amount % Type ) IS BEGIN INSERT INTO receipt ( ID , Receipt_Number , Transaction_ID , Amount ) VALUES ( seq.NextVal , p_receipt , p_transaction , p_amount ) ; END Insert_receipts ; ... END form_pkg ; / When implementing routine see errors ' details ( names , lines etc . ) ` USER_ERRORS ` view : -- ( syntactic ) errors form_pkg select * USER_ERRORS Name = upper ( 'form_pkg ' )"
Stackoverflow,"I 'm trying setup Room database backup functionality . Problem sql database file n't contain latest set data app downloaded . It always misses recent records . Is proper way export room database ? P.S . I n't face similar problems handled db sqliteHelper , I suppose must something Room . Way I 'm : @ Throws ( IOException : :class ) private fun copyAppDbToDownloadFolder ( address : String ) { val backupDB = File ( address , `` studioDb.db '' ) val currentDB = applicationContext.getDatabasePath ( StudioDatabase.DB_NAME ) ( currentDB.exists ( ) ) { val src = FileInputStream ( currentDB ) .channel val dst = FileOutputStream ( backupDB ) .channel dst.transferFrom ( src , 0 , src.size ( ) ) src.close ( ) dst.close ( ) } }","You need use > JournalMode.TRUNCATE AppDatabase.java : private static AppDatabase sInstance ; public static AppDatabase getDatabase ( final Context context ) { ( sInstance == null ) { synchronized ( AppDatabase.class ) { ( sInstance == null ) { sInstance = Room.databaseBuilder ( context , AppDatabase.class , DATABASE_NAME ) .setJournalMode ( JournalMode.TRUNCATE ) .build ( ) ; } } } return sInstance ; } This method create **db.bad** **db.wal** files 's creating hindrance exporting room db . **_For exporting DB file : _** > Link : [ Exporting db creating folder daily basis ] ( https : //stackoverflow.com/questions/6540906/simple-export-and-import- of-a-sqlite-database-on-android/53167018 # 53167018 )"
Stackoverflow,Is recent version SQLHelper class . I 've using one years wondering new version .NET Framework 2.0 3.0 . I prefer small projects vs Microsoft Data App Block ( I use larger projects ) . I came across link > [ http : //www.microsoft.com/downloads/details.aspx ? familyid=f63d1f0a-9877-4a7b-88ec-0426b48df275 & displaylang=en ] ( http : //www.microsoft.com/downloads/details.aspx ? familyid=f63d1f0a-9877-4a7b-88ec-0426b48df275 & displaylang=en ),You use parameters things like table column names . For could whitelist possible values use string concatenation building SQL query .
Stackoverflow,"I 'm strange bug I write website flask package flask- mysql . Here code bug function : @ app.route ( '/calendar/editeventtitle ' , methods= [ 'POST ' ] ) def editeventtitle ( ) : session.get ( 'logged_in ' ) : abort ( 401 ) try : id = request.form.get ( 'id ' , type=int ) title = request.form [ 'title ' ] color = request.form [ 'color ' ] delete = request.form.get ( 'delete ' ) except : pass conn = mysql.connect ( ) cursor = conn.cursor ( ) print ( id , type ( id ) ) # try : # print ( delete , type ( delete ) ) # except : # pass id delete : cursor.execute ( 'delete events id = % ' , id ) conn.commit ( ) flash ( 'Event canceled ! ' ) return redirect ( url_for ( 'calendar ' ) ) elif id title color : cursor.execute ( 'update events set title = % , color = % id = % ' , ( title , color , id ) ) conn.commit ( ) flash ( 'Event updated ! ' ) return redirect ( url_for ( 'calendar ' ) ) When I post four variables page . I succesfully get . And result ` print ( id , type ( id ) ) ` like : 6 < class 'int ' > We see 's really integer , code starts update delete data db , error message : > TypeError : % format : number required , str Really n't know reason =-= , anyone help ? Thank . PS : Python3.6.1 , Flask 0.12.2 , Flask-Mysql 1.4.0","You need keep handle connection ; keep overriding loop . Here simplified example : con = mysql.connect ( ) cursor = con.cursor ( ) def insert ( mysql , insertCmd ) : try : cursor.execute ( insertCmd ) con.commit ( ) return True except Exception e : print ( `` Problem inserting db : `` + str ( e ) ) return False If ` mysql ` connection , commit , directly : def insert ( mysql , insertCmd ) : try : cursor = mysql.cursor ( ) cursor.execute ( insertCmd ) mysql.commit ( ) return True except Exception e : print ( `` Problem inserting db : `` + str ( e ) ) return False return False"
Stackoverflow,Executing command : sqlacodegen < connection-url > -- outfile db.py The db.py contains generated tables : t_table1 = Table ( ... ) classes : Table2 ( Base ) : __tablename__ = 'table2' The problem table generated one way - either table class . I would like make generate models ( classes ) provided flags I could n't find option . Any idea ?,"It Python shell : > > > import sqlacodegen > > > sqlacodegen -- help Traceback ( recent call last ) : File `` < stdin > '' , line 1 , < module > TypeError : bad operand type unary - : '_Helper' You executed ` sqlacodegen -- help ` **Unix command shell / Windows command prompt** : % sqlacodegen -- help usage : sqlacodegen [ -h ] [ -- version ] [ -- schema SCHEMA ] [ -- tables TABLES ] [ -- noviews ] [ -- noindexes ] [ -- noconstraints ] [ -- nojoined ] [ -- noinflect ] [ -- outfile OUTFILE ] [ url ] Generates SQLAlchemy model code existing database . positional arguments : url SQLAlchemy url database optional arguments : -h , -- help show help message exit -- version print version number exit -- schema SCHEMA load tables alternate schema -- tables TABLES tables process ( comma-separated , default : ) -- noviews ignore views -- noindexes ignore indexes -- noconstraints ignore constraints -- nojoined n't autodetect joined table inheritance -- noinflect n't try convert tables names singular form -- outfile OUTFILE file write output ( default : stdout ) An example actual command would : % sqlacodegen -- outfile models.py \ postgresql : //gollyjer : swordfish @ localhost:5432/mydatabase Where ` gollyjer : swordfish ` format ` user : password ` ."
Stackoverflow,"I attempting enable MySQL group replication plugin MySQL 5.7.21 , available 5.7 per documentation ( < https : //dev.mysql.com/doc/refman/5.7/en/group-replication.html > ) $ mysql -- version mysql Ver 14.14 Distrib 5.7.21 , Linux ( x86_64 ) using EditLine wrapper When I attempt enable plugin MySQL : $ mysql > INSTALL PLUGIN group_replication SONAME 'group_replication.so ' ; ERROR 1126 ( HY000 ) : Ca n't open shared library '/usr/lib/mysql/plugin/group_replication.so ' ( errno : 2 /usr/lib/mysql/plugin/group_replication.so : open shared object file : No file directory ) The output plugins MySQL : $ mysql > SHOW PLUGINS ; + -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ | Name | Status | Type | Library | License | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ | binlog | ACTIVE | STORAGE ENGINE | NULL | GPL | | mysql_native_password | ACTIVE | AUTHENTICATION | NULL | GPL | | sha256_password | ACTIVE | AUTHENTICATION | NULL | GPL | | CSV | ACTIVE | STORAGE ENGINE | NULL | GPL | | MEMORY | ACTIVE | STORAGE ENGINE | NULL | GPL | | InnoDB | ACTIVE | STORAGE ENGINE | NULL | GPL | | INNODB_TRX | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_LOCKS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_LOCK_WAITS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMP | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMP_RESET | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMPMEM | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMPMEM_RESET | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMP_PER_INDEX | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_CMP_PER_INDEX_RESET | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_BUFFER_PAGE | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_BUFFER_PAGE_LRU | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_BUFFER_POOL_STATS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_TEMP_TABLE_INFO | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_METRICS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_DEFAULT_STOPWORD | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_DELETED | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_BEING_DELETED | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_CONFIG | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_INDEX_CACHE | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_FT_INDEX_TABLE | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_TABLES | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_TABLESTATS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_INDEXES | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_COLUMNS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_FIELDS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_FOREIGN | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_FOREIGN_COLS | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_TABLESPACES | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_DATAFILES | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | INNODB_SYS_VIRTUAL | ACTIVE | INFORMATION SCHEMA | NULL | GPL | | MyISAM | ACTIVE | STORAGE ENGINE | NULL | GPL | | MRG_MYISAM | ACTIVE | STORAGE ENGINE | NULL | GPL | | PERFORMANCE_SCHEMA | ACTIVE | STORAGE ENGINE | NULL | GPL | | ARCHIVE | ACTIVE | STORAGE ENGINE | NULL | GPL | | BLACKHOLE | ACTIVE | STORAGE ENGINE | NULL | GPL | | FEDERATED | DISABLED | STORAGE ENGINE | NULL | GPL | | partition | ACTIVE | STORAGE ENGINE | NULL | GPL | | ngram | ACTIVE | FTPARSER | NULL | GPL | + -- -- -- -- -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- + -- -- -- -- -+ -- -- -- -- -+ 44 rows set ( 0.01 sec ) This contents plugin directory : $ ls -lah /usr/lib/mysql/plugin/ total 644K drwxr-xr-x 2 root root 4.0K Sep 26 23:24 . drwxr-xr-x 3 root root 4.0K Sep 26 23:24 .. -rw-r -- r -- 1 root root 21K Jul 19 14:10 adt_null.so -rw-r -- r -- 1 root root 6.2K Jul 19 14:10 auth_socket.so -rw-r -- r -- 1 root root 44K Jul 19 14:10 connection_control.so -rw-r -- r -- 1 root root 107K Jul 19 14:10 innodb_engine.so -rw-r -- r -- 1 root root 79K Jul 19 14:10 keyring_file.so -rw-r -- r -- 1 root root 151K Jul 19 14:10 libmemcached.so -rw-r -- r -- 1 root root 9.7K Jul 19 14:10 locking_service.so -rw-r -- r -- 1 root root 11K Jul 19 14:10 mypluglib.so -rw-r -- r -- 1 root root 6.2K Jul 19 14:10 mysql_no_login.so -rw-r -- r -- 1 root root 55K Jul 19 14:10 rewriter.so -rw-r -- r -- 1 root root 56K Jul 19 14:10 semisync_master.so -rw-r -- r -- 1 root root 15K Jul 19 14:10 semisync_slave.so -rw-r -- r -- 1 root root 27K Jul 19 14:10 validate_password.so -rw-r -- r -- 1 root root 31K Jul 19 14:10 version_token.so These contents config : > cat /etc/mysql/my.cnf [ mysqld_safe ] nice = 0 socket = /var/run/mysqld/mysqld.sock [ mysqld ] basedir = /usr bind_address = 123.45.67.89 binlog_checksum = NONE binlog_format = ROW datadir = /var/lib/mysql enforce_gtid_consistency = ON expire_logs_days = 10 general_log = 1 general_log_file = /var/log/mysql/mysql.log gtid_mode = ON key_buffer_size = 8388608 lc_messages_dir = /usr/share/mysql log_bin = binlog log_error = /var/log/mysql/mysql_error.log log_slave_updates = ON long_query_time = 60 loose-group_replication_bootstrap_group = OFF loose-group_replication_enforce_update_everywhere_checks= ON loose-group_replication_group_name = 34dee7cd-d20d-4f59-9500-f56ada9a1abz loose-group_replication_group_seeds = 123.45.67.88:33061,123.45.67.89:33061 loose-group_replication_ip_whitelist = 123.45.67.88,123.45.67.89 loose-group_replication_local_address = 123.45.67.89:33061 loose-group_replication_recovery_use_ssl= 1 loose-group_replication_single_primary_mode= OFF loose-group_replication_ssl_mode = REQUIRED loose-group_replication_start_on_boot = OFF master_info_repository = TABLE max_allowed_packet = 16M max_binlog_size = 100M max_connect_errors = 100000000 pid-file = /var/run/mysqld/mysqld.pid port = 3306 query_cache_limit = 1M query_cache_size = 16M relay_log = my-project-prod-relay-bin relay_log_info_repository = TABLE report_host = 123.45.67.88 require_secure_transport = ON server_id = 2 skip_external_locking slow_query_log = 1 slow_query_log_file = /var/log/mysql/mysql-slow-query.log socket = /var/run/mysqld/mysqld.sock thread_cache_size = 8 thread_stack = 192K tmpdir = /tmp transaction_write_set_extraction = XXHASH64 user = mysql [ mysqldump ] max_allowed_packet = 16M quick quote_names [ mysql ] no-auto-rehash [ isamchk ] key_buffer_size = 16M [ client ] port = 3306 socket = /var/run/mysqld/mysqld.sock ! includedir /etc/mysql/conf.d/ I 'm wondering I 'm using wrong version MySQL step needed install group replication plugin ?","FWIW I downloaded 5.7.19 Community Edihttps : //dev.mysql.com/doc/refman/5.7/en/group-replication.htmltion run sandbox , I confirm group replication .so file present . I got plugin load INSTALL PLUGIN statement , like tried . First I add config settings , documented < https : //dev.mysql.com/doc/refman/5.7/en/group-replication-configuring- instances.html > server_id=1 # value unique among replica set gtid_mode=ON enforce_gtid_consistency=ON master_info_repository=TABLE relay_log_info_repository=TABLE binlog_checksum=NONE log_slave_updates=ON log_bin=binlog binlog_format=ROW And course restart mysqld editing config settings . According discussion [ ] ( https : //bugs.mysql.com/bug.php ? id=85649 ) , looks like group replication plugin may intentionally left normal MySQL server edition . To know running Community Edition , run mysql : > mysql -u root -p Welcome MySQL monitor . Commands end ; \g . Your MySQL connection id 3 Server version : 5.7.20-log MySQL Community Server ( GPL ) ... .. To manually install Community Edition 5.7 ( archived versions : < https : //downloads.mysql.com/archives/community/ > ) , run following commands : sudo apt-get install libaio1 sudo apt-get install libmecab2 curl -OL https : //downloads.mysql.com/archives/get/file/mysql-common_5.7.20-1ubuntu16.04_amd64.deb curl -OL https : //downloads.mysql.com/archives/get/file/mysql-community-client_5.7.20-1ubuntu16.04_amd64.deb curl -OL https : //downloads.mysql.com/archives/get/file/mysql-client_5.7.20-1ubuntu16.04_amd64.deb curl -OL https : //downloads.mysql.com/archives/get/file/mysql-community-server_5.7.20-1ubuntu16.04_amd64.deb sudo dpkg -i mysql-common_5.7.20-1ubuntu16.04_amd64.deb sudo dpkg -i mysql-community-client_5.7.20-1ubuntu16.04_amd64.deb sudo dpkg -i mysql-client_5.7.20-1ubuntu16.04_amd64.deb sudo dpkg -i mysql-community-server_5.7.20-1ubuntu16.04_amd64.deb"
Stackoverflow,"I trying use POstgreSQL database project . I would like learn code first developement.. I download dotconnect PostgreSql ( trial version ) I added Devart.Data.PostgreSql dll references . Then , I added lines app.copnfig < connectionStrings > < add name= '' SchoolDBConnectionString '' connectionString= '' Server=localhost ; Port = 5432 ; Database=MyDataBase ; user Id=postgres ; password=***** '' providerName= '' Devart.Data.PostgreSql '' / > < /connectionStrings > < system.data > < DbProviderFactories > < remove invariant= '' Devart.Data.PostgreSql '' / > < add name= '' dotConnect PostgreSQL '' invariant= '' Devart.Data.PostgreSql '' description= '' Devart dotConnect PostgreSQL '' type= '' Devart.Data.PostgreSql.PgSqlProviderFactory , Devart.Data.PostgreSql , Version= 6.8.333.0 , Culture=neutral , PublicKeyToken=09af7300eec23701 '' / > < /DbProviderFactories > < /system.data > < /configuration > WHen I execute application exeption : An error occurred getting provider information database . This caused Entity Framework using incorrect connection string . Check inner exceptions details ensure connection string correct .","posting question found ca n't done TElement DataTable , made class ( entity ) returning , everything works ok , still must ask , done returns datatable ? ( n't know , hacks.. )"
Stackoverflow,"I 'm writing grammar parse HTSQL syntax stuck deal reuse ` / ` character segment division operators . The [ described grammar ] ( http : //htsql.org/doc/ref/syntax.html # syntax-structure ) n't terribly formal , I 've following exact output Python implementation , cursory glance seems handwritten parser , rather using parser generator - reference parser generator I'm currently using [ ` CL-YACC ` ] ( https : //www.irif.univ-paris- diderot.fr/~jch/software/cl-yacc/ ) [ ` CL- LEX ` ] ( https : //github.com/djr7C4/cl-lex ) . ( FWIW full thing [ ] ( https : //github.com/Ferada/cl-htsql ) , although likely bit outdated . ) The one ambiguity I 'm struggling arises due ` `` /1 '' ` parsed ` ' ( : COLLECT ( : INTEGER `` 1 '' ) ) ' ` , ` `` /1/2 '' ` parsed ` ' ( : COLLECT ( : OPERATOR / ( : INTEGER `` 1 '' ) ( : INTEGER `` 2 '' ) ) ) ' ` , i.e . one segment separator , one division ; ` `` /1//2 '' ` ` ' ( : COLLECT ( : OPERATOR / ( : INTEGER `` 1 '' ) ( : COLLECT ( : INTEGER `` 2 '' ) ) ) ) ' ` . The question thus , I deal grammar specification without resorting switching manual parser ? Would switch different parser generator class ( instead LALR ( 1 ) ) help ? So far I 've tried different variations grammar , however fact precedences fixed whole grammar also interferes interpretations slash . The way I tried disambiguate lexer , i.e . treating first slash ( `` group '' ) differently returning different symbol , e.g . ` DIV ` \- however I could n't find good rule somehow doubt exists purely looking lexical structure . Lastly , I 'm tempted resolve diverging given parser entirely make life easier . Would consider desirable , sense predictable grammar easily understood ? **Exhibit 1 , Python script examine parse tree : ** import htsql application = htsql.HTSQL ( `` sqlite : ///htsql_demo.sqlite '' ) global = None def p ( string ) : global application : = htsql.core.syn.parse.parse ( string ) return def l ( name ) : result = [ ] c name : c.isupper ( ) result : result.append ( `` - '' ) result.append ( c ) return `` '' .join ( result ) def keyword ( name ) : return `` : { } '' .format ( name.upper ( ) ) def n ( expression ) : name = expression.__class__.__name__ name = name [ : name.find ( `` Syntax '' ) ] return keyword ( l ( name ) ) def ( expression ) : arguments = [ n ( expression ) ] = expression.__dict__ `` identifier '' : arguments.append ( ( expression.identifier ) ) `` text '' : arguments.append ( `` \ '' { } \ '' '' .format ( expression.text ) ) `` symbol '' : isinstance ( expression , ( htsql.core.syn.syntax.ProjectSyntax , htsql.core.syn.syntax.FilterSyntax , htsql.core.syn.syntax.CollectSyntax , htsql.core.syn.syntax.DetachSyntax ) ) : arguments.append ( expression.symbol ) `` arm '' : arguments.append ( ( expression.arm ) ) `` larm '' : arguments.append ( ( expression.larm ) ) `` rarm '' : arguments.append ( ( expression.rarm ) ) `` arms '' : arguments.extend ( ( x ) x expression.arms ) `` rarms '' : arguments.extend ( ( x ) x expression.rarms ) return `` ( { } ) '' .format ( `` `` .join ( arguments ) ) # ( p ( `` /school '' ) ) # ' ( : COLLECT ( : IDENTIFIER `` school '' ) ) # ( p ( `` /'school ' '' ) ) # ' ( : COLLECT ( : STRING `` school '' ) ) **Exhibit 2 , current parser , n't deal correctly : ** ( defpackage # : cl-htsql ( : use # : cl # : alexandria # : cl-lex # : yacc ) ( : import-from # : arnesi # : with-collector ) ) ( eval-when ( : compile-toplevel : load-toplevel : execute ) ( defun maybe-intern ( name & optional ( package NIL package-p ) ) `` If NAME SYMBOL , return , otherwise INTERN . '' ( cond ( ( symbolp name ) name ) ( package-p ( intern name package ) ) ( T ( intern name ) ) ) ) ( defmacro define-lexer ( name & body patterns ) `` Shortcut DEFINE-STRING-LEXER . '' ` ( define-string-lexer , name , @ ( mapcar ( lambda ( pattern ) ( etypecase pattern ( ( symbol string ) ( let ( ( symbol ( maybe-intern pattern ) ) ( pattern ( string pattern ) ) ) ` ( , pattern ( return ( values ' , symbol ' , symbol ) ) ) ) ) ( list ( destructuring-bind ( pattern & optional symbol value ) pattern ( let* ( ( symbol ( symbol ( intern pattern ) ) ) ( value ( value symbol ) ) ) ( etypecase symbol ( list ` ( , pattern , symbol ) ) ( symbol ` ( , pattern ( return ( values ' , symbol ' , value ) ) ) ) ) ) ) ) ) ) patterns ) ) ) ) ; ; parser results treated immutable ( define-lexer string-lexer / ( `` \\| '' \| ) ( `` \\ & '' & ) < = > = == = ! == ! = ! ~ ! ~ < > @ ( `` \\ ? '' ? ) ( `` \\ . '' \ . ) ( `` \\ ( `` \ ( ) ( `` \\ ) '' \ ) ) ( `` \\+ '' + ) - ( `` \\* '' * ) \ : ( `` - ? 0| [ 1-9 ] [ 0-9 ] * ( \\. [ 0-9 ] * ) ? ( [ eE ] [ +- ] ? [ 0-9 ] + ) ? '' ( return ( cond ( ( find # \e $ @ ) ( values 'float $ @ ) ) ( ( find # \ . $ @ ) ( values 'decimal $ @ ) ) ( T ( values 'integer $ @ ) ) ) ) ) ( `` ( [ ^\ '' \\.\\ ? ~\'= < > \\ ( \\ ) @ \\|\\ & / : ] ) + '' ( return ( values 'name $ @ ) ) ) ( `` \ ' ( [ ^\\\ ' ] |\\ . ) * ? \ ' '' ( return ( values 'string ( string-trim `` \ ' '' $ @ ) ) ) ) ( `` \ '' ( [ ^\\\ '' ] |\\ . ) * ? \ '' '' ( return ( values 'string ( string-trim `` \ '' '' $ @ ) ) ) ) ) ( define-parser *expression-parser* ( : muffle-conflicts ( 44 0 ) ) ( : start-symbol query ) ( : terminals ( |\|| # + ( ) div & ! |.| ? / = ! = ! == ! ~ ~ < > == < = > = \ ( \ ) + - * @ name integer decimal float string ) ) ( : precedence ( ( : left @ ) ( : left ~ ) ( : left |.| ) ( : left + - ) ( : left * div ) ( : left = ! = == ! == ~ ! ~ < < = > > = ) ( : left ! ) ( : left & ) ( : left |\|| ) ( : left ? ) ( : left / ) ) ) ( query segment ) ( segment ( / segment ( lambda ( x ) ( declare ( ignore x ) ) ( ( eq : skip ) ' ( : skip ) ` ( : collect , ) ) ) ) skip group ) ( skip ( ( constantly : skip ) ) ) ( group ( \ ( segment \ ) ( lambda ( x z ) ( declare ( ignore x z ) ) ` ( : group , ) ) ) sieve ) ( sieve ( segment ? segment ( lambda ( x z ) ( declare ( ignore ) ) ` ( : filter , x , z ) ) ) ) ( ( segment |\|| segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ) ( ( segment & segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ) ( ( ! segment ( lambda ( x ) ` ( : prefix , x , ) ) ) comparison ) ( comparison ( segment = segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment ! = segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment ~ segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment ! ~ segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment == segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment ! == segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment < segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment < = segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment > segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment > = segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) addition ) ( addition ( segment + segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment - segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) multiplication ) ( multiplication ( segment * segment ( lambda ( x z ) ` ( : operator , , x , z ) ) ) ( segment / segment ( lambda ( x z ) ( declare ( ignore ) ) ` ( : operator / , x , z ) ) ) composition ) ( composition ( segment |.| segment ( lambda ( x z ) ( declare ( ignore ) ) ` ( : compose , x , z ) ) ) attach ) ( attach ( segment @ segment ( lambda ( x z ) ( declare ( ignore ) ) ` ( : attach , x , z ) ) ) detach ) ( detach ( @ segment ( lambda ( x ) ( declare ( ignore x ) ) ` ( : detach , ) ) ) term ) ( term ( name ( lambda ( x ) ` ( : identifier , x ) ) ) ( string ( lambda ( x ) ` ( : string , x ) ) ) ( number ( lambda ( x ) ` ( : integer , x ) ) ) ( integer ( lambda ( x ) ` ( : integer , x ) ) ) ( decimal ( lambda ( x ) ` ( : decimal , x ) ) ) ( float ( lambda ( x ) ` ( : float , x ) ) ) ) ) ( defun make-lexer-for-source ( source ) `` Make lexer SOURCE , either STRING STREAM . '' ( etypecase source ( string ( string-lexer source ) ) ( stream ( flet ( ( ignore ( c ) ( declare ( ignore c ) ) ) ) ( stream-lexer # 'read-line # 'string-lexer # 'ignore # 'ignore ) ) ) ) ) ( defun lex-source ( source ) `` Debug helper lex SOURCE list tokens . '' ( let ( ( lexer ( make-lexer-for-source source ) ) ) ( loop ( x ) = ( multiple-value-list ( funcall lexer ) ) x collect ( list x ) ) ) ) ( define-condition htsql-parse-error ( simple-error ) ( ) ) ( defun translate-yacc-error ( error ) ( make-condition 'htsql-parse-error : format-control `` Could n't parse HTSQL query : ~A . '' : format-arguments ( list error ) ) ) ( defun parse-htsql-query ( source ) `` Parse SOURCE syntax tree . The SOURCE may either STRING STREAM . '' ( handler-case ( parse-with-lexer ( make-lexer-for-source source ) *expression-parser* ) ( yacc-parse-error ( error ) ( error ( translate-yacc-error error ) ) ) ) ) ; ; > ( parse-htsql-query `` /1/ '' ) ; ; ( : OPERATOR / ( : COLLECT ( : INTEGER `` 1 '' ) ) : SKIP ) ; ; > ( parse-htsql-query `` /1/2 '' ) ; ; ( : OPERATOR / ( : COLLECT ( : INTEGER `` 1 '' ) ) ( : INTEGER `` 2 '' ) )","When use HTSQL Django shell , open transaction explicitly : > > > django.db import transaction > > > htsql_django import produce > > > transaction.commit_on_success ( ) : ... query = `` /polls_poll { question , total : =sum ( polls_choice.votes ) } '' ... row produce ( query ) : ... print `` % : % '' % ( row.question , row.total ) I 'm sorry documentation n't clear . We may change future release ."
Stackoverflow,"I discovered heavy load pyramid web app throws py-postgresql exceptions like ` postgresql.exceptions.ProtocolError ` . Some search revealed , py-postgresql thread-safe one connection used several threads concurrently . I tried make sort pooling mechanism , I still get ProtocolErrors : ( What I wrong ? First I create number connection objects : x range ( num_db_connections ) : self.pool.append ( Connection ( conn_string , x ) ) Each object pool contains ` db_lock = threading.Lock ( ) ` connection database ` self.conn = postgresql.open ( conn_string ) ` Then I try acquire lock connection work . This code executed many threads concurrently , think two threads run ` work ` one connection concurrently lock . time_start = time.time ( ) time.time ( ) - time_start < self.max_db_lock_wait_time : conn self.pool : acquired = conn.db_lock.acquire ( False ) acquired : try : lst = conn.work ( ) finally : conn.db_lock.release ( ) return lst time.sleep ( 0.05 ) raise Exception ( 'Could get connection lock time ' ) Maybe flaw code , I misunderstood nature `` thread unsafety '' py-postgresql ? Please , help !","I think probably bug [ ` python3-postgresql ` ] ( https : //admin.fedoraproject.org/pkgdb/acls/name/python3-postgresql ? _csrf_token=f882c062f7956b949e845f7efa27455d6ed8869f ) package , looks like works little change . Edit file ( probably ` /usr/lib64 ` 64 bit installations ) : /usr/lib/python3.2/site-packages/postgresql/protocol/client3.py Change ( line 514 ) : element.Startup ( **startup ) , password : element.Startup ( startup ) , password After I made simple connection ( I changed ` pg_hba.conf ` host methods md5 ) looks ok : [ grzegorz @ localhost Desktop ] $ python3 Python 3.2 ( r32:88445 , Feb 21 2011 , 21:12:33 ) [ GCC 4.6.0 20110212 ( Red Hat 4.6.0-0.7 ) ] linux2 Type `` help '' , `` copyright '' , `` credits '' `` license '' information . > > > import postgresql > > > db = postgresql.open ( `` pq : //grzegorz:12345 @ localhost/grzegorz '' ) > > > ps = db.prepare ( `` SELECT version ( ) '' ) > > > ps ( ) [ ( 'PostgreSQL 9.0.4 i386-redhat-linux-gnu , compiled GCC gcc ( GCC ) 4.6.0 20110530 ( Red Hat 4.6.0-9 ) , 32-bit ' , ) ] > > > ps = db.prepare ( `` TABLE '' ) > > > ps ( ) [ ( 1 , 'aaa ' ) , ( 2 , 'bbb ' ) , ( 3 , 'ccc ' ) ] > > >"
Stackoverflow,I trying output files using NZSQL CLI able output tab delimited files . Can somebody worked NZ share thoughts command . Tried far : - nzsql -o sample.txt -F= -A -t -c `` SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5 ; '',"To specify tab delimiter use $ conjunction -F option . nzsql -o sample.txt -F $ '\t ' -A -t -c `` SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5 ; '' This documented nzsql -h output . nzsql -h This nzsql , IBM Netezza SQL interactive terminal . Usage : nzsql [ options ] [ security options ] [ dbname [ username ] [ password ] ] Security Options : -securityLevel Security Level wish request ( default : preferredUnSecured ) -caCertFile ROOT CA certificate file ( default : NULL ) Options : -a Echo input script -A Unaligned table output mode ( -P format=unaligned ) -c < query > Run single query ( slash command ) exit -d < dbname > Specify database name connect ( default : system ) -D < dbname > Specify database name connect ( default : system ) -schema < schemaname > Specify schema name connect ( default : $ NZ_SCHEMA ) -e Echo queries sent backend -E Display queries internal commands generate -f < filename > Execute queries file , exit -F < string > Set field separator ( default : `` | '' ) ( -P fieldsep= ) For binary/control/non-printable character use ' $ ' ( e.g. , nzsql -F $ '\t ' // TAB ) ... If lot data , I 'd recommend using external tables instead perform better . CREATE EXTERNAL TABLE '/tmp/sample.txt ' USING ( DELIMITER '\t ' ) AS SELECT * FROM DW_ETL.USER WHERE datasliceid % 20 = 2 LIMIT 5 ;"
Stackoverflow,"I 'm pretty new node world trying migrate php application node . To able return article data several different queries done depending results first query . Currently data object empty 's returned two queries run . How I `` chain '' queries using promised based approach . I found library < https : //github.com/lukeb-uk/node-promise-mysql > I think could help I idea implement code . exports.getArticleData = function ( req , done ) { pool.getConnection ( function ( error , connection ) { ( error ) throw error ; var data = { article : { } , listicles : [ ] } ; // Inital query connection.query ( ` SELECT article_id , title , is_listicle_article , FROM com_magazine_articles AS article WHERE article_id = $ { req .params.articleId } ` , function ( error , results ) { data.article = results ; } ) ; // This query excuted is_listicle_article = true ( data.article.is_listicle_article ) { connection.query ( ` SELECT * FROM com_magazine_article_listicles WHERE article_id = $ { req.params .articleId } ` , function ( error , results ) { data.listicle = results ; } ) ; } // More queries depending result first one // ... . // ... . // Callback data object done ( data ) ; connection.release ( ) ; } ) ; } ; What would best approach execute queries based queries results ? Any help really appreciated .","Share working example : I use [ Promisified MySQL middleware Node.js ] ( https : //gist.github.com/matthagemann/30cfee724d047007a031eb12b3a95a23 ) read article [ Create MySQL Database Middleware Node.js 8 Async/Await ] ( https : //medium.com/gatemill/create-a-mysql-database-middleware- with-node-js-8-and-async-await-6984a09d49f4 ) database.js var mysql = require ( 'mysql ' ) ; // node -v must > 8.x var util = require ( 'util ' ) ; // ! ! ! ! ! node version < 8.x ! ! ! ! ! // npm install util.promisify //require ( 'util.promisify ' ) .shim ( ) ; // -v < 8.x problem async await upgrade -v v9.6.1 work . // connection pool https : //github.com/mysqljs/mysql [ 1 ] var pool = mysql.createPool ( { connectionLimit : process.env.mysql_connection_pool_Limit , // default:10 host : process.env.mysql_host , user : process.env.mysql_user , password : process.env.mysql_password , database : process.env.mysql_database } ) // Ping database check common exception errors . pool.getConnection ( ( err , connection ) = > { ( err ) { ( err.code === 'PROTOCOL_CONNECTION_LOST ' ) { console.error ( 'Database connection closed . ' ) } ( err.code === 'ER_CON_COUNT_ERROR ' ) { console.error ( 'Database many connections . ' ) } ( err.code === 'ECONNREFUSED ' ) { console.error ( 'Database connection refused . ' ) } } ( connection ) connection.release ( ) return } ) // Promisify Node.js async/await . pool.query = util.promisify ( pool.query ) module.exports = pool You must upgrade node -v > 8.x must use async function able use await . example : var pool = require ( './database ' ) // node -v must > 8.x , -- > async / await router.get ( '/ : template ' , async function ( req , res , next ) { ... try { var _sql_rest_url = 'SELECT * FROM arcgis_viewer.rest_url WHERE id='+ _url_id ; var rows = await pool.query ( _sql_rest_url ) _url = rows [ 0 ] .rest_url // first record , property name 'rest_url' ( _center_lat == null ) { _center_lat = rows [ 0 ] .center_lat } ( _center_long == null ) { _center_long= rows [ 0 ] .center_long } ( _center_zoom == null ) { _center_zoom= rows [ 0 ] .center_zoom } _place = rows [ 0 ] .place } catch ( err ) { throw new Error ( err ) }"
Stackoverflow,"I want export table data csv format app . I used Alasql library XLSX.js . It works fine modern browsers ( Chrome , Firefox .. ) Safari .","Sometimes want use Headers including blank spaces ( Separated By ) ... using headers reserved words ( Deleted ) , cases use [ ] like : alasql ( 'SELECT firstName AS FirstName , [ Deleted ] AS [ Erased ] , Separated AS [ Separated By ] INTO XLSX ( `` test.xlsx '' , ? ) FROM ? ' , [ options , $ scope.users ] ) ;"
Stackoverflow,"For Customer need import data old Centura SQLBase 7.5.1 database . The best way would connect directly .Net customers database , I ca n't find driver .Net Connector use . So far I found company behind product currently : < http : //www.unify.com/ > But I contact , freely available driver / connector SQLBase available ? .","It hard find , eventually I found download : < http : //support.guptatechnologies.com/supportwiki/index.php/SQLBase_Driver_Packs > You query favorite search engine `` SQLBase Driver Packs '' instead `` .NET Data Provider '' get result . The Setup contains ODBC/.NET/OLEDB JDBC drivers . That said , I could connect database anyway , installer ( I tried 9.0.1 10.0.0 setup ) broken . The able get 10.0.0 work 9.x version I manually copied files : - SQLBaseUtil.dll - MFC71.dll - msvcr71.dll - msvcp71.dll installation path system32 folder . That worked Windows XP Windows 7 , Windows 7 I also needed define path ini file connectionstring : var connectionString = `` data source=ISLAND ; '' + `` uid=sysadm ; pwd=sysadm ; '' + `` ini=C : \\Program Files\\Gupta\\SQLBase901\\sql.ini '' ;"
Stackoverflow,"From I make NoSQL databases might good option high intensity data read applications , less good fit need also lot data updates transactionality important ( ACID compliance ) . Right ? Too simplistic maybe . But anyway , supposing I 'm partly right least I 'm concerned NoSQL databases maintain `` read consistent '' view data either reading writing . Or ? And n't , n't really big problem ? I mean , data 're reading ( updating ) changing read 're potentially going get inconsistent/dirty result set . Coming Oracle rdbms background , handled , I find confusing lack read consistency anything big problem . Could well though I 'm missing key point . Can someone set straight ?","I developer Oracle NoSQL Database answer question relative particular NoSQL system . The Oracle NoSQL Database API allows programmer specify -- API call -- level read consistency . The four possible values , ranging strictest loosest , Absolute , Time , Version , None . Absolute says always read replication master current value returned . `` Time '' says system return value replica least within certain time delta master ( e.g . read value replica within 2 seconds master ) . Every read write call system returns `` version handle '' . This version handle may passed read call Consistency.Version specified tells system read replica least date version . This useful Read Modify Write ( aka CAS ) scenarios . The last value , Consistency.None says replica used ( i.e . consistency guaranteed ) . I hope helpful . Charles Lamb"
Stackoverflow,"How I tell Azure SQL Database [ QUERY_STORE ] ( https : //azure.microsoft.com/en-us/blog/query-store-a-flight- data-recorder-for-your-database/ ) turned ? You enable command : ALTER DATABASE < database_name > SET QUERY_STORE = ON ; I figure simple check database , I found trick . FYI , I tried command database enabled , command returned null : SELECT DATABASEPROPERTYEX ( ' < database_name > ' , 'QUERY_STORE ' )","This DMV ` ** [ sys.database_query_store_options ] ( https : //msdn.microsoft.com/en- in/library/dn818146.aspx ) ** ` allow determine ` QUERY_STORE ` enabled : SELECT desired_state_desc , actual_state_desc , readonly_reason , current_storage_size_mb , max_storage_size_mb , max_plans_per_query FROM sys.database_query_store_options ; Description Actual_state_Desc states : **OFF ( 0 ) ** > -Not Enabled **READ_ONLY ( 1 ) ** > Query Store may operate read-only mode even read-write specified user . For example , might happen database read-only mode Query Store size exceeded quota **READ_WRITE ( 2 ) ** > Query store capturing queries **ERROR ( 3 ) ** > Extremely rarely , Query Store end ERROR state internal errors . In case memory corruption , Query Store recovered requesting READ_WRITE mode explicitly , using ALTER DATABASE SET QUERY_STORE statement . In case corruption disk , data must cleared READ_WRITE mode requested explicitly ."
Stackoverflow,"I ` INSERT INTO ... ON DUPLICATE KEY UPDATE ... ` statement executes fine ( warnings ) ` mysql > ` prompt : mysql > INSERT INTO ... ON DUPLICATE KEY UPDATE ... ; Query OK , 2 rows affected , 2 warnings ( 0.00 sec ) Warning ( Code 1364 ) : Field ' x ' n't default value However , I try execute statement via JDBC warning shows ` SQLException ` rows updated : java.sql.SQLException : Field ' x ' n't default value com.mysql.jdbc.SQLError.createSQLException ( SQLError.java:1055 ) com.mysql.jdbc.SQLError.createSQLException ( SQLError.java:956 ) com.mysql.jdbc.MysqlIO.checkErrorPacket ( MysqlIO.java:3536 ) com.mysql.jdbc.MysqlIO.checkErrorPacket ( MysqlIO.java:3468 ) com.mysql.jdbc.MysqlIO.sendCommand ( MysqlIO.java:1957 ) com.mysql.jdbc.MysqlIO.sqlQueryDirect ( MysqlIO.java:2107 ) com.mysql.jdbc.ConnectionImpl.execSQL ( ConnectionImpl.java:2648 ) com.mysql.jdbc.PreparedStatement.executeInternal ( PreparedStatement.java:2086 ) com.mysql.jdbc.PreparedStatement.execute ( PreparedStatement.java:1365 ) **Is JDBC mysql connector setting command ignore suppress warnings ? ** I 'm using MySQL Community Server 5.1.31 MySQL connector 5.1.8 Java 1.5.0_24 .","One servers running strict mode default . If server runs strict mode ( set connection ) try insert NULL value column defined NOT NULL get # 1364 error . Without strict mode NULL value replaced empty string 0 . Example : CREATE TABLE ` test_tbl ` ( ` id ` int ( 11 ) NOT NULL , ` someint ` int ( 11 ) NOT NULL , ` sometext ` varchar ( 255 ) NOT NULL , ` somedate ` datetime NOT NULL , PRIMARY KEY ( ` id ` ) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 SET sql_mode = `` ; INSERT INTO test_tbl ( id ) VALUES ( 1 ) ; SELECT * FROM test_tbl ; + -- -- + -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | id | someint | sometext | somedate | + -- -- + -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ | 1 | 0 | | 0000-00-00 00:00:00 | + -- -- + -- -- -- -- -+ -- -- -- -- -- + -- -- -- -- -- -- -- -- -- -- -+ SET sql_mode = 'STRICT_ALL_TABLES ' ; INSERT INTO test_tbl ( id ) VALUES ( 2 ) ; # 1364 - Field 'someint ' n't default value"
Stackoverflow,"I using DMO API via .NET provide alternative interface job scheduling functionality SQL Server 2000 Agent . The working code looks something like : using SQLDMO ; internal class TestDmo { public void StartJob ( ) { SQLServerClass sqlServer = new SQLServerClass ( ) ; sqlServer.Connect ( `` MyServerName '' , `` sql_user_id '' , `` p @ ssword '' ) ; // trusted/SSPI overload ? foreach ( Job job sqlServer.JobServer.Jobs ) { ( ! job.Name.Equals ( `` MyJob '' ) ) continue ; job.Start ( null ) ; } } } Everything works above-listed form ( SQL Server authentication uid/pwd provided ) I would also like provide option authenticate trusted user ( aka SSPI , Trusted Connection ) Is possible DMO API ? If ? Note : The SQLServerClass.Connect method seem overloads , I already tried pass null values user id password avail Googles helpful yet . Any ideas ?","From [ documentation ] ( http : //msdn.microsoft.com/en- us/library/ms144581.aspx ) : > object.Connect ( [ ServerName ] , [ Login ] , [ Password ] ) > > [ ... ] > > Use _Login_ _Password_ arguments specify values used SQL Server Authentication . To use Windows Authentication connection , set **LoginSecure** property TRUE prior calling **Connect** method . When **LoginSecure** TRUE , values provided _Login_ _Password_ arguments ignored . Thus , set ` LoginSecure ` property ` true ` calling Connect . Then , matter values pass last two parameters ."
Stackoverflow,Currently I 'm trying automatically generate create script SQL jobs MS SQL2005 Server . * One method I found done manually < http : //msdn.microsoft.com/en-us/library/ms191450.aspx > * A second method I found could done automatically I n't direct access SQL server . < http : //relatedterms.com/thread/1916663/Can % 20I % 20script % 20out % 20SQL % 20Server % 20jobs % 20programmatically > Does anyone know good TSQL statement simple program ?,"There several commercial tools create database scripts , e.g . * ApexSQL 's [ SQL Script ] ( http : //www.apexsql.com/sql_tools_script.asp ) * [ EMS DB Extract ] ( http : //sqlmanager.net/en/products/mssql/extract ) Here 's article showing free tool - however , script ALL objects database : [ Eric Moreau's blog ] ( http : //www.emoreau.com/Entries/Articles/2009/02/Scripting-your- Microsoft-SQL-Server-database-objects.aspx ) . If want `` roll '' , look Server Management Objects ( SMO ) - allow inspect database create scripts . See info [ ] ( http : //www.apexsql.com/sql_tools_script.asp ) , [ ] ( http : //davidhayden.com/blog/dave/archive/2006/11/09/ScriptDatabaseUsingSQLServerManagementObjects.aspx ) [ ] ( http : //www.sqlservercentral.com/articles/SMO/scriptdatabaseobjectswithsmo/2342/ ) . Marc"
Stackoverflow,What different database options Windows Mobile available ? I used CEDB EDB linear dataset needs . I heard SQL server 2005 Mobile edition . But advantages others ( ),Also take look [ SQLite Windows CE ] ( http : //sqlite- wince.sourceforge.net/ ) . There also .NET bindings available use Compact Framework .
Stackoverflow,"I trying decrypt one database file using python 3.7 . So decrypt I use pysqlcipher3 version python 3.7 . To install I tried using commands : pip3 install pysqlcipher3 pip install pysqlcipher3 commands showed successful installation pysqlcipher package . But issue I trying import pysqlcipher3 python project using line : pysqlcipher3 import dbapi2 sqlite displays error : ModuleNotFoundError : No module named 'pysqlcipher3 I checked various github projects none provide clear working solution . The python packages website says install **libsqlcipher** OS time issue , documentation link regarding installation libsqlcipher windows 10 . So anyone please provide proper steps installation document video tutorial regarding Or issue import statement ?","Since I n't found way I decided use [ django-fernet- fields ] ( https : //github.com/orcasgit/django-fernet-fields ) . The way works ciphers individual fields database , one still open database check tables structure , however individual entries ciphered . Additionally , easy use integrate ."
Stackoverflow,"I 've installed ` rsqlserver ` like ( errors ) install_github ( 'rsqlserver ' , 'agstudy ' , args = ' -- no-multiarch ' ) And created connection database : > library ( rClr ) > library ( rsqlserver ) Warning message : multiple methods tables found ‘ dbCallProc ’ > drv < - dbDriver ( `` SqlServer '' ) > conn < - dbConnect ( drv , url = `` Server=MyServer ; Database=MyDB ; Trusted_Connection=True ; '' ) > Now I try get data using ` dbGetQuery ` , I get error : > df < - dbGetQuery ( conn , `` select top 100 * public2013.dim_Date '' ) Error clrCall ( sqlDataHelper , `` GetConnectionProperty '' , conn , prop ) : Type : System.MissingMethodException Message : Method found : 'System.Object System.Reflection.PropertyInfo.GetValue ( System.Object ) ' . Method : System.Object GetConnectionProperty ( System.Data.SqlClient.SqlConnection , System.String ) Stack trace : rsqlserver.net.SqlDataHelper.GetConnectionProperty ( SqlConnection _conn , String prop ) > When I try fetch results using ` dbSendQuery ` , I also get error . > res < - dbSendQuery ( conn , `` select top 100 * public2013.dim_Date '' ) > df < - fetch ( res , n = -1 ) Error clrCall ( sqlDataHelper , `` Fetch '' , stride ) : Type : System.InvalidCastException Message : Object stored array type . Method : Void InternalSetValue ( Void* , System.Object ) Stack trace : System.Array.InternalSetValue ( Void* target , Object value ) System.Array.SetValue ( Object value , Int32 index ) rsqlserver.net.SqlDataHelper.Fetch ( Int32 capacity ) c : \projects\R\rsqlserver\src\rsqlserver.net\src\SqlDataHelper.cs : line 116 Strangely , file ` c : \projects\R\rsqlserver\src\rsqlserver.net\src\SqlDataHelper.cs ` actually exist computer . Am I something wrong ?","I agstudy creator ` rsqlserver ` package . Sorry late I finally I get time fix bug . ( actually yet implemented feature ) . I demonstrate read/write data.frame missing values Sql server . First I create data.frame missing values . It important distinguish difference numeric character variables . library ( rsqlserver ) url = `` Server=localhost ; Database=TEST_RSQLSERVER ; Trusted_Connection=True ; '' conn < - dbConnect ( 'SqlServer ' , url=url ) # # create table missing value dat < - data.frame ( txt=c ( ' ' , NA , ' b ' , NA ) , value =c ( 1L , NA , NA,2 ) ) My input looks like : # txt value # 1 1 # 2 < NA > NA # 3 b NA # 4 < NA > 2 I insert dat data base handy function ` dbWriteTable ` : dbWriteTable ( conn , name='T_TABLE_WITH_MISSINGS ' , dat , row.names=FALSE , overwrite=TRUE ) Then I read using 2 methods : # # # dbSendQuery res = dbSendQuery ( conn , 'SELECT * FROM T_TABLE_WITH_MISSINGS ' ) fetch ( res , n=-1 ) dbDisconnect ( conn ) txt value 1 1 2 < NA > NaN 3 b NaN 4 < NA > 2 # # # dbReadTable : rsqlserver DBI compliant implement many convenient functions deal least possible SQL . conn < - dbConnect ( 'SqlServer ' , url=url ) dbReadTable ( conn , name='T_TABLE_WITH_MISSINGS ' ) dbDisconnect ( conn ) txt value 1 1 2 < NA > NaN 3 b NaN 4 < NA > 2"
Stackoverflow,"I 'm trying debug strange discrepency 2 seperate cosmos db collection face value configured . We recently modified code executed following query . OLD QUERY SELECT * FROM c WHERE c.ProductId = `` CODE '' AND c.PartitionKey = `` Manufacturer-GUID '' NEW QUERY SELECT * FROM c WHERE ( c.ProductId = `` CODE '' OR ARRAY_CONTAINS ( c.ProductIdentifiers , `` CODE '' ) ) AND c.PartitionKey = `` Manufacturer-GUID '' The introduction ` Array_Contains ` call production environment tanked performance query ~3 RU/s == > ~6000 RU/s . But production environment . And cause seems Production , 's hitting index . See output two environments . **DEV CONFIGURATION** Collection Scale : 2000 RU/s Index Policy : ( Notice Exclude path ETags ) { `` indexingMode '' : `` consistent '' , `` automatic '' : true , `` includedPaths '' : [ { `` path '' : `` /* '' , `` indexes '' : [ { `` kind '' : `` Range '' , `` dataType '' : `` Number '' , `` precision '' : -1 } , { `` kind '' : `` Range '' , `` dataType '' : `` String '' , `` precision '' : -1 } , { `` kind '' : `` Spatial '' , `` dataType '' : `` Point '' } ] } ] , `` excludedPaths '' : [ { `` path '' : `` /\ '' _etag\ '' / ? '' } ] } **PROD CONFIGURATION** Collection Scale : 10,000 RU/s Index Policy : ( Notice absense Exclude path ETags compared DEV ) { `` indexingMode '' : `` consistent '' , `` automatic '' : true , `` includedPaths '' : [ { `` path '' : `` /* '' , `` indexes '' : [ { `` kind '' : `` Range '' , `` dataType '' : `` Number '' , `` precision '' : -1 } , { `` kind '' : `` Range '' , `` dataType '' : `` String '' , `` precision '' : -1 } , { `` kind '' : `` Spatial '' , `` dataType '' : `` Point '' } ] } ] , `` excludedPaths '' : [ ] } When comparing Output results environments , DEV showing index hit , PROD showing index miss , despite noticeable difference index policies . **Results DEV** Request Charge : 3.490 RUs Showing Results : 1 - 1 Retrieved document count : 1 Retrieved document size : 3118 bytes Output document count : 1 Output document size : 3167 bytes Index hit document count : 1 **Results PROD** Request Charge : 6544.870 RUs Showing Results : 1 - 1 Retrieved document count : 124199 Retrieved document size : 226072871 bytes Output document count : 1 Output document size : 3167 bytes Index hit document count : 0 The thing I 've able find online reference documents change occurred within Cosmos Collection stating `` New Index Layout '' used newer collections , 's mention Index Layouts concept I find anywhere docs . < https : //docs.microsoft.com/en-us/azure/cosmos-db/index-types # index-kind > Anyone got idea I go terms debugging/addressing issue .","Currently ** [ ` Azure Cosmosdb ` ] ( https : //feedback.azure.com/forums/263030-documentdb/suggestions/6333414-implement- wildcards-when-searching ) ** supports ** ` CONTAINS ` ** , ** ` STARTSWITH ` ** , ** ` ENDSWITH ` ** built-in functions equivalent LIKE . The keyword LIKE Cosmosdb COntains . `` SELECT * FROM c WHERE CONTAINS ( c.pi , '09 ' ) ''"
Stackoverflow,"I migrating SQL Server 2008 To 2014 I get error > The data types datetime time incompatible add operator . I figured solution convert time DateTime , I changes many stored procedures views . Is solution problem ?","In function ( ) safeUp ( ) would add following code : $ this- > createTable ( 'tbl_post ' , array ( `` id '' = > `` pk '' , `` title '' = > `` VARCHAR ( 128 ) NOT NULL '' , `` content '' = > `` TEXT NOT NULL '' , `` tags '' = > `` TEXT '' , `` status '' = > `` INT NOT NULL '' , `` create_time '' = > `` INT '' , `` update_time '' = > `` INT '' , `` author_id '' = > `` INT NOT NULL '' , ) , `` ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci '' ) ; $ this- > addForeignKey ( 'FK_post_author ' , 'tbl_post ' , 'author_id ' , 'tbl_user ' , 'id ' , 'CASCADE ' , 'RESTRICT ' ) ; $ this- > insert ( 'tbl_lookup ' , array ( `` name '' = > `` Draft '' , `` type '' = > `` PostStatus '' , `` code '' = > 1 , `` position '' = > 1 , ) ; There update ( ) method available well ( insert shown ) : < http : //www.yiiframework.com/doc/api/1.1/CDbMigration # update-detail >"
Stackoverflow,How connect Sybase custdb.db using SwisSQL ? I work standart Sybase sample . I input appropriate command line : `` mlsrv12.exe -vcrs -zu+ -c `` dsn=SQL Anywhere 12 CustDB ; uid=ml_server ; pwd=sql '' -x http ( port=80 ) '' Database work fine - I test InteractiveSQL Android - client . When I try migrate database Oracle I trouble . Host name : 127.0.0.1 Port number : 80 User name : ml_server Password : sql Service ID ( SID ) : custdb Shema ( optional ) : empty What I wrong ? Any help appreciated !,
Stackoverflow,"I problem discussed [ ] ( https : //stackoverflow.com/questions/26878467/pygresql-error-from-pg- import-importerror-dll-load-failed-the-specified-mo ) , I n't credit comment answer I start new question . I PATH way libpq.dll ( C : \PostgreSql\lib ) n't solve problem . Using Python 2.7.9 32-bit , PostgreSQL 8.4 , Win 8 Traceback ( recent call last ) : File `` < pyshell # 1 > '' , line 1 , < module > import pg File `` C : \Python27\lib\site-packages\pg.py '' , line 21 , < module > _pg import * ImportError : DLL load failed : The specified module could found .",Try running : sudo apt-get install libpq-dev retry .
Stackoverflow,I used official tutorial create default instance < https : //docs.microsoft.com/en-us/sql/linux/sql-server-linux-setup-ubuntu > I want create named-instance ca n't find,"Yes , 's correct remote connection view change MyServerABC MyServerDEF connection strings . There things consider ( @ @ SERVERNAME change default example ) look : [ How : Rename Computer Hosts Stand-Alone Instance SQL Server ] ( http : //msdn.microsoft.com/en- us/library/ms143799.aspx ) Often , 'd use MyServerPermanentAlias network DNS entry actual server name irrelevant ."
Stackoverflow,"I following table : create table dbo.Link ( FromNodeId int null , ToNodeId int null ) Rows table represent links nodes . I want prevent inserts updates table creating cyclic relationship nodes . So table contains : ( 1,2 ) ( 2,3 ) allowed contain following : ( 1,1 ) ( 2,1 ) ( 3,1 ) I 'm happy treat ( 1,1 ) separately ( e.g . using CHECK CONSTRAINT ) makes solution straightforward . I thinking creating AFTER INSERT trigger recursive CTE ( though may easier way ) . Assuming way go , would trigger definition ? If elegant way , ?","The node names inside MATCH repeated . In words , node traversed arbitrary number times query . An edge name repeated inside MATCH . An edge point either direction , must explicit direction . OR NOT operators supported MATCH pattern . MATCH combined expressions using AND WHERE clause . However , combining expressions using OR NOT supported . Find 2 User friends User SELECT Person1.name AS Friend1 , Person2.name AS Friend2 FROM user user1 , friend friend1 , user user2 , friend friend2 , user user0 WHERE MATCH ( user1- ( friend1 ) - > user0 < - ( friend2 ) -user2 ) ; pattern also expressed SELECT user1.name AS Friend1 , user2.name AS Friend2 FROM user user2 , friend friend1 , user user2 , friend friend2 , user user0 WHERE MATCH ( user1- ( friend1 ) - > user0 AND user2- ( friend2 ) - > user0 ) ;"
Stackoverflow,Why teamsql slow execute sql commands ? I 've using tableplus teamsql clients teamsql much slow teable plus .,"TeamSQL n't feature , export result sets using export options ( CSV , JSON , etc.. ) Right click gridview 'll see `` Export Table '' option ; [ ! [ enter image description ] ( https : //i.stack.imgur.com/Q6FWg.png ) ] ( https : //i.stack.imgur.com/Q6FWg.png )"
Stackoverflow,"When query run snowsql shell , get see generated query id UI . Later search history , want search query id define someway tag query . Is possible create query id tag run query ?",You first need specify non-default date format input data . In case example : alter session set date_input_format = 'DD-MON-YY ' ; Then alter session set TWO_DIGIT_CENTURY_START=2000 ; select cast ( '05-FEB-00 ' date ) dual ; yields : 2000-02-05
Stackoverflow,"I 'm sure even within scope MySQL honest php necessary parse data . But ... kind stored procedure likely necessary . I table stores rows timestamp amount . My query dynamic searching based user-provided date range . I would like retrieve SUM ( ) amounts day table date range . **_including 0 entries given day_** Something effect ... SELECT CASE WHEN //there entries present given date THEN SUM ( amount ) ELSE 0 END AS amountTotal , //somehow select day FROM thisTableName T WHERE T.timeStamp BETWEEN ' $ start ' AND ' $ end' GROUP BY //however I select day This two parter ... **is way select section returned column ? Like kind regex within mysql ? Is way return 0 's dates rows ? **","First , n't sample code SELECT SUM ( p.Amount ) , foo.d FROM foo LEFT JOIN ItemTracker_dbo.Payment ON foo.d = p.Datetime WHERE p.ClientId = ClientId GROUP BY foo.d ; missing ` p ` alias table ` ItemTracker_dbo.Payment ` ? ? i.e. , ; read : SELECT SUM ( p.Amount ) , foo.d FROM foo LEFT JOIN ItemTracker_dbo.Payment p ON foo.d = p.Datetime WHERE p.ClientId = ClientId GROUP BY foo.d ; Anyway , reason problem clause conditions applied outer join processed ( rows outer side added back ) So need move condition join condition : SELECT SUM ( p.Amount ) , foo.d FROM foo LEFT JOIN ItemTracker_dbo.Payment p ON foo.d = p.Datetime And p.ClientId = ClientId GROUP BY foo.d ;"
Stackoverflow,"With recent update [ Postres 11 ] ( https : //www.postgresql.org/about/news/1855/ ) . It supports stored procedures . Transactions finally supported stored procedures Postgres . However , I trying perform commit within procedure luck . CREATE OR REPLACE PROCEDURE test_update ( IN pid character , IN pcategoryname character varying , IN pwebcode character varying , IN porder numeric , IN pupdatedby character varying , IN pupdateddate timestamp without time zone , IN plang character varying , INOUT cresults refcursor ) LANGUAGE plpgsql AS $ BODY $ DECLARE cnt bigint ; rtnCode char ( 1 ) ; BEGIN cresults : = 'cur ' ; BEGIN update test_table set FCATEGORYNAME = pCategoryName , FWEBCODE = pWebCode , FORDER = pOrder , FUPDATEDBY = pUpdatedBy , FUPDATEDDATE = pUpdatedDate , FLANGCODE = pLang lower ( FID ) = lower ( pId ) ; COMMIT ; end ; . . EXCEPTION WHEN ... . . . OPEN cresults FOR VALUES ( 'stringresult ' ) ; end ; $ BODY $ ; **UPDATED** Edited Update Commit within block exception . This previously used execute procedure . I able fetch result refcursor . Does work anymore commit added . : begin ; CALL testupdate ( 'ad3caecb-9235-4945-b37a-9b7ff89fdfe0 ' , 'aa ' , '138',0 , 'test ' , '2018/06/29 18:04:03 ' , 'zh-cn ' , '' ) ; fetch cur ; commit ; execute : CALL testupdate ( 'ad3caecb-9235-4945-b37a-9b7ff89fdfe0 ' , 'aa ' , '138',0 , 'test ' , '2018/06/29 18:04:03 ' , 'zh-cn ' , '' ) ; .. runs fine row got updated , way running doesnt allow fetch result refcursor . Is way get result refcursor excuting procedure COMMIT . Any help greatly appreciated . Thanks",I _think_ cast operator ` : : ` higher precedence minus sign . So ` -32768 : :smallint ` executed ` -1 * 32768 : :smallint ` indeed invalid . Using parentheses fixes : ` ( -32768 ) : :smallint ` using SQL standard ` cast ( ) ` operator : ` cast ( -32768 smallint ) `
Stackoverflow,"I used [ electron-vue ] ( https : //github.com/SimulatedGREG/electron-vue ) generate base project . I get application launch , however I attempt run ` yarn test ` , I get following error : Child html-webpack-plugin `` index.html '' : Asset Size Chunks Chunk Names index.html 568 kB 1 db649fa959425ff07a09.hot-update.json 44 bytes [ emitted ] [ 0 ] ./node_modules/html-webpack-plugin/lib/loader.js ! ./src/index.ejs 1.46 kB { 1 } [ built ] [ 1 ] ./node_modules/lodash/lodash.js 540 kB { 1 } [ built ] [ 2 ] ( webpack ) /buildin/module.js 517 bytes { 1 } [ built ] 19 01 2018 02:30:17.603 : INFO [ karma ] : Karma v1.7.1 server started http : //0.0.0.0:9876/ 19 01 2018 02:30:17.605 : INFO [ launcher ] : Launching browser visibleElectron unlimited concurrency 19 01 2018 02:30:17.618 : INFO [ launcher ] : Starting browser Electron 19 01 2018 02:30:19.579 : INFO [ Electron 1.7.9 ( Node 7.9.0 ) ] : Connected socket XnzwV7bi3QYEDoyCAAAA id 16128747 Electron 1.7.9 ( Node 7.9.0 ) ERROR Uncaught Error : Could locate bindings file . Tried : → /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Debug/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/out/Debug/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/Debug/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/out/Release/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/Release/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/default/better_sqlite3.node → /home/cassius/workspace/tagister/node_modules/better-sqlite3/compiled/7.9.0/linux/x64/better_sqlite3.node webpack : ///node_modules/bindings/bindings.js:96:0 < - index.js:30116 Electron 1.7.9 ( Node 7.9.0 ) : Executed 0 0 ERROR ( 1.957 secs / 0 secs ) The bindings actually exist : find $ HOME/workspace/tagister/node_modules/better-sqlite3/ -name `` *node '' /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/test_extension.node /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/better_sqlite3.node /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/obj.target/test_extension.node /home/cassius/workspace/tagister/node_modules/better-sqlite3/build/Release/obj.target/better_sqlite3.node To get ` better-sqlite3 ` work development mode , I add ` `` postinstall '' : `` electron-builder install-app-deps '' ` ` package.json ` . I n't understand tests failing dependent modules clearly . I thought might bug added [ issue electron's github ] ( https : //github.com/electron/electron/issues/11388 ) comments . This reproduced locally following : git clone https : //gitlab.com/djsumdog/tagster.git git checkout ca712c4c yarn yarn test","SQL parameters need formatting ; inserted query text , passed directly database . ( This practical way handle blobs , literally contain anything . )"
Stackoverflow,"I using ` SQL Server 2016 ` ` SSIS ` . I package I calling sql server job . My job one step ` PowerShell command ` transfering files one directory another directory two servers . When I run job SQL server , works well . When I run package ` VisualStudio ` , every things ok , deploying project sql server , I want run package SQL Server , I error : > Execute SQL Server Agent Job Task : Error : Failed lock variable `` RunId '' read access error 0xC0010001 `` The variable found . This occures attempt made retrive variable Variable collection container execution package , variable . The variable name may changed variable created. '' . [ ! [ enter image description ] ( https : //i.stack.imgur.com/XSomW.png ) ] ( https : //i.stack.imgur.com/XSomW.png ) I deleted ` Execute sql server agent job task ` package I redeployed project I do't error . It clear error ` Execute sql server agent job task ` . I 2000 files source directory takes several minutes transfer files destination directory . **Edit1** : I several Task component package last ones ` Execute sql server agent job task ` n't ` Execute T-SQL Statement Task ` .","You get error job step command contains string ` @ backupfilename ` , variable defined context command . If I mistaken ` sp_add_jobstep ` support way could pass parameters command , need create command string without parameters : set @ path ='BACKUP DATABASE AdventureWorks2012 TO DISK = '' '+ @ backupfilename + `` '' Here I concatenated string also added quotes ."
Stackoverflow,"I seen several mentions _ '' upsert mode '' _ dynamic tables based unique key Flink documentation official Flink blog . However , I see examples / documentation regarding enable mode dynamic table . Examples : * [ Blog post ] ( https : //flink.apache.org/news/2017/04/04/dynamic-tables.html ) : > When defining dynamic table stream via update mode , specify **unique key** attribute table . In case , update delete operations performed respect key attribute . The **update mode** visualized following figure . * [ Documentation ] ( https : //ci.apache.org/projects/flink/flink-docs-release-1.4/dev/table/streaming.html ) : > A dynamic table converted **upsert stream** requires ( possibly composite ) **unique key** . So questions : * How I specify unique key attribute dynamic table Flink ? * How I place dynamic table update/upsert/ '' replace '' mode , opposed append mode ?","Apache Flink 's Table API join inner equi-join requires least one equality predicate . Joins without predicates cross products . Flink 's Table API offer operator cross product , cross products **very** expensive compute . With Flink 's DataSet API , cross products computed using cross operator map function Broadcast set ."
Stackoverflow,"I SQL CLR function based .net regex function order split values regular expression . In one cases , I using function split value ` | ` . The issue I found one values double ` || ` . Since , I sure second value ( right value ) number , I know second ` | ` part first value ( left value ) . I : 慂||2215 split : 慂| 2215 I splitting using expression ` [ | ] ` . I think order make work , I need use ` Zero-width negative look ahead assertion. ` I split ` ( ? ! [ | ] ) [ | ] ` I get : 慂||2215 I try look behind - ` ( ? < ! [ | ] ) [ | ] ` I get : 慂 |2215 I need pipe part first value . Could anyone assist ? Looking regex solution able change application right . * * * Here function anyone need : /// < summary > /// Splits input string array substrings positions defined regular expression pattern . /// Index value returned . /// < /summary > /// < param name= '' sqlInput '' > The source material < /param > /// < param name= '' sqlPattern '' > How parse source material < /param > /// < returns > < /returns > [ SqlFunction ( FillRowMethodName = `` FillRowForSplitWithOrder '' ) ] public static IEnumerable SplitWithOrder ( SqlString sqlInput , SqlString sqlPattern ) { string [ ] substrings ; List < Tuple < SqlInt64 , SqlString > > values = new List < Tuple < SqlInt64 , SqlString > > ( ) ; ; ( sqlInput.IsNull || sqlPattern.IsNull ) { substrings = new string [ 0 ] ; } else { substrings = Regex.Split ( sqlInput.Value , sqlPattern.Value ) ; } ( int index = 0 ; index < substrings.Length ; index++ ) { values.Add ( new Tuple < SqlInt64 , SqlString > ( new SqlInt64 ( index ) , new SqlString ( substrings [ index ] ) ) ) ; } return values ; }",You use negative lookahead rather lookbehind [ | ] ( ? ! [ | ] ) See [ regex demo ] ( http : //regexstorm.net/tester ? p= % 5B % 7C % 5D % 28 % 3F ! % 5B % 7C % 5D % 29 & i= % E6 % 85 % 82 % 7C % 7C2215 ) **Details** * ` [ | ] ` \- matches ` | ` char * ` ( ? ! [ | ] ) ` \- negative lookahead requires ` | ` char immediately right current location . [ ! [ enter image description ] ( https : //i.stack.imgur.com/jwvTu.png ) ] ( https : //i.stack.imgur.com/jwvTu.png )
Stackoverflow,"I using MySQL PHP 2 application servers 1 database server . With increase number users ( around 1000 ) , I 'm getting following error : SQLSTATE [ 08004 ] [ 1040 ] Too many connections The parameter ` max_connections ` set ` 1000 ` ` my.cnf ` ` mysql.max_persistent ` set ` -1 ` ` php.ini ` . There 1500 apache processes running time since ` MaxClients ` apache parameter equal 750 2 application servers . * Should I raise ` max_connections ` 1500 indicated [ ] ( https : //stackoverflow.com/questions/2229974/sqlstate08004-1040-too-many-connections ) ? * Or I set ` mysql.max_persistent ` 750 ( use PDO persistent connections performance reasons since database server application servers ) ? * Or I try something else ? Thanks advance !","It sounds like MySQL server shared many accounts , error means server 's ` max_connections ` setting exceeded . It could several high-traffic customers taking connections ( shared MySQL server , I take ? ) I 'm fairly sure PHP automatically closes resources end request ( although 's always best explicitly close connections etc . anyway ) I would n't think 's script keeping connections open . You 'll need keep hosting provider , I 'm afraid , consider moving providers . Also try keep log errors occur long , 'll need evidence refuse acknowledge 's problem . You also refer provider page : < http : //dev.mysql.com/doc/refman/5.1/en/too-many-connections.html > ."
Stackoverflow,"Is possible use Micronaut JNoSQL ? JNoSQL depending CDI implementatio i.e Weld , Micronaut support many set annotations , I think exposing full CDI container , question possible use JNoSQL goodness Micronaut ? Thank . Luis Oscar","If Micronaut implements CDI 2.0 higher work . However , I think works Spring engine . Eclipse JNoSQL initiative work Spring Data soon ."
Stackoverflow,"I want use vagrant , I defined following puppet file : < http : //pastebin.com/GfJK1ziS > When vagrant tries install modules everything works expected . But tries configure ` mysql ` , always get error : Error : Validation Mysql_grant [ $ { username } @ % / $ { db_name } . * ] failed : name must match user table parameters What I ? As far I tell due line ` puppetlabs_mysql ` module < https : //github.com/puppetlabs/puppetlabs- mysql/commit/07b661dcea926981cf5cd1c703a1c982d6eb6ef1 > n't know change","Instead ` ensure ` , use ` package_name ` . This worked : class { ' : :mysql : :server ' : package_name = > 'mysql-server-5.6' } If incorrect version ` mysql-client ` installed , add well : class { ' : :mysql : :client ' : package_name = > 'mysql-client-5.6' }"
Stackoverflow,"I trying read Parquet file Azure Data Lake using following Pyspark code . df= sqlContext.read.format ( `` parquet '' ) .option ( `` header '' , `` true '' ) .option ( `` inferSchema '' , `` true '' ) .load ( `` adl : //xyz/abc.parquet '' ) df = df [ 'Id ' , 'IsDeleted ' ] Now I would like load dataframe df table sql dataware house using following code : df.write \ .format ( `` com.databricks.spark.sqldw '' ) \ .mode ( 'overwrite ' ) \ .option ( `` url '' , sqlDwUrlSmall ) \ .option ( `` forward_spark_azure_storage_credentials '' , `` true '' ) \ .option ( `` dbtable '' , `` test111 '' ) \ .option ( `` tempdir '' , tempDir ) \ .save ( ) This creates table dbo.test111 SQL Datawarehouse datatypes : * Id ( nvarchar ( 256 ) , null ) * IsDeleted ( bit , null ) But I need columns different datatypes say char ( 255 ) , varchar ( 128 ) SQL Datawarehouse . How I loading dataframe SQL Dataware house ?","This n't issue might think . First , limit 128 . ( < https : //docs.microsoft.com/en-us/azure/sql- data-warehouse/memory-and-concurrency-limits # gen2-1 > ) Second , well concurrency next concurrent single cluster warehouse . I 've often wondered marketing mistake made Microsoft concurrency seen limitation ASDW , rarely mentioned less concurrent competitors . Third , best way serve thousands concurrent query users ( ie , dashboards ) PowerBI hybrid queries , ( potentially ) Azure Analysis Services . This gives extremely high concurrency interactivity . Perhaps best evidence I give I work Azure SQL Data Warehouse customers daily basis . I often get questions like customer first exposed ASDW , I never get questions concurrency time 're production . In words , issue `` concurrency '' n't important customers ."
Stackoverflow,"I building phone catalog organization ( AJAX application accesses search.asmx web service ) . I 'd like show list box user could select department ( stored managed property ` Department ` ) . To fill list box values , I need somehow select distinct values property . Is possible search.asmx web service ? What I 've found : * article , [ states possible ] ( http : //blog.robgarrett.com/2009/09/23/pre-search-facets-in-moss-2007/ ) , use web service interface * Microsoft 's [ white paper ] ( http : //download.microsoft.com/download/8/5/8/858F2155-D48D-4C68-9205-29460FD7698F/ % 5BMS-SEARCH % 5D.pdf ) states `` If protocol client specifies least one property , MUST also specify Path property . If , protocol server MUST return status code `` ERROR_BAD_QUERY '' . '' The two findings somewhat inconsistent . ( , yes search really returns ERROR_BAD_QUERY ) .","It could bracket problem , prefix IdOwner . Be careful prefix { 0 } { 1 } parameter : SELECT ROW_NUMBER ( ) OVER ( ORDER BY { 0 } { 1 } ) AS RowNum , Cars.Id , Cars.Make , Cars.Model , Color.Name FROM ( Cars INNER JOIN Color ON Cars.ColorId=Color.Id ) WHERE Cars.IdOwner= { 2 }"
Stackoverflow,"When I use ` create_view ( ) ` method ` sqlalchemy-utils ` module , everything works fine first time I run script . However , every time first call , I encounter error : ` sqlalchemy.exc.OperationalError : ( sqlite3.OperationalError ) table XXX already exists ` . I currently capturing exception ` pass ` avoid script stop seems messy . Is way avoid behavior ?","This last example [ docs ] ( http : //sqlalchemy- utils.readthedocs.io/en/latest/data_types.html # module- sqlalchemy_utils.types.encrypted.encrypted_type ) linked : > The key parameter accepts callable allow key change per-row instead fixed whole table . def get_key ( ) : return 'dynamic-key' class User ( Base ) : __tablename__ = 'user' id = sa.Column ( sa.Integer , primary_key=True ) username = sa.Column ( EncryptedType ( sa.Unicode , get_key ) )"
Stackoverflow,"I want exchange information ExactOnline Freshdesk based deliveries ( Exact Online Accounts - > Freshdesk Contacts , Exact Online deliveries - > Freshdesk tickets ) . The serial number delivered goods available either ` ExactOnlineREST..GoodsDeliveryLines ` table ` ExactOnlineXML..DeliveryLines ` . The following query lists columns also documented [ Exact Online REST API GoodsDeliveryLines ] ( https : //start.exactonline.nl/docs/HlpRestAPIResourcesDetails.aspx ? name=SalesOrderGoodsDeliveryLines ) : select * goodsdeliverylines All fields documentation REST APIs included GoodsDeliveryLines , solely serial numbers batch numbers . I 've tried - ExactOnlineXML tables column come existence actually specified - use : select stockserialnumbers goodsdeliverylines This raises however error : itgensql005 : Unknown identifier 'stockserialnumbers ' . How I retrieve serial numbers ?","Payment conditions referenced account outstanding items . In order get ( sales ) payment conditions account , several options . 1 . Join [ ` Accounts ` ] ( https : //documentation.invantive.com/2017R2/exact-online-data-model/webhelp/invantive-eol-provider-exactonlinerest-crm-accounts.html `` Accounts '' ) table get payment condition ( field ` salespaymentcondition_code_attr ` ` salespaymentcondition_description ` ) . The SQL would look like : select ... , act.salespaymentcondition_code_attr aroutstandingitems aom join exactonlinexml..accounts act aom.outstandingitems_ar_account_code_attr = act.code_attr 2 . Use Excel function get payment condition : ` I_EOL_ACT_SLS_PAY_CODE ` . The formula two parameters : ` division_code ` ` account_code_attr ` . The first optional . A valid call formula would thus : ` =I_EOL_ACT_SLS_PAY_CODE ( , '' 22 '' ) ` payment condition code account code 22 current Exact Online company . You incorporate SQL like : select ... , '=I_EOL_ACT_SLS_PAY_CODE ( `` ' + division_code + ' '' , `` ' + outstandingitems_ar_account_code_attr + ' '' ) ' pcn_code aroutstandingitems That would result model sync receive formula retrieving payment condition code . Remember check check box 'Formula ' ensure SQL outcome treated Excel formula . 3 . The , using column expressions : select ... , '=I_EOL_ACT_SLS_PAY_CODE ( `` $ C { E , . , . , ^ , . } '' ; '' $ C { E , . , . , ^+3 , . } '' ) ' pcn_code aroutstandingitems Remember check check boxes 'Formula ' 'Column expression ' ensure SQL outcome treated Excel formula ` $ C ` column expressions . The recommended option use column expressions , since work across widest range deployment scenarios accountancy hundreds companies formulas upgrade safe . SQL statements may need adapted new releases Exact Online data model Invantive ."
Stackoverflow,"SELECT * FROM dbo.staff WHERE st_position = 'Supervisor ' AND st_salary < AVG ( st_salary ) ; So I 'm trying set query outputs list supervisors salary lower average . putting I get following error . Msg 147 , Level 15 , State 1 , Line 1 An aggregate may appear WHERE clause unless subquery contained HAVING clause select list , column aggregated outer reference .","To get value average salary , use following query : SELECT AVG ( st_salary ) FROM dbo.staff Combining together condition , give following query : SELECT * FROM dbo.staff WHERE st_position = 'Supervisor' AND st_salary < ( SELECT AVG ( st_salary ) FROM dbo.staff )"
Stackoverflow,"I sequence looks like : CREATE SEQUENCE dbo.NextWidgetId AS [ bigint ] START WITH 100 INCREMENT BY 2 NO CACHE GO And table looks like : CREATE TABLE [ dbo ] . [ Widget_Sequenced ] ( [ WidgetId ] [ int ] NOT NULL DEFAULT ( NEXT VALUE FOR dbo.NextWidgetId ) , [ WidgetCost ] [ money ] NOT NULL , [ WidgetName ] [ varchar ] ( 50 ) NOT NULL , [ WidgetCode ] [ int ] NOT NULL , [ LastChangedBy ] [ int ] NOT NULL , [ RowVersionId ] [ timestamp ] NOT NULL , CONSTRAINT [ PK_Widget_Sequenced ] PRIMARY KEY CLUSTERED ( [ WidgetId ] ASC ) ON [ PRIMARY ] ) ON [ PRIMARY ] Is way add new record table structure using Entity Framework ? I tried setting ` StoreGeneratedPattern ` ` WidgetId ` ` computed ` I tried ` Identity ` . Both gave errors . I tried EF 5 . But I could move EF 6 fixes .","It 's totally possible . Changing example post 've linked something like : create sequence mainseq bigint start 1 increment 1 ; create table mytable ( id varchar ( 20 ) null constraint DF_mytblid default ' p ' + CAST ( next value mainseq varchar ( 10 ) ) , code varchar ( 20 ) null ) Test : INSERT INTO MyTable ( Code ) VALUES ( 'asdf ' ) , ( 'cvnb ' ) SELECT * FROM MyTable Results : id code p1 asdf p2 cvnb"
Stackoverflow,"I 've successfully installed MySQL , Boost mysql-connector-c++ macOS High Sierra 10.13.3 via Homebrew , 've ran problems using libs Xcode . So , boost connector libs located ` /usr/local/Cellar/ ` . So wrote simple code sample check everything `` works '' : # include < mysql_driver.h > # include < mysql_error.h > # include < mysql_connection.h > int main ( ) { return 0 ; } compiled c++ -I /usr/local/Cellar/mysql-connector- c++/1.1.9_1/include/mysql_connection.h /usr/local/Cellar/mysql-connector- c++/1.1.9_1/include/mysql_driver.h /usr/local/Cellar/mysql-connector- c++/1.1.9_1/include/mysql_driver.h main.cpp Everything seems working , except I get warnings : > clang : warning : treating ' c-header ' input ' c++-header ' C++ mode , behavior deprecated [ -Wdeprecated ] > > clang : warning : treating ' c-header ' input ' c++-header ' C++ mode , behavior deprecated [ -Wdeprecated ] But 's huge issue , I think . And I try use libs Xcode , : [ ! [ Xcode 9.2 build settings ] ( https : //i.stack.imgur.com/ozrpc.png ) ] ( https : //i.stack.imgur.com/ozrpc.png ) Yet I try compile code libs Xcode , I get : > 'boost/scoped_ptr.hpp ' file found include ; use `` quotes '' instead' many errors alike boost mysql-connector-c++ . Obviosly , changing < > `` '' source files NOT good idea , futhermore , 's extremely tiring . How fix error ?","This line wrong : sqlquery = & sqlbuff ; ` sqlbuff ` type ` char [ ] ` , taking address gives _pointer array_ , type ` char ( * ) [ ] ` . You want pointer first ` char ` ( type ` char * ` ) , evaluating identifier array results pointer contexts ( exceptions like used ` sizeof ` ) . So , write sqlquery = sqlbuff ; That said , 's need separate pointer variable . ` sqlbuff ` evaluates ` char * ` , pointer convertable ` const ` -qualified counterpart implicitly . Just pass ` sqlbuff ` directly ` const char * ` expected work . * * * Another thing mentioned although directly related question : You **never construct SQL queries user input using string operations** . Attackers easily inject SQL code way . Read documentation SQL client library look _prepared statements_ _parameter binding_ use everywhere user input needs parameter SQL query . * * * And one hint : If find writing code like sprintf ( `` % '' , `` foobar '' ) ; e.g. , 're giving constant string ` % ` conversion , 're wrong . Just make constant string part format-string ."
Stackoverflow,"Is product queries using JDBC ( normal SQL ) , sees whether tables query CACHED tables , use cache , otherwise fallback back-end database . I aware two products : Oracle In Memory Database ( IMDB ) Cache , VMware SQLFire . I 'm familiar none , I want know possible query IMDB cache non-cached tables , falls-back underlying database ? Is products support feature ?",One straightforward way accomplish follows : SELECT DISTINCT ( MovieID ) FROM Movies WHERE Format='DVD' AND NOT EXISTS ( SELECT * FROM Movies mm WHERE mm.MovieID=m.MovieID AND mm.Format='Blu-ray' ) This pretty much translation English description problem SQL syntax .
Stackoverflow,"We huge Oracle database I frequently fetch data using SQL Navigator ( v5.5 ) . From time time , I need stop code execution clicking ` Stop ` button I realize missing parts code . The problem , clicking ` Stop ` button , takes long time complete stopping process ( sometimes takes hours ! ) . The program says ` Stopping ... ` bottom bar I lose lot time till finishes . What rationale behind ? How I speed stopping process ? Just case , I 'm admin ; I 'm limited user uses views access database .","Two things need happen stop query : 1 . The actual Oracle process notified want cancel query 2 . If query made modification DB ( DDL , DML ) , work needs rolled back . For first point , Oracle process executing query check time time cancel query . Even long task ( big HASH JOIN example ) , I think checks every 3 seconds ( I 'm looking source info , I 'll update answer I find ) . Now software able communicate correctly Oracle ? I 'm familiar SLQ Navigator I suppose cancel mechanism work like tool I 'm guessing 're waiting second point : Once process notified stop working , undo everything already accomplished query ( statements atomic Oracle , ca n't stopped middle without rolling back ) . Most time DML statement rollback take longer work already accomplished ( I see like : Oracle optimized work forward , backward ) . If case ( big DML ) , patient rollback , much speed process . If query simple SELECT tool wo n't let cancel , could kill session ( needs admin rights another session ) -- instantaneous ."
Stackoverflow,"I need process data SQLite database using Java code . The problem database retrieved external service thus available ` InputStream ` . I would really like avoid saving db file system starting process data , I n't see way . I currently using [ SqlJet ] ( http : //sqljet.com/ ) , provides option create in-memory database . However seems way turn ` InputStream ` / byte array database . There 's nothing forcing use [ SqlJet ] ( http : //sqljet.com/ ) , easily replaced tool provides needed functionality .",Use [ H2 ] ( http : //www.h2database.com/ ) [ TCP server mode ] ( http : //www.h2database.com/html/tutorial.html ) . In one VMs start like : Server server = Server.createTcpServer ( args ) .start ( ) ; Other VM connect using JDBC : JDBC driver class : org.h2.Driver Database URL : jdbc : h2 : tcp : //localhost/~/test
Stackoverflow,"Today I use SQLite database first time I really wondered display ` DATETIME ` column like ` 1411111200 ` . Of course , internally stored integer value able math . But wants see grid output , clearly human eyes ? I even tried two programs , SQLiteStudio SQLite Manager , even option change ( least I could n't find ) . Of course knowledge SQL n't take long find values mean - query displays like I expected : select datetime ( timestamp , 'unixepoch ' , 'localtime ' ) , * MyTable But 's uncomfortable working GUI Tool . So ? Just ? Unix nerds ? Or I get wrong impression I accidentally tried 2 Tools bad ? ( I also appreciate comments tools use I find hidden settings . )","There need join need add HAVING clause SELECT name , SUM ( qty ) AS Total FROM t1 GROUP BY name HAVING SUM ( qty ) BETWEEN 500 AND 1500 ORDER BY name"
Stackoverflow,"I using [ SqlKata ] ( https : //sqlkata.com/docs ) creating dynamic SQL queries . I list conditions , stored database , generated according business rules . code sample : var list = new List < Query > ( ) ; foreach ( var rule rules ) { var q = new Query ( ) .Where ( x= > x.Where ( `` Price '' , `` < `` , rule.Price ) .OrWhere ( `` GoodsType '' , `` = '' , rule.Type ) ) ; list.Add ( q ) ; } Now I want join list item together none **Where ( ) ** extensions overload accepts ` Query ` type parameters . Is way join clauses together ? A VERY LITTLE PART OF expected query I need generate . select * ship_schedule Path = @ path scheduleDate= @ Date AND ( FD.IssueType = ' O ' OR fd.Path ! ='ILMTOP ' OR ( fd.Path='ILMTOP ' AND F.carrier ! ='MAL ' ) ) AND ( FD.IssueType = ' O ' OR fd.Path ! ='TOPILM ' OR ( fd.Path='ILMTOP ' AND F.carrier ! ='MAL ' ) ) I need create second line query end .","The ` Where ` method additive calling multiple times add multiple conditions query , n't need build list conditions . var query = new Query ( `` ship_schedule '' ) .Where ( `` Path '' , path ) ; foreach ( var rule rules ) { // loop rules append query ( col == null ) { query.WhereNull ( col ) ; } else { query.Where ( q = > q.Where ( `` Price '' , `` < `` , rule.Price ) .OrWhere ( `` GoodsType '' , `` = '' , rule.Type ) ) } } # # Other ways using ` When ` method query.When ( condition , q = > q.Where ( ... ) ) ; using ` WhereIf ` method query.WhereIf ( condition , `` Id '' , `` = '' , 10 ) ;"
Stackoverflow,"I used [ following code ] ( http : //mysqlbackupnet.codeplex.com/wikipage ? title=Documentation % 20for % 20MySqlBackup.NET % 20V1.5 ) backup MYSQL database . private void button2_Click ( object sender , EventArgs e ) { string file = `` D : \\backup.sql '' ; //string conn = `` server=localhost ; user=root ; pwd=qwerty ; database=test ; '' ; String str = @ '' server=192.168.1.219 ; database=abc ; userid=sha ; password='123 ' ; '' ; MySqlBackup mb = new MySqlBackup ( str ) ; mb.ExportInfo.FileName = file ; mb.Export ( ) ; } stack trace following - A first chance exception type 'System.NullReferenceException ' occurred MySqlBackup.dll System.Transactions Critical : 0 : < TraceRecord xmlns= '' http : //schemas.microsoft.com/2004/10/E2ETraceEvent/TraceRecord '' Severity= '' Critical '' > < TraceIdentifier > http : //msdn.microsoft.com/TraceCodes/System/ActivityTracing/2004/07/Reliability/Exception/Unhandled < /TraceIdentifier > < Description > Unhandled exception < /Description > < AppDomain > TestAppMysqlDBConnect.vshost.exe < /AppDomain > < Exception > < ExceptionType > System.NullReferenceException , mscorlib , Version=4.0.0.0 , Culture=neutral , PublicKeyToken=b77a5c561934e089 < /ExceptionType > < Message > Object reference set instance object. < /Message > < StackTrace > MySql.Data.MySqlClient.MySqlBackup.ExportExecute ( ) MySql.Data.MySqlClient.MySqlBackup.Export ( ) TestAppMysqlDBConnect.Form1.button2_Click ( Object sender , EventArgs e ) C : \Users\Shashika\Documents\Visual Studio 2010\Projects\TestAppMysqlDBConnect\TestAppMysqlDBConnect\Form1.cs : line 52 System.Windows.Forms.Control.OnClick ( EventArgs e ) .. But exception says null references exception . pass data database C # program . successfully inserted exception . exception occur try backup database C # program . used 2 Dll files link . - MySql.Data.dll MySqlBackup.dll I solve exception . Please help . ! [ Exception ] ( https : //i.stack.imgur.com/onTon.jpg )","According [ documentation section ] ( https : //www.codeproject.com/Articles/256466/MySqlBackup- NET # basicusage ) , specify ` MySqlBackup.ExportInfo ` using ` List < string > ` property called ` TablesToBeExportedList ` . So , something like work : string constring = `` server=localhost ; user=root ; pwd=1234 ; database=test1 ; '' ; string file = `` Y : \\backup.sql '' ; using ( MySqlConnection conn = new MySqlConnection ( constring ) ) { using ( MySqlCommand cmd = new MySqlCommand ( ) ) { using ( MySqlBackup mb = new MySqlBackup ( cmd ) ) { cmd.Connection = conn ; conn.Open ( ) ; mb.ExportInfo.TablesToBeExportedList = new List < string > { `` Table1 '' , `` Table2 '' } ; mb.ExportToFile ( file ) ; } } }"
Stackoverflow,"I created external table using ` polybase ` CREATE EXTERNAL TABLE [ ext ] . [ gendertable ] ( gender_id TINYINT NOT NULL , gender VARCHAR ( 16 ) NOT NULL ) WITH ( LOCATION = '/MovieDB/gender.csv ' , DATA_SOURCE = AzureBlobHDP , FILE_FORMAT = csvformat0 ) ; GO The data source ` HADOOP ` . Is way import table without defining data type _again_ every column ? I search code [ like ] ( https : //docs.microsoft.com/en-us/azure/sql-data-warehouse/load-data- wideworldimportersdw ) : CREATE TABLE [ mov ] . [ gendertable ] WITH ( DISTRIBUTION = REPLICATE , CLUSTERED COLUMNSTORE INDEX ) AS SELECT * FROM [ ext ] . [ gendertable ] Of course code fails , since I use Azure SQL DW ( I get Syntax error ) . I use SQL Server 2019 VM . My question ` SQL ` expression , I declare data type column ?","SQL Server 2019 still CTP new feature . If found bug report Microsoft fixed release ( [ done ] ( https : //feedback.azure.com/forums/908035-sql- server/suggestions/37034101-inline-scalar-udf-bug-multiple-columns-are- specif ) ) . On previous versions try manually inline see error ( cut example ) . This limitation [ discussed ] ( https : //www.itprotoday.com/aggregates-outer-reference ) . WITH T ( m_id , fra , til ) AS ( SELECT 1 , GETDATE ( ) , GETDATE ( ) ) SELECT * FROM T CROSS APPLY ( SELECT SUM ( DATEDIFF ( D , e.FRADATO , til ) ) FROM dbo.mlr_eos_avl e ) CA ( result ) Until inline case fixed Microsoft use WITH INLINE = OFF scalar UDF definition disable inlining UDF"
Stackoverflow,"I seem get query declare statement working SAP HANA . Below I 've put original working T-SQL version HANA version output SQL converter . I 've tried several versions combinations , every time I get errors also find . Anybody willing give `` '' I copy ? I also spelled SAP Documentations , nothing could help . Your help would appreciated . The T-SQL Code : DECLARE @ NumAtCardDuplicate VARCHAR ( 50 ) SET @ NumAtCardDuplicate = ( SELECT TOP 1 DocNum FROM TEST_RSCA.OPCH WHERE CardCode = 'S100424' AND NumAtCard = '118 120 266 805 ' ) IF @ NumAtCardDuplicate IS NOT NULL SELECT 'Invoice number already used entry ' + @ NumAtCardDuplicate + ' ! ' ELSE SELECT '118 120 266 805' The translated HANA query : NumAtCardDuplicate varchar ( 50 ) ; SELECT ( SELECT TOP 1 `` DocNum '' FROM TEST_RSCA.OPCH WHERE `` CardCode '' = 'S100424 ' AND `` NumAtCard '' = '118 120 266 805 ' ) INTO NumAtCardDuplicate FROM DUMMY ; temp_var_0 integer ; SELECT : NumAtCardDuplicate INTO temp_var_0 FROM DUMMY ; IF : temp_var_0 IS NOT NULL THEN SELECT 'Invoice number already used entry ' || : NumAtCardDuplicate || ' ! ' FROM DUMMY ; ELSE SELECT '118 120 266 805 ' FROM DUMMY ; END IF ; The Errors I get : Could execute 'NumAtCardDuplicate varchar ( 50 ) ' 1 ms 989 µs . SAP DBTech JDBC : [ 257 ] : sql syntax error : incorrect syntax near `` NumAtCardDuplicate '' : line 1 col 1 ( pos 1 ) Could execute 'SELECT ( SELECT TOP 1 `` DocNum '' FROM TEST_RSCA.OPCH WHERE `` CardCode '' ='S100424 ' AND `` NumAtCard '' = ... ' 3 ms 578 µs . SAP DBTech JDBC : [ 337 ] ( 119 ) : INTO clause allowed SELECT statement : line 4 col 67 ( pos 119 ) Could execute 'temp_var_0 integer ' 1 ms 701 µs . SAP DBTech JDBC : [ 257 ] : sql syntax error : incorrect syntax near `` temp_var_0 '' : line 1 col 1 ( pos 1 ) Could execute 'SELECT : NumAtCardDuplicate INTO temp_var_0 FROM DUMMY ' 1 ms 976 µs . SAP DBTech JDBC : [ 467 ] : use parameter variable : NUMATCARDDUPLICATE : line 4294967295 col 4294967295 ( pos 4294967295 ) Could execute 'IF : temp_var_0 IS NOT NULL THEN SELECT 'Invoice number already used entry ' || ... ' 1 ms 560 µs . SAP DBTech JDBC : [ 257 ] : sql syntax error : incorrect syntax near `` IF '' : line 1 col 1 ( pos 1 ) Could execute 'ELSE SELECT '118 120 266 805 ' FROM DUMMY ' 1 ms 338 µs . SAP DBTech JDBC : [ 257 ] : sql syntax error : incorrect syntax near `` ELSE '' : line 1 col 1 ( pos 1 ) SAP DBTech JDBC : [ 257 ] : sql syntax error : incorrect syntax near `` END '' : line 1 col 1 ( pos 1 ) Duration 7 statements : 13 ms","You could use ` MAX ( ) ` columns like row single value : CREATE TABLE # data ( col1 INT , col2 INT , col3 INT ) ; INSERT INTO # data ( col1 , col2 , col3 ) VALUES ( 1 , NULL , NULL ) , ( NULL , 2 , NULL ) , ( NULL , NULL , 3 ) ; SELECT MAX ( d.col1 ) AS col1 , MAX ( d.col2 ) AS col2 , MAX ( d.col3 ) AS col3 FROM # data AS ; DROP TABLE # data ;"
Stackoverflow,"In current project need find cheapest paths almost fully connected graph contain lots edges per vertex pair . We developed [ plugin ] ( http : //orientdb.com/docs/2.1/Extend-Server.html ) containing functions 1. special traversal graph lower reoccurences similar paths ` TRAVERSE ` execution . We refer ` search ( ) ` 2. special effective extraction desired information results traverses . We refer ` extract ( ) ` 3. extracting best ` N ` records according target parameter without costly ` ORDER BY ` . We refer ` best ( ) ` But resulted query still unsatisfactory performance full data . So decided modify ` search ( ) ` function could watch best edges first prune paths leading definitely undesired result using current state ` best ( ) ` function . Overall solution effectively flexible implementation [ Branch Bound method ] ( https : //en.wikipedia.org/wiki/Branch_and_bound ) Resulting query ( omitting ` extract ( ) ` step ) look like SELECT best ( path , < limit > ) FROM ( TRAVERSE search ( < params > ) FROM # < starting_point > WHILE < conditions intermediate vertixes > ) WHERE < conditions result elements > This form desired could adapt conditions ` WHILE ` ` WHERE ` current task . The ` path ` field generated ` search ( ) ` containing information ` best ( ) ` proceed . The trouble ` best ( ) ` function executed strictly ` search ( ) ` function , ` search ( ) ` prune non-optimal branches according results already evaluated ` best ( ) ` . So Question : Is way pipeline results ` TRAVERSE ` step ` SELECT ` step way older paths ` TRAVERSE ` ` search ( ) ` earlier paths handled ` SELECT ` ` best ( ) ` ?","You could use query SELECT FROM ( MATCH { CLASS : v1 , AS : v1 , WHERE : ( @ rid= # 29:0 ) } .outE ( 'e1 ' ) { AS : e1 } .inV ( 'e1 ' ) { AS : v2 } RETURN v1 , e1.p , v2 ) I hope clear enough ."
Stackoverflow,"This model Riders : < ? php namespace backend\models ; use Yii ; class Riders extends \yii\db\ActiveRecord { public static function tableName ( ) { return 'riders ' ; } public function rules ( ) { return [ [ [ 'cagories_category_id ' , 'rider_firstname ' , 'rider_no_tlpn ' , 'rider_ucinumber ' , 'countries_id ' , 'rider_province ' , 'rider_city ' , 'rider_dateofbirth ' , 'rider_gender ' ] , 'required ' ] , [ [ 'user_id ' , 'countries_id ' ] , 'integer ' ] , [ [ 'rider_dateofbirth ' , 'cagories_category_id ' ] , 'safe ' ] , [ [ 'rider_gender ' , 'rider_status ' ] , 'string ' ] , [ [ 'rider_firstname ' , 'rider_lastname ' , 'rider_nickname ' , 'rider_province ' , 'rider_city ' ] , 'string ' , 'max ' = > 45 ] , [ [ 'rider_email ' , 'rider_sponsor ' , 'rider_birthcertificate_url ' , 'rider_parental_consent_url ' ] , 'string ' , 'max ' = > 100 ] , [ [ 'rider_no_tlpn ' ] , 'string ' , 'max ' = > 15 ] , [ [ 'rider_ucinumber ' ] , 'string ' , 'max ' = > 11 ] ] ; } /** * @ inheritdoc */ public function attributeLabels ( ) { return [ 'rider_id ' = > 'rider_id ' , 'cagories_category_id ' = > 'Category Name ' , 'user_id ' = > 'User Team ' , 'rider_firstname ' = > 'Rider Firstname ' , 'rider_lastname ' = > 'Rider Lastname ' , 'rider_nickname ' = > 'Rider Nickname ' , 'rider_email ' = > 'Rider Email ' , 'rider_no_tlpn ' = > 'Rider No Tlpn ' , 'rider_ucinumber ' = > 'Rider Ucinumber ' , 'countries_id ' = > 'Country Name ' , 'rider_province ' = > 'Rider Province ' , 'rider_city ' = > 'Rider City ' , 'rider_sponsor ' = > 'Rider Sponsor ' , 'rider_dateofbirth ' = > 'Rider Dateofbirth ' , 'rider_gender ' = > 'Rider Gender ' , 'rider_birthcertificate_url ' = > 'Rider Birthcertificate Url ' , 'rider_parental_consent_url ' = > 'Rider Parental Consent Url ' , 'rider_status ' = > 'Rider Status ' , ] ; } /** * @ return \yii\db\ActiveQuery */ public function getRegistrations ( ) { return $ this- > hasMany ( Registrations : :className ( ) , [ 'riders_rider_id ' = > 'rider_id ' ] ) ; } /** * @ return \yii\db\ActiveQuery */ public function getCagoriesCategory ( ) { return $ this- > hasOne ( Categories : :className ( ) , [ 'category_id ' = > 'cagories_category_id ' ] ) ; } /** * @ return \yii\db\ActiveQuery */ public function getUser ( ) { return $ this- > hasOne ( User : :className ( ) , [ 'id ' = > 'user_id ' ] ) - > ( user : :tableName ( ) . 'ud ' ) ; } /** * @ return \yii\db\ActiveQuery */ public function getUserDesc ( ) { return $ this- > hasOne ( UserDesc : :className ( ) , [ 'desc_id ' = > 'user_id ' ] ) - > ( [ 'ud ' = > userDesc : :tableName ( ) ] ) ; } /** * @ return \yii\db\ActiveQuery */ public function getCountries ( ) { return $ this- > hasOne ( Countries : :className ( ) , [ 'id ' = > 'countries_id ' ] ) ; } } This Controller actionIndex : $ searchModel = new RidersSearch ( ) ; $ dataProvider = $ searchModel- > search ( Yii : : $ app- > request- > queryParams ) ; $ totalCount = Yii : : $ app- > db- > createCommand ( 'SELECT COUNT ( * ) FROM riders WHERE user_id = : user_id ' , [ ' : user_id ' = > Yii : : $ app- > user- > identity- > id ] ) - > queryScalar ( ) ; $ dataProvider = new SqlDataProvider ( [ 'sql ' = > 'SELECT * FROM riders WHERE user_id = : user_id ' , 'params ' = > [ ' : user_id ' = > Yii : : $ app- > user- > identity- > id ] , 'totalCount ' = > $ totalCount , 'key ' = > 'rider_id ' , 'pagination ' = > [ 'pageSize ' = > 10 , ] , 'sort ' = > [ 'attributes ' = > [ 'cagories_category_id ' , 'rider_id ' , 'rider_firstname ' , 'rider_email : email ' , 'rider_no_tlpn ' , ] ] ] ) ; $ models = $ dataProvider- > getModels ( ) ; return $ this- > render ( 'index ' , [ 'searchModel ' = > $ searchModel , 'dataProvider ' = > $ dataProvider , ] ) ; This view index : < ? = GridView : :widget ( [ 'dataProvider ' = > $ dataProvider , // 'filterModel ' = > $ searchModel , 'columns ' = > [ [ 'class ' = > 'yii\grid\SerialColumn ' ] , [ 'label ' = > 'Category Name ' , 'attribute'= > 'cagories_category_id ' , 'value ' = > 'cagoriesCategory.category_name ' , < -- -Ca n't work ] , [ 'label ' = > 'BIB ' , 'attribute'= > 'rider_id ' , ] , 'rider_firstname ' , 'rider_email : email ' , 'rider_no_tlpn ' , [ 'class ' = > 'yii\grid\ActionColumn ' ] , ] , ] ) ; ? > Before I use sqldataprovider , call model function relation , use sqldataprovider ca n't work . How get relation table value ? ? ? use , merge ` rider_firstname ` ` rider_lastname ` ` return $ model- > rider_firstname . `` `` . rider_lastname ; ` use sqldataprovider ca n't work ? ?","Turns unlike mentioned question I followed , Yii accepts ` attributes ` param array values array one CSV value . So : 'sort ' = > array ( 'attributes ' = > array ( 'enabled ' , 'store_name ' , 'rating ' ) , 'defaultOrder ' = > array ( 'store_name'= > false ) ) , And : 'sort ' = > array ( 'attributes ' = > array ( 'enabled , store_name , rating ' ) , 'defaultOrder ' = > array ( 'store_name'= > false ) ) ,"
Stackoverflow,"I configuring sql2java first time . I extracted zip-archive imported files eclipse java project . I n't know correct , I run ant build file eclipse ant function ( `` Run ant build ... '' ) output console . I n't know problem located , sql2java , ant , eclipse ? Its fresh clean install eclipse galileo . How I get sql2java / ant work ? How I get information help locate problem ? Is way use eclipse 's ant installation run build file console ? Any help would appreciated . Thanks advance .","problem may related anthome definitions , generally eclipse plugin directory . 1. goto windows/preferences menu 2. left tree-menu choose ant/Runtime 3. right screen edit anthome variable , set eclipse installition ant plugin directory forexample : ..\eclipse-jee-galileo win32\eclipse\plugins\org.apache.ant_1.7.1.v20090120-1145 hope works"
Stackoverflow,"I 'm trying get sqlalchemy-continuum work alongside flask-sqlalchemy flask-migrate . My ` __init__.py ` file looks like : import os flask import Flask def create_app ( ) : `` '' '' Create configure instance Flask application . '' '' '' app = Flask ( __name__ , instance_relative_config=True ) app.config.from_mapping ( SQLALCHEMY_DATABASE_URI='postgres+psycopg2 : // { } : { } @ { } : { } / { } '.format ( os.environ [ 'POSTGRES_USER ' ] , os.environ [ 'POSTGRES_PASSWORD ' ] , os.environ [ 'POSTGRES_HOST ' ] , os.environ [ 'POSTGRES_PORT ' ] , os.environ [ 'POSTGRES_DB ' ] ) , SQLALCHEMY_TRACK_MODIFICATIONS=False ) try : os.makedirs ( app.instance_path ) except OSError : pass .models import db , migrate db.init_app ( app ) migrate.init_app ( app , db ) return app My models.py file looks like : import sqlalchemy flask_sqlalchemy import SQLAlchemy flask_migrate import Migrate sqlalchemy_continuum import make_versioned db = SQLAlchemy ( ) migrate = Migrate ( ) make_versioned ( user_cls=None ) class User ( db.Model ) : __versioned__ = { } __tablename__ = 'user' id = db.Column ( db.Integer , primary_key=True ) username = db.Column ( db.String ( 80 ) , unique=True , nullable=False ) email = db.Column ( db.String ( 120 ) , unique=True , nullable=False ) password = db.Column ( db.String ( 20 ) , unique=True , nullable=False ) def __repr__ ( self ) : return ' < User { } - { } > '.format ( self.username , self.email ) sqlalchemy.orm.configure_mappers ( ) I run following flask-migrate commands initialise migrate database : flask db init flask db migrate flask db upgrade The output flask db upgrade command seems show correct tables created : INFO [ alembic.runtime.migration ] Context impl PostgresqlImpl . INFO [ alembic.runtime.migration ] Will assume transactional DDL . INFO [ alembic.autogenerate.compare ] Detected added table 'transaction' INFO [ alembic.autogenerate.compare ] Detected added table 'user' INFO [ alembic.autogenerate.compare ] Detected added table 'user_version' INFO [ alembic.autogenerate.compare ] Detected added index 'ix_user_version_end_transaction_id ' ' [ 'end_transaction_id ' ] ' INFO [ alembic.autogenerate.compare ] Detected added index 'ix_user_version_operation_type ' ' [ 'operation_type ' ] ' INFO [ alembic.autogenerate.compare ] Detected added index 'ix_user_version_transaction_id ' ' [ 'transaction_id ' ] ' In python shell I following : > > > test_flask.__init__ import create_app > > > test_flask.models import db , User > > > app = create_app ( ) > > > app.app_context ( ) : ... user = User ( username='devuser ' , email='devuser @ gmail.com ' , password='devpassword ' ) ... db.session.add ( user ) ... db.session.commit ( ) This seems work fine I attempt access element versions attribute using : > > > user.versions [ 0 ] I get following error : Traceback ( recent call last ) : File `` < stdin > '' , line 1 , < module > File `` /usr/local/lib/python3.6/site-packages/sqlalchemy/orm/dynamic.py '' , line 254 , __getitem__ attributes.PASSIVE_NO_INITIALIZE ) .indexed ( index ) File `` /usr/local/lib/python3.6/site-packages/sqlalchemy/orm/dynamic.py '' , line 359 , indexed return list ( self.added_items ) [ index ] IndexError : list index range The command : > > > user.versions returns : < sqlalchemy.orm.dynamic.AppenderQuery object 0x7f6515d3a898 > This n't seem expected behaviour versions attribute , specified [ sqlalchemy-continuum docs ] ( https : //sqlalchemy- continuum.readthedocs.io/en/latest/intro.html # versions-and-transactions ) . Any ideas I 've done wrong ?","If get user global scope ( like flask.g flask.request ) , create plugin , implementing ` transaction_args ( ) ` method : sqlalchemy_continuum.plugins import Plugin class UserPlugin ( Plugin ) : def transaction_args ( self , uow , session ) : return { 'user_id ' : get_user_from_globals ( ) } See [ plugins/flask.py ] ( https : //github.com/kvesteri/sqlalchemy- continuum/blob/master/sqlalchemy_continuum/plugins/flask.py ) [ pyramid_plugin.py ] ( https : //github.com/M4d40/sqlalchemy- continuum/blob/111d943c13d3d3b3a35b008aada6bee8358a0945/sqlalchemy_continuum/plugins/pyramid_plugin.py ) . You return primary key value User model . You save additional data [ TransactionMeta plugin ] ( https : //sqlalchemy- continuum.readthedocs.io/en/latest/plugins.html # module- sqlalchemy_continuum.plugins.transaction_meta ) separate table . There documented API extending Transaction class . It created ` TransactionFactory ` , could change later versions . It probably good idea keep additional data separate ."
Stackoverflow,"Display_Info SQL stored procedure , three input parameters three output parameters . info_Data ( serialized information data may also contain unicode null values ) one output parameters type NVARCHAR ( 1000 ) previously . Since larger size info_Data changed type NVARCHAR ( MAX ) . When like NVARCHAR ( 1000 ) problem executing stored procedure client application , changing NVARCHAR ( MAX ) client application throwing error like '' At least one parameter contained type supported. '' . SQL stored procedure design shown . Create Display_Info @ channel NVARCHAR ( 100 ) , @ infoType INT , @ locationId NVARCHAR ( 50 ) , @ Id BIGINT OUTPUT , @ infoData NVARCHAR ( MAX ) OUTPUT , @ infoStatus TINYINT OUTPUT AS ... The way client application executing stored procedure , try { SACommand conncmd ; CheckConnection ( ) ; conncmd.setConnection ( & mConn ) ; std : :wstring cmdText = COMMAND_TEXT ( `` ReadMessage '' ) ; conncmd.setCommandText ( cmdText.c_str ( ) ) ; conncmd.Param ( `` channel '' ) .setAsString ( ) = SAString ( channel.c_str ( ) , ( int ) channel.length ( ) ) ; conncmd.Param ( `` infoType '' ) .setAsNumeric ( ) = SANumeric ( ( sa_int64_t ) type ) ; conncmd.Param ( `` locationId '' ) .setAsString ( ) = SAString ( locationId.c_str ( ) , ( int ) locationId.length ( ) ) ; conncmd.Execute ( ) ; std : :wstring Id = conncmd.Param ( COMMAND_TEXT ( `` Id '' ) ) .asString ( ) ; infodata = conncmd.Param ( COMMAND_TEXT ( `` info_Data '' ) ) .asString ( ) ; } catch ( SAException & e ) { std : :string errorMessage = ( mb_twine ) e.ErrText ( ) ; std : :cout < < `` \n '' < < errorMessage ; } **Sample Input/output : ** infoData serialized input : Total length **5191** Ä ( Á ( ¼ ( Protocol Buffers method serializing structured data . It useful developing programs communicate wire storing data . The method involves interface description language describes structure data program generates source code description generating parsing stream bytes represents structured data.Google developed Protocol Buffers use internally made protocol compilers C++ , Java Python available public free software , open source license . Various language implementations also available , including C # , JavaScript , Go , Perl , PHP , Ruby , Scala . [ 1 ] The design goals Protocol Buffers emphasized simplicity performance . In particular , designed smaller faster XML . [ 2 ] Third parties reported Protocol Buffers outperforms standardized Abstract Syntax Notation One respect message size decoding performance . [ 3 ] Protocol Buffers widely used Google storing interchanging kinds structured information . The method serves basis custom remote procedure call ( RPC ) system used nearly inter-machine communication Google . [ 4 ] Protocol Buffers similar Apache Thrift protocol ( used Facebook example ) , except public Protocol Buffers implementation include concrete RPC protocol stack use defined services.A software developer defines data structures ( called messages ) services proto definition file ( .proto ) compiles protoc . This compilation generates code invoked sender recipient data structures . For example , example.proto produce example.pb.cc example.pb.h , define C++ classes message service example.proto defines.Canonically , messages serialized binary wire format compact , forwards-compatible , backwards-compatible , self-describing ( , way tell names , meaning , full datatypes fields without external specification ) . There defined way include refer external specification ( schema ) within Protocol Buffers file . The officially supported implementation includes ASCII serialization format , [ 5 ] format â though self-describing â loses forwards-and-backwards-compatibility behavior , thus good choice applications debugging.Though primary purpose Protocol Buffers facilitate network communication , simplicity speed make Protocol Buffers alternative data-centric C++ classes structs , especially interoperability languages systems might needed future.A schema particular use protocol buffers associates data types field names , using integers identify field . ( The protocol buffer data contains numbers , field names , providing bandwidth / storage savings compared systems include field names data . ) //polyline.protomessage Point { required int32 x = 1 ; required int32 = 2 ; optional string label = 3 ; } message Line { required Point start = 1 ; required Point end = 2 ; optional string label = 3 ; } message Polyline { repeated Point point = 1 ; optional string label = 2 ; } The `` Point '' message defines two mandatory data items , x . The data item label optional . Each data item tag . The tag defined equal sign . For example , x tag 1 . The Line `` Polyline '' messages , use Point , demonstrate composition works Protocol Buffers . Polyline repeated field , behaves like vector . This schema subsequently compiled use one programming languages . Google provides compiler called protoc produce output C++ , Java Python . Other schema compilers available sources create language-dependent output 20 languages . [ 6 ] For example , C++ version protocol buffer schema produced , C++ source code file , polyline.cpp , use message objects follows : // polyline.cpp # include polyline.pb.h // generated calling protoc polyline.proto Line* createNewLine ( const std : :string & name ) { // create line ( 10 , 20 ) ( 30 , 40 ) Line* line = new Line ; line- > mutable_start ( ) - > set_x ( 10 ) ; line- > mutable_start ( ) - > set_y ( 20 ) ; line- > mutable_end ( ) - > set_x ( 30 ) ; line- > mutable_end ( ) - > set_y ( 40 ) ; line- > set_label ( name ) ; return line ; } Polyline* createNewPolyline ( ) { // create polyline points ( 10,10 ) ( 20,20 ) Polyline* polyline = new Polyline ; Point* point1 = polyline- > add_point ( ) ; point1- > set_x ( 10 ) ; point1- > set_y ( 10 ) ; Point* point2 = polyline- > add_point ( ) ; point2- > set_x ( 20 ) ; point2- > set_y ( 20 ) ; return polyline ; } When , **NVARCHAR ( 1000 ) ** , infoData value : Total length - **1003** Ä ( Á ( ¼ ( Protocol Buffers method serializing structured data . It useful developing programs communicate wire storing data . The method involves interface description language describes structure data program generates source code description generating parsing stream bytes represents structured data.Google developed Protocol Buffers use internally made protocol compilers C++ , Java Python available public free software , open source license . Various language implementations also available , including C # , JavaScript , Go , Perl , PHP , Ruby , Scala . [ 1 ] The design goals Protocol Buffers emphasized simplicity performance . In particular , designed smaller faster XML . [ 2 ] Third parties reported Protocol Buffers outperforms standardized Abstract Syntax Notation One respect message size dec **NVARCHAR ( 4000 ) ** , infoData : Total length - **4084** Ä ( Á ( ¼ ( Protocol Buffers method serializing structured data . It useful developing programs communicate wire storing data . The method involves interface description language describes structure data program generates source code description generating parsing stream bytes represents structured data.Google developed Protocol Buffers use internally made protocol compilers C++ , Java Python available public free software , open source license . Various language implementations also available , including C # , JavaScript , Go , Perl , PHP , Ruby , Scala . [ 1 ] The design goals Protocol Buffers emphasized simplicity performance . In particular , designed smaller faster XML . [ 2 ] Third parties reported Protocol Buffers outperforms standardized Abstract Syntax Notation One respect message size decoding performance . [ 3 ] Protocol Buffers widely used Google storing interchanging kinds structured information . The method serves basis custom remote procedure call ( RPC ) system used nearly inter-machine communication Google . [ 4 ] Protocol Buffers similar Apache Thrift protocol ( used Facebook example ) , except public Protocol Buffers implementation include concrete RPC protocol stack use defined services.A software developer defines data structures ( called messages ) services proto definition file ( .proto ) compiles protoc . This compilation generates code invoked sender recipient data structures . For example , example.proto produce example.pb.cc example.pb.h , define C++ classes message service example.proto defines.Canonically , messages serialized binary wire format compact , forwards-compatible , backwards-compatible , self-describing ( , way tell names , meaning , full datatypes fields without external specification ) . There defined way include refer external specification ( schema ) within Protocol Buffers file . The officially supported implementation includes ASCII serialization format , [ 5 ] format â though self-describing â loses forwards-and-backwards-compatibility behavior , thus good choice applications debugging.Though primary purpose Protocol Buffers facilitate network communication , simplicity speed make Protocol Buffers alternative data-centric C++ classes structs , especially interoperability languages systems might needed future.A schema particular use protocol buffers associates data types field names , using integers identify field . ( The protocol buffer data contains numbers , field names , providing bandwidth / storage savings compared systems include field names data . ) //polyline.protomessage Point { required int32 x = 1 ; required int32 = 2 ; optional string label = 3 ; } message Line { required Point start = 1 ; required Point end = 2 ; optional string label = 3 ; } message Polyline { repeated Point point = 1 ; optional string label = 2 ; } The `` Point '' message defines two mandatory data items , x . The data item label optional . Each data item tag . The tag defined af **NVARCHAR ( MAX ) : ** infoData input After executing command , conncmd.Execute ( ) ; // statement throws **error** like At least one parameter contained type supported . From error understood clear , type supported anymore . Also While explicitly executing stored procedure SQL Server Management Studio . It working fine , Got complete infoData without truncation . USE [ TestDB ] GO DECLARE @ return_value int , @ Id bigint , @ infoData nvarchar ( max ) , @ infoStatus tinyint EXEC @ return_value = `` DisplayInfo '' @ channel = N'telephoneMessage ' , @ infoType = 1 , @ locationId = N'F6C8B935 ' , @ Id = @ Id OUTPUT , @ infoData = @ infoData OUTPUT , @ infoStatus = @ infoStatus OUTPUT SELECT @ Id N ' @ PayloadId ' , @ infoData N ' @ MessageData ' , @ infoStatus N ' @ Status' SELECT 'Return Value ' = @ return_value GO I also noted [ What maximum characters NVARCHAR ( MAX ) ? ] ( https : //stackoverflow.com/questions/11131958/what-is-the- maximum-characters-for-the-nvarcharmax ) saying `` The max size column type NVARCHAR ( MAX ) 2 GByte storage '' . But I n't understand Why case showing NVARCHAR ( MAX ) **Type supported** . I mentioned SSMS version I using may helps fix error accurately . SQL server Management Studio 2008 R2 . V 10.50.2550.0 : SQLAPI++ - 3.8.3 Help get complete info_Data without loss truncation . Thanks Advance .","My first impression SQLApi++ great . Here bit background . I using ADO long time , 's starting give COM errors users , without helpful information . Also , msado ? ? .tlb backwards-compatible careful users version . I understand might apply , I figured I would share anyway . I started looking SQLApi++ days ago almost good things say . The draw-back I found far way know many rows get back without going result set . Also , n't free . On positive side , API intuitive , documentation good , examples useful . It blazingly fast comparison ADO . Instead copy/pasting , take look < http : //www.sqlapi.com/Examples/step4.cpp > ."
Stackoverflow,I need design stored procedure running functions Netteza database Aginity workbench 4.3 win7 . CREATE OR REPLACE PROCEDURE my_pro ( int ) RETURNS integer EXECUTE AS CALLER LANGUAGE NZPLSQL AS BEGIN_PROC DECLARE int ; BEGIN : = 0 ; WHILE < = 1 loop EXECUTE IMMEDIATE 'select 1 ' ; : = + 1 ; END LOOP ; END ; END_proc ; exec my_pro ( 0 ) But I got null result . Did I miss something ? Thanks,"Are table structures exactly ? If could potentially use Set Operators , though performance might best . Something along lines : Select * ( Select * From TableA MINUS Select * TableB ) A Join ( select * TableB MINUS Select * TableA ) ON *common unique field one* Each MINUS sub queries give records First table exact match ca n't found second . If 's common unique identifier two tables could join results two sub-queries get result 're expecting ."
Stackoverflow,I wrote function app : : Request - > H.Session H.Postgres IO Response accepts web requests builds responses ( consulting database needed ) . To actually send responses I made wrapper runApp : : H.Postgres - > H.SessionSettings - > Application runApp pg sess req respond = respond = < < H.session pg sess ( app req ) I pass function [ Warp ] ( http : //hackage.haskell.org/package/warp-3.0.2.3/docs/Network-Wai- Handler-Warp.html ) ’ ` runSettings ` loop forever handle requests : runSettings appSettings $ runApp pgSettings sessSettings However really bad creates new session every request defeats purpose connection pool prepared statements . I would like call ` runSettings ` inside ` H.session ` rather way around . However ` runSettings ` signature ` Settings - > Application - > IO ( ) ` inside ` IO ` I lost access session . Is way get back inside ` Session b r ` ? _This repost question private email._,"Yes , example create new session every request , unacceptable . First , [ ` Session ` alias reader monad transformer ] ( http : //hackage.haskell.org/package/hasql-0.1.6/docs/Hasql.html # : Session ) , gives direct access pool . So always : session postgresSettings sessionSettings $ -- session ' : : H.Session b r - > r session ' < - flip runReaderT < $ > ask let runApp request respond = respond = < < session ' ( app request ) liftIO $ -- run warp Secondly , ` ReaderT ` [ ` MonadBaseControl ` ] ( http : //hackage.haskell.org/package/monad- control-0.3.3.0/docs/Control-Monad-Trans-Control.html # g:3 ) instance , intended similar patterns ."
Stackoverflow,"I two tables , USER USER_ID | USER_NAME -- -- -- -- -- -- -- -- -- -- 659 | John 660 | Andrew 661 | Bianca -- -- -- -- -- -- -- -- -- -- USER_ADDRESS USER_ID |TYPE | ADDRESS -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 659 | HOME | New York 659 | WORK | New Jersey 660 | HOME | San Francisco 660 | WORK | Fremont -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- Output , USER_ID | USER_NAME | HOME_ADDRESS | WORK_ADDRESS -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 659 | John | New York | New Jersey 660 | Andrew | San Francisco | Fremont How I get output select query ? Thanks advance !","My guess 've actually simply renamed table ` [ dbo.tablename ] ` fully qualified name ` [ dbname ] . [ dbo ] . [ dbo.tablename ] ` . This happens right-click rename table name SSMS I 'd imagine WinSQL thing ( though I n't know 're using tool SSMS included ) . When right-click , takes away schema name makes believe need fully qualify new name , n't . You safe right-click rename table name **_just_** new table name . To sure , though , run : select * sys.schemas name = 'dbo.dbo ' ; confirm 've created new schema . **EDIT** Just sake completeness I 'll incorporate comment made @ billinkc : Run query get exact schema table : select s.name SchemaName , t.name TableName sys.schemas join sys.tables s.schema_id = t.schema_id t.name = 'tablename '"
Stackoverflow,"I set custom activities , used complex workflows . I would like make ( custom activities ) persistable without workflow idle state . It kind failover system , whenever something goes wrong execution workflow either : * paused user ( anytime ) resumed later bookmark/point paused ( example user noticed external system wants pause workflow time ) . * case unhandled exception restart execution last bookmark/point time * stop WorkflowApplication host happen anytime restart execution last bookmark/point time I 've worked days workflow persistance , I sure I achieve goal . Why ? * I could use blocking bookmarks custom activity , blocking workflow restarting purpose persisted n't look promising . * I could use notblocking bookmarks , I able see database resume . Can please advice , workflow bookmarks way go ? I see light notblocking bookmarks , I persist resume later . Can please give hints persist nonblocking bookmark later resume ? **Edit : ** In wf3 attribute ` PersistOnClose ` would enough requirement . wf4 replaced ` Persist ` activity , could also useful , however I want extra activities already complex workflows . Ideally , would great able execute ` context.RequestPersist ( callback ) ` ` NativeActivityContext ` , however method internal ( everything inside visible outside original assembly .","This delete behavior configurable . For example , using WorkflowServiceHost using code configure service host , could set SqlWorkflowInstanceStoreBehavior.InstanceCompletionAction InstanceCompletionAction.DeleteNothing following example : WorkflowServiceHost host = new WorkflowServiceHost ( workflow , baseAddress ) ; SqlWorkflowInstanceStoreBehavior persistenceBehavior = new SqlWorkflowInstanceStoreBehavior ( connString ) ; persistenceBehavior.InstanceCompletionAction = InstanceCompletionAction.DeleteNothing ; host.Description.Behaviors.Add ( persistenceBehavior ) ; For information features SQL workflow instance store configure , check [ MSDN article ] ( http : //msdn.microsoft.com/en-us/library/ee383994 % 28VS.100 % 29.aspx ) ."
Stackoverflow,"I try simple select duplicates sql statement always end error : > Command properly ended What I table like EAN STR ============= ==== 8030524096397 AAAA 8030524096397 BBBB 8030524096731 XXXX 8030524096731 YYYY 8030524096324 CCCC My select actually simple SELECT EAN , COUNT ( * ) FROM ean GROUP BY ean HAVING COUNT ( * ) > 1 ; Reults : EAN COUNT ( * ) ============= ======== 8030524096397 2 8030524096731 2 Everything fine ! Now I want ` STR ` duplicates try SELECT * FROM EAN E INNER JOIN ( SELECT EAN , COUNT ( * ) FROM ean GROUP BY ean HAVING COUNT ( * ) > 1 ) R ON E.EAN = R.EAN ; But results error . It exactly says : SELECT * FROM EAN E INNER JOIN ( SELECT EAN , COUNT ( * ) FROM ean GROUP BY ean HAVING COUNT ( * ) > 1 ) R ON ^ Error : Command properly ended What I wrong ? **Information DB : ** Gupta Centura SQLBase 7.0.1","I n't think SQLBase 7.01 supports proper ANSI ` JOIN ` syntax ( aside : good reason use modern product ) . The error indicates problem ` INNER JOIN ` . Here two possible solutions . First , yucky archaic ` join ` syntax : SELECT * FROM EAN E , ( SELECT EAN , COUNT ( * ) cnt FROM ean GROUP BY ean HAVING COUNT ( * ) > 1 ) R WHERE E.EAN = R.EAN ; Second , ` IN ` : SELECT * FROM EAN E WHERE E.EAN IN ( SELECT EAN FROM ean GROUP BY ean HAVING COUNT ( * ) > 1 )"
Stackoverflow,Does anyone know location 64 bit version `` Osm2pgsql.exe '' windows ? I find 32 bit version .,The ` type `` geometry '' exist ` part clearly states postgis extension installed . Maybe missed/forgot following part tutorial creating extension postgres db ? > substitute username _username_
Stackoverflow,"I 'm learning add view programmatically . But , I 'm confusing . I data , idpatient , idheader . Patient one ID header . When I input idpatient , add listview ( custom adapter ) programmatically . The number listview number ID header . I want set listview data patient group ID header.. So far , I add search view loop adding listview , I input one ID header seacrh view , listview view data according ID header search view.. I 'm sorry long explanation . Can anybody help solve problem ? Thanks advance [ ! [ enter image description ] ( https : //i.stack.imgur.com/GJD0Zm.jpg ) ] ( https : //i.stack.imgur.com/GJD0Zm.jpg ) This My Adapter : /** * Created RKI 11/9/2016 . */ public class AdapterHistory extends BaseAdapter implements Filterable , View.OnClickListener { private Activity activity ; LayoutInflater inflater ; HistoryHeaderActivity main ; public int count = 0 ; Context context ; public ModelHistory product ; ArrayList < ModelHistory > mStringFilterList ; ModelHistory tempValues = null ; ValueFilter valueFilter ; public Cart cart ; public AdapterHistory ( HistoryHeaderActivity main , ArrayList < ModelHistory > arraylist ) { this.main = main ; this.main.historyModel = arraylist ; mStringFilterList = arraylist ; } @ Override public int getCount ( ) { return main.historyModel.size ( ) ; } @ Override public Object getItem ( int position ) { return main.historyModel.get ( position ) ; } @ Override public long getItemId ( int position ) { return 0 ; } public View getView ( final int position , View convertView , ViewGroup parent ) { int pos = position ; final Cart carts = CartHelper.getCart ( ) ; View vi = convertView ; ViewHolderItem holder = new ViewHolderItem ( ) ; ( convertView == null ) { LayoutInflater inflater = ( LayoutInflater ) main.getSystemService ( Context.LAYOUT_INFLATER_SERVICE ) ; vi = inflater.inflate ( R.layout.list_history , null ) ; holder.h_id = ( TextView ) vi.findViewById ( R.id.id_header_ ) ; holder.h_type = ( TextView ) vi.findViewById ( R.id.servicetype_ ) ; holder.h_qty = ( TextView ) vi.findViewById ( R.id.qty_ ) ; holder.h_ps_id = ( TextView ) vi.findViewById ( R.id.patient_id ) ; holder.h_ps_name = ( TextView ) vi.findViewById ( R.id.patient_ ) ; holder.h_dokid = ( TextView ) vi.findViewById ( R.id.doctor_id_ ) ; holder.h_dokname = ( TextView ) vi.findViewById ( R.id.doctor_ ) ; holder.h_item = ( TextView ) vi.findViewById ( R.id.item_ ) ; holder.h_date = ( TextView ) vi.findViewById ( R.id.date_ ) ; holder.checkToCart = ( CheckBox ) vi.findViewById ( R.id.checkBox ) ; holder.checkToCart.setTag ( position ) ; holder.checkToCart.setOnCheckedChangeListener ( new CompoundButton.OnCheckedChangeListener ( ) { @ Override public void onCheckedChanged ( CompoundButton buttonView , boolean isChecked ) { int getPosition = ( Integer ) buttonView.getTag ( ) ; // Here get position set checkbox using setTag . tempValues.setSelected ( isChecked ) ; } } ) ; vi.setTag ( holder ) ; vi.setTag ( R.id.checkBox , holder.checkToCart ) ; } else { holder = ( ViewHolderItem ) vi.getTag ( ) ; } ( main.historyModel.size ( ) < = 0 ) { holder.h_date.setText ( `` No Data '' ) ; } else { /*** Get Model object Arraylist ****/ tempValues = null ; tempValues = ( ModelHistory ) main.historyModel.get ( position ) ; holder.h_id.setText ( tempValues.getH_id ( ) ) ; holder.h_type.setText ( tempValues.getH_service ( ) ) ; holder.h_qty.setText ( tempValues.getH_qty ( ) ) ; holder.h_ps_name.setText ( tempValues.getH_p_name ( ) ) ; holder.h_ps_id.setText ( tempValues.getH_p_id ( ) ) ; holder.h_dokid.setText ( tempValues.getH_d_id ( ) ) ; holder.h_dokname.setText ( tempValues.getH_d_name ( ) ) ; holder.h_item.setText ( tempValues.getH_item ( ) ) ; holder.h_date.setText ( tempValues.getH_date ( ) ) ; holder.checkToCart.setTag ( position ) ; holder.checkToCart.setOnCheckedChangeListener ( new CompoundButton.OnCheckedChangeListener ( ) { @ Override public void onCheckedChanged ( CompoundButton buttonView , boolean isChecked ) { main.historyModel.get ( position ) .setSelected ( buttonView.isChecked ( ) ) ; } } ) ; holder.checkToCart.setChecked ( main.historyModel.get ( position ) .isSelected ( ) ) ; ( pos = 1 ; pos < = 1 ; pos++ ) { main.u_pname = tempValues.getH_p_name ( ) ; main.u_pid = tempValues.getH_p_id ( ) ; main.u_service = tempValues.getH_service ( ) ; main.up_iddetail = tempValues.getH_id ( ) ; } } return vi ; } @ Override public void onClick ( View v ) { } public static class ViewHolderItem { TextView h_id , h_type , h_ps_id , h_ps_name , h_dokid , h_dokname , h_item , h_qty , h_date ; CheckBox checkToCart ; } private List < TransactionsItem > getCartItems ( Cart cart ) { List < TransactionsItem > cartItems = new ArrayList < > ( ) ; Map < Saleable , Integer > itemMap = cart.getItemWithQuantity ( ) ; ( Map.Entry < Saleable , Integer > entry : itemMap.entrySet ( ) ) { TransactionsItem cartItem = new TransactionsItem ( ) ; cartItem.setProduct ( ( ModelInventory ) entry.getKey ( ) ) ; cartItem.setQuantity ( entry.getValue ( ) ) ; cartItems.add ( cartItem ) ; } return cartItems ; } @ Override public Filter getFilter ( ) { ( valueFilter == null ) { valueFilter = new ValueFilter ( ) ; } return valueFilter ; } private class ValueFilter extends Filter { @ Override protected FilterResults performFiltering ( CharSequence constraint ) { FilterResults results = new FilterResults ( ) ; ( constraint ! = null & & constraint.length ( ) > 0 ) { List < ModelHistory > filterList = new ArrayList < ModelHistory > ( ) ; ( int = 0 ; < mStringFilterList.size ( ) ; i++ ) { ( ( mStringFilterList.get ( ) .getH_id ( ) .toUpperCase ( ) ) .contains ( constraint.toString ( ) .toUpperCase ( ) ) ) { ModelHistory country = new ModelHistory ( mStringFilterList.get ( ) .getH_id ( ) , mStringFilterList.get ( ) .getH_p_id ( ) , mStringFilterList.get ( ) .getH_date ( ) , mStringFilterList.get ( ) .getH_p_name ( ) , mStringFilterList.get ( ) .getH_d_id ( ) , mStringFilterList.get ( ) .getH_d_name ( ) , mStringFilterList.get ( ) .getH_item ( ) , mStringFilterList.get ( ) .getH_qty ( ) , mStringFilterList.get ( ) .getH_service ( ) ) ; filterList.add ( country ) ; } } results.count = filterList.size ( ) ; results.values = filterList ; } else { results.count = mStringFilterList.size ( ) ; results.values = mStringFilterList ; } return results ; } @ Override protected void publishResults ( CharSequence constraint , FilterResults results ) { main.historyModel = ( ArrayList < ModelHistory > ) results.values ; notifyDataSetChanged ( ) ; } } } This My Activity : import com.mobileproject.rki.mobile_his_receipt.view.adapter.AdapterHistory ; import org.w3c.dom . * ; import java.io.Serializable ; import java.util . * ; public class HistoryHeaderActivity extends AppCompatActivity implements SearchView.OnQueryTextListener { static final String URL = `` http : // ... /GetDataInv '' ; static final String KEY_TABLE = `` Table '' ; // parent node static final String KEY_REG_ID = `` Sales_Aptk_ID '' , KEY_DATE = `` Sales_Aptk_Date '' , KEY_PATIENT_ID = `` Sales_Aptk_Patient_ID '' , KEY_SERVICE_ID = `` Sales_Aptk_Type '' , KEY_PATIENT_NAME = `` Sales_Aptk_Patient_Name '' , KEY_DOCTOR_ID = `` Sales_Aptk_Doctor_ID '' , KEY_DOCTOR_NAME = `` Sales_Aptk_Doctor_Name '' , KEY_ITEM_ID = `` Sales_Aptk_Detail_Item_ID '' , KEY_DETAIL_UNIT = `` Sales_Aptk_Detail_Unit '' , KEY_ITEM_QTY = `` Sales_Aptk_Detail_Qty '' ; static final String KEY_ID_HEADER = `` Sales_Aptk_ID '' ; static final String KEY_TABLE_INV = `` Table '' ; // parent node static final String KEY_ITEM_ID_INV = `` Item_ID '' , KEY_ITEM_NAME = `` Item_Name '' , KEY_MAX_STOCK = `` Item_Max_Stock '' ; public static final int DIALOG_DOWNLOAD_DATA_PROGRESS = 0 , DIALOG_NO_DATA = 1 , DIALOG_DOWNLOAD_FULL_PHOTO_PROGRESS = 2 ; Element e ; final Context context = ; public List < ModelInventory > invModels ; private ProgressDialog mProgressDialog ; public static String id , up_iddetail , up_user , u_pid , u_pname , u_service , xml ; public ArrayList < ModelInventory > invModel = new ArrayList < ModelInventory > ( ) ; public ArrayList < ModelHistory > historyModel = new ArrayList < ModelHistory > ( ) ; public ArrayList < ModelIDHeader > headerModel = new ArrayList < ModelIDHeader > ( ) ; public Cart cart ; private Menu menu ; AdapterHistory hstAdpt , idhstAdpt ; private ProgressDialog progressDialog ; public ModelInventory productInv ; int mPosition , invPosition ; EditText p_id ; ListView listHistory ; XMLParser parser ; LinearLayout lm ; LinearLayout.LayoutParams params ; @ Override protected void onCreate ( Bundle savedInstanceState ) { super.onCreate ( savedInstanceState ) ; setContentView ( R.layout.activity_history_header ) ; lm = ( LinearLayout ) findViewById ( R.id.linearMain ) ; params = new LinearLayout.LayoutParams ( LayoutParams.WRAP_CONTENT , LayoutParams.WRAP_CONTENT ) ; p_id = ( EditText ) findViewById ( R.id.patientID ) ; listHistory = ( ListView ) findViewById ( R.id.listhistory ) ; hstAdpt = new AdapterHistory ( HistoryHeaderActivity.this , historyModel ) ; listHistory.setAdapter ( hstAdpt ) ; listHistory.setOnItemClickListener ( new AdapterView.OnItemClickListener ( ) { @ Override public void onItemClick ( AdapterView < ? > parent , View view , int position , long id ) { mPosition = position ; invPosition = position ; } } ) ; SharedPreferences login2 = getSharedPreferences ( `` USERLOGIN '' , 0 ) ; String doktername = login2.getString ( `` userlogin '' , `` 0 '' ) ; up_user = doktername ; p_id.addTextChangedListener ( new TextWatcher ( ) { public void afterTextChanged ( Editable ) { } public void beforeTextChanged ( CharSequence , int start , int count , int ) { } public void onTextChanged ( CharSequence , int start , int , int count ) { onGetHistory ( ) ; } } ) ; } /** * Check Form Input * * @ return */ private boolean isFormValid ( ) { String aTemp = p_id.getText ( ) .toString ( ) ; ( aTemp.isEmpty ( ) ) { Toast.makeText ( , `` Please Input Patient ID.. '' , Toast.LENGTH_SHORT ) .show ( ) ; return false ; } else { id = aTemp.toString ( ) ; } return true ; } protected void dismissDialogWait ( ) { ( progressDialog ! = null ) { ( progressDialog.isShowing ( ) ) { progressDialog.dismiss ( ) ; } } } public void onGetHistory ( ) { listHistory.setAdapter ( null ) ; ( isFormValid ( ) ) { GetDataHistoryTask syncTask = new GetDataHistoryTask ( ) ; syncTask.execute ( new IntroductingMethod ( ) ) ; GetDataHeaderTask syncTaskHeader = new GetDataHeaderTask ( ) ; syncTaskHeader.execute ( new IntroductingMethod ( ) ) ; } } private class GetDataHistoryTask extends AsyncTask < IntroductingMethod , String , String > { @ Override protected String doInBackground ( IntroductingMethod ... params ) { IntroductingMethod REGService = params [ 0 ] ; return REGService.getHistoryData ( id ) ; } @ Override protected void onPostExecute ( String result ) { dismissDialogWait ( ) ; ( result ! = null ) { try { ArrayList < HashMap < String , String > > menuItems = new ArrayList < HashMap < String , String > > ( ) ; ( Build.VERSION.SDK_INT > 9 ) { StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder ( ) .permitAll ( ) .build ( ) ; StrictMode.setThreadPolicy ( policy ) ; } XMLParser parser = new XMLParser ( ) ; Document doc = parser.getDomElement ( result ) ; // getting DOM element NodeList nl = doc.getElementsByTagName ( KEY_TABLE ) ; // looping item nodes < item > ( int = 0 ; < nl.getLength ( ) ; i++ ) { // creating new HashMap HashMap < String , String > map = new HashMap < String , String > ( ) ; Element e = ( Element ) nl.item ( ) ; ModelHistory add = new ModelHistory ( ) ; add.setH_id ( parser.getValue ( e , KEY_REG_ID ) ) ; add.setH_p_name ( parser.getValue ( e , KEY_PATIENT_NAME ) ) ; add.setH_p_id ( parser.getValue ( e , KEY_PATIENT_ID ) ) ; add.setH_service ( parser.getValue ( e , KEY_SERVICE_ID ) ) ; add.setH_date ( parser.getValue ( e , KEY_DATE ) ) ; add.setH_detail_unit ( parser.getValue ( e , KEY_DETAIL_UNIT ) ) ; add.setH_d_id ( parser.getValue ( e , KEY_DOCTOR_ID ) ) ; add.setH_d_name ( parser.getValue ( e , KEY_DOCTOR_NAME ) ) ; add.setH_item ( parser.getValue ( e , KEY_ITEM_ID ) ) ; add.setH_qty ( parser.getValue ( e , KEY_ITEM_QTY ) ) ; historyModel.add ( add ) ; } ShowAllContentHistory ( ) ; } catch ( Exception e ) { Toast.makeText ( HistoryHeaderActivity.this.getApplicationContext ( ) , `` Koneksi gagal . Silahkan coba kembali . `` , Toast.LENGTH_LONG ) .show ( ) ; } } else { Toast.makeText ( HistoryHeaderActivity.this.getApplicationContext ( ) , `` Koneksi gagal . Silahkan coba kembali . `` , Toast.LENGTH_LONG ) .show ( ) ; } } } private class GetDataHeaderTask extends AsyncTask < IntroductingMethod , String , String > { @ Override protected String doInBackground ( IntroductingMethod ... params ) { IntroductingMethod REGService = params [ 0 ] ; return REGService.getHistoryHeaderData ( id ) ; } @ Override protected void onPostExecute ( String result ) { dismissDialogWait ( ) ; ( result ! = null ) { try { ArrayList < HashMap < String , String > > menuItems = new ArrayList < HashMap < String , String > > ( ) ; ( Build.VERSION.SDK_INT > 9 ) { StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder ( ) .permitAll ( ) .build ( ) ; StrictMode.setThreadPolicy ( policy ) ; } XMLParser parser = new XMLParser ( ) ; Document doc = parser.getDomElement ( result ) ; // getting DOM element NodeList nl = doc.getElementsByTagName ( KEY_TABLE ) ; // looping item nodes < item > ( int = 0 ; < nl.getLength ( ) ; i++ ) { // creating new HashMap HashMap < String , String > map = new HashMap < String , String > ( ) ; Element e = ( Element ) nl.item ( ) ; ModelIDHeader add = new ModelIDHeader ( ) ; add.setIdSalesHeader ( parser.getValue ( e , KEY_ID_HEADER ) ) ; headerModel.add ( add ) ; } ( result.contains ( `` < Sales_Aptk_Patient_ID > '' +id+ '' < /Sales_Aptk_Patient_ID > '' ) ) { Log.d ( `` NilaiID `` , id ) ; AddNewList ( ) ; } else { Log.d ( `` Kenapayahh '' , id ) ; } } catch ( Exception e ) { Toast.makeText ( HistoryHeaderActivity.this.getApplicationContext ( ) , `` Koneksi gagal . Silahkan coba kembali . `` , Toast.LENGTH_LONG ) .show ( ) ; } } else { Toast.makeText ( HistoryHeaderActivity.this.getApplicationContext ( ) , `` Koneksi gagal . Silahkan coba kembali . `` , Toast.LENGTH_LONG ) .show ( ) ; } } } public void AskUpdate ( View v ) { ( hstAdpt.getCount ( ) == 0 ) { Toast.makeText ( HistoryHeaderActivity.this.getApplicationContext ( ) , `` Tidak ada rekaman pasien . `` , Toast.LENGTH_LONG ) .show ( ) ; } else { SharedPreferences id_header = getSharedPreferences ( `` IDSALESAPTK '' , 0 ) ; SharedPreferences.Editor editorID = id_header.edit ( ) ; editorID.putString ( `` idsalesaptk '' , up_iddetail ) ; editorID.commit ( ) ; SharedPreferences sendPref2 = getSharedPreferences ( `` DATAPATIENTNAME '' , 0 ) ; SharedPreferences.Editor editor2 = sendPref2.edit ( ) ; editor2.putString ( `` datapatientname '' , u_pname ) ; editor2.commit ( ) ; editor2.clear ( ) ; SharedPreferences sendPref3 = getSharedPreferences ( `` DATAPATIENTID '' , 0 ) ; SharedPreferences.Editor editor3 = sendPref3.edit ( ) ; editor3.putString ( `` datapatientid '' , u_pid ) ; editor3.commit ( ) ; editor3.clear ( ) ; SharedPreferences regID = getSharedPreferences ( `` DATAREGID '' , 0 ) ; String reg_ID = regID.getString ( `` dataregid '' , `` 0 '' ) ; SharedPreferences sendPref4 = getSharedPreferences ( `` DATAREGID '' , 0 ) ; SharedPreferences.Editor editor4 = sendPref4.edit ( ) ; editor4.putString ( `` dataregid '' , reg_ID ) ; editor4.commit ( ) ; editor4.clear ( ) ; SharedPreferences sendPref1 = getSharedPreferences ( `` DATASERVICE '' , 0 ) ; SharedPreferences.Editor editor1 = sendPref1.edit ( ) ; editor1.putString ( `` dataservice '' , u_service ) ; editor1.commit ( ) ; editor1.clear ( ) ; new LoadingDataAsync ( ) .execute ( ) ; } } public class LoadingDataAsync extends AsyncTask < String , Void , Void > { @ Override protected Void doInBackground ( String ... params ) { updateDetail ( ) ; return null ; } protected void onPostExecute ( Void unused ) { dismissDialog ( DIALOG_DOWNLOAD_DATA_PROGRESS ) ; removeDialog ( DIALOG_DOWNLOAD_DATA_PROGRESS ) ; } protected void onPreExecute ( ) { super.onPreExecute ( ) ; showDialog ( DIALOG_DOWNLOAD_DATA_PROGRESS ) ; } } public void updateDetail ( ) { ArrayList < ModelHistory > candidateModelArrayList = new ArrayList < ModelHistory > ( ) ; ( ModelHistory model : historyModel ) { ( model.isSelected ( ) ) { candidateModelArrayList.add ( model ) ; } } ArrayList < HashMap < String , String > > menuItems = new ArrayList < HashMap < String , String > > ( ) ; ( Build.VERSION.SDK_INT > 9 ) { StrictMode.ThreadPolicy policy = new StrictMode.ThreadPolicy.Builder ( ) .permitAll ( ) .build ( ) ; StrictMode.setThreadPolicy ( policy ) ; } parser = new XMLParser ( ) ; xml = parser.getXmlFromUrl ( URL ) ; Document doc = parser.getDomElement ( xml ) ; NodeList nl = doc.getElementsByTagName ( KEY_TABLE_INV ) ; ( int = 0 ; < nl.getLength ( ) ; i++ ) { e = ( Element ) nl.item ( ) ; ModelInventory add = new ModelInventory ( ) ; add.setItem_ID ( parser.getValue ( e , KEY_ITEM_ID_INV ) ) ; add.setItem_Name ( parser.getValue ( e , KEY_ITEM_NAME ) ) ; add.setItem_Max_Stock ( parser.getValue ( e , KEY_MAX_STOCK ) ) ; invModel.add ( add ) ; } invModels = new ArrayList < ModelInventory > ( ) ; invModels = invModel ; ArrayAdapter < ModelInventory > adptInv = new ArrayAdapter < ModelInventory > ( context , android.R.layout.simple_spinner_dropdown_item , invModels ) ; ArrayAdapter < ModelHistory > adptChkItem = new ArrayAdapter < ModelHistory > ( context , android.R.layout.simple_spinner_dropdown_item , candidateModelArrayList ) ; ( adptInv.isEmpty ( ) ) { Toast.makeText ( HistoryHeaderActivity.this , `` Empty Patient '' , Toast.LENGTH_SHORT ) .show ( ) ; } cart = CartHelper.getCart ( ) ; ( mPosition = 0 ; mPosition < adptChkItem.getCount ( ) ; mPosition++ ) { ModelHistory historyItem = ( ModelHistory ) adptChkItem.getItem ( mPosition ) ; ( int j = mPosition ; j < adptInv.getCount ( ) ; j++ ) { ModelInventory inventoryItem = ( ModelInventory ) adptInv.getItem ( j ) ; ( candidateModelArrayList.get ( mPosition ) .getH_item ( ) .equals ( inventoryItem.getItem_ID ( ) ) ) { ( cart.getProducts ( ) .toString ( ) .contains ( inventoryItem.getItem_Name ( ) ) ) { } else { productInv = ( ModelInventory ) ( Serializable ) adptInv.getItem ( j ) ; int qty = Integer.parseInt ( historyItem.getH_qty ( ) ) ; cart.add ( productInv , qty ) ; } } else { } } } Intent = new Intent ( getBaseContext ( ) , CartActivity.class ) ; i.putExtra ( `` PersonID '' , `` try '' ) ; startActivity ( ) ; } @ Override protected Dialog onCreateDialog ( int id ) { } @ Override public boolean onQueryTextSubmit ( String query ) { return false ; } @ Override public boolean onQueryTextChange ( String newText ) { hstAdpt.getFilter ( ) .filter ( newText ) ; idhstAdpt.getFilter ( ) .filter ( newText ) ; return false ; } public void AddNewList ( ) { ArrayAdapter < ModelIDHeader > adptIDHeader = new ArrayAdapter < ModelIDHeader > ( context , android.R.layout.simple_spinner_dropdown_item , headerModel ) ; ( int count =0 ; count < adptIDHeader.getCount ( ) ; count++ ) { idhstAdpt = new AdapterHistory ( HistoryHeaderActivity.this , historyModel ) ; ModelIDHeader idHeader = ( ModelIDHeader ) adptIDHeader.getItem ( count ) ; Button btn = new Button ( ) ; btn.setId ( count ) ; btn.setText ( idHeader.getIdSalesHeader ( ) ) ; lm.addView ( btn ) ; SearchView search = new SearchView ( ) ; search.setQuery ( idHeader.getIdSalesHeader ( ) , false ) ; search.setOnQueryTextListener ( ) ; lm.addView ( search ) ; ListView tv = new ListView ( ) ; tv.setId ( count ) ; tv.setLayoutParams ( params ) ; tv.setDividerHeight ( 2 ) ; tv.setAdapter ( idhstAdpt ) ; lm.addView ( tv ) ; } } public void ShowAllContentHistory ( ) { listHistory = ( ListView ) findViewById ( R.id.listhistory ) ; hstAdpt = new AdapterHistory ( HistoryHeaderActivity.this , historyModel ) ; listHistory.setAdapter ( hstAdpt ) ; listHistory.setOnItemClickListener ( new AdapterView.OnItemClickListener ( ) { @ Override public void onItemClick ( AdapterView < ? > parent , View view , int position , long id ) { } } ) ; } @ Override public boolean onCreateOptionsMenu ( Menu menu ) { MenuInflater inflater = getMenuInflater ( ) ; inflater.inflate ( R.menu.activity_history_actions , menu ) ; return super.onCreateOptionsMenu ( menu ) ; } @ Override public boolean onOptionsItemSelected ( MenuItem item ) { int id = item.getItemId ( ) ; ( id == R.id.action_logout ) { new AlertDialog.Builder ( ) .setMessage ( `` Are sure want exit ? '' ) .setCancelable ( false ) .setPositiveButton ( `` Yes '' , new DialogInterface.OnClickListener ( ) { public void onClick ( DialogInterface dialog , int id ) { ClearPrefs ( ) ; logout ( ) ; Intent intent = new Intent ( HistoryHeaderActivity.this , LoginActivity.class ) ; startActivity ( intent ) ; } } ) .setNegativeButton ( `` No '' , null ) .show ( ) ; return true ; } ( id == R.id.action_home ) { Intent intent = new Intent ( HistoryHeaderActivity.this , MainActivity.class ) ; startActivity ( intent ) ; return true ; } return super.onOptionsItemSelected ( item ) ; } @ Override public void onBackPressed ( ) { Intent intent = new Intent ( HistoryHeaderActivity.this , MainActivity.class ) ; startActivity ( intent ) ; } public void ClearPrefs ( ) { } public void logout ( ) { SharedPreferences preferences5 = getSharedPreferences ( `` IDLOGIN '' , Context.MODE_PRIVATE ) ; SharedPreferences.Editor editor5 = preferences5.edit ( ) ; editor5.clear ( ) ; editor5.commit ( ) ; } }",It seems phrases used : * Spread data amongst many containers best performance * Modeling data via Entities * Process queries parallel best performance * Caching data service hosted middle tier This would imply start thinking like OO modellers rather relational mind-set . Performance seems rely ability massively parallelise object query smiliar way creating LINQ query take advantage parallelisation .
Stackoverflow,"I 'm trying read FIC file encrypted ( indeed , data almost read displaying ) . I want convert file convenient format , CSV , XML , SQL , etc ... When I try open Windev Express 19 , I error telling file password protected . But If really password , file would encrypted ( I think ) . If someone idea could problem . Or suggestion , I 'd glad .","Here ^^ : 1 : You get OleDb Provider hyperfileSql files [ page ] ( http : //www.pcsoft.fr/st/telec/modules-communs-15/wx15_63j.htm ) . 2 : Here simple exemple code used extract data : string connectionString = @ '' Provider=PCSOFT.HFSQL ; Initial Catalog=C : \MyDataFolder '' ; string sql = @ '' SELECT * FROM MyTable '' ; //MyTable = The .FIC file DataTable table = new DataTable ( ) ; using ( OleDbConnection connection = new OleDbConnection ( connectionString ) ) { using ( OleDbDataAdapter adapter = new OleDbDataAdapter ( sql , connection ) ) { adapter.Fill ( table ) ; //Fill table extracted data } } gridControl1.DataSource = table ; //Set DataSource grid control For connection strings : visite [ page ] ( http : //doc.pcsoft.fr/fr- FR/ ? 9000059 )"
Stackoverflow,"I 'm trying switch one VBS scripts using SQLOLEDB ODBC driver . So long works like expected - one thing : When fetching _SERVERPROPERTY ( `` is_clustered '' ) _ MSSQL instance resulting value different using driver . Here 's output example script ( script follows ) : > > C : \ > cscript test.vbs > > Provider : sqloledb > > is_clustered ( name ) : is_clustered > is_clustered ( type ) : 12 > is_clustered ( value ) : 0 > > Driver : ( SQL Server ) > > is_clustered ( name ) : is_clustered > is_clustered ( type ) : 204 > C : \test.vbs ( 33 , 1 ) Microsoft VBScript runtime error : Type mismatch > Does anyone know I 'm wrong I 'm missing code ? Oh , yes , code ... 's example script : Option Explicit Dim RS , CONN1 , CONN2 Set RS = CreateObject ( `` ADODB.Recordset '' ) Set CONN1 = CreateObject ( `` ADODB.Connection '' ) CONN1.ConnectionTimeout = 2 CONN1.Provider = `` sqloledb '' CONN1.Properties ( `` Integrated Security '' ) .Value = `` SSPI '' CONN1.Properties ( `` Data Source '' ) .Value = `` HOSTNAME\INST01 '' CONN1.Open WScript.echo `` Provider : sqloledb '' & vbLf RS.Open `` SELECT SERVERPROPERTY ( 'IsClustered ' ) AS is_clustered '' , CONN1 WScript.echo `` is_clustered ( name ) : `` & RS.fields ( 0 ) .Name WScript.echo `` is_clustered ( type ) : `` & RS.fields ( 0 ) .Type WScript.echo `` is_clustered ( value ) : `` & RS ( `` is_clustered '' ) & vbLf RS.Close Set CONN2 = CreateObject ( `` ADODB.Connection '' ) CONN2.ConnectionTimeout = 2 CONN2.ConnectionString = `` driver= { SQL Server } ; '' & _ `` server=HOSTNAME\INST01 ; '' & _ `` Trusted_Connection=yes '' CONN2.Open WScript.echo `` Driver : ( SQL Server ) '' & vbLf RS.Open `` SELECT SERVERPROPERTY ( 'IsClustered ' ) AS is_clustered '' , CONN2 WScript.echo `` is_clustered ( name ) : `` & RS.fields ( 0 ) .Name WScript.echo `` is_clustered ( type ) : `` & RS.fields ( 0 ) .Type WScript.echo `` is_clustered ( value ) : `` & RS ( `` is_clustered '' ) RS.Close Many Thanks advance ! BR , Marcel","Since solution , ADO ( _dead_ ) done , hack **is** answer . Never issue ` MERGE ` statement without leading comment line : -- Dummy leading comment line thwart ADO mangling MERGE Windows XP/2003R2 MERGE Users USING ( VALUES ..."
Stackoverflow,"Having table jsonb column containing array , best way select records containing specific tag ` rom-sql ` ? Example query . < https : //www.db-fiddle.com/f/u4CFkUUpnHZj67j1RJ5YRe/0 > CREATE TABLE posts ( id INT , tags JSONB ) ; INSERT INTO posts ( id , tags ) VALUES ( 1 , ' [ `` cats '' , `` dogs '' ] ' ) ; INSERT INTO posts ( id , tags ) VALUES ( 2 , ' [ `` dogs '' ] ' ) ; SELECT * FROM posts WHERE tags @ > ' [ `` cats '' ] ' ; So , build query ` rom-sql ` ?","Complete example : < https : //github.com/gotar/sinatra-rom > add require 'bundler/setup' require 'rom/sql/rake_task' task : setup # Load ROM related stuff . You n't need specify manually connection end Rakefile get Raketasks ( rake -T ) list , $ rake db : create_migration [ any_name ] file create , add migration . Thats"
Stackoverflow,"I trying build query tableau dashboard connected Google BigQuery . We tables month data , I want present last 30 days data given time ( go across multiple tables ) . The current query I gives error `` Timestamp literal explicit conversion timestamp required . '' I 've looking around help convert timestamp n't found anything helpful . This code . SELECT DATE ( date_time ) AS date , FROM TABLE_QUERY ( myTable , `` date ( concat ( left ( table_id,4 ) , '- ' , right ( table_id,2 ) , '- ' , '01 ' ) ) > = '2017-06-01 ' '' ) WHERE DATE ( date_time ) > = DATE_ADD ( day , -30 , current_date ( ) ) DATE ( date_time ) < = current_date ( ) ORDER BY date Any help would get work greatly appreciated . Note : using legacy SQL","> problem data problem split ? ? To help troubleshooting - I would recommend running logic BigQuery Standard SQL # standardSQL SELECT ColA , SPLIT ( ColA , '| ' ) [ SAFE_OFFSET ( 0 ) ] AS part1 , SPLIT ( ColA , '| ' ) [ SAFE_OFFSET ( 1 ) ] AS part2 FROM TabA"
Stackoverflow,"I 'm trying add fields existing report runs fine . To add fields , I need join another table . The problem field different type table Table A / FieldA = Varchar ( 20 ) Table B / FieldB = Decimal ( 19,0 ) This join I : inner join TableA ta ta.FieldA = b.FieldB With join , I get ` SELECT Failed [ 3754 ] Precision error FLOAT type constant implicit conversions ` . I 'm thinking I use CAST statement like : inner join TableA ta ta.FieldA = cast ( b.FieldB Varchar ( 20 ) ) When I run report , I n't get results I 'm expecting least 1 row . Any help inner join would greatly appreciated . Thanks .","This returns result Standard SQL 's ` LAG ( END_dt ) OVER ( PARTITION BY CUST_ID ORDER BY END_dt ` , i.e . previous row 's ` END_dt ` ( NULL 1st row per CUST_ID ) . When switch ` FOLLOWING ` instead ` PRECEDING ` 's next row , ` LEAD ` Standard SQL . Both ` LAG ` ` LEAD ` finally implemented TD16.10 . As simply want find gaps n't access actual difference also simplify : SELECT DISTINCT CUST_ID FROM table QUALIFY STRT_dt - MIN ( END_dt ) OVER ( PARTITION BY CUST_ID ORDER BY END_dt ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING ) > 1"
Stackoverflow,"ALTER PROCEDURE [ dbo ] . [ sp_helptext3 ] ( @ ProcName VARCHAR ( 256 ) ) AS SET NOCOUNT ON BEGIN DECLARE @ path VARCHAR ( MAX ) DECLARE @ filepath_loction VARCHAR ( MAX ) SET @ filepath_loction = '\\SOLVERLAPT292\tempfolder\new.txt' EXEC @ path = sp_helptext2 @ ProcName SET @ filepath_loction = @ path SET @ sql = 'bcp `` ' + @ sql + ' '' queryout `` ' + @ filename + ' '' -c -r '' ''\n '' -t '' '' , '' '' -S '+ @ @ servername+ ' -T' SELECT @ filepath_loction END Output I got convert text file format I get error : > Msg 214 , Level 16 , State 201 , Procedure xp_cmdshell , Line 1 > Procedure expects parameter 'command_string ' type 'varchar ' .","What I believe missing total script stops . Your ` pause ` command moment holds script temporarily , full script . You use label jump instead . In script I added label ` : error_happened ` jump case error . In case error , command ` goto : EOF ` skip past ` : error_happened ` label . bcp.exe `` select * OrderXpress.dbo.Customers CustId < 1000 '' queryout `` D : \Customer.dat '' -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened bcp.exe OrderXpress.dbo.Customers `` D : \Customer2.dat '' -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened bcp.exe OrderXpress.dbo.Orders `` D : \Orders.dat '' -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened goto : EOF : error_happened echo , echo An error occurred . Script stopped echo pause A remark must made however errorlevel comes BCP . In case data row exported correctly , I sure happens errorlevel . I know import row error lead problems . See also : < https : //groups.google.com/forum/ # ! topic/microsoft.public.sqlserver.tools/qzpWuZzJnr4 > I workaround using -e parameter check errors . The script would : bcp.exe `` select * OrderXpress.dbo.Customers CustId < 1000 '' queryout `` D : \Customer.dat '' -e errors.txt -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened call : check_errorfile bcp.exe OrderXpress.dbo.Customers `` D : \Customer2.dat '' -e errors.txt -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened call : check_errorfile bcp.exe OrderXpress.dbo.Orders `` D : \Orders.dat '' -e errors.txt -S localhost -U sa -P Sa12345 -E -n IF ERRORLEVEL 1 GOTO error_happened call : check_errorfile goto : EOF : check_errorfile exist errors.txt ( FOR % % A IN ( errors.txt ) DO ( % % ~zA GTR 0 ( goto error_happened ) else ( del errors.txt ) ) ) exit /b : error_happened echo , echo An error occurred . Script stopped echo pause"
Stackoverflow,"I installed PC Docker . I installed SQlServer linux instance run correctly . I need connect Docker instance SSMS . In picture see personal config . For installation I follow link < https : //docs.microsoft.com/en- us/sql/linux/sql-server-linux-setup-docker > Thanks advance [ ! [ enter image description ] ( https : //i.stack.imgur.com/iSy4G.jpg ) ] ( https : //i.stack.imgur.com/iSy4G.jpg ) When I lunch server I message PS C : \Users\Daniele > docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES PS C : \Users\Daniele > docker run microsoft/mssql-server-linux This evaluation version . There [ 171 ] days left evaluation period . 2017-05-23 07:19:15.12 Server Setup step copying system data file ' C : \templatedata\master.mdf ' '/var/opt/mssq l/data/master.mdf ' . 2017-05-23 07:19:15.19 Server Setup step copying system data file ' C : \templatedata\mastlog.ldf ' '/var/opt/mss ql/data/mastlog.ldf ' . 2017-05-23 07:19:15.19 Server Setup step copying system data file ' C : \templatedata\model.mdf ' '/var/opt/mssql /data/model.mdf ' . 2017-05-23 07:19:15.21 Server Setup step copying system data file ' C : \templatedata\modellog.ldf ' '/var/opt/ms sql/data/modellog.ldf ' . 2017-05-23 07:19:15.22 Server Setup step copying system data file ' C : \templatedata\msdbdata.mdf ' '/var/opt/ms sql/data/msdbdata.mdf ' . 2017-05-23 07:19:15.24 Server Setup step copying system data file ' C : \templatedata\msdblog.ldf ' '/var/opt/mss ql/data/msdblog.ldf ' . 2017-05-23 07:19:15.34 Server Microsoft SQL Server 2017 ( CTP2.1 ) - 14.0.600.250 ( X64 ) May 10 2017 12:21:23 Copyright ( C ) 2017 Microsoft Corporation . All rights reserved . Developer Edition ( 64-bit ) Linux ( Ubuntu 16.04.2 LTS ) 2017-05-23 07:19:15.34 Server UTC adjustment : 0:00 2017-05-23 07:19:15.35 Server ( c ) Microsoft Corporation . 2017-05-23 07:19:15.35 Server All rights reserved . 2017-05-23 07:19:15.35 Server Server process ID 4116 . 2017-05-23 07:19:15.35 Server Logging SQL Server messages file '/var/opt/mssql/log/errorlog ' . 2017-05-23 07:19:15.36 Server Registry startup parameters : -d /var/opt/mssql/data/master.mdf -l /var/opt/mssql/data/mastlog.ldf -e /var/opt/mssql/log/errorlog 2017-05-23 07:19:15.37 Server SQL Server detected 1 sockets 2 cores per socket 2 logical processors per cket , 2 total logical processors ; using 2 logical processors based SQL Server licensing . This informational mes sage ; user action required . 2017-05-23 07:19:15.38 Server SQL Server starting normal priority base ( =7 ) . This informational message . No user action required . 2017-05-23 07:19:15.39 Server Detected 3944 MB RAM . This informational message ; user action required . 2017-05-23 07:19:15.39 Server Using conventional memory memory manager . 2017-05-23 07:19:15.63 Server Buffer pool extension already disabled . No action necessary . 2017-05-23 07:19:15.76 Server InitializeExternalUserGroupSid failed . Implied authentication disabled . 2017-05-23 07:19:15.76 Server Implied authentication manager initialization failed . Implied authentication disabled . 2017-05-23 07:19:15.77 Server Successfully initialized TLS configuration . Allowed TLS protocol versions [ ' 1 .0 1.1 1.2 ' ] . Allowed TLS ciphers [ 'ECDHE-ECDSA-AES128-GCM-SHA256 : ECDHE-ECDSA-AES256-GCM-SHA384 : ECDHE-RSA-AES128-GCM -SHA256 : ECDHE-RSA-AES256-GCM-SHA384 : ECDHE-ECDSA-AES128-SHA256 : ECDHE-ECDSA-AES256-SHA384 : ECDHE-RSA-AES128-SHA256 : ECDHE-RS A-AES256-SHA384 : ECDHE-ECDSA-AES256-SHA : ECDHE-ECDSA-AES128-SHA : ECDHE-RSA-AES256-SHA : ECDHE-RSA-AES128-SHA : AES256-GCM-SHA38 4 : AES128-GCM-SHA256 : AES256-SHA256 : AES128-SHA256 : AES256-SHA : AES128-SHA ' ] . 2017-05-23 07:19:15.81 Server The maximum number dedicated administrator connections instance ' 1' 2017-05-23 07:19:15.81 Server Node configuration : node 0 : CPU mask : 0x0000000000000003:0 Active CPU mask : 0x0000000 000000003:0 . This message provides description NUMA configuration computer . This informational essage . No user action required . 2017-05-23 07:19:15.82 Server Using dynamic lock allocation . Initial allocation 2500 Lock blocks 5000 Lock Owner blocks per node . This informational message . No user action required . 2017-05-23 07:19:15.83 Server In-Memory OLTP initialized lowend machine . 2017-05-23 07:19:15.88 Server Database Instant File Initialization : enabled . For security performance considera tions see topic 'Database Instant File Initialization ' SQL Server Books Online . This informational message . No user action required . 2017-05-23 07:19:15.89 Server Query Store settings initialized enabled = 1 , 2017-05-23 07:19:15.90 spid7s Starting database 'master ' . 2017-05-23 07:19:15.91 Server Software Usage Metrics disabled . 2017-05-23 07:19:16.12 spid7s The tail log database master rewritten match new sector si ze 4096 bytes . 3584 bytes offset 418304 file /var/opt/mssql/data/mastlog.ldf written . 2017-05-23 07:19:16.18 spid7s Converting database 'master ' version 862 current version 868 . 2017-05-23 07:19:16.19 spid7s Database 'master ' running upgrade step version 862 version 863 . 2017-05-23 07:19:16.21 spid7s Database 'master ' running upgrade step version 863 version 864 . 2017-05-23 07:19:16.23 spid7s Database 'master ' running upgrade step version 864 version 865 . 2017-05-23 07:19:16.23 spid7s Database 'master ' running upgrade step version 865 version 866 . 2017-05-23 07:19:16.24 spid7s Database 'master ' running upgrade step version 866 version 867 . 2017-05-23 07:19:16.25 spid7s Database 'master ' running upgrade step version 867 version 868 . 2017-05-23 07:19:16.42 spid7s Buffer pool extension already disabled . No action necessary . 2017-05-23 07:19:16.42 spid7s Resource governor reconfiguration succeeded . 2017-05-23 07:19:16.43 spid7s SQL Server Audit starting audits . This informational message . No user ac tion required . 2017-05-23 07:19:16.43 spid7s SQL Server Audit started audits . This informational message . No user ac tion required . 2017-05-23 07:19:16.49 spid7s SQL Trace ID 1 started login `` sa '' . 2017-05-23 07:19:16.50 spid7s Server name '29e275920103 ' . This informational message . No user action required . 2017-05-23 07:19:16.50 spid7s The NETBIOS name local node running server '29e275920103 ' . This informational message . No user action required . 2017-05-23 07:19:16.52 spid19s Password policy update successful . 2017-05-23 07:19:16.52 spid22s Always On : The availability replica manager starting . This informational mes sage . No user action required . 2017-05-23 07:19:16.53 spid9s Starting database 'mssqlsystemresource ' . 2017-05-23 07:19:16.53 spid7s Starting database 'msdb ' . 2017-05-23 07:19:16.54 spid22s Always On : The availability replica manager waiting instance SQL Server allow client connections . This informational message . No user action required . 2017-05-23 07:19:16.55 spid9s The resource database build version 14.00.600 . This informational message ly . No user action required . 2017-05-23 07:19:16.57 spid9s Starting database 'model ' . 2017-05-23 07:19:16.88 spid7s The tail log database msdb rewritten match new sector size 4096 bytes . 512 bytes offset 52736 file /var/opt/mssql/data/MSDBLog.ldf written . 2017-05-23 07:19:16.94 spid7s Converting database 'msdb ' version 862 current version 868 . 2017-05-23 07:19:16.95 spid7s Database 'msdb ' running upgrade step version 862 version 863 . 2017-05-23 07:19:16.98 spid19s A self-generated certificate successfully loaded encryption . 2017-05-23 07:19:17.00 spid7s Database 'msdb ' running upgrade step version 863 version 864 . 2017-05-23 07:19:17.01 spid19s Server listening [ 0.0.0.0 < ipv4 > 1433 ] . 2017-05-23 07:19:17.02 Server Server listening [ 127.0.0.1 < ipv4 > 1434 ] . 2017-05-23 07:19:17.02 Server Dedicated admin connection support established listening locally port 1434 . 2017-05-23 07:19:17.02 spid9s The tail log database model rewritten match new sector siz e 4096 bytes . 2048 bytes offset 75776 file /var/opt/mssql/data/modellog.ldf written . 2017-05-23 07:19:17.02 spid19s SQL Server ready client connections . This informational message ; user action required . 2017-05-23 07:19:17.05 spid7s Database 'msdb ' running upgrade step version 864 version 865 . 2017-05-23 07:19:17.05 spid7s Database 'msdb ' running upgrade step version 865 version 866 . 2017-05-23 07:19:17.06 spid9s Converting database 'model ' version 862 current version 868 . 2017-05-23 07:19:17.06 spid9s Database 'model ' running upgrade step version 862 version 863 . 2017-05-23 07:19:17.07 spid7s Database 'msdb ' running upgrade step version 866 version 867 . 2017-05-23 07:19:17.08 spid7s Database 'msdb ' running upgrade step version 867 version 868 . 2017-05-23 07:19:17.09 spid9s Database 'model ' running upgrade step version 863 version 864 . 2017-05-23 07:19:17.10 spid9s Database 'model ' running upgrade step version 864 version 865 . 2017-05-23 07:19:17.10 spid9s Database 'model ' running upgrade step version 865 version 866 . 2017-05-23 07:19:17.10 spid9s Database 'model ' running upgrade step version 866 version 867 . 2017-05-23 07:19:17.11 spid9s Database 'model ' running upgrade step version 867 version 868 . 2017-05-23 07:19:17.22 spid9s Polybase feature disabled . 2017-05-23 07:19:17.22 spid9s Clearing tempdb database . 2017-05-23 07:19:17.71 spid9s Starting database 'tempdb ' . 2017-05-23 07:19:17.97 spid9s The tempdb database 1 data file ( ) . 2017-05-23 07:19:17.99 spid22s The Service Broker endpoint disabled stopped state . 2017-05-23 07:19:17.99 spid22s The Database Mirroring endpoint disabled stopped state . 2017-05-23 07:19:18.00 spid22s Service Broker manager started . 2017-05-23 07:19:18.14 spid7s Recovery complete . This informational message . No user action requir ed .","I issue . Luckily case linked instance set listen another static tcp port . So I could use workaround . If option change server linking , set tcp port _SQL Server Configuration manager - Network configuration - Protocols INST2 - TCP/IP - properties - IP Addresses - TCP port._ After I could add linked server like . Port number 1435 . EXEC master.dbo.sp_addlinkedserver @ server = N'Server20_inst2 ' , @ srvproduct=N '' , @ provider=N'SQLNCLI ' , @ datasrc=N'192.168.1.112,1435 ' , @ catalog=N'Test' EXEC master.dbo.sp_addlinkedsrvlogin @ rmtsrvname=N'Server20_inst2 ' , @ useself=N'False ' , @ locallogin=NULL , @ rmtuser=N'sa ' , @ rmtpassword= ' # # # # # # # # # # '"
Stackoverflow,"I using SQL.js SQLite chrome app , I loading external db file perform query , want save changes local storage make persistent , already define here- < https : //github.com/kripken/sql.js/wiki/Persisting-a-Modified-Database > using way defined article- function toBinString ( arr ) { var uarr = new Uint8Array ( arr ) ; var strings = [ ] , chunksize = 0xffff ; // There maximum stack size . We call String.fromCharCode many arguments want ( var i=0 ; i*chunksize < uarr.length ; i++ ) { strings.push ( String.fromCharCode.apply ( null , uarr.subarray ( i*chunksize , ( i+1 ) *chunksize ) ) ) ; } return strings.join ( `` ) ; } function toBinArray ( str ) { var l = str.length , arr = new Uint8Array ( l ) ; ( var i=0 ; < l ; i++ ) arr [ ] = str.charCodeAt ( ) ; return arr ; } save data storage - var data =toBinString ( db.export ( ) ) ; chrome.storage.local.set ( { `` localDB '' : data } ) ; get data storage- chrome.storage.local.get ( 'localDB ' , function ( res ) { var data = toBinArray ( res.localDB ) ; //sample example usage db = new SQL.Database ( data ) ; var result = db.exec ( `` SELECT * FROM user '' ) ; } ) ; Now make query , getting error - > Error : file encrypted database differnces storing values chrome.storage localStorage ? working fine using localStorage , find working example here- < http : //kripken.github.io/sql.js/examples/persistent.html > document sugggested - < https : //developer.chrome.com/apps/storage > n't need use stringify parse chrome.storage API unlike localStorage , directly saves object array . try save result return db.export without conversion , getting error- > Can serialize value JSON So please help guys approach save db export chrome's storage , anything wrong ?","` sql.Database ( ) ` needs called constructor ( _e.g._ ` db = new sql.Database ( ) ; ` ) . Note ` new ` , [ Usage example ] ( https : //github.com/kripken/sql.js/ # usage ) . var sql = window.SQL ; var db = new sql.Database ( ) ; sqlstr = `` CREATE TABLE hello ( int , b char ) ; '' ; sqlstr += `` INSERT INTO hello VALUES ( 0 , 'hello ' ) ; '' sqlstr += `` INSERT INTO hello VALUES ( 1 , 'world ' ) ; '' db.run ( sqlstr ) ; var res = db.exec ( `` SELECT * FROM hello '' ) ; console.log ( res ) ; < script src= '' https : //rawgit.com/kripken/sql.js/master/js/sql.js '' type= '' text/javascript '' > < /script >"
Stackoverflow,"To select N records per category one : SELECT category , category_id , value FROM ( SELECT category , value , row_number ( ) OVER ( PARTITION category ) category_id FROM myTable ) WHERE category_id < N ; The inner SELECT first partition records per category assign record per category id called category_id . The outer query use category_id limit number records queries per category . This **extremely inefficient BIG tables** going assigning ids records even though interested N records per category . The following work sql engine I working - sure works engine . SELECT category , value , row_number ( ) OVER ( PARTITION category ) category_id FROM myTable WHERE category_id < N Does anyone know ways achieving better time complexity ? More thoughts : Time profiling following algorithm query might provide insights query runs behind scene : 1 . SELECT DISTINCT ( category ) FROM myTable 2 . FOREACH category SELECT N rows * * * info : data physically partitioned ` category ` , able explicitly leverage would useful","As @ Lamak mentioned comment , avoid sorting rows table , reason stated . A sort required determine distinct categories result set partitioned , , absence explicit ordering within partition , row numbers easily determined side effect category sort . How query runs `` behind scenes '' , , using correct term , execution plan determined presence ( absence ) index might help avoid category sort . If covering index ` ( category , value ) ` , whatever columns need result , query would run much efficiently . In latter case simplified algorithm might look like : 1 . Read pre-sorted records containing necessary columns , including row numbers , index . 2 . Discard records row number greater ` n ` . Your `` ideal '' query > > SELECT category , value , row_number ( ) OVER ( PARTITION category ) > category_id FROM myTable WHERE category_id < N > probably would n't run SQL database , ` SELECT ` list processed _after_ ` WHERE ` clause predicates , ` category_id ` unknown predicates evaluated ."
Stackoverflow,I following batch code executes SQL files folder . /r % % G ( *.sql ) sqlcmd /S localhost /d superDB /U sa /P topsecret -i '' % % G '' pause But I want execute files start two digit number name like : 01_file.sql 02_file.sql 05_file.sql 43_file.sql But files like : optional_01_file.sql XYZ04_file.sql Can someone tell change code filter files file name starts 2 digits ?,Findstr 's rudimentary RegEx capabilities sufficient match files . ~~Un~~ **Tested : ** : : Q : \Test\2018\07\21\SO_51456661.cmd @ echo /r % % G ( *.sql ) ( Echo= % % ~nG|Findstr `` ^ [ 0-9 ] [ 0-9 ] _ '' 2 > & 1 > Nul & & ( echo sqlcmd /S localhost /d superDB /U sa /P topsecret -i '' % % G '' ) ) In folder files > dir /B *.sql 01_file.sql 0815_file.sql 55_file.sql abc0855_file.sql I get commands ( echoed demonstration ) > SO_51456661.cmd sqlcmd /S localhost /d superDB /U sa /P topsecret -i '' Q : \Test\2018\07\21\01_file.sql '' sqlcmd /S localhost /d superDB /U sa /P topsecret -i '' Q : \Test\2018\07\21\55_file.sql ''
Stackoverflow,"When creating new postgresql instance azure portal even CLI , username automatically made like user @ databasename ca n't use database string , funny enough Azure entire section connection strings , example **node.js** example postgres : // { your_username } : { your_password } @ { host_name } :5432/ { your_database } ? ssl=true So substitute values would end something like postgres : //user @ databasename : password @ host.azure.com:5432/database_name ? ssl=true Can see problem ? two ` @ ` , completely wrong , ` node.js ` can't parse , I problem ` ruby ` , valid url . How I change username remove @ part ?",In case OP solved problem upgrading Postgre tier implementing vnet rules allow traffic .
Stackoverflow,"I use ` mysql-client 5.55.9999 ` I would like know prevent mysql-client save passwords ` ~/.mysql_history ` ? Let 's say , I create new user : CREATE USER `` username '' @ '' host '' IDENTIFIED BY `` thePassword '' ; This statement put exactly ` ~/.mysql_history ` . This I n't want . I remember earlier versions mysql-client , rows including passwords put history file , I remember exact option settings .","The ` $ ` part shell variables , unintentionally get replaced . You escape ` $ ` character keep string literal ` $ ` . $ echo `` $ 1 $ Hat7oFty $ mA.L2vsQdD3MxvxAuDFKp0 '' .L2vsQdD3MxvxAuDFKp0 $ echo `` \ $ 1\ $ Hat7oFty\ $ mA.L2vsQdD3MxvxAuDFKp0 '' $ 1 $ Hat7oFty $ mA.L2vsQdD3MxvxAuDFKp0"
Stackoverflow,"INT ( 10 ) unsigned mysql limit 4b+ use getLong row , throws following error : java.lang.ClassCastException : java.lang.Integer cast java.lang.Long com.github.jasync.sql.db.RowData $ DefaultImpls.getLong ( RowData.kt:48 ) ~ [ jasync-common-0.9.23.jar : ? ] com.github.jasync.sql.db.general.ArrayRowData.getLong ( ArrayRowData.kt:5 ) ~ [ jasync-common-0.9.23.jar : ? ] com.richdevt.hthookserver.repository.ProjectRepository.findProjectEnvironment ( ProjectRepository.kt:29 ) ~ [ classes/ : ? ] com.richdevt.hthookserver.repository.ProjectRepository $ findProjectEnvironment $ 3.invokeSuspend ( ProjectRepository.kt ) ~ [ classes/ : ? ] kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith ( ContinuationImpl.kt:32 ) [ kotlin-stdlib-1.3.21.jar:1.3.21-release-158 ( 1.3.21 ) ] kotlinx.coroutines.DispatchedTask.run ( Dispatched.kt:233 ) [ kotlinx-coroutines-core-1.1.1.jar : ? ] io.netty.util.concurrent.AbstractEventExecutor.safeExecute $ $ $ capture ( AbstractEventExecutor.java:163 ) [ netty-common-4.1.33.Final.jar:4.1.33.Final ] io.netty.util.concurrent.AbstractEventExecutor.safeExecute ( AbstractEventExecutor.java ) [ netty-common-4.1.33.Final.jar:4.1.33.Final ] io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks ( SingleThreadEventExecutor.java:404 ) [ netty-common-4.1.33.Final.jar:4.1.33.Final ] io.netty.channel.nio.NioEventLoop.run ( NioEventLoop.java:495 ) [ netty-transport-4.1.33.Final.jar:4.1.33.Final ] io.netty.util.concurrent.SingleThreadEventExecutor $ 5.run ( SingleThreadEventExecutor.java:905 ) [ netty-common-4.1.33.Final.jar:4.1.33.Final ] io.netty.util.concurrent.FastThreadLocalRunnable.run ( FastThreadLocalRunnable.java:30 ) [ netty-common-4.1.33.Final.jar:4.1.33.Final ] java.lang.Thread.run ( Thread.java:748 ) [ ? :1.8.0_152 Why conversion possible ?","The type ` INT ( 10 ) UNSIGNED ` size int ( 4 bytes ) unsigned : [ What maximum size int ( 10 ) Mysql ] ( https : //stackoverflow.com/questions/10879345/what-is-the-maximum-size- of-int10-in-mysql ) However , Java type ( ie maximum value bigger maximum Java int ) . On hand , type returned MySQL INT defined : < https : //dev.mysql.com/doc/refman/8.0/en/integer-types.html > without additional info unsigned . In case [ jasync- sql ] ( https : //github.com/jasync-sql/jasync-sql ) converts regular Int type . More details : < https : //github.com/jasync-sql/jasync- sql/issues/102 > As workaround case possible widening conversion like suggested : [ Best way convert signed integer unsigned long ? ] ( https : //stackoverflow.com/questions/9578639/best-way-to-convert-a- signed-integer-to-an-unsigned-long )"
Stackoverflow,mysql supports passwordless login using stored local authentication credentials file named .mylogin.cnf ( see [ ] ( http : //dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html ) details ) . example : mysql -- login-path=local My questions : perl using DBD : :mysql ?,"So I finally found . Of course , locating libraries include files properly . So I went cpan directory system /Users/robert/.cpan/build/DBD-mysql-4.048-0 After checking brew installations openssl mysql , I woke . sudo perl Makefile.PL -- libs= '' -L/usr/local/opt/openssl/lib -lssl -lcrypto -L/usr/local/lib -L/usr/local/Cellar/mysql/8.0.12/lib -lmysqlclient '' Now . worked looking mysaql lib directory , I see library 'mysqlclient ' mysql . As see . Now worked Makefile.PL thing . Then 'make ' worked . As . Not showing errors ... Then **sudo make test worked** \- time . All tests successful . Files=73 , Tests=9 , 11 wallclock secs ( 0.24 usr 0.15 sys + 8.25 cusr 1.23 csys = 9.87 CPU ) Result : PASS Finally - make install . Worked zeus : DBD-mysql-4.048-0 robert $ sudo make install `` /usr/local/Cellar/perl/5.28.0/bin/perl '' -MExtUtils : :Command : :MM -e 'cp_nonempty ' -- mysql.bs blib/arch/auto/DBD/mysql/mysql.bs 644 Manifying 3 pod documents Files found blib/arch : installing files blib/lib architecture dependent library tree Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/auto/DBD/mysql/mysql.bundle Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/Bundle/DBD/mysql.pm Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql.pm Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql/INSTALL.pod Installing /usr/local/Cellar/perl/5.28.0/lib/perl5/site_perl/5.28.0/darwin-thread-multi-2level/DBD/mysql/GetInfo.pm Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/Bundle : :DBD : :mysql.3 Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/DBD : :mysql : :INSTALL.3 Installing /usr/local/Cellar/perl/5.28.0/share/man/man3/DBD : :mysql.3 Appending installation info /usr/local/Cellar/perl/5.28.0/lib/perl5/5.28.0/darwin-thread-multi-2level/perllocal.pod zeus : DBD-mysql-4.048-0 robert $ pwd /Users/robert/.cpan/build/DBD-mysql-4.048-0 Well finally . Now I want ? I ca n't remember ."
Stackoverflow,"Consider method angular service : select : function ( table ) { window.sqlitePlugin.openDatabase ( { name : 'smartLab.db ' , location : 'default' } , function success ( db ) { var defer = window.Q.defer ( ) ; console.dir ( defer ) ; db.executeSql ( 'SELECT * FROM ' + table , [ ] , function success ( rows ) { var resp = [ ] ; ( var = 0 ; < rows.rows.length ; i++ ) { resp.push ( rows.rows.item ( ) ) ; } defer.resolve ( resp ) ; } , defer.reject ) } , function error ( err ) { } ) } I calling another service following way : DatabaseService.select ( TBL_NAME ) .then ( function ( rows ) { // logic goes } ) ; The error I get : Uncaught TypeError : Can read property 'then ' undefined I never used Q familiar promises , anything jump inherently wrong ? Clearly something ...","You never returning promise defered object ( click [ ] ( https : //github.com/kriskowal/q/blob/master/README.md # using-deferreds ) ) select : function ( table ) { var defer = window.Q.defer ( ) ; window.sqlitePlugin.openDatabase ( { name : 'smartLab.db ' , location : 'default' } , function success ( db ) { console.dir ( defer ) ; db.executeSql ( 'SELECT * FROM ' + table , [ ] , function success ( rows ) { var resp = [ ] ; ( var = 0 ; < rows.rows.length ; i++ ) { resp.push ( rows.rows.item ( ) ) ; } defer.resolve ( resp ) ; } , defer.reject ) } , function error ( err ) { } ) ; return defer.promise ; }"
Stackoverflow,"I trying run large script creates table , inserts almost 15,000 rows . The table gets created fine , 833 INSERT , I get error : Error : Query empty ( 1065 ) Here 833rd INSERT statement ( one failing ) : INSERT INTO CLASSCODE ( CLASS_CODE , CLASS_CODE_NAME , RATE_GROUP , PROGRAM_NM , ST_CODE , EFF_DT , EXP_DT ) VALUES ( 10255 , `` Funeral Directors - incl PL Crematory - 10255 '' , 3 , `` Service '' , `` AZ '' , 19980801 , NULL ) ; I ca n't see syntax errors differences line , one works . FOr reference , example INSERT statement works fine : INSERT INTO CLASSCODE ( CLASS_CODE , CLASS_CODE_NAME , RATE_GROUP , PROGRAM_NM , ST_CODE , EFF_DT , EXP_DT ) VALUES ( 10425 , `` Frame Shop - Picture/Posters - 10425 '' , 2 , `` Retail '' , `` AZ '' , 19980801 , NULL ) ; The part puzzles error sounds like something would happen I populating new row using data another SELECT statement , coming empty . That case , though , INSERT statements using static data . My table definition looks like : CREATE TABLE CLASSCODE ( CLASS_CODE INTEGER NOT NULL , CLASS_CODE_NAME VARCHAR ( 60 ) NOT NULL , RATE_GROUP SMALLINT NOT NULL , PROGRAM_NM VARCHAR ( 20 ) NOT NULL , ST_CODE CHAR ( 2 ) , EFF_DT DATE , EXP_DT DATE ) I running script GUI MySQL Query Browser . Could something number rows I 'm trying insert ? Do I need periodically commit ? Is something simple I overlooking ? Thanks !","The common scenario run script file double semi-colon somewhere : INSERT INTO CLASSCODE ( CLASS_CODE , CLASS_CODE_NAME , RATE_GROUP , PROGRAM_NM , ST_CODE , EFF_DT , EXP_DT ) VALUES ( 10255 , `` Funeral Directors - incl PL Crematory - 10255 '' , 3 , `` Service '' , `` AZ '' , 19980801 , NULL ) ; ; I 'd quick search script see 's ` ; ; ` around line 833 ."
Stackoverflow,"Hi pretty basic question , I able find definitive answer . What IF evaluate NULL SQL ? What happen following code fieldXYZ NULL IF fieldXYZ=SomeValue //do something ELSE //do something else Update : I found one result suggests NULL considered falsy < https : //technet.microsoft.com/en-us/library/ms182587 ( v=sql.110 ) .aspx >","First , I think [ ` if..else ` ] ( http : //www.techonthenet.com/oracle/loops/if_then.php ) statement incomplete defining expression column ` fieldXYZ ` . Generally , write ` if..else ` statement , would want write expression compares column value something . You explicitly check null values want handle . Here gist syntax : -- This handles null check explicitly IF fieldXYZ IS NOT NULL AND fieldXYZ = 'somevalue' //do something ELSE //do something else Also , ` fieldXYZ ` passed ` NULL ` value statement , first condition met , ` else ` condition applies immediately scenario.Hope helps !"
Stackoverflow,"I want connect MySQL database machine Vapor 3 app . My current ` configure.swift ` file looks follows : try services.register ( FluentMySQLProvider ( ) ) ... let mysqlConfig = MySQLDatabaseConfig ( username : `` dev '' , password : `` '' , database : `` test '' ) let mysql = MySQLDatabase ( config : mysqlConfig ) var databases = DatabasesConfig ( ) databases.add ( database : mysql , : .mysql ) services.register ( databases ) This works fine . However , since I need add Model migration configuration , I also need add : var migrations = MigrationConfig ( ) migrations.add ( model : Posts.self , database : .mysql ) services.register ( migrations ) When running app time , I 'm seeing error saying : > Full authentication supported insecure connections . After research , seems error overcome changing password logic ` caching_sha2_password ` ` mysql_native_password ` . However , leaves error saying : > Unrecognized basic packet . How I fix ?","From MySQL 8 want use ` localhost ` ( unsecured connection ) need disable MySQL transport layer security . Use ` unverifiedTLS ` ` transport ` ` MySQLDatabaseConfig ` initializer . Your ` MySQLDatabaseConfig ` initializer look something like : let config = MySQLDatabaseConfig ( hostname : `` 127.0.0.1 '' , port : 3306 , username : `` dev '' , password : `` '' , database : `` test '' , transport : MySQLTransportConfig.unverifiedTLS ) It work fine configuration ."
Stackoverflow,"I created alerts notify us SQL Database DTU usage peaks , alerts still `` Not activated '' status : ! [ Azure Alert Notifications ] ( https : //i.stack.imgur.com/2uEd8.png ) The alert details also showing `` Not activated '' state : ! [ Azure Alert Details ] ( https : //i.stack.imgur.com/2AQ5S.png ) But alerts set `` Enabled '' : ! [ Azure Alert Enabled ] ( https : //i.stack.imgur.com/DyXTb.png ) **How I go activating ? **","To find list applicable/available metrics set particular resource , powerShell Command . You use powershell command get list metrics logical names . **Get-AzureRmMetricDefinition** For example , want lookup list metrics alerts elastic pool , simple use command , **Get-AzureRmMetricDefinition -ResourceId `` ElasticPoolResourceId '' ** You provide resourceID elastic Pool paramter . And give list applicable metrics setting alerts . Hope helps !"
Stackoverflow,There 's command optimize repair databases mysqlcheck -- user=root -- password=PASSWORD_FOR_ROOT -- auto-repair -- optimize -- all-databases How I skip database process ? -- skip-database=DATABASE_NAME seems n't work,"It 's ` -- optimize ` uses space . It basically rebuilds tables regain unused space ( data deleted ) , configured server [ innodb_file_per_table ] ( https : //dev.mysql.com/doc/refman/8.0/en/innodb- multiple-tablespaces.html ) This rarely necessary , skip . That said , really _all databases_ ? This also necessary . I one database I weekly check , purpose host solely verify backups work . Once week backup restored host , mysqlcheck verifies tables work . And 's . My boss would fire , I would servers production : )"
Stackoverflow,"I 'm trying implement incredibly basic use SQLite . I ` Button ` ` EditText ` . I want store contents ` EditText ` ` OnClick ` . I 'm following : < https : //developer.xamarin.com/guides/android/application_fundamentals/data/part_3_using_sqlite_orm/ > < https : //developer.xamarin.com/guides/xamarin-forms/application- fundamentals/databases/ > I get passed following starting code without getting subsequent errors : ` var db = new SQLiteConnection ( dbPath ) ; ` Error : > The type initializer 'SQLite.SQLiteConnection ' threw exception . Inner Exception : > System.Exception : This 'bait ' . You probably need add one SQLitePCLRaw.bundle_* nuget packages platform project . > SQLitePCL.Batteries_V2.Init ( ) [ 0x00000 ] < 9baed10c674b49e0b16322f238b8ecc1 > :0 SQLite.SQLiteConnection..cctor ( ) [ 0x00000 ] /Users/vagrant/git/src/SQLite.cs:169 } I 've installed NuGet package PCL Android projects . I see following packages installed : SQLitePCLRaw.provider.e_sqlite3.android SQLitePCLRaw.lib.e_sqlite3.android I 've tried installing : SQLitePCLRaw.bundle_e_sqlite3 As mentioned , code basic implementation possible : try { string dbPath = Path.Combine ( System.Environment.GetFolderPath ( System.Environment.SpecialFolder.Personal ) , `` TestDB-DEV.db3 '' ) ; var db = new SQLiteConnection ( dbPath ) ; db.CreateTable < PersonName > ( ) ; } I 've spent couple days tried numerous resources like : [ https : //forums.xamarin.com/discussion/87289/sqlite-net-pcl-bait- issue ] ( https : //stackoverflow.com/questions/41344485/xamarin-sqlite-this-is- the-bait ) ultimately success . Unfortunately , nonsense like `` works '' , `` sure I '' , '' clean/rebuild '' answers I 've seen , e.g . previous link , SO posts like [ Xamarin SQLite `` This 'bait ' '' ] ( https : //stackoverflow.com/questions/41344485/xamarin-sqlite-this-is- the-bait ) Here ` package.config ` Android project : < ? xml version= '' 1.0 '' encoding= '' utf-8 '' ? > < packages > < package id= '' sqlite-net-pcl '' version= '' 1.4.118 '' targetFramework= '' monoandroid60 '' / > < package id= '' SQLitePCLRaw.bundle_green '' version= '' 1.1.5 '' targetFramework= '' monoandroid60 '' / > < package id= '' SQLitePCLRaw.core '' version= '' 1.1.5 '' targetFramework= '' monoandroid60 '' / > < package id= '' SQLitePCLRaw.lib.e_sqlite3.android '' version= '' 1.1.5 '' targetFramework= '' monoandroid60 '' / > < package id= '' SQLitePCLRaw.provider.e_sqlite3.android '' version= '' 1.1.5 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.Animated.Vector.Drawable '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.Design '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.v4 '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.v7.AppCompat '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.v7.CardView '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.v7.MediaRouter '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.v7.RecyclerView '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Android.Support.Vector.Drawable '' version= '' 23.3.0 '' targetFramework= '' monoandroid60 '' / > < package id= '' Xamarin.Forms '' version= '' 2.4.0.282 '' targetFramework= '' monoandroid60 '' / > < /packages > Here package.config PCL project : < ? xml version= '' 1.0 '' encoding= '' utf-8 '' ? > < packages > < package id= '' sqlite-net-pcl '' version= '' 1.4.118 '' targetFramework= '' portable45-net45+win8+wpa81 '' / > < package id= '' SQLitePCLRaw.bundle_green '' version= '' 1.1.5 '' targetFramework= '' portable45-net45+win8+wpa81 '' / > < package id= '' SQLitePCLRaw.core '' version= '' 1.1.5 '' targetFramework= '' portable45-net45+win8+wpa81 '' / > < package id= '' Xamarin.Forms '' version= '' 2.3.4.247 '' targetFramework= '' portable45-net45+win8+wpa81 '' / > < /packages >","I hate put `` I n't know I fixed '' boat , that's happened . I started clean copy+pasted code repulled Nuget packages everything worked . Maybe I overlooked something initially , maybe version mismatch , I say . However , I tried adding dependencies mentioned Trevor problem still existed , I think I missing anything ."
Stackoverflow,"I 'm working ` Django Project ` localhost I would like use distant ` MySQL Database ` . My localhost IP : **172.30.10.XX** My MySQL distant server : **172.30.10.XX** In **Django settings.py** file , I wrote : DATABASES = { 'default ' : { 'ENGINE ' : 'django.db.backends.mysql ' , 'NAME ' : 'DatasystemsEC ' , 'USER ' : 'root ' , 'PASSWORD ' : '***** ' , 'HOST ' : '172.30.10.XX ' , 'PORT ' : '3306 ' , 'OPTIONS ' : { 'init_command ' : 'SET innodb_strict_mode=1 ' , } , } } My Database name : DatasystemsEC But , I run : ` python manage.py migrate ` , I get error : Traceback ( recent call last ) : File `` manage.py '' , line 22 , < module > execute_from_command_line ( sys.argv ) File `` /usr/local/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 367 , execute_from_command_line utility.execute ( ) File `` /usr/local/lib/python2.7/site-packages/django/core/management/__init__.py '' , line 359 , execute self.fetch_command ( subcommand ) .run_from_argv ( self.argv ) File `` /usr/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 294 , run_from_argv self.execute ( *args , **cmd_options ) File `` /usr/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 342 , execute self.check ( ) File `` /usr/local/lib/python2.7/site-packages/django/core/management/base.py '' , line 374 , check include_deployment_checks=include_deployment_checks , File `` /usr/local/lib/python2.7/site-packages/django/core/management/commands/migrate.py '' , line 61 , _run_checks issues = run_checks ( tags= [ Tags.database ] ) File `` /usr/local/lib/python2.7/site-packages/django/core/checks/registry.py '' , line 81 , run_checks new_errors = check ( app_configs=app_configs ) File `` /usr/local/lib/python2.7/site-packages/django/core/checks/database.py '' , line 10 , check_database_backends issues.extend ( conn.validation.check ( **kwargs ) ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/mysql/validation.py '' , line 9 , check issues.extend ( self._check_sql_mode ( **kwargs ) ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/mysql/validation.py '' , line 13 , _check_sql_mode self.connection.cursor ( ) cursor : File `` /usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 231 , cursor cursor = self.make_debug_cursor ( self._cursor ( ) ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 204 , _cursor self.ensure_connection ( ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 199 , ensure_connection self.connect ( ) File `` /usr/local/lib/python2.7/site-packages/django/db/utils.py '' , line 94 , __exit__ six.reraise ( dj_exc_type , dj_exc_value , traceback ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 199 , ensure_connection self.connect ( ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/base/base.py '' , line 171 , connect self.connection = self.get_new_connection ( conn_params ) File `` /usr/local/lib/python2.7/site-packages/django/db/backends/mysql/base.py '' , line 263 , get_new_connection conn = Database.connect ( **conn_params ) File `` /Library/Python/2.7/site-packages/MySQLdb/__init__.py '' , line 81 , Connect return Connection ( *args , **kwargs ) File `` /Library/Python/2.7/site-packages/MySQLdb/connections.py '' , line 193 , __init__ super ( Connection , self ) .__init__ ( *args , **kwargs2 ) django.db.utils.OperationalError : ( 2003 , `` Ca n't connect MySQL server '172.30.10.XX ' ( 61 ) '' ) On MySQL phpmyadmin , I : [ enter image description ] ( https : //i.stack.imgur.com/9SNMD.png ) I assume I need configure new user order register MacOSX localhost ? So I created new user named ` osx ` granted privileges . But still n't work . Thank could help","You class model_name ( models.Model ) : id = models.UUIDField ( primary_key=True , default=uuid.uuid4 , editable=True ) user = models.ForeignKey ( User ) foo = models.CharField ( max_length=51 ) def save ( self , *args , **kwargs ) : self.__class__.objects.filter ( user=self.user ) .count ( ) > =10 : return None return super ( model_name , self ) .save ( *args , **kwargs ) # return super ( ) .save ( *args , **kwargs ) python3.x You thing using ` forms ` updating ` Form.clean ` method def clean ( self ) : super ( MyForm , self ) .clean ( ) model_name.objects.filter ( user=self.cleaned_data [ 'user ' ] ) .count ( ) > =10 : raise forms.ValidationError ( `` You exceeded limit . '' )"
Stackoverflow,"I developing simple application Motorola MC32N0 WinCE7.0 using VS2008 SP1 .net Compact Framework 3.5 SQL Server CE 3.5SP1 . Whenever I try read database I get error . > A native exception occurred projectName.exe . Select Quit restart program select details information . When I go details I get > ExceptionCode : 0xc0000005 > ExceptionAddress : 0xc0000000 > Reading : 0xc0000000 > NativeMethods.GetKeyInfo ( IntPtrpTx , String pwszBase Table , IntPtr prgDbKeyInfo , Int32 cDbKeyInfo , IntPtr pError ) > SqlCeDataReader.FillMetaData ( SqlCeDataReader reader , Int32 resultType ) . . . continues . This database read code public List < Users > SelectByUserName ( string UserName ) { var list = new List < Users > ( ) ; using ( var command = EntityBase.CreateCommand ( Transaction ) ) { ( UserName ! = null ) { command.CommandText = `` SELECT * FROM Users WHERE UserName= @ UserName '' ; command.Parameters.Add ( `` @ UserName '' , SqlDbType.NVarChar ) ; command.Parameters [ `` @ UserName '' ] .Value = UserName ; } else command.CommandText = `` SELECT * FROM Users WHERE UserName IS NULL '' ; using ( var reader = command.ExecuteReader ( ) ) { list = fetchData ( reader ) ; } } return list ; } When reaches ` command.ExecuteReader ( ) ` exception occurs . After lot searching guys saying might mismatch SQL Server CE version I fix ? One file copied folder application deployed . How check 's version change version SQL Server CE used ?","From MSDN : [ The Dispose method ] Releases resources used DbDataReader calls Close . So seems sensible call dispose 're finished reader . Better still , wrap Using block forget ."
Stackoverflow,"I MySQL function **2 parameters** namely **user_id** **post_id** Here 's function : CREATE FUNCTION isliked ( pid INT , uid INT ) RETURN TABLE AS RETURN ( EXISTS ( SELECT 1 FROM likedata ld WHERE post_id = pid AND user_id = uid ) ) is_liked END I tried call query : SELECT posts.id , posts.title , isliked ( 111,123 ) FROM posts It returns following error : You error SQL syntax ; check manual corresponds MySQL server version right syntax use near 'RETURN TABLE AS RETURN ( EXISTS ( SELECT 1 FROM likedata ld WHERE post_id = pid AN ' line 2 It return results like < http : //sqlfiddle.com/ # ! 9/91040/5 > I'm new sql , help great , thanks advance","You complicating code , need dynamically generate sql code work anyway . Just create function takes field value json field value parameter need dynamic sql : DROP FUNCTION IF EXISTS get_select_expr ; CREATE FUNCTION get_select_expr ( field_name VARCHAR ( 255 ) , json_field_name varchar ( 255 ) ) RETURNS VARCHAR ( 255 ) DETERMINISTIC RETURN json_unquote ( json_extract ( field_name , concat ( SUBSTRING_INDEX ( json_unquote ( JSON_SEARCH ( field_name , 'one ' , 'pref . ' , json_field_name , `` , NULL , ' $ .f [ * ] .q ' ) ) , ' . ' , 2 ) , ' . ' , 'value ' ) ) ) ; SELECT get_select_expr ( my_table.response , 'field_1 ' ) AS field_1 FROM my_table ;"
Stackoverflow,I new phpmysqli . Here I trying achieve : I update based recommendations ; [ Database sample data ] ( https : //i.stack.imgur.com/hJryA.png ) I want display data one page separate tables student based sid . This I tried far ; < ? php include_once 'dbcon.php ' ; $ results = $ MySQLiconn- > query ( 'SELECT * FROM activitybook ' ) ; $ students = [ ] ; foreach ( $ results- > fetch_array ( ) $ activity ) { $ students [ $ activity [ 'sid ' ] ] [ ] = $ activity ; } foreach ( $ students $ sid= > $ activities ) { foreach ( $ activities $ activity ) { echo `` < table > < tr > < th > SID < /th > < th > Date < /th > < th > FName < /th > < th > LName < /th > < th > activity < /th > < th > time < /th > < th > score < /th > < /tr > < tr > < td > '' . $ sid . `` < /td > < td > '' . $ activity [ 'fname ' ] . `` < /td > < td > '' . $ activity [ 'lname ' ] . `` < /td > < td > '' . $ activity [ 'activity ' ] . `` < /td > < td > '' . $ activity [ 'atime ' ] . `` < /td > < td > '' . $ activity [ 'ascore ' ] . `` < /td > < /tr > < /table > '' ; } } ? > [ This I get ] ( https : //i.stack.imgur.com/BR3rH.png ) What trying achieve separate tables ` sid ` . [ This sample I want archive ] ( https : //i.stack.imgur.com/bGz1L.png ),"> I ca n't figure I 'm missing . Missing ? Nothing . Extra ? : ) ` mysqli_fetch_array ` take ` $ result ` , already knows ` $ conn ` ; ` mysqli_fetch_array ( $ result ) ` correct , adding ` $ conn ` confuses ."
Stackoverflow,"When parsing SQL query like : SELECT g.A , g.B , g.C FROM dbo.Goat g inner join dbo.Badger b g.A=b.A iterating TSqlParserToken Property called TokenType . I get following : SELECT Select WhiteSpace g Identifier . Dot A Identifier , Comma WhiteSpace g Identifier . Dot B Identifier , Comma WhiteSpace g Identifier . Dot C Identifier WhiteSpace FROM From WhiteSpace dbo Identifier . Dot Goat Identifier WhiteSpace g Identifier WhiteSpace inner Inner WhiteSpace join Join WhiteSpace dbo Identifier . Dot Badger Identifier WhiteSpace b Identifier WhiteSpace On WhiteSpace g Identifier . Dot A Identifier = EqualsSign b Identifier . Dot A Identifier I understand order parser return values need awareness underlying schema . Is possible feed parser underlying objects return aware set tokens ? The TSqlTokenType enum also values ‘ Table ’ , ’ Schema ’ , ‘ View ’ I ’ sure must possible . It ’ documentation scarce . Dan",RegEx wo n't get job done - definitely ... need real parser - like [ one ] ( http : //www.sqlparser.com/sql-parser-dotnet.php ) ( commercial ) .
Stackoverflow,"Note output `` stylized '' reads better SO . What I 've got ... ( sql/format ( - > ( sqlh/select : * ) ( sqlh/from : event ) ( sqlh/merge-where [ : : field_id [ `` 1673576 '' , `` 1945627 '' , `` 1338971 '' ] ] ) ( sqlh/merge-where [ : : layer [ `` fha.abs '' `` fha.rank '' `` fha.true-color '' ] ] ) ( sqlh/merge-order-by : field_id ) ( sqlh/merge-order-by : layer ) ( sqlh/merge-order-by : event_date ) ( sqlh/limit 5 ) ) ) = > [ `` SELECT * FROM event WHERE ( ( field_id ( ? , ? , ? ) ) AND ( layer ( ? , ? , ? ) ) ) ORDER BY field_id , layer , event_date LIMIT ? '' `` 1673576 '' `` 1945627 '' `` 1338971 '' `` fha.abs '' `` fha.rank '' `` fha.true-color '' 5 ] What I want ... ( sql/format ( - > ( sqlh/select : * ) ( sqlh/from : event ) ( sqlh/merge-where [ : : field_id [ `` 1673576 '' , `` 1945627 '' , `` 1338971 '' ] ] ) ( sqlh/merge-where [ : : layer [ `` fha.abs '' `` fha.rank '' `` fha.true-color '' ] ] ) ; ; ; n't work , conceptually I 'm looking ( sqlh/merge-order-by [ : field_id : layer : event_date ] ) ( sqlh/limit 5 ) ) ) = > [ `` SELECT * FROM event WHERE ( ( field_id ( ? , ? , ? ) ) AND ( layer ( ? , ? , ? ) ) ) ORDER BY ( field_id , layer , event_date ) LIMIT ? '' `` 1673576 '' `` 1945627 '' `` 1338971 '' `` fha.abs '' `` fha.rank '' `` fha.true-color '' 5 ] How I get HoneySQL emit SQL treats order clause compound key table using Primary Key ? It seems HoneySQL able `` right thing '' presented challenge clause like ... ( sql/format ( - > ( sqlh/select : * ) ( sqlh/from : event ) ( sqlh/merge-where [ : = [ : field_id : layer : event_date ] [ `` 1338971 '' `` fha.abs '' ( c/from-string `` 2011-08-02T10:54:55-07 '' ) ] ] ) ) ) = > [ `` SELECT * FROM event WHERE ( field_id , layer , event_date ) = ( ? , ? , ? ) '' `` 1338971 '' `` fha.abs '' # object [ org.joda.time.DateTime 0xe59f807 `` 2011-08-02T17:54:55.000Z '' ] ]","First need look format behavior ` order-by ` ( sql/format { : order-by [ : c1 : c2 ] } ) = > [ `` ORDER BY c1 , c2 '' ] ( sql/format { : order-by [ [ : c1 : desc ] : c2 ] } ) = > [ `` ORDER BY c1 DESC , c2 '' ] struct order-by generated . If look macro ` defhelper ` two things . 1. defrecord spec type 2. define function call mutimethod ` ( ( defmethod build-clause : order-by [ _ fields ] ( assoc : order-by ( collify fields ) ) ) ( defn order-by [ & args__14903__auto__ ] ( let [ [ m__14904__auto__ args__14903__auto__ ] ( ( plain-map ? ( first args__14903__auto__ ) ) [ ( first args__14903__auto__ ) ( rest args__14903__auto__ ) ] [ { } args__14903__auto__ ] ) ] ( build-clause : order-by m__14904__auto__ args__14903__auto__ ) ) ) ( alter-meta ! # 'order-by assoc : arglists ' ( [ fields ] [ fields ] ) ) ) ` The ` collify ` simple . ( defn collify [ x ] ( ( coll ? x ) x [ x ] ) ) So , need look ` defn order-by ` function . When call ` ( sqlh/merge- order-by { } [ : : b ] ) ` , ` args__14903__auto__ = ' ( { } [ : : b ] ) ` The first ` ` create two var ` m__14904__auto__ = { } ` ` args__14903__auto__ = ( rest args__14903__auto__ ) = ( [ : : b ] ) ` . So , I guess merge-order-by function wrong . I solve problem like . ( sql/format ( - > ( sqlh/select : * ) ( sqlh/from : event ) ( sqlh/merge-where [ : : field_id [ `` 1673576 '' , `` 1945627 '' , `` 1338971 '' ] ] ) ( sqlh/merge-where [ : : layer [ `` fha.abs '' `` fha.rank '' `` fha.true-color '' ] ] ) ( sqlh/merge-order-by [ : field_id : desc ] : layer : event_date ) ( sqlh/limit 5 ) ) )"
Stackoverflow,"Based example forking , I build little script : # ! /usr/bin/env python # -*- coding : utf-8 -*- import sqlanydb import os def child ( ) : conn = sqlanydb.connect ( uid='dba ' , pwd='sql ' , eng='somedb_IQ ' , dbn='somedb ' ) curs = conn.cursor ( ) curs.execute ( `` '' '' SELECT * FROM foobaa ; '' '' '' ) os.exit ( 0 ) def parent ( ) : True : newpid = os.fork ( ) newpid == 0 : child ( ) else : pids = ( os.getpid ( ) , newpid ) print `` parent : % , child : % '' % pids raw_input ( ) == ' q ' : break parent ( ) The intention database action inside seperate process ( big goal later run huge number queries time ) . But running script , I 'm getting : parent : 20580 , child : 20587 Traceback ( recent call last ) : File `` connectiontest.py '' , line 25 , < module > parent ( ) File `` connectiontest.py '' , line 19 , parent child ( ) File `` connectiontest.py '' , line 8 , child conn = sqlanydb.connect ( uid='dba ' , pwd='sql ' , eng='somedb_IQ ' , dbn='somedb ' ) File `` /usr/local/lib/python2.6/dist-packages/sqlanydb.py '' , line 461 , connect return Connection ( args , kwargs ) File `` /usr/local/lib/python2.6/dist-packages/sqlanydb.py '' , line 510 , __init__ self.handleerror ( *error ) File `` /usr/local/lib/python2.6/dist-packages/sqlanydb.py '' , line 520 , handleerror eh ( self , None , errorclass , errorvalue ) File `` /usr/local/lib/python2.6/dist-packages/sqlanydb.py '' , line 342 , standardErrorHandler raise errorclass ( errorvalue ) sqlanydb.OperationalError : Failed initialize connection object What I might miss ?","Since Sybase IQ based Sybase ASA , sure 're using proper keys credentials ? This ( albeit , old ) documentation looks like wants DSN DSF instead ENG DBN . < http : //dcx.sybase.com/1101/en/dbprogramming_en11/python-writing-open.html >"
Stackoverflow,"The window user details different Sql Server user I log . So I tried use pyodbc connect database using username ( Admin_JJack ) password . But connection show fails Window User ( Jack ) I n't know goes wrong . connection string : connection = pyodbc.connect ( `` Driver= { `` SQL Driver '' } ; '' `` Server= `` ServerName '' ; '' `` Database= '' DatabaseName '' ; '' `` UID= '' UserName '' ; '' `` PWD= '' Password '' ; '' `` Trusted_Connection=yes '' ) > pyodbc.InterfaceError : ( '28000 ' , `` [ 28000 ] [ Microsoft ] [ SQL Server Native Client 11.0 ] [ SQL Server ] Login failed user 'Jack ' . ( 18456 ) ( SQLDriverConnect ) ; How connect database using sql server authentication ?","Some details SQL Server mixed authentication . To enable , following : 1 . Connect DB server ( presumably via Windows Authentication 2 . Right click properties server icon 3 . On properties dialog go Security 4 . Select `` SQL Server Windows Authentication mode '' . Save . [ ! [ enter image description ] ( https : //i.stack.imgur.com/rzYgR.png ) ] ( https : //i.stack.imgur.com/rzYgR.png )"
Stackoverflow,"Not duplicate [ User-Defined Table Type insertion sometimes causing conversion error ] ( https : //stackoverflow.com/questions/6092390/user-defined- table-type-insertion-sometimes-causing-conversion-error `` User-Defined Table Type insertion sometimes causing conversion error '' ) I user defined table type : CREATE TYPE [ dbo ] . [ udtImport ] AS TABLE ( Name varchar ( 256 ) null Code varchar ( 32 ) null StartDate varchar ( 256 ) null EndDate varchar ( 256 ) null DateCreated datetime null The DateCreated field populated DB layer using ` DateTime.Now ( ) , ` fields imported table . When I import file date fields populated I get SQL error : > Conversion failed converting date and/or time character string . Intercepting generated code using SQL Profiler shows : DECLARE @ p1 dbo.udtImport ; INSERT INTO @ p1 VALUES ( N'Kit A1 ' , N'A002 ' , '2016-04-02 00:00:00.000 ' , '2016-10-10 00:01:00.000 ' , '2018-10-22 16:08:28.6823468 ' ) ; exec impSaveImport @ ImportList= @ p1 impSaveImport stored procedure one parameter : table type var straight insert table [ Import ] . No logic , triggers , reference tables . Executing code SSMS shows error expected . Trimming last 4 digits last DateTime field causes insert query succeed . So far good . When I import file StartDate EndDate fields empty , I get error , data successfully inserted Import table . When I intercept successful insert using profiler I get : DECLARE @ p1 dbo.udtImport ; INSERT INTO @ p1 VALUES ( N'Kit A1 ' , N'A002 ' , null , null , '2018-10-22 16:34:11.5243245 ' ) ; exec impSaveImport @ ImportList= @ p1 Keep mind query SUCCESSFULLY insert one row table Import . When I run latest query SSMS I get conversion error , **but ran without error within MVC app ! ** This last part stumped . How ? Project using MVC SQL2016 .","You could use ` DATETIME2 ` : CREATE TYPE [ dbo ] . [ udtImport ] AS TABLE ( Name varchar ( 256 ) null , Code varchar ( 32 ) null , StartDate varchar ( 256 ) null , -- datetime2 format EndDate varchar ( 256 ) null , -- datetime2 format DateCreated datetime2 null ) ; DECLARE @ p1 dbo.udtImport ; INSERT INTO @ p1 ( Name , Code , StartDate , EndDate , DateCreated ) VALUES ( N'Kit A1 ' , N'A002 ' , '2016-04-02 00:00:00.000 ' , '2016-10-10 00:01:00.000 ' , '2018-10-22 16:08:28.6823468 ' ) ; ** [ db < > fiddle demo ] ( https : //dbfiddle.uk/ ? rdbms=sqlserver_2017 & fiddle=1cefe235953d2330670186fead36c420 ) **"
Stackoverflow,Does DeepSQL require new MySQL installation I add existing environment ?,( Disclaimer : I work Deep ) DeepSQL installed pluggable storage engine MySQL . We currently support MySQL 5.5 5.6 well Percona Server 5.5 5.6 . The DeepSQL plugin designed install binary distributions mysq.com well ones Ubuntu IUS repositories . Details found < http : //deepis.com/product- documentation/deepsql-installation-guide-rel-330 >
Stackoverflow,"It seems like tremendously easy , I 've luck thus far .","I 'd like try break , work problem . There number things address . # # # Logic How going control uniqueness across tables ? There might ALBERT students _and_ administration . It might even person , student employee facility . I 've person . # # # Suggested SQL There joins tables , would turn result cartesion product , I 'm pretty sure could return true ALBERT logging NIKOLA 's password . I think probably meant use set operator select ... student ... union select ... admin ... The UNION _ALL_ means need check uniqueness , need [ extra sort ] ( http : //www.grassroots-oracle.com/2009/07/union-all-performance- accuracy.html ) . # # # Password protection To able UPPER password means 're storing clear text . People today inherit enough digital fluency passwords **should not** stored clear text . Ever . See article example set custom authentication APEX hashing passwords . A rare one amongst disappointing number hash passwords . This one also hashes username salt , better . < http : //barrybrierley.blogspot.com.au/2015/05/custom-authentication- in-apex.html > It also starts cover needs regarding user type . I 'm sure example APEX documentation , I could n't find . # # User Type - Authorisation Once established valid user , determine type user , control access various components using [ Authorization Schemes ] ( https : //docs.oracle.com/database/apex-18.1/HTMDB/providing-security- through-authorization.htm # HTMDB12004 ) . For flexible system , I would abstract use authorisation schemes control _privileges_ certain components , allocate _business roles_ , turn granted _users_ . This serves `` multiple types people '' . # # 'Record Exists ' check From early I learned something AskTom regarding checking existence rows , seems hold well across versions declare ln_exists pls_integer ; begin select count ( * ) ln_exists dual exists ( select null your_table -- whatever 're looking id = p_id ) ; return ln_exists = 1 ; -- true exists end ; Oracle knows spend least effort solving problem . Many variations select many rows database . # # Shared applications You _can_ actually define multiple entry points using different authentication application . < http : //www.grassroots-oracle.com/2014/04/shared- authentication-across-multiple-apex-apps.html >"
Stackoverflow,"running issues trying figure Azure Function ( node.js- based ) connect mysql database ( also hosted Azure ) . We 're using mysql2 following tutorials pretty much exactly ( < https : //docs.microsoft.com/en-us/azure/mysql/connect-nodejs > , similar ) Here 's meat call : const mysql = require ( 'mysql2 ' ) ; const fs = require ( 'fs ' ) ; module.exports = async function ( context , req ) { context.log ( 'JavaScript HTTP trigger function processed request . ' ) ; ( req.query.fname || ( req.body & & req.body.fname ) ) { context.log ( 'start ' ) ; var config = { host : process.env [ 'mysql_host ' ] , user : process.env [ 'mysql_user ' ] , password : process.env [ 'mysql_password ' ] , port:3306 , database : 'database_name ' , ssl : { ca : fs.readFileSync ( __dirname + '\\certs\\cacert.pem ' ) } , connectTimeout:5000 } ; const conn = mysql.createConnection ( config ) ; /*context.log ( conn ) ; */ conn.connect ( function ( err ) { context.log ( 'here ' ) ; ( err ) { context.error ( 'error connecting : ' + err.stack ) ; context.log ( `` shit broke '' ) ; throw err ; } console.log ( `` Connection established . `` ) ; } ) ; context.log ( 'mid ' ) ; conn.query ( 'SELECT 1+1 ' , function ( error , results , fields ) { context.log ( 'here ' ) ; context.log ( error ) ; context.log ( results ) ; context.log ( fields ) ; } ) ; Basically , running issue conn.connect ( function ( err ) ... n't return anything - error message , logs , etc . conn.query works similarly . Everything seems set properly , I n't even know look next resolve issue . Has anyone come across advice handle ? Thanks ! ! Ben","Yes , possible using [ Azure Python SDK ] ( https : //docs.microsoft.com/en-us/python/azure/python-sdk-azure- install ? view=azure-python ) . This might looking < https : //docs.microsoft.com/en- us/python/api/azure-mgmt- rdbms/azure.mgmt.rdbms.mysql.operations.servers_operations.serversoperations ? view=azure- python >"
Stackoverflow,"using ` global sql parser ( gsp ) ` extracting column sorting type order sql query extract condition SELECT employee_id , dept , name , age , salary FROM employee_info WHERE dept = 'Sales ' ID=1 ORDER BY salary , age DESC , ID ; I extracting column name extract order type > 1- extract order type ? > > 2- extract , sql ?","If _pSqlstmt_ _gsp_selectStatement_ * something like : ( pSqlstmt- > orderbyClause ! = nullptr ) { string sortType ; int colNumOrderBy = pSqlstmt- > orderbyClause- > items- > length ; ( int = 0 ; < colNumOrderBy ; i++ ) { gsp_orderByItem *field = reinterpret_cast < gsp_orderByItem * > ( gsp_list_celldata ( pSqlstmt- > orderbyClause- > items- > head ) ) ; //get order column name char *sortCol = gsp_node_text ( reinterpret_cast < gsp_node* > ( field- > sortKey ) ) ; ( field- > sortToken== nullptr ) { //ERROR } else { //find sorting type ( ASC/DESC ) sortType = sortType.substr ( 0 , field- > sortToken- > nStrLen ) ; } free ( sortCol ) ; pSqlstmt- > orderbyClause- > items- > head = pSqlstmt- > orderbyClause- > items- > head- > nextCell ; } }"
Stackoverflow,"I simple SQL query Elasticsearch I know returns less 100 rows results . How I get results ( i.e. , without using scroll ) ? I tried ` limit n ` clause works ` n ` less equal 10 n't work ` n ` great 10 . The Python code calling Elasticsearch SQL API . import requests import json url = 'http : //10.204.61.127:9200/_xpack/sql' headers = { 'Content-Type ' : 'application/json ' , } query = { 'query ' : `` ' select date_start , sum ( spend ) spend some_index campaign_id = 790 campaign_id = 490 group date_start `` ' } response = requests.post ( url , headers=headers , data=json.dumps ( query ) ) The query returns cursor ID . I tried feed cursor ID SQL API n't gave result . I also tried translated SQL query native Elasticsearch query using SQL translate API wrapped following Python code , n't work either . I still got 10 rows results . import requests import json url = 'http : //10.204.61.127:9200/some_index/some_doc/_search' headers = { 'Content-Type ' : 'application/json ' , } query = { `` size '' : 0 , `` query '' : { `` bool '' : { `` '' : [ { `` term '' : { `` campaign_id.keyword '' : { `` value '' : 790 , `` boost '' : 1.0 } } } , { `` term '' : { `` campaign_id.keyword '' : { `` value '' : 490 , `` boost '' : 1.0 } } } ] , `` adjust_pure_negative '' : True , `` boost '' : 1.0 } } , `` _source '' : False , `` stored_fields '' : `` _none_ '' , `` aggregations '' : { `` groupby '' : { `` composite '' : { `` size '' : 1000 , `` sources '' : [ { `` 2735 '' : { `` terms '' : { `` field '' : `` date_start '' , `` missing_bucket '' : False , `` order '' : `` asc '' } } } ] } , `` aggregations '' : { `` 2768 '' : { `` sum '' : { `` field '' : `` spend '' } } } } } } response = requests.post ( url , headers=headers , data=json.dumps ( query ) ) .json ( )","With [ elasticsearch-sql ] ( https : //www.elastic.co/products/stack/elasticsearch- sql ) , ` LIMIT 100 ` translate ` `` size '' : 100 ` [ traditional query DSL ] ( https : //www.elastic.co/guide/en/elasticsearch/reference/current/search- request-from-size.html ) . This return 100 matching results . Given request : POST _xpack/sql/translate { `` query '' : '' SELECT FlightNum FROM flights LIMIT 100 '' } The translated query : { `` size '' : 100 , `` _source '' : { `` includes '' : [ `` FlightNum '' ] , `` excludes '' : [ ] } , `` sort '' : [ { `` _doc '' : { `` order '' : `` asc '' } } ] } So syntax-wise , ` LIMIT N ` expect . As seeing results , likely something specific index , query , data . There setting ` index.max_result_window ` cap size query , defaults 10K also return error rather limiting results ."
Stackoverflow,"I seems valid syntax : SELECT A.email , A.handle foo FROM ( user_table A INNER JOIN ( klass_table K LEFT JOIN user_table B ON ( B.x = A.y ) ) ) I re-use alias ( user_table A x2 ) : SELECT A.email , A.handle foo FROM ( user_table A INNER JOIN ( klass_table K LEFT JOIN user_table A ON ( A.x = K.y ) ) ) I get error : > Not unique table/alias : ' A' Can anyone explain logic aliasing works case ? If 's table , need different alias ? Note nonsense queries - I concerned semantics/syntax rules .","This : SELECT A.email , A.handle foo FROM ( user_table A INNER JOIN ( klass_table K LEFT JOIN user_table B ON ( B.x = A.y ) ) ) selects 2 columns , table subquery ? What ` A ` ? ` A ` alias ` user_table ` exists inside subquery : ( user_table A INNER JOIN ( klass_table K LEFT JOIN user_table B ON ( B.x = A.y ) ) ) Outside subquery exist unless alias whole subquery like : ( user_table A INNER JOIN ( klass_table K LEFT JOIN user_table B ON ( B.x = A.y ) ) ) A Of course ` A ` previous ` A ` . The 1st ` A ` alias table ` user_table ` 2nd ` A ` alias subquery ."
Stackoverflow,"What right way color-code rows ` QTableView ` ? I 'm developing spreadsheet application color-code rows based specific value set one columns . I use ` QSqlRelationalTableModel ` ` QSqlRelationalDelegate ` ; , value determine color foreign key . Why ca n't simple following ? Any ideas ? model- > setData ( model- > index ( index.row ( ) , index.column ( ) ) , QBrush ( Qt : :red ) , Qt : :BackgroundRole ) ;","So show unreadable text PDF must return empty string ask ` Qt : :DisplayRole ` role return icon ask ` Qt : :DecorationRole ` role . I also recommend reading icon unique , I show : ***.h** private : QIcon icon ; ***.cpp** RelationalTableModelWithIcon : :RelationalTableModelWithIcon ( QObject *parent , QSqlDatabase db ) : QSqlRelationalTableModel ( parent , db ) { icon = QIcon ( `` : /icons/Art/Icons/Iynque-Flat-Ios7-Style-Documents-Pdf.ico '' ) ; } QVariant RelationalTableModelWithIcon : :data ( const QModelIndex & index , int role ) const { ( index.column ( ) == 3 ) { ( role == Qt : :DisplayRole ) return `` '' ; else ( role == Qt : :DecorationRole ) return icon ; } return QSqlRelationalTableModel : :data ( index , role ) ; }"
Stackoverflow,"I alter query ALTER TABLE Table_name ADD ( coumn1 DOUBLE , coumn2 DOUBLE ) ; I using this.namedParameterJdbcTemplate.execute ( alterTableSQL , namedValues , new PreparedStatementCallback < Boolean > ( ) { @ Override public Boolean doInPreparedStatement ( PreparedStatement ps ) throws SQLException , DataAccessException { return ps.execute ( ) ; } } ) ; While executing statement , throwing following exception . org.springframework.jdbc.BadSqlGrammarException : PreparedStatementCallback ; bad SQL grammar [ ALTER TABLE < sample > ADD ( coumn1 DOUBLE , coumn2 DOUBLE ) ; ] ; nested exception org.h2.jdbc.JdbcSQLException : Syntax error SQL statement `` ALTER TABLE TEMPLATE_1 ADD ( [ * ] coumn1 DOUBLE , coumn2 DOUBLE ) ; `` ; expected `` identifier '' ; SQL statement : ALTER TABLE template_1 ADD ( coumn1 DOUBLE , coumn2 DOUBLE ) ; [ 42001-160 ] How I proceed ?","OVER MariaDB keyword : < https : //mariadb.com/kb/en/library/window- functions-overview/ > Rename column something else . I would also strongly advise using database environments , otherwise tests detect bugs n't happen production , wo n't detect bugs happen production ."
Stackoverflow,"I 'm trying use query engine [ SQLike ] ( http : //www.thomasfrank.se/sqlike.html ) struggling basic concept . The JSON I 'm using data source comes PHP code , like : var placesJSON= < ? echo json_encode ( $ arrPlaces ) ? > ; Here 's sample JSON : var placesJSON= [ { `` id '' : '' 100 '' , '' name '' : '' Martinique '' , '' type '' : '' CTRY '' } , { `` id '' : '' 101 '' , '' name '' : '' Mauritania '' , '' type '' : '' CTRY '' } , { `` id '' : '' 102 '' , '' name '' : '' Mauritius '' , '' type '' : '' CTRY '' } , { `` id '' : '' 103 '' , '' name '' : '' Mexico '' , '' type '' : '' CTRY '' } , { `` id '' : '' 799 '' , '' name '' : '' Northern Mexico '' , '' type '' : '' SUBCTRY '' } , { `` id '' : '' 800 '' , '' name '' : '' Southern Mexico '' , '' type '' : '' SUBCTRY '' } , { `` id '' : '' 951 '' , '' name '' : '' Central Mexico '' , '' type '' : '' SUBCTRY '' } , { `` id '' : '' 104 '' , '' name '' : '' Micronesia , Federated States '' , '' type '' : '' CTRY '' } , { `` id '' : '' 105 '' , '' name '' : '' Moldova '' , '' type '' : '' CTRY '' } ] ; I understand ( via [ reference ] ( http : //www.thomasfrank.se/SQLike/ ) ) I first need unpack JSON like : var placesData = SQLike.q ( { Unpack : placesJSON , Columns : [ 'id ' , 'name ' , 'type ' ] } ) And next step would query results like : var selectedPlaces = SQLike.q ( { Select : [ '* ' ] , From : placesData , OrderBy : [ 'name ' , '|desc| ' ] } Lastly , display results browser I use something like : document.getElementById ( `` myDiv '' ) .innerHTML=selectedPlaces [ 0 ] .name This n't work . The error I get : selectedPlaces [ 0 ] .name undefined . I 'm pretty sure I 'm missing something simple . Any hints ?",return this.userData = username ^^^ ` = ` assignment since returning assignment true every record returned . return this.userData === username ; ^^^
Stackoverflow,I using sql-interactive-mode connect 2 databases : MySQL SQLite . I created yasnippets mysql ` yasnippets/sql-interactive-mode ` folder . For example add column MySQL I use following snippet : # -*- mode : snippet -*- # name : Add column # key : addcol # -- ALTER TABLE $ 1 ADD COLUMN \ ` $ 2\ ` $ 3 ; But SQLite uses different syntax . How I create different yasnippets different databases ?,"As explained [ ] ( https : //capitaomorte.github.io/yasnippet/snippet- development.html # sec-3-2 ) add arbitrary Emacs Lisp code ( enclosed backquotes ) ` yasnippet ` snippets evaluated expand . In ` sql-mode ` ` sql-interactive-mode ` variable called ` sql- product ` check determine type database ( ` mysql ` , ` sqlite ` , ` postgres ` , etc . ) currently working . That 's basically need know . Here example modify ` addcol ` snippet : # ... ALTER TABLE $ 1 ` ( ( eq sql-product 'mysql ) `` ADD '' `` FROB '' ) ` COLUMN \ ` $ 2\ ` $ 3 ; This expand ALTER TABLE $ 1 ADD COLUMN \ ` $ 2\ ` $ 3 ; ` mysql ` ALTER TABLE $ 1 FROB COLUMN \ ` $ 2\ ` $ 3 ; types databases ."
Stackoverflow,"In system , two similar ( ) databases . So I built sqlalchemy models : # base.py Base = declarative_base ( ) class T1 ( Base ) : __tablename__ = 't1' id = Column ( Integer , primary_key=True ) name = Column ( String ) # production1.py . import base class T1 ( base.T1 ) : status1 = Column ( String ) # production2.py . import base class T1 ( base.T1 ) : status2 = Column ( String ) # sessions.py engine1 = create_engine ( **production1_params ) session1 = scoped_session ( sessionmaker ( bind=engine1 ) ) engine2 = create_engine ( **production2_params ) session2 = scoped_session ( sessionmaker ( bind=engine2 ) ) Then I access different database : import production1 , production2 session1 ( ) .query ( production1.T1 ) session2 ( ) .query ( production2.T2 ) Now , I want build API system graphql . First , I inherit ` SQLAlchemyConnectionField ` support database switching . class SwitchableConnectionField ( SQLAlchemyConnectionField ) : def __init__ ( self , type , *args , **kwargs ) : kwargs.setdefault ( 'db_type ' , String ( ) ) super @ classmethod def get_query ( self , model , info , sort=None , **args ) : session = get_query ( args [ 'db_type ' ] ) query = session.query ( model ) ... But I want define nodes , I found definitions must : import production1 , production2 class Production1Node ( SQLAlchemyObjectType ) : class Meta : model = production1 , T1 interfaces = ( Node , ) class Production2Node ( SQLAlchemyObjectType ) : class Meta : model = production2.T1 interfaces = ( Node , ) There two nodes definitions support different databases . But I want something like : import base class ProductionNode ( SQLAlchemyObjectType ) : class Meta : model = base.T1 interfaces = ( Node , ) So I switch similar model run time . However , even though I try inherit ` Node ` , I ca n't implement . Does anyone know I ?","Based error message , seems ` project.models.site ` ( imported second snippet ` project.models import site site_model ` ) Python module rather subclass ` db.Model ` similar . Did perhaps mean import ` Site ` ( uppercase ) instead ` site ` ?"
Stackoverflow,"First , background info , maybe someone suggests better way I try . I need export SQLite database text file . For I use C++ chosen use CppSQLite lib . That I collecting create queries export every table data , problem tables like ` sqlite_sequence ` ` sqlite_statN ` . During import I create tables special purpose , main question , would affect stability tables gone ? Another part question . Is way export import SQLite database using CppSQLite SQLite lib C++ ? P.S . Solution copy database file appropriate particular situation .","Searching [ source code ] ( https : //github.com/neosmart/CppSQLite/blob/master/CppSQLite3.cpp ) shows ` sqlite3_exec ( ) ` called : int CppSQLite3DB : :execDML ( const char* szSQL ) ; But callback supported ; want read returned data , must use query object ."
Stackoverflow,"I 've got problem sorting entries JqGrid . Orderby seem work . I set breakpoint code I noticed , orderby n't change order elements . Any idea could wrong ? I 'm using LINQ SQL MySQL ( DbLinq project ) . My action code : public ActionResult All ( string sidx , string sord , int page , int rows ) { var tickets = ZTRepository.GetAllTickets ( ) .OrderBy ( sidx + `` `` + sord ) .ToList ( ) ; var rowdata = ( ticket tickets select new { = ticket.ID , cell = new String [ ] { ticket.ID.ToString ( ) , ticket.Hardware , ticket.Issue , ticket.IssueDetails , ticket.RequestedBy , ticket.AssignedTo , ticket.Priority.ToString ( ) , ticket.State } } ) .ToArray ( ) ; var jsonData = new { total = 1 , // 'll implement later page = page , records = tickets.Count ( ) , rows = rowdata } ; return Json ( jsonData , JsonRequestBehavior.AllowGet ) ; }",You probably want Skip first Take . ( Doing take first skip n't make much sense . )
Stackoverflow,"I 'm windows 10 docker version 1.9.1 using docker toolbox I wanted put quick postgres container , something I 've done dockerfile I laying around . FROM postgres ADD create-db.sql /tmp/ ADD drop_create_table.sql /tmp/ ADD db.sql /tmp/ ADD create-db.sh /docker-entrypoint-initdb.d/ It 's pretty simple . run resulting image . starts fine . However end says : > ... > > server started ALTER ROLE > > /docker-entrypoint-sh : running > > /docker-entrypoint-initdb.d/create-db.sh : No file directory If I try ` docker run -it < imagename > //bin/bash ` I see file indeed : > root @ xxxx : /docker-entrypoint-initdb.d # ls > > create-db.sh whenever I run tells 's . The container promptly stops n't find file , I ca n't try ssh running container .","` pg_ctl ` option ` -E ` . If want run ` initdb ` ` pg_ctl ` , need pass ` initdb ` options using ` -o ` e.g . ` pg_ctl initdb -D ... -o `` -E=UTF8 '' ` But 's lot easier call ` inidb ` directly : initdb -D=D : \testdata -E=UTF8 -U=postgres"
Stackoverflow,"So I 'm trying create table SQL insert values . However , I seem getting error : > [ Error Code : -12101 , SQL State : 42000 ] Syntax error , 'CHECK ' assumed missing > [ Error Code : -12233 , SQL State : 42000 ] The number insert values number object columns Here SQL code : CREATE TABLE Server ( Nummer INTEGER NOT NULL PRIMARY KEY ( Nummer ) ) ; INSERT INTO Server ( Nummer ) VALUES ( 1,2,3,4,5 ) ; So I want create table named ` Server ` primary key named ` nummer ` . Nummer values 1,2,3,4,5 UPDATE -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- So new code : CREATE TABLE Server ( Nummer INTEGER NOT NULL , PRIMARY KEY ( Nummer ) , ) ; INSERT INTO Server ( Nummer ) VALUES ( 1 ) ; INSERT INTO Server ( Nummer ) VALUES ( 2 ) ; INSERT INTO Server ( Nummer ) VALUES ( 3 ) ; INSERT INTO Server ( Nummer ) VALUES ( 4 ) ; INSERT INTO Server ( Nummer ) VALUES ( 5 ) ; I solved check problem simply putting comma every statement create section . But I got new problem error code : > [ Error Code : -12101 , SQL State : 42000 ] Syntax error , IDENTIFIER IDENTIFIER assumed missing","If want one row , DBMS 's version ` 10.1+ ` use ` fetch first ` : CREATE VIEW HighestValue AS SELECT s.dept , d.name , SUM ( s.quantity ) TotalQuantity FROM sale INNER JOIN dept ON d.number = s.dept GROUP BY s.dept , d.name ORDER BY TotalQuantity FETCH FIRST 1 ROW ONLY ;"
Stackoverflow,"I data : Id Name amount Comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- - 1 n1 421762 Hello 2 n2 421 Bye 3 n2 262 null 4 n2 5127 `` Each name may may extra rows null empty comments . How I group name sum ( amount ) ignores/absorbs null empty comments grouping shows 2 groups . Output I want : Id Name sum ( amount ) Comments -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- 1 n1 421762 Hello 2 n2 5180 Bye I ca n't figure . I hoped would ignore null/empty values I always end 4 groups select id , name , sum ( amount ) , comments table group id , name , comments","The reason provided count column value ` null ` . Instead , use ` count ( * ) ` : SELECT COUNT ( * ) FROM Table_Name WHERE [ Current_Status ] IS NULL * * * Sample data : current_status -- -- -- -- -- -- -- Processed Null Not Processed Null And difference two queries : **count ( current_status ) ** SELECT count ( current_status ) FROM table_name WHERE current_status IS NULL 0 **count ( * ) ** SELECT count ( * ) FROM table_name WHERE current_status IS NULL 2"
Stackoverflow,Assume table unique clustered index . We query row table using index key value . SQL Server use index seek find related data pages . But SQL Server read data pages physical disk file_id : page_id ? How know position disk sector/cluster ?,"SQL Server deal disk sector/cluster . It reads ` files ` logically divided ` pages ` ( 8Kb ) . ` Extent ` 8 consecutive pages . Every table ` IAM ` page ( ) associated extents allocated table listed ( 's bitmap page 1 means extent allocated object 0 means ) . SQL Server maintains internal pointers first IAM page first data page heap . Those pointers found system view ` sys.system_internals_allocation_units ` . In case clustered index data lives pages usual , still read using IAM page ( ) , also accessed `` ordered '' way using ` binary tree ` index . This means data ( index leaf pages ) server builds additional searching structure also consists pages , root page directs lower index levels base clustered index key . Conclusion : addresses presented index pages consist ` file_id : page_id ` , cluster/sector . Here find database structures organized : [ Inside Microsoft SQL Server 2008 T-SQL Querying ] ( https : //books.google.it/books ? id=FZlCAwAAQBAJ & pg=PT332 & lpg=PT332 & dq= % 22The % 20structure % 20is % 20called % 20a % 20heap % 20because % 20the % 20data % 20is % 20not % 20organized % 20in % 20any % 20order ; % 20rather % 22 & source=bl & ots=S6RYRc19Q3 & sig=FpxLMmHIdCrwd3nWsZj1ZA1pmTc & hl=en & sa=X & ved=0ahUKEwj9o9vVoIvXAhWE2hoKHVMJC- EQ6AEIKzAB # v=onepage & q= % 22The % 20structure % 20is % 20called % 20a % 20heap % 20because % 20the % 20data % 20is % 20not % 20organized % 20in % 20any % 20order % 3B % 20rather % 22 & f=false )"
Stackoverflow,"I set SQL Server EC2 instance NOT domain . I need SQL Server Agent run SSIS scripts , back ups , send DB Mail , . I set sql server user read / write db permissions , tried follow instructions < https : //docs.microsoft.com/en-us/sql/ssms/agent/configure-a-user-to-create- and-manage-sql-server-agent-jobs > After research Job needs run Proxy , requires Credential . When I try create new credential I click elipsis next Identity I get error '' The program open required dialog box determine whether computer named 'BLAHBLAHBLAH ' joined domain . Close message try . '' To clear I need account run SSIS jobs , back ups send occasional mail . What I wrong ? Thanks",In OrmLite ` [ AutoIncrement ] ` attribute specifying Auto Incrementing primary key . So ca n't used non-Primary Key column .
Stackoverflow,"Using ` F # ` , ` FsSql ` ` PostGres ` So I 'm using function let getSqlParameter value = let uniqueKey = Guid.NewGuid ( ) .ToString ( `` N '' ) let key = ( sprintf `` @ % '' uniqueKey ) ( key , Sql.Parameter.make ( key , value ) ) get parameter anything I pass dynamically Which I append query I get something like select * ( select * mytable ) innerQuery @ a29c575b69bb4629a9971dac2808b445 LIKE ' % @ 9e3485fdf99249e5ad6adb6405f5f5ca % ' Then I take collection pass ` Sql.asyncExecReader connectionManager query parameters ` The problem I 'm I n't run parameterization engine , works fine . When I , n't work . It returns empty sets . The thing I think column names ca n't parameterized . This problem 're coming client . Is way ?","This 're using checked-cast operator ` : > ` , compiler verify cast valid . In case , since know data ` obj ` ` int64 ` , tell compiler know better using unsafe-cast operator ` : ? > ` ."
Stackoverflow,"I trying run docker container [ microsoft/mssql-server- linux ] ( https : //hub.docker.com/r/microsoft/mssql-server-linux/ ) sudo docker run -d -e 'ACCEPT_EULA=Y ' -e 'SA_PASSWORD=SecertP @ ssW0rd ! ' -p 1433:1433 -- name TestName-SqlServer microsoft/mssql-server-linux The container start I inspcts logs : docker logs e2 I see following errors : > 2018-10-16 09:06:42.54 spid19s Error : 17182 , Severity : 16 , State : 1 . 2018-10-16 09:06:42.54 spid19s TDSSNIClient initialization failed error 0xffffffff , status code 0x80 . Reason : Unable initialize SSL support . > > 2018-10-16 09:06:42.55 spid19s Error : 17182 , Severity : 16 , State : 1 . 2018-10-16 09:06:42.55 spid19s TDSSNIClient initialization failed error 0xffffffff , status code 0x1 . Reason : Initialization failed infrastructure error . Check previous errors . > > 2018-10-16 09:06:42.56 spid19s Error : 17826 , Severity : 18 , State : 3 . > > 2018-10-16 09:06:42.56 spid19s Could start network library internal error network library . To determine cause , review errors immediately preceding one error log . > > 2018-10-16 09:06:42.56 spid19s Error : 17120 , Severity : 16 , State : 1 . > 2018-10-16 09:06:42.56 spid19s SQL Server could spawn FRunCommunicationsManager thread . Check SQL Server error log operating system error log information possible related problems .",
Stackoverflow,"I using Oracle Database 12.2 ORDS 18.3 . Is possible make HTTP POST request raw body ( parameters using ` application/x-www-form- urlencoded ` ` multipart/form-data ` ) ORDS , knowing ORDS used PL/SQL Gateway part , REST Data Services part . In scenario , ORDS standalone mode ( Jetty ) HTTP server . The HTTP client makes request external program ( Postman , Java , ... ) . The idea send JSON data could binary data like image something else . It looks like PL/SQL Gateway `` '' able invoke procedures using parameters ( parameter also course body ) . I able make request dummy parameter multipart form somehow simulate I want ( tested ` VARCHAR2 ` , ` CLOB ` ` BLOB ` ) I wanted know possibility exists .","To answer question , possible . We use parameter MIME type ` application/x-www-form-urlencoded ` ( ` multipart/form-data ` ) ."
Stackoverflow,"Please help figure one . I 've already tried deleting re- creating data files , flushing tables , restarting database entire server . InnoDB Force Recovery = 4 > mysql -u root Welcome MySQL monitor . Commands end ; \g . Your MySQL connection id 3 Server version : 5.5.16 MySQL Community Server ( GPL ) Copyright ( c ) 2000 , 2011 , Oracle and/or affiliates . All rights reserved . Oracle registered trademark Oracle Corporation and/or affiliates . Other names may trademarks respective owners . Type 'help ; ' '\h ' help . Type '\c ' clear current input statement . mysql > use wotstats_info_gold ; Database changed mysql > show tables ; Empty set ( 0.00 sec ) mysql > create table player_team_role ( id int ( 11 ) ) ; ERROR 1050 ( 42S01 ) : Table ' ` wotstats_info_gold ` . ` player_team_role ` ' already exists mysql > drop table player_team_role ; ERROR 1051 ( 42S02 ) : Unknown table 'player_team_role' mysql > flush tables ; Query OK , 0 rows affected ( 0.00 sec ) mysql > create table player_team_role ( id int ( 11 ) ) ; ERROR 1050 ( 42S01 ) : Table ' ` wotstats_info_gold ` . ` player_team_role ` ' already exists mysql >","Should ` fb-celebotd ` ( dash ) ` fb_celebotd ` ( underscore ) ? If table name dash , 'll quote table name : ` `` fb- celebotd '' ` . Otherwise dash treated minus sign tries subtract celebotd fb ( unknown ) ."
Stackoverflow,"I attempted apply solution posted [ How I search ( case- insensitive ) column using LIKE wildcard ? ] ( https : //stackoverflow.com/questions/2876789/case-insensitive-for- sql-like-wildcard-statement ) , I 'm given error > 1253 - COLLATION 'utf8_general_ci ' valid CHARACTER SET 'latin1' **How I apply case insensitivity ` latin1 ` table ? **",SELECT * FROM mytable WHERE mycolumn = CAST ( 'CamelCaseWord ' AS CHAR CHARACTER SET latin1 ) COLLATE latin1_general_ci See [ SQLFiddle ] ( http : //sqlfiddle.com/ # ! 2/d76b2/2 ) .
Stackoverflow,"I 'm using [ Mongoose ODM wrapper ] ( https : //mongoosejs.com/ ) NodeJS I 'm concerned injection attacks . Let 's assume I following schema : const UserSchema = new mongoose.Schema ( { userName : String , password : String } ) ; If I perform login request looks like following : router.post ( '/login ' , ( request , response ) = > { const userName = request.body.userName ; const password = request.body.password ; User.findOne ( { userName : userName } , function ( error , user ) { // ... check password , logic } ) ; } ) ; I would open injection attack following JSON payload always find user : { `` email '' : { `` $ gte '' : `` '' } , `` password '' : { `` $ gte '' : `` '' } } I 'm concerned password hashed user found prevents actual log I want make sure input sanitized attacker would n't even make point . I 'm aware [ mongo-sanitize ] ( https : //www.npmjs.com/package/mongo- sanitize ) NPM package referenced similar [ StackOverflow post ] ( https : //stackoverflow.com/questions/28710345/sanitize-user-input-in- mongoose ) appears remove JSON keys begin ' $ ' . I plan using anyway I never allow user submit raw , unparsed JSON . Is good practice case call toString ( ) userName assuming I correct ` null ` checks ? const userName = request.body.userName.toString ( ) ; That would eliminate query executed n't feel secure . I assume following better approach tries convert ` userName ` ` String ` : User.findOne ( { userName : { `` $ eq '' : userName } } , function ( error , user ) { // ... logic } ) ; I ca n't find anything concerning [ Model.findOne ( ) documentation ] ( https : //mongoosejs.com/docs/api.html # model_Model.findOne ) leads believe I 'm overlooking something . Any insight would appreciated . **Other References : ** 1 . < https : //blog.websecurify.com/2014/08/hacking-nodejs-and-mongodb.html > 2 . < https : //ckarande.gitbooks.io/owasp-nodegoat-tutorial/content/tutorial/a1_-_sql_and_nosql_injection.html >","Both JPQL injection SQL injection examples broader category [ Code Injection ] ( https : //en.wikipedia.org/wiki/Code_injection ) . _Any_ language parsed runtime susceptible Code Injection . JPQL [ Java Persistence Query Language ] ( https : //www.objectdb.com/java/jpa/query ) similar SQL syntax , fact written strings parsed runtime . > Building queries passing JPQL query strings directly createQuery method , shown , referred JPA dynamic query construction query string built dynamically runtime . When description says `` built dynamically runtime '' mean code formats JPQL query Java string , submits string parsed executed . Therefore code opportunity combine fixed strings variable content . Here 's example using parameters safely combine variable JPQL statement . This comes < https : //www.objectdb.com/java/jpa/query/parameter > **SAFE : ** TypedQuery < Country > query = em.createQuery ( `` SELECT c FROM Country c WHERE c.name = : name '' , Country.class ) ; return query.setParameter ( `` name '' , name ) .getSingleResult ( ) ; Here 's query written unsafe way , combining variable directly string . **UNSAFE : ** TypedQuery < Country > query = em.createQuery ( `` SELECT c FROM Country c WHERE c.name = ' '' + name + `` ' '' , Country.class ) ; Do n't use string concatenation form JPQL queries avoid . That 's unsafe content sneaks JPQL ."
Stackoverflow,"I 've scratching head past month I still ca n't figure going . The problem I serious memory leak C++ application running Windows Server 2008 , compiled using Visual Studio 2005 . This managed project . The application starts around 5-6MB ( according Task Manager ) starts exhibit symptoms failure around ~200MB mark . I know Task Manager crude tool , given scale leak seems OK use . I 've narrowed problem MySQL Database interaction . If application interact database , memory leaked . All database interactions use mysql++ . I 've followed build instructions man pages tangentsoft.net . We 've evaluated code thread safety ( , ensured thread uses mysqlpp object thread ) checked make sure destructors called dynamically generated objects created using 'new ' . Looking internet I keep seeing various reports users mysqlpp class indicate leak somewhere . In particular , discussion Win C API would leak mysqlpp used : < http : //www.phpmarks.com/6-mysql-plus/ffd713579bbb1c3e.htm > This discussion seems conclude fix , however , I try fixes application still leaks . I implemented version application cited thread , advice man pages added : int _tmain ( int argc , TCHAR* argv [ ] , TCHAR* envp [ ] ) { ( true ) { //Initialise MySQL API mysql_library_init ( 0 , NULL , NULL ) ; Sleep ( 50 ) ; //Connect Database . mysqlpp : :Connection c ; c.connect ( `` myDatabase '' , '' localhost '' , '' username '' , '' password '' ) ; Sleep ( 50 ) ; //Disconnect Database c.disconnect ( ) ; Sleep ( 50 ) ; //Free memory allocated heap thread c.thread_end ( ) ; Sleep ( 50 ) ; //Free memory allocated MySQL C API mysql_library_end ( ) ; Sleep ( 50 ) ; } return 1 ; } I added Sleep ( 50 ) throttle stage loop , function time `` settle '' . I know probably n't necessary least way I eliminate cause . Nevertheless , program leaks quite rapidly ( ~1mb per hour ) . I 've seen similar questions mine asked places , conclusions made : ( So 'm alone issue . It occurs mysqlpp class reputation usefulness must quite robust . Given case , I still ca n't see 've done wrong . Does anyone experience mysqlpp Visual Studio 2005 might shed light problem ? Cheers , Adam . **EDIT** I created another example using pointer , case c duplicated loop : //LEAKY int _tmain ( int argc , TCHAR* argv [ ] , TCHAR* envp [ ] ) { mysqlpp : :Connection * c ; ( true ) { mysql_library_init ( 0 , NULL , NULL ) ; c = new mysqlpp : :Connection ; Sleep ( 50 ) ; c- > connect ( `` myDatabase '' , '' localhost '' , '' username '' , '' password '' ) ; Sleep ( 50 ) ; c- > disconnect ( ) ; Sleep ( 50 ) ; c- > thread_end ( ) ; Sleep ( 50 ) ; mysql_library_end ( ) ; Sleep ( 50 ) ; delete c ; c = NULL ; } return 1 ; } This also leaks . I created control example based code , n't leak : //NOT LEAKY int _tmain ( int argc , TCHAR* argv [ ] , TCHAR* envp [ ] ) { char * ch ; ( true ) { mysql_library_init ( 0 , NULL , NULL ) ; //Allocate 4000 bytes ch = new char [ 4000 ] ; Sleep ( 250 ) ; mysql_library_end ( ) ; delete ch ; ch = NULL ; } return 1 ; } Note I also left calls MySQL C API prove cause leak . I created example using pointer without calls connect/ disconnect : //NOT LEAKY int _tmain ( int argc , TCHAR* argv [ ] , TCHAR* envp [ ] ) { mysqlpp : :Connection * c ; ( true ) { mysql_library_init ( 0 , NULL , NULL ) ; c = new mysqlpp : :Connection ; Sleep ( 250 ) ; mysql_library_end ( ) ; delete c ; c = NULL ; } return 1 ; } This n't leak . So difference use mysqlpp : :connect / disconnect methods . I 'll dig mysqlpp class try see whats . Cheers , Adam . **EDIT** Here example leaky code checks made . //LEAKY int _tmain ( int argc , TCHAR* argv [ ] , TCHAR* envp [ ] ) { mysqlpp : :Connection * c ; ( true ) { mysql_library_init ( 0 , NULL , NULL ) ; c = new mysqlpp : :Connection ; Sleep ( 50 ) ; ( c- > connect ( `` myDatabase '' , '' localhost '' , '' username '' , '' password '' ) == false ) { cout < < `` Connection Failure '' ; return 0 ; } Sleep ( 50 ) ; c- > disconnect ( ) ; Sleep ( 50 ) ; c- > thread_end ( ) ; Sleep ( 50 ) ; mysql_library_end ( ) ; Sleep ( 50 ) ; delete c ; c = NULL ; } return 1 ; } Cheers , Adam .","mysqlpp : :String ` operator int ( ) ` code snippet work . What problem ? If want explicit , use mysqlpp : :String's [ conv ] ( http : //tangentsoft.net/mysql++/doc/html/refman/classmysqlpp_1_1String.html # a12 ) function : int = futureItemsets [ j ] .conv < int > ( 0 ) ; timeFrameItemsets.push_back ( ) ;"
Stackoverflow,"I wanted open IntelliJ NonStop/SQLMX database using driver com.tandem.t4jdbc . After configuration connection successful , I also open table double click , looks like column immediately recognized label loaded instead name column header . When I connect I error : java.lang.NullPointerException com.tandem.t4jdbc.SQLMXResultSetMetaData.getColumnLabel ( SQLMXResultSetMetaData.java:145 ) com.intellij.database.remote.jdbc.impl.RemoteResultSetMetaDataImpl.getColumnLabel ( RemoteResultSetMetaDataImpl.java:69 ) sun.reflect.GeneratedMethodAccessor1.invoke ( Unknown Source ) sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) java.lang.reflect.Method.invoke ( Method.java:498 ) sun.rmi.server.UnicastServerRef.dispatch ( UnicastServerRef.java:346 ) sun.rmi.transport.Transport $ 1.run ( Transport.java:200 ) sun.rmi.transport.Transport $ 1.run ( Transport.java:197 ) java.security.AccessController.doPrivileged ( Native Method ) sun.rmi.transport.Transport.serviceCall ( Transport.java:196 ) sun.rmi.transport.tcp.TCPTransport.handleMessages ( TCPTransport.java:568 ) sun.rmi.transport.tcp.TCPTransport $ ConnectionHandler.run0 ( TCPTransport.java:826 ) sun.rmi.transport.tcp.TCPTransport $ ConnectionHandler.lambda $ run $ 0 ( TCPTransport.java:683 ) java.security.AccessController.doPrivileged ( Native Method ) sun.rmi.transport.tcp.TCPTransport $ ConnectionHandler.run ( TCPTransport.java:682 ) java.util.concurrent.ThreadPoolExecutor.runWorker ( ThreadPoolExecutor.java:1142 ) java.util.concurrent.ThreadPoolExecutor $ Worker.run ( ThreadPoolExecutor.java:617 ) java.lang.Thread.run ( Thread.java:745 ) sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer ( StreamRemoteCall.java:276 ) sun.rmi.transport.StreamRemoteCall.executeCall ( StreamRemoteCall.java:253 ) sun.rmi.server.UnicastRef.invoke ( UnicastRef.java:162 ) java.rmi.server.RemoteObjectInvocationHandler.invokeRemoteMethod ( RemoteObjectInvocationHandler.java:227 ) java.rmi.server.RemoteObjectInvocationHandler.invoke ( RemoteObjectInvocationHandler.java:179 ) com.sun.proxy. $ Proxy199.getColumnLabel ( Unknown Source ) sun.reflect.GeneratedMethodAccessor351.invoke ( Unknown Source ) sun.reflect.DelegatingMethodAccessorImpl.invoke ( DelegatingMethodAccessorImpl.java:43 ) java.lang.reflect.Method.invoke ( Method.java:498 ) com.intellij.execution.rmi.RemoteUtil.invokeRemote ( RemoteUtil.java:179 ) com.intellij.execution.rmi.RemoteUtil.access $ 300 ( RemoteUtil.java:39 ) com.intellij.execution.rmi.RemoteUtil $ RemoteInvocationHandler.invoke ( RemoteUtil.java:275 ) com.sun.proxy. $ Proxy200.getColumnLabel ( Unknown Source ) com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil $ ResultSetWrapper. < init > ( MetaDataUtil.java:86 ) com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil $ RemoteResultSetWrapper. < init > ( MetaDataUtil.java:224 ) com.intellij.database.introspection.jdbcMetadataWrappers.MetaDataUtil $ ResultSetWrapper.wrap ( MetaDataUtil.java:96 ) com.intellij.database.introspection.jdbcMetadataWrappers.ClosableIt $ ResultSetDelegateIt. < init > ( ClosableIt.java:253 ) com.intellij.database.introspection.jdbcMetadataWrappers.ClosableIt $ ResultSetClosableIt. < init > ( ClosableIt.java:184 ) com.intellij.database.introspection.jdbcMetadataWrappers.UserDefinedTypeIt. < init > ( UserDefinedTypeIt.java:17 ) com.intellij.database.introspection.jdbcMetadataWrappers.DatabaseMetaDataWrapper.userDefinedTypes ( DatabaseMetaDataWrapper.java:649 ) com.intellij.database.introspection.GenericIntrospector.introspectTypesInSchema ( GenericIntrospector.java:451 ) com.intellij.database.introspection.GenericIntrospector.lambda $ introspectSchemasByCatalogs $ 7 ( GenericIntrospector.java:214 ) java.lang.Iterable.forEach ( Iterable.java:75 ) com.intellij.database.introspection.GenericIntrospector.lambda $ forEachSchemaInCatalog $ 15 ( GenericIntrospector.java:249 ) com.intellij.database.introspection.GenericIntrospector.forEachCatalog ( GenericIntrospector.java:244 ) com.intellij.database.introspection.GenericIntrospector.forEachSchemaInCatalog ( GenericIntrospector.java:249 ) com.intellij.database.introspection.GenericIntrospector.introspectSchemasByCatalogs ( GenericIntrospector.java:211 ) com.intellij.database.introspection.GenericIntrospector.introspectSchemas ( GenericIntrospector.java:200 ) com.intellij.database.introspection.GenericIntrospector.lambda $ null $ 0 ( GenericIntrospector.java:157 ) org.jetbrains.dekaf.core.BaseFacade.inSession ( BaseFacade.java:125 ) com.intellij.database.introspection.GenericIntrospector.lambda $ introspectAuto $ 1 ( GenericIntrospector.java:151 ) com.intellij.database.model.impl.BaseModel.modify ( BaseModel.java:114 ) com.intellij.database.model.impl.BaseModel.modify ( BaseModel.java:99 ) com.intellij.database.model.impl.BaseModel.modify ( BaseModel.java:84 ) com.intellij.database.introspection.GenericIntrospector.introspectAuto ( GenericIntrospector.java:151 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.introspectDatabases ( DatabaseModelLoader.java:419 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.lambda $ null $ 2 ( DatabaseModelLoader.java:312 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.withFacade ( DatabaseModelLoader.java:533 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.lambda $ introspect $ 3 ( DatabaseModelLoader.java:295 ) com.intellij.database.dataSource.DataSourceSyncManager.lambda $ null $ 0 ( DataSourceSyncManager.java:40 ) com.intellij.database.dataSource.DatabaseConnectionManager $ Executor.perform ( DatabaseConnectionManager.java:363 ) com.intellij.database.dataSource.DatabaseConnectionManager $ Executor.lambda $ sync $ 2 ( DatabaseConnectionManager.java:302 ) com.intellij.database.dataSource.AsyncUtil.withAsyncFriendly ( AsyncUtil.java:158 ) com.intellij.database.dataSource.DatabaseConnectionManager $ Executor.sync ( DatabaseConnectionManager.java:298 ) com.intellij.database.dataSource.DatabaseConnectionManager $ Builder.sync ( DatabaseConnectionManager.java:112 ) com.intellij.database.dataSource.DataSourceSyncManager.lambda $ static $ 1 ( DataSourceSyncManager.java:39 ) com.intellij.database.dataSource.DataSourceSyncManager $ SyncProcessor $ 1.perform ( DataSourceSyncManager.java:242 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.introspect ( DatabaseModelLoader.java:292 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.lambda $ run $ 0 ( DatabaseModelLoader.java:272 ) com.intellij.database.dataSource.LocalDataSource.performBatch ( LocalDataSource.java:1184 ) com.intellij.database.dataSource.DatabaseModelLoader $ IntrospectionSession.run ( DatabaseModelLoader.java:270 ) com.intellij.database.dataSource.DataSourceSyncManager $ SyncProcessor.performSync ( DataSourceSyncManager.java:244 ) com.intellij.database.dataSource.AsyncUtil.lambda $ null $ 6 ( AsyncUtil.java:55 ) com.intellij.database.dataSource.AsyncUtil.lambda $ underProgress $ 14 ( AsyncUtil.java:127 ) com.intellij.openapi.progress.impl.CoreProgressManager.a ( CoreProgressManager.java:543 ) com.intellij.openapi.progress.impl.CoreProgressManager.executeProcessUnderProgress ( CoreProgressManager.java:488 ) com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress ( ProgressManagerImpl.java:94 ) com.intellij.database.dataSource.AsyncUtil.underProgress ( AsyncUtil.java:133 ) com.intellij.database.dataSource.AsyncUtil.underProgress ( AsyncUtil.java:127 ) com.intellij.database.dataSource.AsyncUtil.lambda $ captureIndicator $ 7 ( AsyncUtil.java:55 ) java.util.concurrent.CompletableFuture $ AsyncSupply.run ( CompletableFuture.java:1590 ) com.intellij.openapi.application.impl.ApplicationImpl $ 1.run ( ApplicationImpl.java:315 ) java.util.concurrent.Executors $ RunnableAdapter.call ( Executors.java:511 ) java.util.concurrent.FutureTask.run ( FutureTask.java:266 ) java.util.concurrent.ThreadPoolExecutor.runWorker ( ThreadPoolExecutor.java:1142 ) java.util.concurrent.ThreadPoolExecutor $ Worker.run ( ThreadPoolExecutor.java:617 ) java.lang.Thread.run ( Thread.java:745 ) ( stack trace ) . java.lang.NullPointerException","This might help , althought standard SQL im sure much variation comes sqlmx SELECT TableName = t.NAME , TableSchema = s.Name , RowCounts = p.rows FROM sys.tables INNER JOIN sys.schemas ON t.schema_id = s.schema_id INNER JOIN sys.indexes ON t.OBJECT_ID = i.object_id INNER JOIN sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id WHERE t.is_ms_shipped = 0 GROUP BY t.NAME , s.Name , p.Rows ORDER BY s.Name , t.Name Obviously example , replace example data table info"
Stackoverflow,"I started HyperSQL like : java -cp hsqldb.jar org.hsqldb.server.Server -- database.0 file : /data/db -- dbname.0 some_db Then I try connect like : DriverManager.getConnection ( `` jdbc : hsqldb : hsql : //localhost/some_db '' , `` SA '' , `` '' ) ; Everything works fine . Now I add new user like : CREATE USER new_user PASSWORD `` some_password '' ADMIN ; I connect HyperSQL server using new user data ( also restarting ) : DriverManager.getConnection ( `` jdbc : hsqldb : hsql : //localhost/some_db '' , `` new_user '' , `` some_password '' ) ; Any suggestions ?","Why n't try locally ? I ca n't seem find whether limit amount login attempts , could try local instance database something like : ( int = 1 ; < = 1000 ; i++ ) { System.out.println ( `` Trying log , try number `` + ) ; DriverManager.getConnection ( `` jdbc : hsqldb : hsql : //localhost/fsdb '' , `` SA '' , `` some_password '' ) ; try { Thread.sleep ( 100 ) ; } catch ( InterruptedException ex ) { System.out.println ( `` Sleep interrupted . `` ) ; Thread.currentThread ( ) .interrupt ( ) ; } } **Edit : ** database server probably open connections external IP 's anyways . So 's reason allow external IP 's make connections database server , make whitelist help [ guide ] ( http : //hsqldb.org/doc/guide/listeners-chapt.html # lsc_acl ) ."
Stackoverflow,"I created ` SQLite3 ` database protected password ( `` test '' ) thanks application ` DB browser SQLite ` . In order connect database via ` Python ` , I need provide password I ca n't figure . I tried following code : conn=sqlite3.connect ( `` mydatabase.db '' , Password= '' test '' ) cur=conn.cursor ( ) **EDIT : ** My ` SQLite3 ` database encrypted ` SQLCipher ` ( see image ) . [ ! [ enter image description ] ( https : //i.stack.imgur.com/kSXYf.png ) ] ( https : //i.stack.imgur.com/kSXYf.png ) If I run following code : conn=sqlite3.connect ( `` mydatabase.db '' ) cur=conn.cursor ( ) I get error : sqlite3.DatabaseError : file encrypted database How I pass password order connect ` db ` via Python ? **EDIT 2** Here brief summary I try achieve . I developing application ` Python 3 ` requiring pre-populated database database needs protected password . After extensive research , seems complicated connect encrypted ` SQLite3 ` database via ` Python 3 ` . A library calls ` pysqlcipher ` exists Python 2.7 . My next question maybe broadly I apology advance . Is another portable database exists allowing protect password still get access Python ? Another idea I mind order troubleshoot problem use ` zipfile ` library . This [ link ] ( https : //www.sqlite.org/zipfile.html ) mentions ` zipfile ` module support encryption ’ clear encryption refers ` SQLite3 ` database zip file . The idea would zip unprotected ` DB ` protected zip file seems I ( [ link ] ( https : //docs.python.org/3/library/zipfile.html # zipfile.ZipFile.setpassword ) ) . The goal edit get new ideas solve problem . Thanks",You need [ SQLCipher ] ( https : //www.zetetic.net/sqlcipher/ ) module read database . The default SQLite3 module support . See < https : //github.com/sqlitebrowser/sqlitebrowser/wiki/Encrypted-Databases >
Stackoverflow,"We 're using [ gorm ] ( https : //github.com/jinzhu/gorm ) I 'd like able specify database specific annotations . For convenience , development/test use sqlite3 database , MySQL production . Unfortunately sqlite3 n't accept ` CHARACTER SET ` ` COLLATE ` keywords . Which means following breaks : type User struct { Name string ` gorm : '' primary_key ; type : varchar ( 200 ) CHARACTER SET utf8 COLLATE utf8_general_ci '' ` } Has anyone found work around ? I 'd rather use mysql test I 'd also rather manually manage columns .","The usual pattern 're trying achieve uses [ ` WaitGroup ` ] ( https : //golang.org/pkg/sync/ # WaitGroup ) . I think problem 're facing ` ` captured goroutine keeps getting incremented outer loop . Your inner loop starts ` ` since outer loop continued , goroutine starts 5 . Try passing iterator parameter goroutine get new copy time . func someFunction ( ) { ... . gos : = 5 var wg sync.WaitGroup wg.Add ( gos ) : =0 ; < gos ; i++ { go func ( n int ) { defer wg.Done ( ) j : =n ; j < len ( tables ) ; j+=gos { r , err : = db.Exec ( tables [ j ] ) fmt.Println ( r ) err ! = nil { methods.CheckErr ( err , err.Error ( ) ) } } } ( ) } wg.Wait ( ) ; } I 'm sure 're trying achieve , goroutine ` db.Exec ` tables one started first one treats tables , second one treats first one . Is intended ?"
Stackoverflow,"I tried prepare ` sqlite3_prepare_v2 ` ` sqlite3_step ` following two statements : @ '' delete students ; @ '' insert students select * buffer.students ; '' ` 'buffer ' ` database attached source . However , guarantee two statements executed bundle .",You get path database using : NSString *path = [ [ NSBundle mainBundle ] pathForResource : @ '' someDB '' ofType : @ '' sqlite '' ] ; Change parameters match database . And provide path framework access .
Stackoverflow,"I trying connect mysqlshell client linux command line I try connect , using mysqlsh mysql -h IP -u user -ppassword -e `` show databases '' mysqlsh mysql -h IP : port -u user -ppassword -e `` show databases '' mysqlsh mysql -h IP : port -u -port 3306 user -ppassword -e `` show databases '' I get error . Conflicting options : provided host differs host URI . Could please help ?",The proper format ` mysqlsh ` bit different : mysqlsh -- user=user -- password=password -- port=5721 -- host < hostname IP >
Stackoverflow,"I using [ Roland Bock 's sqlpp11 ] ( https : //github.com/rbock/sqlpp11 ) library ` mysql ` queries [ Howard Hinnant's date ] ( https : //github.com/HowardHinnant/date ) library operation date project . And getting following error one update query . /usr/local/include/sqlpp11/rhs_wrap.h : In instantiation ‘ struct sqlpp : :rhs_wrap_t < date : :year_month_day , false > ’ : /usr/local/include/sqlpp11/assignment.h:63:12 : required ‘ struct sqlpp : :assignment_t < sqlpp : :column_t < changestreet : :Goals , changestreet : :Goals_ : :GoalEndDate > , date : :year_month_day > ’ sqlOperations/sqlppDbConnection.cpp:286:65 : required ‘ bool setEmergencyFundGoal ( T1 , T2 , T2 , T3 , T3 ) [ T1 = int ; T2 = const char* ; T3 = double ] ’ main.cpp:705:113 : required /usr/local/include/sqlpp11/rhs_wrap.h:119:43 : error : type named ‘ _traits ’ ‘ class date : :year_month_day ’ using _traits = typename Expr : :_traits ; And update statement auto efGoal = db_cs.run ( update ( g ) .set ( g.goalAmount = emergencyFund , g.goalEndDate = contributionEndDate , // Line number 286 g.goalContributionStartDate = currentDate ( ) , g.goalContributionEndDate = contributionEndDate , g.goalInitialContribution = initialContribution , g.goalMaximumAchievableAmount = emergencyFund , g.goalCreatedOn = currentDateTime ( ) , g.goalUpdatedOn = currentDateTime ( ) ) .where ( g.goalName == goalName g.goalType == goalType g.usersUserId == userId ) ) ; And value used rhs auto contributionEndDate = lastDateOfMonth ( currentDate ( ) , date : :months { contributionTenure } ) ; Here definition contribution ` lastDateOfMonth ( ) ` function . date : :year_month_day lastDateOfMonth ( date : :year_month_day givenDate , date : :months monthsNum ) { date : :year_month_day newDate = year_month_day { givenDate } + monthsNum ; newDate = newDate.year ( ) /newDate.month ( ) /last ; return newDate ; } And ` currentDate ( ) ` function date : :year_month_day currentDate ( ) { auto currentTime = system_clock : :now ( ) ; auto currentDate = floor < days > ( currentTime ) ; return currentDate ; } Column name ` goal_end_date ` type ` DATE ` mysql table structure .",I needed change mysql datatype ` DATE ` ` DATETIME ` ` system_clock : :time_point ` type variable assignment variables works like charm . It also work mysql ` DATE ` dataype variable assignment type ` system_clock : :day_point ` . P.S : I needed modify two given functin return type ` syste_clock : :time_point ` .
Stackoverflow,"We 're trying use [ SqlDependencyEx ] ( https : //github.com/dyatchenko/ServiceBrokerListener '' SqlDependencyEx '' ) track updates made database table . This create update listener : var listener = new SqlDependencyEx ( ConfigUtils.GetConnectionString ( ) , ConfigUtils.GetDbName ( ) , `` codeTriggers '' , listenerType : SqlDependencyEx.NotificationTypes.Update ) ; Here update statement manually trigger sql management studio : UPDATE codeTriggers SET type = 'clearListsCache' WHERE type = 'clearCache' **PROBLEM** We get notifications . What missing ? When define listener follows ( without specifying type notification type ) : ` var listener = new SqlDependencyEx ( ConfigUtils.GetConnectionString ( ) , ConfigUtils.GetDbName ( ) , `` codeTriggers '' ) ; ` run update statement ( updating column different value actual ) , get Insert Delete notifications . Is expected behavior ?","Problem solved , here´s code : private const string CONNECTION_STRING = `` Server=LFTCMCPTP83 ; Database=Database ; Trusted_Connection=True ; MultipleActiveResultSets=true ; Integrated Security=false ; User ID=used_id ; Password=password '' ; private const string DATABASE_NAME = `` db_name '' ; private const string TABLE_NAME = `` table_name '' ; private const string SCHEMA_NAME = `` dbo '' ; private SqlDependencyEx sqlDependency = new SqlDependencyEx ( CONNECTION_STRING , DATABASE_NAME , TABLE_NAME , SCHEMA_NAME ) ; listener.TableChanged += ( , args ) = > { //Code ... } ; listener.Start ( ) ; listener.Stop ( ) ; Best Regards"
Stackoverflow,"SQL/MP SQL dialect used proprietary HP NonStop SQL database systems go back 1980ies ... ( Who ever worked system old , huh ? : ) ) SQL/MP somewhat ANSI compliant , example ` CURRENT_DATE ` exist . I however hard time figuring functions _do_ exist help get current date queries . Can anyone help dinosaur SQL language ?","Try CURRENT CURRENT_TIMESTAMP depending context . The HP NonStop SQL/MP Reference Manual excellent sort thing - one advantage using older product , documentation good ."
Stackoverflow,"documentation seems somewhat date ( last edit 2007 ) , quick look related forums shows references problem : < https : //rt.cpan.org/Public/Bug/Display.html ? id=90700 > anyone using SQLFairy visualize modern version SQLServer ? Is supported/meant work ?",
Stackoverflow,"I 'm writing [ CustomSqlChange ] ( https : //www.liquibase.org/javadoc/liquibase/change/custom/CustomSqlChange.html ) first time want test outcome running current database . Of course I could start application execute change sets via liquibase ( including one executes ` CustomSqlChange ` ) , takes lot time . Is way manually execute java class implementing ` CustomSqlChange ` IDE ( IntelliJ ) would liquibase ? Could one maybe even debug execution ?","You create separate changelog file , 's custom change included . Point Liquibase use instead base one . This give ability debug well . < ? xml version= '' 1.0 '' encoding= '' UTF-8 '' ? > < databaseChangeLog ... .. > < changeSet id= '' custom-change '' author= '' author '' runOnChange= '' true '' > < customChange param= '' ... '' / > < /changeSet > < /databaseChangeLog >"
Stackoverflow,"I seem get CommandTimeout work code . I set timeout value 1 second test ensure works seems ignored . The SELECT statement takes 15 seconds run runs completion . I expecting timeout exception occur 1 second . I searched internet find example I able find anything . What I wrong ? DbProviderFactory factory = DbProviderFactories.GetFactory ( `` System.Data.SqlClient '' ) ; DataSet dataSet = new DataSet ( ) ; DbCommand command = factory.CreateCommand ( ) ; command.Connection = factory.CreateConnection ( ) ; command.Connection.ConnectionString = `` Server=localhost ; Database=MyDatabase ; User Id= ; Password=**** '' ; command.CommandType = CommandType.Text ; command.CommandText = `` SELECT * FROM table '' ; command.CommandTimeout = 1 ; using ( DbDataAdapter adapter = factory.CreateDataAdapter ( ) ) { adapter.SelectCommand = command ; adapter.Fill ( dataSet ) ; } **UPDATE : ** I wrote SELECT command takes longer 30 seconds run . This also timeout . I even commented CommandTimeout line still ran completion . So , seems even default CommandTimeout 30 seconds ignored . I stumped ... help would greatly appreciated . **UPDATE : ** I think I might figured . It seems I wrote really complex SELECT statement command timeout exception occurred . I guess happening , simple SELECT command , taking less 1 second execute command takes additional 15 seconds load data set . Am I right track ? Does seem likely ?","You ca n't break normal string literals across multiple lines , also closing quote misplaced : SqlConnection con = new SqlConnection ( `` Data Source=local ; Initial Catalog=DB ; User ID=sa ; Password=pass '' ) ; Or use verbatim literal , _can_ break across multiple lines : SqlConnection con = new SqlConnection ( @ '' Data Source=local ; Initial Catalog=DB ; User ID=sa ; Password=pass '' ) ; That said , code vulnerable [ SQL injection ] ( http : //en.wikipedia.org/wiki/SQL_injection ) attacks . For sake , sake users , really use parameterized queries instead concatenating SQL queries like . Here 's quick example : using ( var con = new SqlConnection ( ... ) ) { var cmd = new SqlCommand ( `` select * Table1 ID = @ ID '' , con ) ; con.Open ( ) ; cmd.Parameters.AddWithValue ( `` @ ID '' , LbLID.Text.Trim ( ) ) ; var da = new SqlDataAdapter ( cmd ) ; var dt = new DataTable ( ) ; da.Fill ( dt ) ; lblS1.Text = dt.Rows [ 0 ] [ 4 ] .ToString ( ) ; lblS1.DataBind ( ) ; } Some tips : You avoid using ` select * ` queries , since database schema might change , would break existing code . It would better select column 're interested make simple call [ ` ExecuteScalar ` ] ( http : //msdn.microsoft.com/en- us/library/system.data.sqlclient.sqlcommand.executescalar % 28v=vs.110 % 29.aspx ) ."
Stackoverflow,"I trying install Microsoft Drivers PHP SQL Server using document provided Microsoft . Link < https : //github.com/Microsoft/msphpsql > It provides installation steps Ubuntu Redhat Amazon Linux . To install Microsoft Drivers Amazon , I followed steps provided Redhat ( sure correct ) . When I run command sudo ACCEPT_EULA=Y yum install msodbcsql mssql-tools got error Error : Package : msodbcsql-13.1.7.0-1.x86_64 ( packages-microsoft-com-prod ) Requires : unixODBC > = 2.3.1 Available : unixODBC-2.2.14-14.7.amzn1.i686 ( amzn-main ) unixODBC = 2.2.14-14.7.amzn1 The error clearly says install msodbcsql , unixODBC version > = 2.3.1 . But updated/latest unixODBC package available amazon unixODBC-2.2.14 . I need help install Microsoft Drivers PHP Amazon Linux I use _Sqlsrv_ PHP functions connect SQL server . PHP7 , Apache , SQL server already set .","After googling trying ways , I found answer question . Instead using sudo ACCEPT_EULA=Y yum install msodbcsql mssql-tools I used sudo ACCEPT_EULA=Y yum install msodbcsql-13.0.1.0-1 mssql-tools-14.0.2.0-1 issue fixed ."
Stackoverflow,I configured Java application connect multiple routers InnoDB cluster one router stops working router take routing task ( I 6 router configured ) Now Can one help finding router request sent multirouter setuP ?,"The client always check errors . This necessity system , network errors , power outages , etc , occur configuration . When client discovers connection failure ( failure connect / dropped connection ) , start reconnecting replaying transaction middle . For transaction integrity , client _must_ involved process ; recovery _cannot_ provide proxy ."
Stackoverflow,"I 'm using MySQL Server 7.0 Windows Server 2008 trying return result GROUP_CONCAT function . General SQL follows : DELIMITER ; DROP FUNCTION IF EXISTS MyFunction ; DELIMITER $ $ CREATE FUNCTION MyFunction ( MyVar INT ) RETURNS VARCHAR ( 255 ) BEGIN SELECT @ MyRetVar = GROUP_CONCAT ( MyColumn ) FROM MyTable WHERE MyID = MyVar ; RETURN @ MyRetVar ; END $ $ DELIMITER ; This yields following result : > ERROR 1415 ( 0A000 ) : Not allowed return result set function I checked manual ( < http : //dev.mysql.com/doc/refman/5.0/en/group-by- functions.html > ) read > The result type TEXT BLOB unless group_concat_max_len less equal 512 , case result type VARCHAR VARBINARY . I changed value group_concat_max_len default value 512 also 256 My.ini ( restarted MySQL service ) . I 've verified change using > mysql > show variables like `` % concat % '' ; Any help appreciated !",I think 's actually debugging ` SELECT ` calls . From [ docs ] ( http : //dev.mysql.com/doc/refman/5.0/en/create-procedure.html ) : > Statements return result set used within stored procedure within stored function . This prohibition includes SELECT statements INTO var_list clause ...
Stackoverflow,"In database fields characters html entities converted , I try search words like _übung_ _ärzte_ nothing found . I tried add SQL query ` COLLATE utf8_general_ci ` I get following error : ` COLLATION 'utf8_general_ci ' valid CHARACTER SET 'latin1 ' ` My fields I search utf8_general_ci encoding . Is possible make search convert submit letters html entities ?",something like ? var field = `` < href='/Default.aspx ? Email= '' +dr [ `` id '' ] + '' ' > '' + dr [ `` First_Name '' ] .ToString ( ) + '' < /a > '' ;
Stackoverflow,I 've MySQL master-slave replication setup slave marked read- only=ON . I 've stored procedures I want route ProxySQL slave prefer run master . Each stored procedure logs exception-logging table using exception-logging stored procedure exception arises . I set definer root user SUPER privilege . But would break replication set primary key auto increment values would sync master exception table . Would using replicate-ignore-table make sense ? How safe ROW based replication ? Please suggest 's better way .,"Port 6032 administrative CLI . Instead , would instead want connect port 6033 listens traffic load balancing towards backend PXC nodes . Good luck !"
Stackoverflow,"I custom sitemapprovider loads pages database . > **Pages** ( pageid , fk_pageid ( parent ) , title , url , show_in_menu ) I would like globalize/localize ` title ` page . What 's best method ?",Have look ` mysqldump ` . mysqldump db_name tbl_name > backupfile.sql dump db / table overwrite backupfile.sql exists . Use ` rsync ` ` scp ` copy another host needed .
Stackoverflow,"I 'm using Aster Basket_Generator function calculate basket table purchases ( retail_purchases ) . I create basket without issue using following code : SELECT order_number , gsi_sku1 , gsi_sku2 , Count ( 1 ) FROM basket_generator ( ON retail_purchases PARTITION BY order_number BASKET_SIZE ( 2 ) BASKET_ITEM ( 'gsi_sku ' ) ACCUMULATE ( 'order_number ' ) ) WHERE gsi_sku1 ( 11001788 , 12002389 ) GROUP BY 1 , 2 , 3 ; LIMIT 10 ; What I 'd like additionally , calculate average value basket . Ideally would returned one column , I 'd completely satisfied average sale price item basket . I 've tried following : SELECT order_number , gsi_sku1 , gsi_sku2 , avg ( sales_amt ) , Count ( 1 ) FROM basket_generator ( ON retail_purchases PARTITION BY order_number BASKET_SIZE ( 2 ) BASKET_ITEM ( 'gsi_sku ' ) ACCUMULATE ( 'order_number ' , 'sales_amt ' ) ) WHERE gsi_sku1 ( 11001788 , 12002389 ) GROUP BY 1 , 2 , 3 ; LIMIT 10 ; But avg ( sales_amt ) column n't seem returning correct value . What 's recommended way calculate basket aggregates using Aster Basket_Generator analytic function ?",There good research study based lab study compares time develop execute analytic functions Teradata Aster discovery platform Hadoop/Hive . The systems run side side show workloads appropriate system . There good working example `` day life '' analyst time/effort required . ( disclosure : work Teradata acquired Aster Data 2 years ago ) < http : //www.asterdata.com/resources/assets/ESG-Lab-Validation-Teradata-Aster- MapReduce-Platform.pdf >
Stackoverflow,"It 's pretty hard understand nuget packages install Xamrin solution searching around web , dozens packages , dozens different solutions . Right , solution 2 projects , Android one PCL one . Our model data access defined PCL . Our platform implementation defined Android one . We need SQLite , SQLite.Net ( data annotations table relations ) , SQLiteExtentions *withchildren methods . We stuck old versions anytime try update anything , frail magical way installed packages working together falls apart . We need upgrade find way add SQLCipher weird snafu nuget packages . Our current packages installed , works : **Android project** * [ Mono.Data.Sqlite.Portable 1.0.3.5 ] ( https : //www.nuget.org/packages/Mono.Data.Sqlite.Portable/ ) ( sure ... ) * [ SQLite.Net.Core-PCL 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net.Core-PCL ) * [ SQLite.Net.Platform.XamarinAndroidN 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net.Platform.XamarinAndroidN ) * [ SQLite.Net-PCL 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net-PCL ) * [ SQLiteNetExtensions 1.3.0 ] ( https : //www.nuget.org/packages/SQLiteNetExtensions ) ( required GetWithChildren etc . ) * [ SQLitePCL.raw 0.9.3 ] ( https : //www.nuget.org/packages/SQLitePCL.raw ) **PCL Project** ( model definition data access methods ) * [ Mono.Data.Sqlite.Portable 1.0.3.5 ] ( https : //www.nuget.org/packages/Mono.Data.Sqlite.Portable/ ) ( sure ... ) * [ SQLite.Net.Core-PCL 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net.Core-PCL ) * [ SQLite.Net.Platform.XamarinAndroidN 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net.Platform.XamarinAndroidN ) * [ SQLite.Net-PCL 3.1.1 ] ( https : //www.nuget.org/packages/SQLite.Net-PCL ) * [ SQLiteNetExtensions 1.3.0 ] ( https : //www.nuget.org/packages/SQLiteNetExtensions ) ( required GetWithChildren etc . ) * [ SQLitePCL.raw 0.9.3 ] ( https : //www.nuget.org/packages/SQLitePCL.raw ) Currently , update SQLiteExtensions 2.0 , bunch SQLite nuget packages installed , breaking frail stability data access code ( fails *WithChildren methods : Severity Code Description Project File Line Suppression State Error CS1929 'SQLiteConnection ' contain definition 'GetWithChildren ' best extension method overload 'ReadOperations.GetWithChildren < TEntity > ( SQLiteConnection , object , bool ) ' requires receiver type 'SQLiteConnection ' We 'd also need incorporate SQLiteCipher combinations packages made work solution . Our Android platform specific implementation : # region Usings using OURPCLLib.DataAccess ; using Serilog ; using SQLite.Net ; using SQLite.Net.Interop ; using SQLite.Net.Platform.XamarinAndroid ; using System ; using System.IO ; # endregion Usings public class AndroidSQLiteDatabase : SQLiteDatabaseAccess { protected ISQLitePlatform SQLitePlatform { get { return new SQLitePlatformAndroidN ( ) ; } } protected override SQLiteConnection GetConnection ( ) { var conn = new SQLiteConnection ( SQLitePlatform , `` dbpathforus.sqlite '' , SQLiteOpenFlags.ReadWrite | SQLiteOpenFlags.FullMutex | SQLiteOpenFlags.ProtectionCompleteUnlessOpen | SQLiteOpenFlags.Create | SQLiteOpenFlags.SharedCache ) ; return conn ; } } The ( simplified ) base data access class PCL : # region Usings using OURPCLLib.DataAccess.Entities ; using SQLite.Net ; using SQLite.Net.Attributes ; using SQLite.Net.Interop ; using SQLiteNetExtensions.Extensions ; using System ; using System.Collections.Generic ; using System.Diagnostics ; using System.Linq ; using System.Linq.Expressions ; # endregion Usings public abstract class SQLiteDatabaseAccess { protected abstract SQLiteConnection GetConnection ( ) ; // Example one many methods accessing DB using SQLite.Net public bool Any < TEntity > ( Expression < Func < TEntity , bool > > expression ) TEntity : class , IBaseEntity , new ( ) { using ( var currentConnection = this.GetConnection ( ) ) { return currentConnection.Table < TEntity > ( ) .Where ( expression ) .FirstOrDefault ( ) ! = null ; } } // Example one methods accessing DB using SQLiteExtentions public TEntity GetWithChildren < TEntity > ( int id , bool recursive = false ) TEntity : class , IBaseEntity , new ( ) { using ( var currentConnection = this.GetConnection ( ) ) { return currentConnection.GetWithChildren < TEntity > ( id , recursive ) ; } } } Anyone help us use SQLite SQLIte.net , SQLiteExtentions SQLIte cipher project like ? ( data access pcl connection implementation android project ?","I considering closing question felt leaving answer would useful others . The SQLite JSON1 documentation fact explain things though explanations immediaately apparent . The SQL issue instance would SELECT users.name FROM users , json_each ( users.phone , ' $ .cell ' ) WHERE json_each.value = '234567890 '"
Stackoverflow,"I wrote utPL/SQL Test PL/SQL Package , put maven project let execute jenkins . I wonder way get rid test packages created database ? It feels bit weird test artefacts stay database . I would either consider maven goal within utPL/SQL Plugin delete created test packages seperate goal , I execute PL/SQL drop packages . I would also appreciate ideas .","I recently gone pain . Most utPLSQL examples utPLSQL v2 . It transpires appears assertions deprecated , replaced `` Expects '' . I found [ great blog post Jacek Gebal ] ( http : //www.oraclethoughts.com/utplsql/what-is-the-equivalent-of- utassert-eqtable-utplsql-v2-v3/ ) describes . I 've tried put useful links [ page unit testing fits Redgate's Oracle DevOps pipeline ] ( https : //documentation.red- gate.com/display/DDFO/Unit+testing ) ( I work Redgate often get asked best implement automated unit testing Oracle ) ."
Stackoverflow,"I WCF Data Service I intend use **session-based table functions** ( creates temporary tables usable current session ) upon _insert_ _update_ . I tried use ` SaveChanges ` method like : public partial class MyContext : DbContext { public override int SaveChanges ( ) { var res = SetValues ( true ) ; var = Database.SqlQuery < string > ( `` SELECT [ Key ] FROM TempContextView '' ) .ToList ( ) ; System.IO.File.AppendAllText ( @ '' c : \Temp\session.txt '' , $ '' SIZE S : { s.Count } , script res : { res } '' ) ; foreach ( var element ) { System.IO.File.AppendAllText ( @ '' c : \Temp\session.txt '' , $ '' RES : { element } '' ) ; //never reached } return base.SaveChanges ( ) ; } public int SetValues ( bool insert ) { System.IO.File.AppendAllText ( @ '' c : \Temp\session.txt '' , `` SetV : `` + insert ) ; return Database.ExecuteSqlCommand ( insert ? `` INSERT INTO TempContextView ( [ Key ] , [ Value ] ) VALUES ( 'Flag ' , ' 1 ' ) '' : `` DELETE FROM TempContextView WHERE [ Key ] = 'Flag ' '' ) ; } } The _TempContextView_ _view_ provides _temporary table_ created function : SELECT TOP ( 32 ) [ Key ] , Value FROM Schema1.getContextTable ( ) ORDER BY [ Key ] function [ Schema1 ] . [ getContextTable ] ( ) RETURNS @ Context TABLE ( [ Key ] varchar ( 126 ) , [ Value ] varchar ( 126 ) ) WITH SCHEMABINDING ... However , I select values table created function , returns nothing ( query size 0 , yet insert returns 1 ) . Does mean , I ca n't use EF sessions ? Or every EF function uses _its context_ ? As session table used triggers , I need proper key value . What I ? Any hint EF able use type functionality ? **UPDATE : ** I learned EF uses ` exec sp_reset_connection ` executed command , _resets temporary variables tables_ . So I tried create transaction force EF execute commands one session : using ( var scope = new TransactionScope ( TransactionScopeOption.RequiresNew ) ) { Database.ExecuteSqlCommand ( `` INSERT INTO TempContextView ( [ Key ] , [ Value ] ) VALUES ( 'Flag ' , ' 1 ' ) '' ) ; //session # 1 ? base.SaveChanges ( ) ; //session # 2 ? : ( scope.Complete ( ) ; } It still creates new sessions , I ca n't really merge two commands . [ ! [ enter image description ] ( https : //i.stack.imgur.com/7P7rv.png ) ] ( https : //i.stack.imgur.com/7P7rv.png ) Any suggestions ?",A ` ' I/O Error : Socket closed ' ` exception means connection JDBC client server closed reason . My advice use connection pool library using pool . The reason could one following : 1 . You tried execute query connection closed . 2 . May OS terminated connection network problem . 3 . Database server shut . 4 . Because result long read query taking lot time execute
Stackoverflow,"As per [ hangfire documentation ] ( http : //docs.hangfire.io/en/latest/configuration/configuring- logging.html ) > Starting Hangfire 1.3.0 , required anything , application already uses one following libraries reflection ( Hangfire depend ) . Logging implementation automatically chosen checking presence corresponding types order shown . > > Serilog > NLog > Log4Net > EntLib > Logging > Loupe > Elmah I ASP.NET Core 2.0 application using ` Serilog ` like public class Program { public static void Main ( string [ ] args ) { BuildWebHost ( args ) .Run ( ) ; } public static IWebHost BuildWebHost ( string [ ] args ) = > WebHost.CreateDefaultBuilder ( args ) .UseStartup < Startup > ( ) .UseApplicationInsights ( ) .UseUrls ( `` http : //*:40006 '' ) .ConfigureAppConfiguration ( ( hostingContext , config ) = > { // removed bravity } ) .ConfigureLogging ( ( hostingContext , logging ) = > { Log.Logger = new LoggerConfiguration ( ) .ReadFrom.Configuration ( hostingContext.Configuration ) .CreateLogger ( ) ; logging.AddSerilog ( ) ; } ) .Build ( ) ; } The application configured use Hangfire . During background job processing exception occurs , Hangfire re-tries job 10 times increasing delay expected , shows exceptions dashboard . **Issue** The Hangfire dashboard shows exception UI however log exception configured Serilog sink . Note : The Hangfire dashboard shows exception formats exception [ see ] ( https : //stackoverflow.com/questions/49245955/hanfire-not-logging-custom- exception-with-metadata ) hides critical information exception . I think logs exception , Serilog logger would log complete exception .",Sorry . I realized someone renamed [ Key ] column one hangfire tables . Is working .
Stackoverflow,"I 'm struggling following situation : I need extract '10000' ... name= '' stateid '' > < > 10000 < /from > I 've managed extract I need first node audittrail SELECT ref.value ( ' @ datetime ' , 'nvarchar ( 364 ) ' ) [ LastTimeActioned ] , ref.value ( ' @ changedby ' , 'nvarchar ( 364 ) ' ) [ Actioned_By ] FROM Audit CROSS APPLY audit.Xml.nodes ( '/audittrail ' ) R ( ref ) < audittrail id= '' 137943 '' datetime= '' 29-Feb-2016 15:42:06 '' changedby= '' quality '' type= '' update '' > < fields > < field id= '' 10022 '' name= '' Content Control '' > < mergedValue > dsad < /mergedValue > < / > < > dsad < /to > < /field > < field id= '' 10027 '' name= '' Document Controller '' > < mergedValue > quality < /mergedValue > < / > < > quality < /to > < /field > < field id= '' 10028 '' name= '' Document Owner '' > < mergedValue > quality < /mergedValue > < / > < > quality < /to > < /field > < field id= '' 10029 '' name= '' Document Type '' > < mergedValue > Contract/Agreement < /mergedValue > < / > < > Contract/Agreement < /to > < /field > < field id= '' 10067 '' name= '' StateId '' > < > 10000 < /from > < > 10000 < /to > < /field > ... . < /fields > < /audittrail >","There several problems cause behavior : * > This error occurs try use special character SQL statement . If special character $ , _ , # used name column table , name must enclosed double quotations . * > This error may occur 've pasted SQL editor another program . Sometimes non-printable characters may present . In case , try retyping SQL statement re-execute . * > This error occurs special character used SQL WHERE clause value enclosed single quotations ."
Stackoverflow,I created local service . My server name like ` sqlitesync.com ` . In database folder created ` configuration ( web.config ) ` ` WCF web service ( AuroraSyncService.svc ) ` . When I test service browser shows nothing . followed tutorial [ Link ] ( http : //sqlite- sync.com/documentation/installation/ ) When start service < http : //sqlitesync.com/sqlitesyncservice/AuroraSyncService.svc > browser shows empty like ! [ enter image description ] ( https : //i.stack.imgur.com/GtOfC.png ) When I create ( ` index.html ` ) ` http : //sqlitesync.com/index.html ` works fine ! [ http : //sqlitesync.com/index.html ] ( https : //i.stack.imgur.com/Wjxww.png ) I n't know solve also I new SQL Server . Anybody provide solution solve this.. configuration code : < connectionStrings > < add name= '' AuroraSyncCn '' connectionString= '' Data Source= ( local ) ; Initial Catalog=SQLite-Sync.com ; User ID=sa ; Password=pass '' providerName= '' System.Data.SqlClient '' / > ! [ enter image description ] ( https : //i.stack.imgur.com/mz36K.png ),Create stored procedure separate schema ( use ` CREATE SCHEMA ` ) GRANT REVOKE execution right stored procedures schema users charge .
Stackoverflow,"Today university lecturer databases class brought PSP . I used LAMP stack open source , I bias Oracle . I understand however , Oracle databases used industry good learn use . However covered PL/SQL Server pages ( Stored procedures Oracle database output HTML access web browser ) I cringed . Is bias getting way much , advantages using compared using something like PHP dynamic SQL queries ? How popular PSP web applications using Oracle ?","[ PL/SQL Server Pages ] ( http : //download.oracle.com/docs/cd/B19306_01/appdev.102/b14251/adfns_psp.htm ) ( PSPs ) ( sort ) variant general-purpose [ PL/SQL Web Toolkit ] ( http : //download.oracle.com/docs/cd/B19306_01/appdev.102/b14251/adfns_web.htm ) ( also known OWA ; called Oracle Web Agent early days ) . There much buzz around PSPs , I assume used . The PL/SQL Web Toolkit ( OWA ) , however , another matter altogether . Lots lots people use building ( data-centric ) web applications power PL/SQL . Also , 2006 thereabouts , Oracle released free ( no-cost option ) database called [ Oracle Application Express ] ( http : //apex.oracle.com/i/index.html ) ( Apex ) , built top OWA , ( time writing ) version 4.1 . Apex gives browser-based IDE developing PL/SQL web applications ( powered OWA ) . Apex also includes whole bunch built-in features session management , authentication authorization , interactive reports , flash charts , page templates , navigation , Ajax support , much , much . So interested looking web development Oracle , I suggest forget PSPs look Apex instead . In terms popularity , according [ Powerpoint ] ( http : //www.oracle.com/technetwork/developer-tools/apex/apex -- introduction-157585.ppt ) presentation Oracle 2010 , Apex time : * 80,000 downloads per year * 4,000,000 page views per week apex.oracle.com * Third largest discussion forum forums.oracle.com * 100 consulting companies delivering Apex services * 60 blogs Apex * Used user interface Oracle Store Oracle Audit Vault"
Stackoverflow,I facing strange issue executing query mysql manager tool exported database phpmyadmin using wamp server client's server trying run query gives error . one help resolve issue ? blank space query . help would appreciated . ! [ enter image description ] ( https : //i.stack.imgur.com/KQqJw.png ),"Is numberOfRows. $ _SESSION always 20 ? # # follow ok , I understand problem 's better described comments : ) ) miss **SQL_CALC_FOUND_ROWS** use like : SELECT SQL_CALC_FOUND_ROWS * FROM tab_a limit 50 ; SELECT FOUND_ROWS ( ) ; omit , always return 50 , include actual count would without limit"
Stackoverflow,"We migrating data Oracle Hadoop requirement continue use existing reporting tool ( Crystal Report ) generate reports Hadoop ( instead Oracle ) In current scenario using Oracle Stored PROC aggregations /logic . Now requirement migrated data , The Options considered 1 . Use ` HPLSQL ` ( instead Oracle ` PLSQL ` ) call Crystal Reports.However appears challenges approach unlike Oracle Stored Procs , ` HPLSQL ` Stored Procs registered DB catalouge hence Crystal reports may unable find / access ` HPLSQL ` STored PROCS . 2 . Create custom aggregation /logic java expose webservice invoked /consumed Crystal Reports The help/guidance needed find whether ) Did someone successfully called ` HPLSQL ` Stored Procs form external tool/ reports like Crystal ` ODBC/JDBC ` . If yes , share details ? b ) Is better option mentioned two options achieve requirement ? c ) Are challenges using webservices fetch data run reports Thanks advance help really appreciated .","What logging procedure might fail ? Is pure ` INSERT ` \+ ` COMMIT ` ? Anyway : simplest ( probably worst ) option use exception handler section , e.g . your_procedure begin something ; exception others null ; end ; It would OK really n't care whether something _bad_ happened , - 's purpose , ? You think something , Oracle n't complain , procedure nothing idea 's going . Therefore , 'd better make sure `` logging '' procedure n't fail ."
Stackoverflow,"I working [ iModel.js ] ( https : //imodeljs.github.io/iModelJs-docs- output/ ) project involves OpenPlant schema . The iModel file uploaded [ iModelHub ] ( https : //imodeljs.github.io/iModelJs-docs- output//learning/imodelhub/ ) using MicroStation bridge . I able successfully ; 1 . Get IMS access token 2 . Open iModelDb passing access token , CONNECT project id iModel id 3 . Run ECSql queries iModelDb object using [ executeQuery ] ( https : //imodeljs.github.io/iModelJs-docs-output/reference/imodeljs-backend/imodels/imodeldb/executequery/ ) command . I trying get list Process Piping Process Equipment classes display pane . I ` SELECT * FROM Bis.Element GROUP BY ECClassId ` . This gives list elements corresponding classIDs , list seems incomplete . I get 4 process equipment classes ( ` CentrifugalPump ` , ` Drum ` , ` Vessel ` , ` Tank ` ) 5 process piping classes ( ` ConcentricPipeReducer ` , ` Nozzle ` , ` GateValve ` , ` Pipeline ` , ` PipeRun ` , ` ThreeWayValve ` ) . How I get list classes ?","It looks like querying instance data iModel . The resulting limited list classes contains actual elements exist iModel file . The file contains physical ( functional ) elements 4 process equipment class 5 process piping class types , ones see . Luckily , iModel file ` ECDbMeta ECSchema ` . An ` ECDbMeta ECSchema ` schema provides information ABOUT schemas ( turn give instance data meaning ) . All class definitions inside schemas exist iModel separate dataset . This information exists sole purpose making context instance data portable . For example , ` SELECT * FROM Bis.Element ` , get list ` Bis.Elements ` currently exist within iModel . As noticed , elements class Ids . A ` Nozzle ` another ` Bis.Element ` ` Nozzle ` subclass type defined ` ProcessPipingFunctional ` schema . You query ` ECDbMeta ECSchema ` , get list classes defined within Process Piping Piping equipment schemas . Here sample ` SELECT ` statements : > `` SELECT Name AS name , Alias AS alias , ECInstanceId AS id FROM ECDbMeta.ECSchemaDef ORDER BY Name '' The statement give list schemas imported iModel querying . Once get list , get ` schema.id ` process piping schemas looking . You use ids run following query : > `` SELECT Name AS name FROM ECDbMeta.ECClassDef WHERE ECClassDef.Schema.Id= ? ORDER BY Name '' This query give list classes defined within schemas . For information metaschemas : < https : //imodeljs.github.io/iModelJs- docs-output/learning/ecdbmeta.ecschema/ >"
Stackoverflow,I DBA using postgresql production server using postgresql 10 database . I using Bigsql started replication production server server replication server everything working production server space left . And du command production server getting ` pg_wal ` folder 17 gb file file 16 mb size . After google search change postgresql.conf file : wal_level = logical archive_mode = archive_command = 'cp -i % p /etc/bigsql/data/pg10/pg_wal/archive_status/ % f' install postgresql 10 Bigsql changes . After changes dir /pg_wal/archive_status 16 gb log . So question delete manually wait system delete automatically . And write archive_mode wal file getting removed automatically ? ? Thanks precious time .,This depends backups whether 'd ever need restore database point time . Only full offline filesystem backup ( offline meaning database turned ) on-line logical backup pg_dumpall need files restore . You 'd need files restore filesystem backup created database running . Without backup fail restore . Though exist backup solutions copy needed WAL files automatically ( like [ Barman ] ( http : //www.pgbarman.org/ ) ) . You 'd also need files replica database ever fall behind master reason . Or 'd need restore database past point-in-time . But files compress pretty well - less 10 % size compression - write archive_command compress automatically instead copying . And delete eventually archive . I 'd recommend delete 're least month old also least 2 full successful backups done creating .
Stackoverflow,"I trying convert date 's stored string date , e.g . YYYYMMDD ( string ) YYYY-MM-DD ( date ) As far I know conversion function checks input format output format , I tried manual logic , e.g . CASE WHEN CHAR_LENGTH ( TRIM ( some_string_date ) ) = 8 THEN CAST ( SUBSTRING ( TRIM ( some_string_date ) FROM 1 FOR 4 ) || '-' || SUBSTRING ( TRIM ( some_string_date ) FROM 5 FOR 2 ) ||'-' || SUBSTRING ( TRIM ( some_string_date ) FROM 7 FOR 2 ) DATE ) ELSE NULL END However accepted Apache SQL Validator , anyone see problem ?","Not directly answering question , maybe related , date literals declared ` DATE ` keyword , e.g . see examples tests Beam tests : [ one ] ( https : //github.com/apache/beam/blob/b06e28bf9605f232c270993728361bd0fc2ee08a/sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/BeamSqlDslSqlStdOperatorsTest.java # L559 ) , [ two ] ( https : //github.com/apache/beam/blob/bdd0081b49f8e7df6733dc8e8bc90dda3efc6621/sdks/java/extensions/sql/jdbc/src/test/java/org/apache/beam/sdk/extensions/sql/jdbc/BeamSqlLineTest.java # L90 ) Calcite [ docs ] ( https : //calcite.apache.org/docs/reference.html # scalar- types ) . **Update : ** What seems happen Calcite adds indirection ` CASE ` . Casting strings dates works expected general . For example , input rows schema ` ( INT f_int , VARCHAR f_string ) ` dates ` 'YYYYMMDD ' ` ( e.g . ` ( 1 , '2018 ' ) ` , works : SELECT f_int , CAST ( SUBSTRING ( TRIM ( f_string ) FROM 1 FOR 4 ) ||'-' ||SUBSTRING ( TRIM ( f_string ) FROM 5 FOR 2 ) ||'-' ||SUBSTRING ( TRIM ( f_string ) FROM 7 FOR 2 ) DATE ) FROM PCOLLECTION Even directly casting ` 'YYYYMMDD ' ` works : SELECT f_int , CAST ( f_string AS DATE ) FROM PCOLLECTION You see supported date formats [ ] ( https : //github.com/apache/beam/blob/06128f27d1780f25c23ca65cc7ace693a78dac80/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/interpreter/operator/BeamSqlCastExpression.java # L45 ) . But soon wrap ` 'CASE ... ELSE NULL ' ` , Beam/Calcite seem infer expression type ` 'String ' ` . This means ` 'THEN CAST ( ... AS DATE ) ' ` succeeds returns 'Date ' , 's converted ` 'String ' ` wrapped ` 'CASE ' ` . Then , returning result test seems try cast back ` 'Date ' ` , string format ` 'YYYYMMDD ' ` default format . Unfortunately format list supported , fails . **Workaround : ** As soon change ` 'ELSE NULL ' ` something 's known ` 'Date ' ` , e.g . ` 'ELSE DATE `` 2001-01-01 '' ' ` works Beam/Calcite n't seem go though ` 'String'- > 'Date'- > 'String'- > 'Date ' ` path works : SELECT f_int , CASE WHEN CHAR_LENGTH ( TRIM ( f_string ) ) = 8 THEN CAST ( SUBSTRING ( TRIM ( f_string ) FROM 1 FOR 4 ) ||'-' ||SUBSTRING ( TRIM ( f_string ) FROM 5 FOR 2 ) ||'-' ||SUBSTRING ( TRIM ( f_string ) FROM 7 FOR 2 ) AS DATE ) ELSE DATE '2001-01-01' END FROM PCOLLECTION I filed [ BEAM-5789 ] ( https : //issues.apache.org/jira/browse/BEAM-5789 ) track better solution . **Update 2 : ** So , Calcite generates plan telling Beam , 's Beam actually casts/parses dates case . There 's effort use Calcite 's built-in implementations basic operations instead re- implementing everything Beam : < https : //github.com/apache/beam/pull/6417 > . After pull request merged , ` CASE ... ELSE NULL ` path work automatically , I 'm reading right ( I assume [ class ] ( https : //github.com/apache/calcite- avatica/blob/3cfafde9f5f0f1fc3ddc60b5a4db19762c73b96b/core/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java # L374 ) used handling date/time values ) . It still go strings , probably unnecessarily , work ."
Stackoverflow,"I using Dapper [ ExpressionToSQL ] ( https : //github.com/andygjp/ExpressionToSql/tree/master/src/ExpressionToSql ) nuget package . I query : Set = new Table { Schema = `` Sch '' , Name = `` tbl1 '' } ; Columns = x = > new { x.Id } ; Conditions = x = > x.Name== request.Name & & x.Date == request.Date ; Where < TQueryIn , object > query = Sql.Select ( Columns , Set ) .Where ( Conditions ) ; ... connection.QueryAsync ( query ) ; Where value ` request.Name ` `` T1 '' ` request.Date ` `` 20121020 '' . When I run above-mentioned query , I get following query : SELECT . [ Id ] FROM [ Sch ] . [ tbl1 ] AS WHERE . [ Name ] = @ Name AND . [ Date ] = @ Date As see ` request.Name ` ` request.Date ` 's values replaced ` @ Name ` ` @ Date ` ! What I need see : SELECT . [ Id ] FROM [ Sch ] . [ tbl1 ] AS WHERE . [ Name ] = 'T1 ' AND . [ Date ] = '20121020' Definitely problem Dapper problem ExpressionToSQL package . Is way overcome issue , convert ` Expression < Func < T , bool > > ` string value mentioned output ?","I 'm familiar Dapper , looking tests [ ExpressionToSQL ] ( https : //github.com/andygjp/ExpressionToSql/blob/master/tests/ExpressionToSql.Dapper.UnitTests/ExtensionTests.cs ) seems provide parameters second argument ` QueryAsync ` , therefore , I think work : Set = new Table { Schema = `` Sch '' , Name = `` tbl1 '' } ; Columns = x = > new { x.Id } ; Conditions = x = > x.Name== request.Name & & x.Date == request.Date ; Where < TQueryIn , object > query = Sql.Select ( Columns , Set ) .Where ( Conditions ) ; ... connection.QueryAsync ( query , request ) ; Also found , may useful : < https : //dapper-tutorial.net/parameter- anonymous >"
Stackoverflow,"I would like compose query way : $ query = $ orm- > table ( ) ; - > ( 'foo_id ' , $ foo [ 'id ' ] ) - > like ( 'foo_name ' , ' % DP ' ) - > fetch ( ) ; The error : > PDOException : SQLSTATE [ 42S02 ] : Base table view found : 1146 Table 'database_name.like ' n't exist /var/www/webapp/app/vendor/morris/lessql/src/LessQL/Database.php:117","You make ` LIKE ` ` ( ) ` call : $ query = $ orm- > table ( ) - > ( 'foo_id ' , $ foo [ 'id ' ] ) - > ( 'foo_name LIKE ? ' , array ( ' % DP ' ) ) - > fetch ( ) ;"
Stackoverflow,"Given simple table like : var1 | var2 0 | 3 2 | 4 6 | 5 I would need return lowest value ( var1 , multiple 2 ) either record exist ( case , 4 ) var2 equal 5 ( case , 6 ) . I 'd need single query , unfortunately I 'm new MySQL functions . I tried create function like : DELIMITER // CREATE FUNCTION Prova ( starting_value INT ) returns int ; BEGIN DECLARE testNum INT DEFAULT 0 ; test_loop : LOOP IF ( SELECT db.tabella WHERE var1 = testNum AND var2 < = 5 ) OR WHERE NOT EXISTS ( SELECT * FROM db.tabella WHERE var1 = 'testNum' ) LIMIT 1 ; THEN RETURN testNum ; END IF ; SET testNum = testNum + 2 ; SELECT testNum ; END LOOP ; END ; n't seem work . Thank .","could use ` CASE ` mysql ( see 's doc [ MySQL CASE Syntax ] ( https : //dev.mysql.com/doc/refman/5.7/en/case.html ) ) I create MySQL table test test inserts , see bellow : CREATE TABLE test ( id INT , hp INT ) ; INSERT INTO test ( id , hp ) VALUES ( 1 , 1 ) ; INSERT INTO test ( id , hp ) VALUES ( 2 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 3 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 4 , 1 ) ; INSERT INTO test ( id , hp ) VALUES ( 5 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 6 , 1 ) ; INSERT INTO test ( id , hp ) VALUES ( 7 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 8 , 1 ) ; INSERT INTO test ( id , hp ) VALUES ( 9 , 1 ) ; INSERT INTO test ( id , hp ) VALUES ( 10 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 11 , 0 ) ; INSERT INTO test ( id , hp ) VALUES ( 12 , 1 ) ; For I understand want return list table ` hp ` ` 0 1 ` , ` hp ` filter used want bring data regardless ` hp ` field . Here way , see bellow : SET @ hp_from_html = null ; SELECT * FROM test WHERE ( CASE WHEN @ hp_from_html null true ELSE hp = @ hp_from_html END ) ; You try code : [ MySQLFiddle ] ( https : //www.db- fiddle.com/f/7qXi81NodZy8wZAidwhqQx/2 ) I hope works ."
Stackoverflow,"I 'm trying create JSF based web application netbeans , struck caching sha2 password error , replaced JDBC drivers ( 8.0.13 version ) sorted issues . Now run application browser shows error An Error Occurred : Can open file : C : \Users\Siva\AppData\Roaming\NetBeans\8.2\config\GF_4.1.1\domain1/config/keystore.jks [ Keystore tampered , password incorrect ] searched online tried solutions couldnt get sorted , missing ?","You need sum ( ) aggregation getting error becuase group clause putting wrong place - outside subquery select SUBSTRING ( name , 1 , CHARINDEX ( '* ' , name ) - 1 ) name , sum ( duration ) duration FROM ( SELECT b . `` identity '' || '* ' || session_id || '- ' || a.user_id AS `` name '' , MIN ( time ) AS `` start '' , MAX ( time ) AS `` last '' , ( ( DATEDIFF ( 'milliseconds ' , MIN ( time ) , MAX ( time ) ) : :FLOAT / 1000 ) / 60 ) AS `` duration '' FROM abc_app_production.all_events , abc_app_production.users b a.user_id = b.user_id AND b.user_type IS NOT NULL AND b . `` identity '' IS NOT NULL AND b . `` identity '' NOT IN ( 'Shubham ' ) AND time > = convert ( datetime , '2018-10-01 ' ) AND time < = convert ( datetime , '2018-11-01 ' ) group b . `` identity '' || '* ' || session_id || '- ' || a.user_id ) X group SUBSTRING ( name , 1 , CHARINDEX ( '* ' , name ) - 1 )"
Stackoverflow,"I testing configuration availability groups ( AG ) windows Server 2016 sql server vNext CTP 1.4 within clusterless environment . After recent conference , I learnt clusterless AG configuration possible recent version windows OS vNext . I setup everything correctly ( viz . HADR enable , two nodes pinging , endpoints , certificates , logins permissions ) within correct parameters . Have even successfully executed tsql script creating AG using CLUSTER_TYPE=None primary node AG . However , seems I unable join secondary replica onto AG encountering screenshot error . Also , whilst creating AG I noticed secondary replica connected . I get error whilst joining secondary AG . ! [ enter image description ] ( https : //i.stack.imgur.com/E9vj9.png )",I managed rectify error within availability group creation re-read MS docs . Basically error within AG group TSQL script I using create Clusterless AG . Essentially ( I think ) important create AG syntax ( CLUSTER_TYPE=NONE ) within CREATE AVAILABILITY GROUP ... .TSQL nothing else viz . ( DB_FAILOVER=ON/OFF etc ) executing create availability group node1 ... .. hop node2 ( read replica ) execute join syntax ALTER AVAILABILITY GROUP **AGNAME** JOIN WITH ( CLUSTER_TYPE = NONE ) ; Hope helps future someone 's trying scale read-only AG using AG'less and/or cluster'less environment .
Stackoverflow,"As example : I two tables firebird : TB_CUSTOMER * IDCUSTOMER ( autoincrement generator ) * CUSTOMERNAME TB_PHONE * IDPHONE * IDCUSTOMER ( foreing key TB_CUSTOMER ) * PHONE I registration form developed Delphi . The table data TB_PHONE handled using dbgrid . I assign value field IDCUSTOMER TB_PHONE , generated Firebird generator . How I make relationship tables ? I want implement without first saving table data TB_CUSTOMER . I 'm using datamodules IBDAC . Any sugest ?","> I want implement without first saving table data TB_CUSTOMER There 's problem . You need primary key master table save detail . That 's way works . But want make sure values get saved together , transaction . In Firebird , like : * Begin transaction . Exactly depends DB library 're using access Firebird database . * Run [ INSERT INTO ... RETURNING statement ] ( http : //www.firebirdsql.org/refdocs/langrefupd21-insert.html # langrefupd21-insert-returning ) insert row master table retrieve generated value single operation . * Use generated PK value fill FK value detail table . * Insert detail row . * Commit transaction ."
Stackoverflow,"My team I working Wordpress site together . We one person data entry live server rest us working locally . What would like local solutions point live server database develop around actual database content entered . I added IP address `` Access Host '' Remote MySQL section hosting account confirmed I able connect database remotely ( using live server 's IP ) . Since ` home ` / ` site ` url pointing live server domain database , understanding need edit ` hosts ` file order mask local URL live URL . The local URL I use < http : //localhost:8888/wordpress > I added line hosts file : 127.0.0.1 ourliveserver.com www.ourliveserver.com But I try access < http : //ourliveserver.com/wordpress > I getting ` ERR_CONNECTION_REFUSED ` Can anyone tell I went wrong ?",Did try ` php artisan config : clear ` clear cached config ? Maybe cached old config file testing server .
Stackoverflow,"I working way around problem : [ Create multiple results single row joining either 2 3 tables based conditional table1 results ? ] ( https : //stackoverflow.com/questions/48464936/create-multiple- results-from-single-row-joining-either-2-or-3-tables-based-off-c ) I wish I could take ` Strawberry 's ` advice , I ca n't , I trying C # rather DB , also trying smart much CPU I utilize . For scale , table1 may 50,000 records 20 ` codetype ` fields evaluated matched table2 , 2,000 rows , table3 , could 200,000 rows . To avoid hammering DB , I going store possible memory , limit results date much possible , I want avoid 2,000 foreach loops per 20 codetype matches . To start , I getting results I need table2 loading C # DataTable stored variable named ` descriptionLookup ` : id , description 13 Item 13 Description 15 Item 15 Description 17 Item 17 Description 18 Item 18 Description 21 Item 21 Description 28 Item 28 Description 45 Item 45 Description And table3 ` lookupTable ` : id , table2id 1 15 33 17 21 28 simple ( showing surrounding code , relevant ) : var rawData = new DataTable ( ) ; using ( OdbcCommand com = new OdbcCommand ( `` SELECT id , description table2 '' , conn ) ) { using ( OdbcDataReader reader = com.ExecuteReader ( ) ) { rawData.Load ( reader ) ; conn.Close ( ) ; return rawData ; } } I assigned variable called function . Now I deal table1 : codeid1 , codeid2 , codeid3 , ... codeid20 ... codetype1 , codetype2 , codetype3 , ... ..codetype20 18 13 1 33 0 0 1 1 13 21 45 0 0 1 0 0 Using foreach row , I need evaluate codetype column 1 0 . When codetype=1 I need grab associated codeid , lookup data I holding memory ` descriptionLookup ` see table2id matches ` id ` ` lookuptable ` use lookup description associated field table2 . If codetype 0 , I need match ` codeid ` associated description field table2 . I looking lay , I think : DataTable descriptionLookup= DB.ExecuteQuery ( `` SELECT id , description table2 '' ) ; DataTable lookupTable= DB.ExecuteQuery ( `` SELECT id , table2id table3 '' ) ; DataTable mainData= DB.ExecuteQuery ( `` SELECT * table1 '' ) ; foreach ( var row mainData ) { var manDataId = row.GetColumn ( `` id '' ) ; var subroutine = new Dictionary < string , string > ( ) ; ( var index = 1 ; index < 20 ; index++ ) { string description ; ( row.GetColumn ( `` codetype '' + index ) == `` 1 '' ) { int idLookup = row.GetColumn ( [ `` codeid '' +index ] ) ; foreach ( var row2 lookupTable ) { ( row3.GetColumn ( `` id '' ) == idLookup ) { descriptionId = row3.GetColumn ( `` table2id '' ) ; foreach ( var row2 descriptionLookup ) { ( row.GetColumn ( `` id '' ) == descriptionId ) { description = row2.GetColumn ( `` description '' ) .ToString ( ) ; } } } } } elseif ( row.GetColumn ( `` codetype '' + index ) == `` 0 '' ) { descriptionId = row.GetColumn ( [ `` codeid '' +index ] ) ; foreach ( var row2 descriptionLookup ) { ( row.GetColumn ( `` id '' ) == descriptionId ) { description = row2.GetColumn ( `` description '' ) .ToString ( ) ; } } } subroutine.Add ( manDataId.ToString ( ) , description.ToString ( ) ) ; } ArrayData.Add ( subroutine ) ; } I n't tried running code , probably problem two , gets point across looping thousands records using ` foreach ( var row3 idLookup ) ` . The alternative seems making DB query , seems intensive looping memory , seems like better way I missing get ` id ` ` table2id ` without using foreach . To make longest looking question history : ) Here SQL I far : SELECT table1.id , table1.field1 , table1.field2 , table2.description , fee.amt fee FROM table2 INNER JOIN table1 ON table2.id = table1.codeid1 OR table2.id = table1.codeid2 OR table2.id = table1.codeid3 OR table2.id = table1.codeid4 OR table2.id = table1.codeid5 OR table2.id = table1.codeid6 OR table2.id = table1.codeid7 OR table2.id = table1.codeid8 OR table2.id = table1.codeid9 OR table2.id = table1.codeid10 OR table2.id = table1.codeid11 OR table2.id = table1.codeid12 OR table2.id = table1.codeid13 OR table2.id = table1.codeid14 OR table2.id = table1.codeid15 OR table2.id = table1.codeid16 OR table2.id = table1.codeid17 OR table2.id = table1.codeid18 OR table2.id = table1.codeid19 OR table2.id = table1.codeid20 INNER JOIN fee ON table2.id = fee.id WHERE table1.codetype1 = 0 AND table1.codetype2 = 0 AND table1.codetype3 = 0 AND table1.codetype4 = 0 AND table1.codetype5 = 0 AND table1.codetype6 = 0 AND table1.codetype7 = 0 AND table1.codetype8 = 0 AND table1.codetype9 = 0 AND table1.codetype10 = 0 AND table1.codetype11 = 0 AND table1.codetype12 = 0 AND table1.codetype13 = 0 AND table1.codetype14 = 0 AND table1.codetype15 = 0 AND table1.codetype16 = 0 AND table1.codetype17 = 0 AND table1.codetype18 = 0 AND table1.codetype19 = 0 AND table1.codetype20 = 0 This works great long n't ` codetype ` 1 , otherwise record skipped . There likely single row ` codeid / codetype ` filled , ever case ` codetype ` 1 across board match inverse query .","I suppose mean want connect database within automated test case ? ( opposed using Tosca 's database , would supported ) * When get error message ? * How test step look like connect database ? * Does connection string work , create ODBC data source via Windows ? * Which version Tosca using ? Did check follow [ manual ] ( https : //support.tricentis.com/community/manuals_detail.do ? lang=en & version=11.1.0 & url=engines_3.0/database/tbox_database_engine.htm ) ? **Update : Looks like connection string words `` connection string : '' ? This look right . **"
Stackoverflow,"I 've playing around lately SQL Data Services . Although ( perhaps ) I knock well-structured relational database sleep , I 'm struggling get head round design performant database environment ( example ) enforcement referential integrity indexes columns primary key . Does anyone know guidelines ? Maybe place start would create many-to-many join traversed either side performant manner , even vast numbers ~~rows~~ entities ?",It seems phrases used : * Spread data amongst many containers best performance * Modeling data via Entities * Process queries parallel best performance * Caching data service hosted middle tier This would imply start thinking like OO modellers rather relational mind-set . Performance seems rely ability massively parallelise object query smiliar way creating LINQ query take advantage parallelisation .
Stackoverflow,"Folks , I installed msodbcsql package least several dozen times . Never issue come . Even spent entire day yesterday trying fix . Step 1 : add apt key repo sourced.list.d Step 2 : apt-get install msodbcsql17 , mssql-tools , php odbc_pdo extensions , etc Step 3 : create basic test.php ( existing older servers WORKS ) makes test connection mssql db . Fails : SQLSTATE [ 01000 ] SQLDriverConnect : 0 [ unixODBC ] [ Driver Manager ] Ca n't open lib '/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1 ' : file found Yes , file exists . I tried chmod 755 file , still says exist . I done ldd file , output : linux-vdso.so.1 ( 0x00007ffe13bf8000 ) libdl.so.2 = > /lib/x86_64-linux-gnu/libdl.so.2 ( 0x00007fc1e865a000 ) librt.so.1 = > /lib/x86_64-linux-gnu/librt.so.1 ( 0x00007fc1e8452000 ) libodbcinst.so.2 = > /usr/lib/x86_64-linux-gnu/libodbcinst.so.2 ( 0x00007fc1e823d000 ) libcrypto.so.1.0.2 = > found libkrb5.so.3 = > /usr/lib/x86_64-linux-gnu/libkrb5.so.3 ( 0x00007fc1e7f67000 ) libgssapi_krb5.so.2 = > /usr/lib/x86_64-linux-gnu/libgssapi_krb5.so.2 ( 0x00007fc1e7d1c000 ) libssl.so.1.0.2 = > found libuuid.so.1 = > /lib/x86_64-linux-gnu/libuuid.so.1 ( 0x00007fc1e7b15000 ) libstdc++.so.6 = > /usr/lib/x86_64-linux-gnu/libstdc++.so.6 ( 0x00007fc1e778c000 ) libm.so.6 = > /lib/x86_64-linux-gnu/libm.so.6 ( 0x00007fc1e73ee000 ) libgcc_s.so.1 = > /lib/x86_64-linux-gnu/libgcc_s.so.1 ( 0x00007fc1e71d6000 ) libpthread.so.0 = > /lib/x86_64-linux-gnu/libpthread.so.0 ( 0x00007fc1e6fb7000 ) libc.so.6 = > /lib/x86_64-linux-gnu/libc.so.6 ( 0x00007fc1e6bc6000 ) /lib64/ld-linux-x86-64.so.2 ( 0x00007fc1e8c65000 ) libltdl.so.7 = > /usr/lib/x86_64-linux-gnu/libltdl.so.7 ( 0x00007fc1e69bc000 ) libk5crypto.so.3 = > /usr/lib/x86_64-linux-gnu/libk5crypto.so.3 ( 0x00007fc1e678a000 ) libcom_err.so.2 = > /lib/x86_64-linux-gnu/libcom_err.so.2 ( 0x00007fc1e6586000 ) libkrb5support.so.0 = > /usr/lib/x86_64-linux-gnu/libkrb5support.so.0 ( 0x00007fc1e637b000 ) libkeyutils.so.1 = > /lib/x86_64-linux-gnu/libkeyutils.so.1 ( 0x00007fc1e6177000 ) libresolv.so.2 = > /lib/x86_64-linux-gnu/libresolv.so.2 ( 0x00007fc1e5f5c000 ) Two libraries seem missing . I found I libssl1.1 1.0.0 . I tried creating symlinks BOTH 1.1 1.0.0 versions libssl shared objects . Still get error . Again , YES - I copied/pasted exact path error screams . The path correct . I tried using isql command line well - exact error . So certainly something odbc < - > msodbcsql library . This NOT specific PHP/PDO/ODBC stuff . Interestingly , mssql-cli command line tool ( Microsoft repo ) DOES work . Running ldd says NOT dynamic executable . Installed versions libraries involved : unixodbc = 2.3.7 libodbc1 = 2.3.7 odbcinst = 2.3.7 msodbcsql = 17.3.1.1-1 All versions latest available Microsoft repo . All Ubuntu 18.04 . I re-tried procedure ( I done countless times ) laptop - result . Complaining driver library found . Output odbcinst -j follows : unixODBC 2.3.7 DRIVERS ... ... ... ... : /etc/odbcinst.ini SYSTEM DATA SOURCES : /etc/odbc.ini FILE DATA SOURCES.. : /etc/ODBCDataSources USER DATA SOURCES.. : /root/.odbc.ini SQLULEN Size ... ... . : 8 SQLLEN Size ... ... .. : 8 SQLSETPOSIROW Size . : 8 Yes , files exist /etc/odbcinst.ini /etc/odb.ini . Contents /etc/odbcinst.ini : [ ODBC Driver 17 SQL Server ] Description=Microsoft ODBC Driver 17 SQL Server Driver=/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1 UsageCount=1 Running `` stat /opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.3.so.1.1 '' : Size : 2046672 Blocks : 4000 IO Block : 4096 regular file I found little info regarding issue . The posts I came across either NOT solve issue , NOT answered . So.. I missing something ? Or I file bug report ? On side note , I nothing issues Ubuntu 18.04 msodbc stuff since day one . Originally , libcurl3/4 issue . I suppose fixed . But seems may ... bug ?","Found answer , could come earlier , I 'm going leave case someone else stumbles across issue : I already thought something access rights/permissions , basically working everywhere Apache , I could n't figure , I found caused systemd : Systemd feature preventing services accessing ` /tmp ` called SecureTmp . Deactivating feature Apache described [ ] ( https : //support.plesk.com/hc/en-us/articles/115000063849-Directories- like-tmp-systemd-private-overflow-cause-server-crash-due-to-lack-of-disk- space ) testing solved issue . I see I differently I want leave feature disabled . Hope helpful someone : - )"
Stackoverflow,"I ’ using SQL+ dot net really liking far , I ’ trouble formatting phone number . With following code : -- +SqlPlusRoutine -- & Author=Vincent Grazapoli -- & Comment=Inserts new Customer Billing Record returns new CustomerBillingId -- & SelectType=NonQuery -- +SqlPlusRoutine CREATE PROCEDURE [ dbo ] . [ CustomerBillingInsert ] ( @ CustomerBillingId int output , -- +Required @ CustomerId int , -- +Required -- +StringLength=8,64 -- +Email @ Email varchar ( 64 ) , -- +Required -- +StringLength=16,20 -- +CreditCard @ CreditCard varchar ( 20 ) , -- +Required -- +StringLength=10,16 -- +Phone @ Phone varchar ( 16 ) , -- ... -- parameters The email credit card tags work expected , phone number seems prevent letters , allows numbers like 1234 etc . What I wrong ? The generated code follows : public class CustomerBillingInsertInput { [ Required ( AllowEmptyStrings = false ) ] public int ? CustomerId { set ; get ; } [ EmailAddress ] [ DataType ( DataType.EmailAddress ) ] [ Required ( AllowEmptyStrings = false ) ] [ StringLength ( 64 , MinimumLength = 8 ) ] public string Email { set ; get ; } [ CreditCard ] [ DataType ( DataType.CreditCard ) ] [ Required ( AllowEmptyStrings = false ) ] [ StringLength ( 20 , MinimumLength = 16 ) ] public string CreditCard { set ; get ; } [ Phone ] [ DataType ( DataType.PhoneNumber ) ] [ Required ( AllowEmptyStrings = false ) ] [ StringLength ( 16 , MinimumLength = 10 ) ] public string Phone { set ; get ; } //Other properties /// < summary > /// Use method validate instance . /// If method returns false , ValidationResults list populated . /// < /summary > public bool IsValid ( ) { ValidationResults = new List < ValidationResult > ( ) ; return Validator.TryValidateObject ( , new ValidationContext ( ) , ValidationResults , true ) ; } /// < summary > /// ValidationResults populated IsValid ( ) call . /// < /summary > public List < ValidationResult > ValidationResults { set ; get ; } } } }","So I received response sqlplus.net feedback answer gave follows , might help someone even 're using sql+ dot net . The -- +Phone tag expected , however , logic annotation c # quite liberal since apply globally . For case one following : Use -- +RexExPattern tag along appropriate supplemental tag error message like . -- +Required -- +StringLength=10,16 -- +Phone -- +RegExPattern=^ [ 0-9 ] { 10,12 } $ -- & ErrorMessage=Phone vaid format . @ Phone varchar ( 16 ) , That would translate following data annotation . [ RegularExpression ( @ '' ^ [ 0-9 ] { 10,12 } $ '' , ErrorMessage = `` Phone vaid format . '' ) ] Or , would preference , leave semantic tags way , use service like twilio send text verification code . Have user confirm verification code subsequent form post , golden . Confirming phone number , email matter , really way sure , since looks like persisting customer billing information , would worth extra work ."
Stackoverflow,"After installing csvkit following command $ sudo -HE pip install -- upgrade -e git+git : //github.com/wireservice/csvkit.git @ master # egg=csvkit trying import ` .csv ` follows : csvsql -- db mysql : //root : root @ 127.0.0.1:3306/jira_test -- insert -- table bugs_temp -- no-constraints -- overwrite -- create-if-not-exists -- no-inference -- blanks bugs_temp.csv I get following error ( ) /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_2 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_3 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_4 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_5 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_6 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_7 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_8 '' . /usr/local/lib/python2.7/dist-packages/agate/utils.py:292 : DuplicateColumnWarning : Column name `` 0 '' already exists Table . Column renamed `` 0_9 '' . ( table 'bugs_temp ' , column 'ESS-3146 ' ) : VARCHAR requires length dialect mysql","( This answer might somewhat uncomplete . I n't experience ` csvsql ` , I 'm making wiki . ) ` csvkit ` [ depends ] ( https : //github.com/wireservice/csvkit/blob/d1cc54ffab1eb4f0a3a5afc8b7183db4ee1ec800/requirements- py3.txt # L6 ) [ ` agate-sql ` ] ( https : //agate-sql.readthedocs.io/en/latest/ ) , [ depends ] ( https : //github.com/wireservice/agate- sql/blob/1ad8c6548ca9653a5f4bd796311ddbd6ce2e7fcb/requirements-py3.txt # L8 ) [ ` sqlalchemy ` ] ( https : //www.sqlalchemy.org/ ) . ` sqlalchemy ` contains [ line code ] ( https : //github.com/sqlalchemy/sqlalchemy/blob/201c4a60e4b8af56d9c02a3675d1443ba4171c89/lib/sqlalchemy/dialects/mysql/base.py # L1951 ) : `` VARCHAR requires length dialect % '' % self.dialect.name It seems like ” _you need specify lengths Strings_ “ , citing [ answer ] ( https : //stackoverflow.com/questions/5569963/varchar-requires-a-length- when-rendered-on-mysql # 5586870 ) . That means use ` -- no-constraints ` option : -- no-constraints Generate schema without length limits null checks . Useful sampling big tables . See < https : //csvkit.readthedocs.io/en/latest/scripts/csvsql.html >"
Stackoverflow,I ca n't find anywhere documentation way execute raw SQL string [ nanoSQL ] ( https : //docs.nanosql.io/setup ) . If possible could anyone recommend another in-memory SQL solution I run raw SQLs ?,"Library author , 's possible since nanoSQL 's query language . However , planned feature nanoSQL 2.0 . Keep eye readme.md repo `` Query Support '' < https : //github.com/ClickSimply/Nano-SQL >"
Stackoverflow,"I query like : SET @ = ( SELECT GROUP_CONCAT ( Id ) FROM MyTable1 WHERE Id < 10 ) ; SELECT * FROM MyTable2 WHERE find_in_set ( IdLite , @ ) ; SELECT * FROM MyTable3 WHERE find_in_set ( IdLite , @ ) ; SELECT * FROM MyTable4 WHERE find_in_set ( IdLite , @ ) ; I 've tryed use code get resut : Using ds As DataSet = MySqlHelper.ExecuteDataset ( CnStr , SqlStr ) I get error : > Fatal error encountered command execution . Error message : > Parameter ' @ ' must defined . I 've also tryed : SELECT * FROM MyTable2 WHERE find_in_set ( IdLite , @ : = ( SELECT GROUP_CONCAT ( Id ) FROM MyTable1 WHERE Id < 10 ) ) ; SELECT * FROM MyTable3 WHERE find_in_set ( IdLite , @ ) ; SELECT * FROM MyTable4 WHERE find_in_set ( IdLite , @ ) ; I get error . What 's correct way get result ` DataSet ` ?","This might get part way . First get rid ` DB_Functions ` . MySQLHelper method create DataSet ; general , db Ops query-specific little generic reusable . The exception building ConnectionString : MySQL [ gobs cool options ] ( https : //dev.mysql.com/doc/connector-net/en/connector-net-connection- options.html ) enable/disable via connection string . But need standard ` MySqlConnectionStringBuilder ` . # # Build DataSet : ' form/class level vars Private dsSample As DataSet Private MySqlConnStr As String = `` ... '' ... Dim SQL = `` SELECT Id , FirstName , Middle , LastName FROM Employee '' Using dbcon As New MySqlConnection ( MySQLConnStr ) dsSample = MySqlHelper.ExecuteDataset ( dbcon , SQL ) dsSample.Tables ( 0 ) .TableName = `` Emps '' End Using There appear way specify tablename build , separate step . # # Update Record To update single row , want ` ExecuteNonQuery ` ; also allow use Parameters : Dim uSQL = `` UPDATE Employee SET Middle = @ p1 WHERE Id = @ p2 '' Using dbcon As New MySqlConnection ( MySQLConnStr ) Dim params ( 1 ) As MySqlParameter params ( 0 ) = New MySqlParameter ( `` @ p1 '' , MySqlDbType.String ) params ( 0 ) .Value = `` Q '' params ( 1 ) = New MySqlParameter ( `` @ p2 '' , MySqlDbType.Int32 ) params ( 1 ) .Value = 4583 dbcon.Open ( ) Dim rows = MySqlHelper.ExecuteNonQuery ( dbcon , uSQL , params ) End Using Again , really simpler using fully configured DataAdapter , would simply : dsSample.Tables ( `` Emps '' ) .Rows ( 1 ) .Item ( `` Middle '' ) = `` X '' daSample.Update ( dsSample.Tables ( `` Emps '' ) ) I exactly sure value ` UpdateDataSet ` method adds . I _think_ `` helper '' counterpart , since n't provide Parameters , I n't much use . The docs sketchy . The ` commandtext ` would appear SQL single row . Note DataAdapter.Update method would add new rows added , delete deleted ones update values row changed values - potentially dozens even hundreds db Ops one line code ."
Stackoverflow,"I playing around MySQL 8.0.14 InnoDB cluster . I currently stuck creating Group replication via mySQL shell . Since I want use SSL , I required set ipWhitelist dba.createCluster ( ) shown : ` var cluster = dba.createCluster ( 'testCluster4 ' , { ipWhitelist : 'somedns-1.tosqlnode ' } ) ` The cluster successfully created . Now I want add another instance . ` cluster.addInstance ( 'ca @ somedns-2.tosqlnode ' , { ipWhitelist : 'somedns-1.tosqlnode , somedns-2.tosqlnode ' } ) ` This fails first instance showing error states non- whitelisted instance trying connect . * * * So create another one : ` var cluster = dba.createCluster ( 'testCluster5 ' , { ipWhitelist : 'somedns-1.tosqlnode , somedns-2.tosqlnode ' } ) ` The cluster successfully created . Now I want add another instance . ` cluster.addInstance ( 'ca @ somedns-2.tosqlnode ' , { ipWhitelist : 'somedns-1.tosqlnode , somedns-2.tosqlnode ' } ) ` Instance successfully added . * * * Is really necessary know instance addresses cluster creation ? I find way via MySQL shell change initial ipWhitelist .","If want able add nodes fly , need set ` group_replication_ip_whitelist ` ` AUTOMATIC ` . This done specifying ipWhitelist Shell configuration ( default ) . If , Miguel wrote ."
Stackoverflow,"We timestamp column , queried , displays syntax : 12/18/2018 11:27:35 AM However , makes confusing timestamp actually stores milisecond values , well . This means querying '12/18/2018 11:27:35 AM ' , return results unless use date_trunc ( ) function . Is anyway force engine display literal value ? We using SQL Manager PostgreSQL","Use To_Char function column set output display format : to_char ( yourColumn , 'mm/dd/yyyy hh : mi : ss.us ' )"
Stackoverflow,"I problem I get column names values Tables schema show result grid . I used direct approach I implement SqlSiphon structure . For I make getters setters column Table schema impossible . What I use get Column names values dynamically table . SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = ' '' + @ Tablename1 + `` ' AND TABLE_SCHEMA='dbo ' '' What best dynamic solution ? And Best use List , Dictionay something like 2d Array give column names well column values ?","Might worth taking step back comparing people solve similar problems . Typically , table database represents entity , also class per entity , may use ORM system avoid duplication work . So , typical system , table customers , table invoices , table invoice lines , etc . class represents customer , class invoice , class invoice line , etc . As later add functionality ( possible columns/properties ) change classes , rather seeing columns database - course decorate XML documentation get Intelisense goodness . There many ORM systems , strengths weaknesses , I personally like [ LINQ SQL ] ( http : //msdn.microsoft.com/en- us/library/bb387007 % 28v=vs.110 % 29.aspx ) adding onto existing data model ."
Stackoverflow,"I 've using JoSQL quite months today I came across problem I sure solve . I probably could solve binding variables/placeholders , I 'd like include fields query . SELECT * FROM ... MyObject WHERE getType ! = com.mypackage.myclass.TYPE_A This query I . TYPE_A public static final int attribute `` myclass '' class . Accessing methods ( getType ) easy , getType expected method MyObject - I write round brackets ( JoSQL works far I know ) . Does anyone happen idea access public static final field ? JoSQL uses gentlyweb-utils ; seems sort Accessor/Getter/Setter framework . I 'd love access attribute without bind variables , I n't able . Thanks help advance ! I really appreciate .",You use : ` Query.getColumns ` return list : ` SelectItemExpression ` use : ` getAlias ` return '' column '' name . So : Query q = new Query ( ) ; q.parse ( `` SELECT FROM java.lang.String '' ) ; List cols = q.getColumns ( ) ; ( int = 0 ; < cols.size ( ) ; i++ ) { SelectItemExpresion sie = ( SelectItemExpression ) cols.get ( ) ; System.out.println ( + `` : `` + sie.getAlias ( ) ) ; } Of course column n't alias could try using ` toString ` method instead digging around ` Expression ` find suitable data .
Stackoverflow,How get elements XML know nodename.I sample XML like . < ? xml version= '' 1.0 '' ? > < ! DOCTYPE PARTS SYSTEM `` parts.dtd '' > < ? xml-stylesheet type= '' text/css '' href= '' xmlpartsstyle.css '' ? > < PARTS > < TITLE > Computer Parts < /TITLE > < PART > < ITEM > Motherboard < /ITEM > < MANUFACTURER > ASUS < /MANUFACTURER > < MODEL > P3B-F < /MODEL > < COST > 123.00 < /COST > < /PART > < PART > < ITEM > Video Card < /ITEM > < MANUFACTURER > ATI < /MANUFACTURER > < MODEL > All-in-Wonder Pro < /MODEL > < COST > 160.00 < /COST > < /PART > < PART > < ITEM > Sound Card < /ITEM > < MANUFACTURER > Creative Labs < /MANUFACTURER > < MODEL > Sound Blaster Live < /MODEL > < COST > 80.00 < /COST > < /PART > < PART > < ITEMᡋ inch Monitor < /ITEM > < MANUFACTURER > LG Electronics < /MANUFACTURER > < MODEL > 995E < /MODEL > < COST > 290.00 < /COST > < /PART > I want get element array using esql . How ?,Assuming parsing xml entry simply use following : SET myvar [ ] = InputRoot.XMLNSC.PARTS.PART [ ]
Stackoverflow,[ Here ] ( https : //github.com/yandex-qatools/postgresql-embedded ) embedpostgresql library starting postgres process via java . The [ instruction ] ( https : //github.com/yandex-qatools/postgresql-embedded # how-to- use-avoid-archive-extraction-on-every-run ) reusing already extracted postgres zip archive . But change archive directory ? By default store maven ` % USER_HOME\.embedpostgresql % ` I want change maven repository needed invoke set property ?,"You use **artifactStorePath ( ) ** method _DownloadConfigBuilder_ class : final FixedPath cachedDir = new FixedPath ( `` /tmp/cacheddir '' ) ; final FixedPath artifactStoreDir = new FixedPath ( `` /tmp/artstoredir '' ) ; IRuntimeConfig runtimeConfig = new RuntimeConfigBuilder ( ) .defaults ( cmd ) .artifactStore ( new CachedArtifactStoreBuilder ( ) .defaults ( cmd ) .tempDir ( cachedDir ) .download ( new DownloadConfigBuilder ( ) .defaultsForCommand ( cmd ) .artifactStorePath ( artifactStoreDir ) .packageResolver ( new PackagePaths ( cmd , cachedDir ) ) .build ( ) ) ) .build ( ) ; //starting Postgres final PostgresStarter < PostgresExecutable , PostgresProcess > runtime = PostgresStarter.getInstance ( runtimeConfig ) ; I cloned [ git repository ] ( https : //github.com/yandex-qatools/postgresql- embedded ) , 'grep -R embedpostgresql * ' found first hint ru.yandex.qatools.embed.postgresql.config.DownloadConfigBuilder class . Then I tested works !"
Stackoverflow,"I 'm trying PL/SQL online Oracle SQL Worksheet - [ Live Oracle SQL ] ( https : //livesql.oracle.com/apex/f ? p=590:1:3323436307214 : :NO : : : ) . I 'm unable display output block , spite adding ` SET SERVEROUTPUT ON ; ` This code SET SERVEROUTPUT ON ; declare number : =2 ; j number : =0 ; counter number : =0 ; flag number ; begin loop ( i=2 ) counter : =counter+1 ; dbms_output.put ( || ' ' ) ; else j : =2 ; flag : =0 ; loop ( mod ( , j ) =0 ) flag : =1 ; end ; exit ( i=j ) flag=1 ; end loop ; ( flag=0 ) counter : =counter+1 ; dbms_output.put ( j || ' ' ) ; end ; end ; : =i+1 ; exit counter=10 ; end loop ; end ; / This console message Unsupported Command Statement processed . Any idea get working ?",I actually changed ` dbms_output.put ( ) ` ` dbms_output.put_line ( ) ` worked . Any idea make ` dbms_output.put ( ) ` work ? I want output single line .
Stackoverflow,"I want simulate simple SQL statement ( Create & Select ) processing Java . For example , consider following two relations CREATE TABLE X ( , b , c ) ; CREATE TABLE Y ( c , , e ) ; **Ques 1 : ** Now data structures I use store relation name X Y along attributes . Also consider select statement : SELECT , FROM X , Y WHERE X.c = Y.c ; **Ques 2 : ** How confirm whether **a belongs X Y** **d belongs X Y** ? How processing taken care inside SQL query processing engine . I find numerous query evaluation plans pushes SELECT statement JOIN operation Selection depends single relation . For purpose I need know **How confirm relation attribute belongs ? ** Thanks advance .","> Ques 1 : Now data structures I use store relation name X Y along attributes . Use structure like , Java 'd probably use classes correspond X Y fields correspond , b , c X c , , e Y . > SELECT , FROM X , Y WHERE X.c = Y.c ; > > Ques 2 : How confirm whether belongs X Y belongs X Y ? How processing taken care inside SQL query processing engine . Since ` ` exists X ` ` exists Y database knows what's meant . If 'd exist table well 'd need specify mean ( e.g . ` SELECT x.a , FROM X x , Y ... ` ) ."
Stackoverflow,"So I need import ITIS database pdadmin4 1.5 ( postgresql ) The read.me file gives information : 1 . Open terminal command prompt navigate folder unzipped ITIS download file . 2 . Enter following : psql -U root -f ITIS.sql If PostgreSql user root , substitute user name root . Also , updating remote server , need add -h < servername > command 3 . When prompted PostgreSql user 's password , enter load process start . The directory file : /Users/kostas/Desktop/itisPostgreSql063017/ITIS.sql How I open terminal command prompt navigate folder I unzipped ITIS download file I see PostgreSql user root ? Thanks !",Issue solved using following maven plugin : < plugin > < groupId > org.apache.maven.plugins < /groupId > < artifactId > maven-surefire-plugin < /artifactId > < configuration > < argLine > -Dfile.encoding=UTF8 < /argLine > < /configuration > < /plugin > plugin solve encoding issue **command line build** . **UPDATE : ** solve issue **run project server** : 1- Run As 2- Run Configurations 3- Change encoding server UTF-8 image [ ! [ enter image description ] ( https : //i.stack.imgur.com/Tqyvz.png ) ] ( https : //i.stack.imgur.com/Tqyvz.png )
Stackoverflow,"I want show result Oracles ` dbms_sqltune.report_sql_monitor ` browser ( Firefox ) , I get page : > Alternate HTML content placed . This content requires Adobe Flash Player . Get Flash In detail I first run query bellow SQL_ID traced : select dbms_sqltune.report_sql_monitor ( sql_id = > 'c9uht72r7gga8 ' , report_level = > 'ALL ' , type = > 'ACTIVE ' ) dual The result query HTML document , I cut paste file , I open browser ( Firefox ) But instead get required query overview , I get message . [ ! [ enter image description ] ( https : //i.stack.imgur.com/rhZkA.png ) ] ( https : //i.stack.imgur.com/rhZkA.png ) My Flash installed recent version I see ` Add-on ` overview [ ! [ enter image description ] ( https : //i.stack.imgur.com/4kG0I.png ) ] ( https : //i.stack.imgur.com/4kG0I.png ) Re-install Flash using admin account ( Windows 7 Profesional ) n't help .",
Stackoverflow,"I 've trying measure execution time queries executed application . I 've trying use sys.dm_exec_query_stats.last_elapsed_time I 'm confused values returning . When I execute query SELECT TOP 1 qs.plan_handle , qs.last_execution_time , qs.last_elapsed_time , qs.min_elapsed_time FROM sys.dm_exec_query_stats qs ORDER BY qs.last_execution_time DESC ssms , clearly executes returns results soon display refresh , clearly much faster one second . However , values returned last_elapsed_time consistently around 1700-1800 milliseconds . What I missing ? Here [ msdn info page ] ( http : //msdn.microsoft.com/en- us/library/ms189741.aspx ) sys.dm_exec_query_stats . I 'm using sql server 2012 .","They millisecond values , MICROsecond values , stated clearly msdn info page table ."
Stackoverflow,"I basic question database programming , 's problem : I want create/read/edit/etc.. data database without using Entity Framework , job I 've chosen SqlFu . I want put stored procedures create , update , delete database views get entities . My doubt : If I table _Employee_ , one-to-many relationship _Tasks_ table , I create Sql View retrieve _Employee_ entity , I retrieve data _Tasks_ table related employee ? If , single View SQL Server ? If , I different Sql Views retrieve data table _bind_ relationship application ? I 'm bit lost subject : S","You ca n't , SqlFu data mapper , mapping query result poco . The attributes used decorate Poco table creation querying . So , 's mapping ORM sense ."
